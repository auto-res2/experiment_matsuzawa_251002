
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
AUTOMATA integrates three core components: a hyper-parameter search algorithm (e.g., Random Search, TPE) to identify configurations, a gradient-based subset selection (GSS) algorithm for efficient configuration evaluation, and a hyper-parameter scheduling algorithm (e.g., Hyperband, ASHA) for early stopping. The GSS method selects a subset and associated weights such that the weighted subset loss gradient best approximates the entire training loss gradient, formulated as an optimization problem and solved using a greedy algorithm like Orthogonal Matching Pursuit (OMP). A 'per-batch' variant of GSS is used for efficiency, selecting subsets of mini-batches. Additionally, 'warm-starting' involves initial training on the entire dataset for a few epochs to obtain informative loss gradients, particularly when using the ASHA scheduler.

# Repository Content
File Path: configs/HPO/config_hp_1.py
Content:
from ray import tune

config = dict(setting= "hyperparamtuning",

# parameter for subset selection
# all settings for subset selection will be fetched from here
subset_config = "configs/SL/config_gradmatchpb_glove_sst2.py",

# parameters for hyper-parameter tuning
# search space for hyper-parameter tuning
space = dict(learning_rate=tune.uniform(0.001, 0.1), 
        # optimizer= tune.choice(['sgd', 'adam']),
        hidden_size = tune.choice([64, 128, 256]),
        trn_batch_size= tune.choice([16, 32, 64]),
        # num_layers = tune.choice([1, 2])
        ),

# tuning algorithm 
search_algo = "TPE",

# number of hyper-parameter set to try
num_evals = 27,

# metric to be optimized, for 'mean_loss' metric mode should be 'min'
metric = "mean_accuracy",
mode = "max",

# scheduler to be used (i.e ASHAScheduler)
# scheduler terminates trials that perform poorly
# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html
# scheduler = 'hyperband',
scheduler = 'ASHA',

# where to store logs
log_dir = "RayLogs/",

# resume hyper-parameter tuning from previous log
# specify 'name' (i.e main_2021-03-09_18-33-56) below
resume = False,

# only required if you want to resume from previous checkpoint
# it can also be specified if you don't want to resume
name = None,

# specify resources to be used per trial
# i.e {'gpu':1, 'cpu':2}
# resources = {'gpu':1, 'cpu':2},
resources = {'gpu':0.5, 'cpu':1},

# if True, trains model on Full dataset with the best parameter selected.
final_train = True,

final_train_type = 'full' # full, gmpb

)

File Path: configs/HPO/config_hyper-param_tuning.py
Content:
from ray import tune

config = dict(setting= "hyperparamtuning",

# parameter for subset selection
# all settings for subset selection will be fetched from here
subset_config = "configs/SL/config_gradmatchpb-warm_cifar10.py",

# parameters for hyper-parameter tuning
# search space for hyper-parameter tuning
space = dict(learning_rate=tune.uniform(0.001, 0.01), 
        optimizer= tune.choice(['sgd', 'adam']),
        trn_batch_size= tune.choice([20, 32, 64]),        
        ),

# tuning algorithm 
search_algo = "TPE",

# number of hyper-parameter set to try
num_evals = 20,

# metric to be optimized, for 'mean_loss' metric mode should be 'min'
metric = "mean_accuracy",
mode = "max",

# scheduler to be used (i.e ASHAScheduler)
# scheduler terminates trials that perform poorly
# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html
scheduler = 'hyperband',

# where to store logs
log_dir = "RayLogs/",

# resume hyper-parameter tuning from previous log
# specify 'name' (i.e main_2021-03-09_18-33-56) below
resume = False,

# only required if you want to resume from previous checkpoint
# it can also be specified if you don't want to resume
name = None,

# specify resources to be used per trial
# i.e {'gpu':1, 'cpu':2}
resources = {'gpu' : 0.5},

# if True, trains model on Full dataset with the best parameter selected.
final_train = True

)

File Path: configs/HPO/config_hyper-param_tuning_cifar100.py
Content:
from ray import tune

config = dict(setting= "hyperparamtuning",
# parameters for hyper-parameter tuning
# search space for hyper-parameter tuning
space = dict(
        learning_rate=tune.uniform(0.001, 0.01), 
        learning_rate1=tune.uniform(0.001, 0.01),
        learning_rate2=tune.uniform(0.001, 0.01),
        learning_rate3=tune.uniform(0.001, 0.01),
        scheduler= tune.choice(['cosine_annealing', 'linear_decay']),
        nesterov= tune.choice([True, False]),
        gamma= tune.uniform(0.05, 0.5),    
        ),

# tuning algorithm 
search_algo = "TPE",

# number of hyper-parameter set to try
num_evals = 27,

# metric to be optimized, for 'mean_loss' metric mode should be 'min'
metric = "mean_accuracy",
mode = "max",

# scheduler to be used (i.e ASHAScheduler)
# scheduler terminates trials that perform poorly
# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html
scheduler = 'asha',

# where to store logs
log_dir = "RayLogs/",

# resume hyper-parameter tuning from previous log
# specify 'name' (i.e main_2021-03-09_18-33-56) below
resume = False,

# only required if you want to resume from previous checkpoint
# it can also be specified if you don't want to resume
name = None,

# specify resources to be used per trial
# i.e {'gpu':1, 'cpu':2}
resources = {'gpu':0.5},

# if True, trains model on Full dataset with the best parameter selected.
final_train = True
)

File Path: configs/HPO/config_hyper-param_tuning_sst2.py
Content:
from ray import tune

config = dict(setting= "hyperparamtuning",

# parameter for subset selection
# all settings for subset selection will be fetched from here
subset_config = "configs/SL/config_gradmatchpb_glove_sst2.py",

# parameters for hyper-parameter tuning
# search space for hyper-parameter tuning
space = dict(learning_rate=tune.uniform(0.001, 0.1), 
        # optimizer= tune.choice(['sgd', 'adam']),
        hidden_size = tune.choice([64, 128, 256]),
        trn_batch_size= tune.choice([16, 32, 64]),
        num_layers = tune.choice([1, 2])
        ),

# tuning algorithm 
search_algo = "",

# number of hyper-parameter set to try
num_evals = 54,

# metric to be optimized, for 'mean_loss' metric mode should be 'min'
metric = "mean_accuracy",
mode = "max",

# scheduler to be used (i.e ASHAScheduler)
# scheduler terminates trials that perform poorly
# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html
# scheduler = 'hyperband',
scheduler = 'ASHA',

# where to store logs
log_dir = "RayLogs/",

# resume hyper-parameter tuning from previous log
# specify 'name' (i.e main_2021-03-09_18-33-56) below
resume = False,

# only required if you want to resume from previous checkpoint
# it can also be specified if you don't want to resume
name = None,

# specify resources to be used per trial
# i.e {'gpu':1, 'cpu':2}
# resources = {'gpu':1, 'cpu':2},
resources = {'gpu':0.5, 'cpu':1},

# if True, trains model on Full dataset with the best parameter selected.
final_train = True,

final_train_type = 'full' # full, gmpb

)

File Path: configs/HPO/config_hyper-param_tuning_trec6.py
Content:
from ray import tune

config = dict(setting= "hyperparamtuning",

# parameter for subset selection
# all settings for subset selection will be fetched from here
subset_config = "configs/SL/config_milo_glove_trec6.py",

# parameters for hyper-parameter tuning
# search space for hyper-parameter tuning
space = dict(learning_rate=tune.uniform(0.001, 0.1), 
        # optimizer= tune.choice(['sgd', 'adam']),
        hidden_size = tune.choice([64, 128, 256]),
        trn_batch_size= tune.choice([16, 32, 64]),
        num_layers = tune.choice([1, 2])
        ),

# tuning algorithm 
search_algo = "TPE",

# number of hyper-parameter set to try
num_evals = 54,

# metric to be optimized, for 'mean_loss' metric mode should be 'min'
metric = "mean_accuracy",
mode = "max",

# scheduler to be used (i.e ASHAScheduler)
# scheduler terminates trials that perform poorly
# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html
# scheduler = 'hyperband',
scheduler = 'ASHA',

# where to store logs
log_dir = "RayLogs/",

# resume hyper-parameter tuning from previous log
# specify 'name' (i.e main_2021-03-09_18-33-56) below
resume = False,

# only required if you want to resume from previous checkpoint
# it can also be specified if you don't want to resume
name = None,

# specify resources to be used per trial
# i.e {'gpu':1, 'cpu':2}
# resources = {'gpu':1, 'cpu':2},
resources = {'gpu':0.5},

# if True, trains model on Full dataset with the best parameter selected.
final_train = True,

final_train_type = 'full' # full, gmpb

)

File Path: configs/SL/Regression/config_craig_Cadata.py
Content:
# Learning setting
config = dict(setting="supervisedlearning",

              dataset=dict(name="cadata",
                           datadir="reg_data",
                           feature="dss",
                           type="pre-defined"),

              dataloader=dict(shuffle=True,
                              batch_size=200,
                              pin_memory=True),

              model=dict(architecture='RegressionNet',
                         type='pre-defined',
                         numclasses=1),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='MeanSquaredLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type="None",
                             T_max=500),

              dss_args=dict(type="CRAIG",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                linear_layer=True,
                                optimizer='lazy',
                                selection_type='PerBatch'
                                ),

              train_args=dict(num_epochs=500,
                              device="cuda",
                              print_every=10,
                              results_dir='results/',
                              print_args=["val_loss", "tst_loss", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/Regression/config_craig_boston.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = True,
              dataset=dict(name="boston",
                           datadir="../data",
                           feature="dss",
                           type="tabular"),

              dataloader=dict(shuffle=True,
                              batch_size=20,
                              pin_memory=True),

              model=dict(architecture='ThreeLayerNet',
                         type='pre-defined',
                         input_dim=13,
                         numclasses=1,
                         h1 = 16,
                         h2 = 32),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='MeanSquaredLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             lr=1e-2),

              scheduler=dict(type="none"),

              dss_args=dict(type="CRAIG",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            linear_layer=False,
                            optimizer='lazy',
                            selection_type='Supervised'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              results_dir='results/',
                              print_args=["val_loss", "tst_loss", "trn_loss", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/Regression/config_full_boston.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = True,
              dataset=dict(name="boston",
                           datadir="../data",
                           feature="dss",
                           type="tabular"),

              dataloader=dict(shuffle=True,
                              batch_size=20,
                              pin_memory=True),

              model=dict(architecture='ThreeLayerNet',
                         type='pre-defined',
                         input_dim=13,
                         numclasses=1,
                         h1 = 16,
                         h2 = 32),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='MeanSquaredLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             lr=0.01),

              scheduler=dict(type="none"),

              dss_args=dict(type="Full"),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              results_dir='results/',
                              print_args=["val_loss", "tst_loss", "trn_loss", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/Regression/config_glister_Cadata.py
Content:
# Learning setting
config = dict(setting="supervisedlearning",

              dataset=dict(name="cadata",
                           datadir="reg_data",
                           feature="dss",
                           type="pre-defined"),

              dataloader=dict(shuffle=True,
                              batch_size=200,
                              pin_memory=True),

              model=dict(architecture='RegressionNet',
                         type='pre-defined',
                         numclasses=1),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='MeanSquaredLoss',
                        use_sigmoid=False),

              optimizer=dict(type='sgd',#"adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type="None",
                             T_max=500),

              dss_args=dict(type="GLISTER",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            linear_layer=True,#False,
                            selection_type='Supervised',
                            greedy='Stochastic'),

              train_args=dict(num_epochs=500,
                              device="cuda",
                              print_every=10,
                              results_dir='results/',
                              print_args=["val_loss",  "tst_loss", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/Regression/config_glister_boston.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = True,
              dataset=dict(name="boston",
                           datadir="../data",
                           feature="dss",
                           type="tabular"),

              dataloader=dict(shuffle=True,
                              batch_size=20,
                              pin_memory=True),

              model=dict(architecture='ThreeLayerNet',
                         type='pre-defined',
                         input_dim=13,
                         numclasses=1,
                         h1 = 16,
                         h2 = 32),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='MeanSquaredLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             lr=1e-2),

              scheduler=dict(type="none"),

              dss_args=dict(type="GLISTER",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                linear_layer=False,
                                selection_type='Supervised',
                                greedy='Stochastic'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              results_dir='results/',
                              print_args=["val_loss", "tst_loss", "trn_loss", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/__init__.py
Content:

File Path: configs/SL/config_adapfacloc_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',
                           ss_path = '../data/SST/facloc.pkl',
                           use_ss_if_exists = False),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdapFacLoc",
                            fraction=0.3,
                            select_every=5,
                            kappa = 0,
                            collate_fn = collate_fn_pad_batch,
                            size_chunk = 8534,
                            selection_type='PerBatch',
                            linear_layer = True,
                            valid = False),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              run=1,
                              print_every=3,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom-warm_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom-Warm",
                                fraction=0.1,
                                select_every=1,
                                kappa=0.5,
                                collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="AdaptiveRandom",
                                fraction=0.1,
                                select_every=1,
                                kappa=0,
                                collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              wandb=False,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom",
                                fraction=0.1,
                                select_every=1,
                                kappa=0,
                                collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              wandb=False,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_glove_glue_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_glove_imdb.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_glove_rotten_tomatoes.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_glove_trec6.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_glove_tweet_eval.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_mnist.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="AdaptiveRandom",
                                fraction=0.1,
                                select_every=1,
                                kappa=0,
                                collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              wandb=False,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_adaptiverandom_tinyimagenet.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="AdaptiveRandom",
                                fraction=0.1,
                                select_every=1,
                                kappa=0,
                                collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craig-warm_cifar10.py
Content:
# Learning setting
from torch.optim import optimizer


config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
							 nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIG-Warm",
                                fraction=0.1,
                                select_every=20,
                                kappa=0.5,
                                if_convex=False,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerClass',
                                collate_fn = None
                                ),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craig_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.1,
                             weight_decay=5e-4,
							 nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIG",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            linear_layer=False,
                            if_convex=False,
                            optimizer='lazy',
                            selection_type='PerClass',
                            collate_fn = None
                            ),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              run=1,
                              print_every=10,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb-warm_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
							 nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB-Warm",
                                fraction=0.1,
                                if_convex=False,
                                select_every=20,
                                kappa=0.5,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                collate_fn = None
                                ),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb-warm_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             nesterov = True,
                             weight_decay=5e-4),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="CRAIGPB-Warm",
                                fraction=0.1,
                                select_every=20,
                                if_convex=False,
                                kappa=0.5,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                collate_fn = None
                                ),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
							 nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="CRAIGPB",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                if_convex=False,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                collate_fn = None
                                ),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="CRAIGPB",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            if_convex=False,
                            linear_layer=False,
                            optimizer='lazy',
                            selection_type='PerBatch',
                            collate_fn = None
                            ),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_glove_glue_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB-Warm",
                                fraction=0.1,
                                select_every=5,
                                kappa=0,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                if_convex = False,
                                collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_glove_imdb.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB",
                                fraction=0.1,
                                select_every=5,
                                kappa=0,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                if_convex = False,
                                collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_glove_rotten_tomatoes.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB",
                                fraction=0.1,
                                select_every=5,
                                kappa=0,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                if_convex = False,
                                collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB-Warm",
                                fraction=0.1,
                                select_every=5,
                                kappa=0,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                if_convex = False,
                                collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_glove_sst5.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst5",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=5,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB-Warm",
                                fraction=0.1,
                                select_every=5,
                                kappa=0,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                if_convex = False,
                                collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_glove_trec6.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB-Warm",
                                fraction=0.1,
                                select_every=5,
                                kappa=0,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                if_convex = False,
                                collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_glove_tweet_eval.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="CRAIGPB",
                                fraction=0.1,
                                select_every=5,
                                kappa=0,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                if_convex = False,
                                collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_mnist.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
							 nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="CRAIGPB",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                if_convex=False,
                                linear_layer=False,
                                optimizer='lazy',
                                selection_type='PerBatch',
                                collate_fn = None
                                ),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_craigpb_tinyimagenet.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="CRAIGPB",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            if_convex=False,
                            linear_layer=False,
                            optimizer='lazy',
                            selection_type='PerBatch',
                            collate_fn = None
                            ),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_facloc_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst2_facloc",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',
                           ss_path = '../data/SST/facloc.pkl'),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="FacLoc",
                            fraction=0.3,
                            select_every=5,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch,
                            size_chunk=8534),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=3,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="Full",
                            select_every=1,
                            kappa=0,
                            fraction=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            select_every=1,
                            kappa=0,
                            fraction=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_glove_glue_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            fraction=1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_glove_imdb.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            fraction=1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_glove_rotten_tomatoes.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            fraction=1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            fraction=1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_glove_sst5.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst5",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=5,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            fraction=1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_glove_trec6.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            fraction=1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_glove_tweet_eval.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            fraction=1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_mnist.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="Full",
                            select_every=1,
                            kappa=0,
                            fraction=1,
                            collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_full_tinyimagenet.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Full",
                            select_every=1,
                            kappa=0,
                            fraction=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister-warm_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTER-Warm",
                                fraction=0.1,
                                select_every=20,
                                kappa=0.5,
                                linear_layer=False,
                                selection_type='Supervised',
                                collate_fn = None,
                                greedy='Stochastic'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTER",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                linear_layer=False,
                                selection_type='Supervised',
                                collate_fn = None,
                                greedy='Stochastic'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTER",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                linear_layer=False,
                                selection_type='Supervised',
                                collate_fn = None,
                                greedy='Stochastic'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister_glove_imdb.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTER",
                            fraction=0.3,
                            select_every=1,
                            selection_type='Supervised',
                            greedy='Stochastic',
                            linear_layer=False,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister_glove_rotten_tomatoes.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTER",
                            fraction=0.3,
                            select_every=1,
                            selection_type='Supervised',
                            greedy='Stochastic',
                            linear_layer=False,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister_glove_trec6.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTER",
                            fraction=0.3,
                            select_every=1,
                            selection_type='Supervised',
                            greedy='Stochastic',
                            linear_layer=False,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister_mnist.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200),

              dss_args=dict(type="GLISTER",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                linear_layer=False,
                                selection_type='Supervised',
                                collate_fn = None,
                                greedy='Stochastic'),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=10,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glister_tinyimagenet.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTER",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                linear_layer=False,
                                selection_type='Supervised',
                                collate_fn = None,
                                greedy='Stochastic'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glisterpb-warm_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTERPB-Warm",
                                fraction=0.1,
                                select_every=20,
                                kappa=0.5,
                                linear_layer=False,
                                selection_type='PerBatch',
                                collate_fn = None,
                                greedy='Stochastic'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_glisterpb_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GLISTERPB",
                                fraction=0.1,
                                select_every=20,
                                kappa=0,
                                linear_layer=False,
                                selection_type='PerBatch',
                                collate_fn = None,
                                greedy='Stochastic'),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatch-warm_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),
              
              dss_args=dict(type="GradMatch-Warm",
                            fraction=0.1,
                            select_every=20,
                            lam=0.5,
                            selection_type='PerClassPerGradient',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True, 
                            kappa=0.5,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatch_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatch",
                            fraction=0.1,
                            select_every=20,
                            lam=0.5,
                            selection_type='PerClassPerGradient',
                            v1=True,
                            valid=False,
                            kappa=0,
                            eps=1e-100,
                            linear_layer=True,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatch_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatch",
                            fraction=0.3,
                            select_every=5,
                            lam=0.5,
                            selection_type='PerClassPerGradient',
                            v1=True,
                            valid=False,
                            kappa=0,
                            eps=1e-100,
                            linear_layer=True,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=3,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb-warm_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB-Warm",
                            fraction=0.1,
                            select_every=20,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0.5,
                            collate_fn = None),


              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb-warm_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             lr1=0.1,
                             lr2=0.1,
                             lr3=0.1,
                             nesterov = True,
                             weight_decay=5e-4),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="GradMatchPB-Warm",
                            fraction=0.1,
                            select_every=20,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0.5,
                            collate_fn = None),


              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb-warm_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB-Warm",
                            fraction=0.3,
                            select_every=5,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0.5,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=12,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_glove_glue_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_glove_imdb.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_glove_rotten_tomatoes.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_glove_sst5.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst5",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=5,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_glove_trec6.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_glove_tweet_eval.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_mnist.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_gradmatchpb_tinyimagenet.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),

              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="GradMatchPB",
                            fraction=0.1,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_cifar10.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar10",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )

    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )

    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              wandb=False,
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_cifar100.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              wandb=False,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_glove_glue_sst2.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="imdb",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/glue_sst2/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            fraction=0.1,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_glove_imdb.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="imdb",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            fraction=0.1,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_glove_rotten_tomatoes.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            fraction=0.1,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_glove_trec6.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="disp_min_pc",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.01,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            fraction=0.1,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_glove_tweet_eval.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            fraction=0.1,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_mnist.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar10",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    parser.add_argument("-f", "--fff",
                        help= "a dummy argument for colab to fool ipython",
                        default="1",
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200),

              dss_args=dict(type="MILO",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              wandb=False,
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milo_tinyimagenet.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILO",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            gc_ratio=1/6,
                            gc_stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milofixed_cifar10.py
Content:
import argparse
import os

def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar10",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()
#global_order, _, _ = generate_image_global_order(args.dataset, args.model, args.submod_function, data_dir=args.data_dir, device=args.device)


# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="MILOFixed",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            temperature=1,
                            kappa=0,
                            per_class=True,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              wandb=False,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

#
File Path: configs/SL/config_milofixed_cifar100.py
Content:
from cords.utils.data.data_utils import generate_image_global_order
import argparse, os

def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()
# global_order, _, _ = generate_image_global_order(args.dataset, args.model, args.submod_function, data_dir=args.data_dir, device=args.device)


# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILOFixed",
                            fraction=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + 'rbf_kernel' + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            temperature=1,
                            kw=0.1,
                            per_class=True,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milofixed_glove_glue_sst2.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="glue_sst2",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting


config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILOFixed",
                            fraction=0.3,
                            select_every=1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milofixed_glove_imdb.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="imdb",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILOFixed",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milofixed_glove_rotten_tomatoes.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="rotten_tomatoes",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILOFixed",
                            fraction=0.3,
                            select_every=1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milofixed_glove_trec6.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILOFixed",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milofixed_glove_tweet_eval.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILOFixed",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_milofixed_mnist.py
Content:
import argparse
import os

def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="mnist",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()
#global_order, _, _ = generate_image_global_order(args.dataset, args.model, args.submod_function, data_dir=args.data_dir, device=args.device)


# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="MILOFixed",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            temperature=1,
                            kappa=0,
                            per_class=True,
                            collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

#
File Path: configs/SL/config_milofixed_tinyimagenet.py
Content:
from cords.utils.data.data_utils import generate_image_global_order
import argparse, os

def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()
# global_order, _, _ = generate_image_global_order(args.dataset, args.model, args.submod_function, data_dir=args.data_dir, device=args.device)


# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="MILOFixed",
                            fraction=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + 'rbf_kernel' + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            temperature=1,
                            kw=0.1,
                            per_class=True,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random-warm_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random-Warm",
                                fraction=0.1,
                                select_every=20,
                                kappa=0.5,
                                collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random-warm_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             lr1=0.1,
                             lr2=0.1,
                             lr3=0.1,
                             nesterov = True,
                             weight_decay=5e-4),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="Random-Warm",
                                fraction=0.1,
                                select_every=20,
                                kappa=0.5,
                                collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=10,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_cifar10.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="Random",
                            fraction=0.1,
                            select_every=1,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_cifar100.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.1,
                            select_every=1,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_glove_glue_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_glove_imdb.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_glove_rotten_tomatoes.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_glove_sst2.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.3,
                            select_every=5,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_glove_sst5.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="sst5",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=5,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.3,
                            select_every=5,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_glove_trec6.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_glove_tweet_eval.py
Content:
# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.3,
                            select_every=1,
                            kappa=0,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_mnist.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="Random",
                            fraction=0.1,
                            select_every=1,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_random_tinyimagenet.py
Content:
# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="Random",
                            fraction=0.1,
                            select_every=1,
                            kappa=0,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_selcon_lawschool.py
Content:
# Learning setting
config = dict(setting="SL",

              dataset=dict(name="LawSchool_selcon",
                           datadir="../data",
                           feature="dss",
                           type="pre-defined"),

              dataloader=dict(shuffle=True,
                              batch_size=100,
                              pin_memory=False),

              model=dict(architecture='RegressionNet',
                         type='pre-defined',
                         input_dim=10,
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='MeanSquaredLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam", 
                             lr=0.01),

              scheduler=dict(type="StepLR", # added this new scheduler type
                             step_size=1,
                             gamma=0.1),

              dss_args=dict(type="SELCON",
                                fraction=0.01,
                                select_every=35,
                                kappa=0,
                                delta=0.04,
                                linear_layer=False,
                                lam=1e-5,
                                batch_sampler='sequential',
                                selection_type='Supervised'),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["val_loss", "tst_loss", "trn_loss", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_sge_cifar10.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="SGE",
                            fraction=0.1,
                            kw=0.1,
                            stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_sge_cifar100.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="SGE",
                            fraction=0.1,
                            kw=0.1,
                            stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_sge_glove_imdb.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="imdb",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="SGE",
                            kw=0.1,
                            stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_sge_glove_rotten_tomatoes.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="rotten_tomatoes",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
from cords.utils.data.data_utils.collate import *

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="SGE",
                            fraction=0.3,
                            select_every=1,
                            kw=0.1,
                            stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_sge_glove_trec6.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="SGE",
                            kw=0.1,
                            stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            #stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            fraction=0.3,
                            select_every=1,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            temperature=1,
                            per_class=True,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_sge_mnist.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200),

              dss_args=dict(type="SGE",
                            fraction=0.1,
                            kw=0.1,
                            stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_sge_tinyimagenet.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="SGE",
                            fraction=0.1,
                            kw=0.1,
                            stochastic_subsets_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_0.1_stochastic_subsets.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_cifar10.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar10",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar10",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300,
                             stepsize=20,
                             gamma=0.1),

              dss_args=dict(type="WRE",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_cifar100.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="cifar100",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="cifar100",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=100),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="WRE",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_glove_glue_sst2.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="glue_sst2",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="glue_sst2",
                           datadir="../data/SST/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="WRE",
                            fraction=0.3,
                            select_every=1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_glove_imdb.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="imdb",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="imdb",
                           datadir="../data/imdb/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="WRE",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            fraction=0.3,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_glove_rotten_tomatoes.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="rotten_tomatoes",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="rotten_tomatoes",
                           datadir="../data/rotten_tomatoes/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=2,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="WRE",
                            fraction=0.3,
                            select_every=1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              # print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_glove_trec6.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="hf_trec6",
                           datadir="../data/TREC6/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=6,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="WRE",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            fraction=0.3,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_glove_tweet_eval.py
Content:
from cords.utils.data.data_utils.collate import *
import argparse, os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="trec6",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tweet_eval",
                           datadir="../data/tweet_eval/",
                           feature="dss",
                           type="text",
                           wordvec_dim=300,
                           weight_path='../data/glove.6B/',),

              dataloader=dict(shuffle=True,
                              batch_size=16,
                              pin_memory=True,
                              collate_fn = collate_fn_pad_batch),

              model=dict(architecture='LSTM',
                         type='pre-defined',
                         numclasses=20,
                         wordvec_dim=300,
                         weight_path='../data/glove.6B/',
                         hidden_size=128,
                         num_layers=1),

              ckpt=dict(is_load=False,
                        is_save=False,
                        dir='results/',
                        save_every=5),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="adam",
                             momentum=0.9,
                             lr=0.001,
                             weight_decay=5e-4),

              scheduler=dict(type=None,
                            #  type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="WRE",
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            fraction=0.3,
                            select_every=1,
                            submod_function = 'fl',
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = collate_fn_pad_batch
                            ),

              train_args=dict(num_epochs=20,
                              device="cuda",
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              wandb=False,
                            #   print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_mnist.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="mnist",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="mnist",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='MnistNet',
                         type='pre-defined',
                         numclasses=10),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=200),

              dss_args=dict(type="WRE",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=200,
                              device="cuda",
                              wandb=False,
                              print_every=1,
                              run=1,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SL/config_wre_tinyimagenet.py
Content:
import argparse
import os


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="tinyimagenet",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="clip-ViT-L-14",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="../data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="fl",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:0',
                        help= "Device used for computing the embeddings"
                        )
    parser.add_argument(
                        "--kw",
                        type=float,
                        default=0.1,
                        help= "Multiplier for RBF Kernel"
                        )
    parser.add_argument(
                        "--r2_coefficient",
                        type=float,
                        default=3,
                        help= "Multiplier for R2 Variant"
                        )
    parser.add_argument(
                        "--knn",
                        type=int,
                        default=25,
                        help= "No of nearest neighbors for KNN variant"
                        )
    args=parser.parse_args()
    return args

args = parse_args()

# Learning setting
config = dict(setting="SL",
              is_reg = False,
              dataset=dict(name="tinyimagenet",
                           datadir="../data",
                           feature="dss",
                           type="image"),

              dataloader=dict(shuffle=True,
                              batch_size=128,
                              pin_memory=True),

              model=dict(architecture='ResNet18',
                         type='pre-defined',
                         numclasses=200),
              
              ckpt=dict(is_load=False,
                        is_save=True,
                        dir='results/',
                        save_every=20),
              
              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.05,
                             weight_decay=5e-4,
                             nesterov=True),

              scheduler=dict(type="cosine_annealing",
                             T_max=300),

              dss_args=dict(type="WRE",
                            fraction=0.1,
                            kw=0.1,
                            global_order_file=os.path.join(os.path.abspath(args.data_dir), args.dataset + '_' + args.model + '_' + args.submod_function + '_' + str(args.kw) + '_global_order.pkl'),
                            submod_function = 'fl',
                            select_every=1,
                            kappa=0,
                            per_class=True,
                            temperature=1,
                            collate_fn = None),

              train_args=dict(num_epochs=300,
                              device="cuda",
                              print_every=1,
                              run=1,
                              wandb=False,
                              results_dir='results/',
                              print_args=["trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"],
                              return_args=[]
                              )
              )

File Path: configs/SSL/__init__.py
Content:
from .parser import get_args
File Path: configs/SSL/config_gradmatch_vat_cifar10.py
Content:
# Learning setting
config = dict(setting="SSL",
              dataset=dict(name="cifar10",
                           root="../data",
                           feature="dss",
                           type="pre-defined",
                           num_labels=4000,
                           val_ratio=0.1,
                           ood_ratio=0.5,
                           random_split=False,
                           whiten=False,
                           zca=True,
                           labeled_aug='WA',
                           unlabeled_aug='WA',
                           wa='t.t.f',
                           strong_aug=False),

              dataloader=dict(shuffle=True,
                              pin_memory=True,
                              num_workers=2,
                              l_batch_size=50,
                              ul_batch_size=50),

              model=dict(architecture='wrn',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        checkpoint_model='model.ckpt',
                        checkpoint_optimizer='optimizer.ckpt',
                        start_iter=None,
                        checkpoint=10000),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.03,
                             weight_decay=0,
                             nesterov=True,
                             tsa=False,
                             tsa_schedule='linear'),

              scheduler=dict(lr_decay="cos",
                             warmup_iter=0),

              ssl_args=dict(alg='vat',
                            coef=0.3,
                            ema_teacher=False,
                            ema_teacher_warmup=False,
                            ema_teacher_factor=0.999,
                            ema_apply_wd=False,
                            em=0,
                            threshold=None,
                            sharpen=None,
                            temp_softmax=None,
                            consis='ce',
                            eps=6,
                            xi=1e-6,
                            vat_iter=1
                            ),

              ssl_eval_args=dict(weight_average=False,
                                 wa_ema_factor=0.999,
                                 wa_apply_wd=False),

              dss_args=dict(type="GradMatch",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            linear_layer=True,
                            selection_type='PerClassPerGradient',
                            greedy='Stochastic',
                            valid=False,
                            v1=True,
                            eps=1e-100,
                            lam=0.5
                            ),

              train_args=dict(iteration=500000,
                              max_iter=-1,
                              device="cuda",
                              results_dir='results/',
                              disp=256,
                              seed=96)
              )

File Path: configs/SSL/config_retrieve-warm_vat_cifar10.py
Content:
# Learning setting
config = dict(setting="SSL",
              dataset=dict(name="cifar10",
                           root="../data",
                           feature="dss",
                           type="pre-defined",
                           num_labels=4000,
                           val_ratio=0.1,
                           ood_ratio=0.5,
                           random_split=False,
                           whiten=False,
                           zca=True,
                           labeled_aug='WA',
                           unlabeled_aug='WA',
                           wa='t.t.f',
                           strong_aug=False),

              dataloader=dict(shuffle=True,
                              pin_memory=True,
                              num_workers=8,
                              l_batch_size=50,
                              ul_batch_size=50),

              model=dict(architecture='wrn',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        checkpoint_model='model.ckpt',
                        checkpoint_optimizer='optimizer.ckpt',
                        start_iter=None,
                        checkpoint=10000),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.03,
                             weight_decay=0,
                             nesterov=True,
                             tsa=False,
                             tsa_schedule='linear'),

              scheduler=dict(lr_decay="cos",
                             warmup_iter=0),

              ssl_args=dict(alg='vat',
                            coef=0.3,
                            ema_teacher=False,
                            ema_teacher_warmup=False,
                            ema_teacher_factor=0.999,
                            ema_apply_wd=False,
                            em=0,
                            threshold=None,
                            sharpen=None,
                            temp_softmax=None,
                            consis='ce',
                            eps=6,
                            xi=1e-6,
                            vat_iter=1
                            ),

              ssl_eval_args=dict(weight_average=False,
                                 wa_ema_factor=0.999,
                                 wa_apply_wd=False),

              dss_args=dict(type="RETRIEVE-Warm",
                            fraction=0.1,
                            select_every=20,
                            kappa=0.5,
                            linear_layer=False,
                            selection_type='Supervised',
                            greedy='Stochastic',
                            valid=True),

              train_args=dict(iteration=500000,
                              max_iter=-1,
                              device="cuda",
                              results_dir='results/',
                              disp=256,
                              seed=96)
              )

File Path: configs/SSL/config_retrieve_vat_cifar10.py
Content:
# Learning setting
config = dict(setting="SSL",
              dataset=dict(name="cifar10",
                           root="../data",
                           feature="dss",
                           type="pre-defined",
                           num_labels=4000,
                           val_ratio=0.1,
                           ood_ratio=0.5,
                           random_split=False,
                           whiten=False,
                           zca=True,
                           labeled_aug='WA',
                           unlabeled_aug='WA',
                           wa='t.t.f',
                           strong_aug=False),

              dataloader=dict(shuffle=True,
                              pin_memory=True,
                              num_workers=8,
                              l_batch_size=50,
                              ul_batch_size=50),

              model=dict(architecture='wrn',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        checkpoint_model='model.ckpt',
                        checkpoint_optimizer='optimizer.ckpt',
                        start_iter=None,
                        checkpoint=10000),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.03,
                             weight_decay=0,
                             nesterov=True,
                             tsa=False,
                             tsa_schedule='linear'),

              scheduler=dict(lr_decay="cos",
                             warmup_iter=0),

              ssl_args=dict(alg='vat',
                            coef=0.3,
                            ema_teacher=False,
                            ema_teacher_warmup=False,
                            ema_teacher_factor=0.999,
                            ema_apply_wd=False,
                            em=0,
                            threshold=None,
                            sharpen=None,
                            temp_softmax=None,
                            consis='ce',
                            eps=6,
                            xi=1e-6,
                            vat_iter=1
                            ),

              ssl_eval_args=dict(weight_average=False,
                                 wa_ema_factor=0.999,
                                 wa_apply_wd=False),

              dss_args=dict(type="RETRIEVE",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            linear_layer=False,
                            selection_type='Supervised',
                            greedy='Stochastic',
                            valid=True),

              train_args=dict(iteration=500000,
                              max_iter=-1,
                              device="cuda",
                              results_dir='results/',
                              disp=256,
                              seed=96)
              )

File Path: configs/SSL/config_retrievepb-warm_vat_cifar10.py
Content:
# Learning setting
config = dict(setting="SSL",
              dataset=dict(name="cifar10",
                           root="../data",
                           feature="dss",
                           type="pre-defined",
                           num_labels=4000,
                           val_ratio=0.1,
                           ood_ratio=0.5,
                           random_split=False,
                           whiten=False,
                           zca=True,
                           labeled_aug='WA',
                           unlabeled_aug='WA',
                           wa='t.t.f',
                           strong_aug=False),

              dataloader=dict(shuffle=True,
                              pin_memory=True,
                              num_workers=8,
                              l_batch_size=50,
                              ul_batch_size=50),

              model=dict(architecture='wrn',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        checkpoint_model='model.ckpt',
                        checkpoint_optimizer='optimizer.ckpt',
                        start_iter=None,
                        checkpoint=10000),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.03,
                             weight_decay=0,
                             nesterov=True,
                             tsa=False,
                             tsa_schedule='linear'),

              scheduler=dict(lr_decay="cos",
                             warmup_iter=0),

              ssl_args=dict(alg='vat',
                            coef=0.3,
                            ema_teacher=False,
                            ema_teacher_warmup=False,
                            ema_teacher_factor=0.999,
                            ema_apply_wd=False,
                            em=0,
                            threshold=None,
                            sharpen=None,
                            temp_softmax=None,
                            consis='ce',
                            eps=6,
                            xi=1e-6,
                            vat_iter=1
                            ),

              ssl_eval_args=dict(weight_average=False,
                                 wa_ema_factor=0.999,
                                 wa_apply_wd=False),

              dss_args=dict(type="RETRIEVEPB",
                            fraction=0.1,
                            select_every=20,
                            kappa=0.5,
                            linear_layer=False,
                            selection_type='PerBatch',
                            greedy='Stochastic',
                            valid=True),

              train_args=dict(iteration=500000,
                              max_iter=-1,
                              device="cuda",
                              results_dir='results/',
                              disp=256,
                              seed=96)
              )

File Path: configs/SSL/config_retrievepb_vat_cifar10.py
Content:
# Learning setting
config = dict(setting="SSL",
              dataset=dict(name="cifar10",
                           root="../data",
                           feature="dss",
                           type="pre-defined",
                           num_labels=4000,
                           val_ratio=0.1,
                           ood_ratio=0.5,
                           random_split=False,
                           whiten=False,
                           zca=True,
                           labeled_aug='WA',
                           unlabeled_aug='WA',
                           wa='t.t.f',
                           strong_aug=False),

              dataloader=dict(shuffle=True,
                              pin_memory=True,
                              num_workers=8,
                              l_batch_size=50,
                              ul_batch_size=50),

              model=dict(architecture='wrn',
                         type='pre-defined',
                         numclasses=10),

              ckpt=dict(is_load=False,
                        is_save=True,
                        checkpoint_model='model.ckpt',
                        checkpoint_optimizer='optimizer.ckpt',
                        start_iter=None,
                        checkpoint=10000),

              loss=dict(type='CrossEntropyLoss',
                        use_sigmoid=False),

              optimizer=dict(type="sgd",
                             momentum=0.9,
                             lr=0.03,
                             weight_decay=0,
                             nesterov=True,
                             tsa=False,
                             tsa_schedule='linear'),

              scheduler=dict(lr_decay="cos",
                             warmup_iter=0),

              ssl_args=dict(alg='vat',
                            coef=0.3,
                            ema_teacher=False,
                            ema_teacher_warmup=False,
                            ema_teacher_factor=0.999,
                            ema_apply_wd=False,
                            em=0,
                            threshold=None,
                            sharpen=None,
                            temp_softmax=None,
                            consis='ce',
                            eps=6,
                            xi=1e-6,
                            vat_iter=1
                            ),

              ssl_eval_args=dict(weight_average=False,
                                 wa_ema_factor=0.999,
                                 wa_apply_wd=False),

              dss_args=dict(type="RETRIEVEPB",
                            fraction=0.1,
                            select_every=20,
                            kappa=0,
                            linear_layer=False,
                            selection_type='PerBatch',
                            greedy='Stochastic',
                            valid=True),

              train_args=dict(iteration=500000,
                              max_iter=-1,
                              device="cuda",
                              results_dir='results/',
                              disp=256,
                              seed=96)
              )

File Path: configs/SSL/parser.py
Content:
import argparse


def get_args():
    parser = argparse.ArgumentParser()
    # dataset config
    parser.add_argument("--root", "-r", default="./data", type=str, help="/path/to/dataset")
    parser.add_argument("--dataset", "-d", default="cifar10", choices=['stl10', 'svhn', 'cifar10', 'cifar100', 'cifarOOD', 'mnistOOD', 'cifarImbalance'], type=str, help="dataset name")
    parser.add_argument("--num_labels", default=4000, type=int, help="number of labeled data")
    parser.add_argument("--ood_ratio", default=0.5, type=float, help="the ratio of OOD in unlabeled data")
    parser.add_argument("--val_ratio", default=0.1, type=float, help="the ratio of evaluation data to training data.")
    parser.add_argument("--random_split", action="store_true", help="random sampling from training data for validation")
    parser.add_argument("--num_workers", default=8, type=int, help="number of thread for CPU parallel")
    parser.add_argument("--whiten", action="store_true", help="use whitening as preprocessing")
    parser.add_argument("--zca", action="store_true", help="use zca whitening as preprocessing")
    parser.add_argument("--ood", action="store_true", help="OOD indicator")
    parser.add_argument("--classimb", action="store_true", help="ClassImbalance indicator")
    # augmentation config
    parser.add_argument("--labeled_aug", default="WA", choices=['WA', 'RA'], type=str, help="type of augmentation for labeled data")
    parser.add_argument("--unlabeled_aug", default="WA", choices=['WA', 'RA'], type=str, help="type of augmentation for unlabeled data")
    parser.add_argument("--wa", default="t.t.f", type=str, help="transformations (flip, crop, noise) for weak augmentation. t and f indicate true and false.")
    parser.add_argument("--strong_aug", action="store_true", help="use strong augmentation (RandAugment) for unlabeled data")
    # optimization config
    parser.add_argument("--model", default="wrn", choices=['wrn', 'shake', 'cnn13', 'cnn'], type=str, help="model architecture")
    parser.add_argument("--ul_batch_size", "-ul_bs", default=50, type=int, help="mini-batch size of unlabeled data")
    parser.add_argument("--l_batch_size", "-l_bs", default=50, type=int, help="mini-batch size of labeled data")
    parser.add_argument("--optimizer", "-opt", default="sgd", choices=['sgd', 'adam'], type=str, help="optimizer")
    parser.add_argument("--lr", default=3e-2, type=float, help="learning rate")
    parser.add_argument("--weight_decay", "-wd", default=0.0005, type=float, help="weight decay")
    parser.add_argument("--momentum", default=0.9, type=float, help="momentum for sgd or beta_1 for adam")
    parser.add_argument("--iteration", default=500000, type=int, help="number of training iteration")
    parser.add_argument("--lr_decay", default="cos", choices=['cos', 'step'], type=str, help="way to decay learning rate")
    parser.add_argument("--lr_decay_rate", default=0.2, type=float, help="decay rate for step lr decay")
    parser.add_argument("--only_validation", action="store_true", help="only training and validation for hyperparameter tuning")
    parser.add_argument("--warmup_iter", default=0, type=int, help="number of warmup iteration for SSL loss coefficient")
    parser.add_argument("--tsa", action="store_true", help="use training signal annealing proposed by UDA")
    parser.add_argument("--tsa_schedule", default="linear", choices=['linear', 'exp', 'log'], type=str, help="tsa schedule")
    # SSL common config
    parser.add_argument("--alg", default="cr", choices=['ict', 'cr', 'pl', 'vat'], type=str, help="ssl algorithm")
    parser.add_argument("--coef", default=1, type=float, help="coefficient for consistency loss")
    parser.add_argument("--ema_teacher", action="store_true", help="use mean teacher")
    parser.add_argument("--ema_teacher_warmup", action="store_true", help="warmup for mean teacher")
    parser.add_argument("--ema_teacher_factor", default=0.999, type=float, help="exponential mean avarage factor for mean teacher")
    parser.add_argument("--ema_apply_wd", action="store_true", help="apply weight decay to ema model")
    parser.add_argument("--entropy_minimization", "-em", default=0, type=float, help="coefficient of entropy minimization")
    parser.add_argument("--threshold", default=None, type=float, help="pseudo label threshold")
    parser.add_argument("--sharpen", default=None, type=float, help="temperature parameter for sharpening")
    parser.add_argument("--temp_softmax", default=None, type=float, help="temperature for softmax")
    parser.add_argument("--consistency", "-consis", default="ce", choices=['ce', 'ms', 'kld'], type=str, help="consistency type")
    ## SSL alg parameter
    ### ICT config
    parser.add_argument("--alpha", default=0.1, type=float, help="parameter for beta distribution in ICT")
    ### VAT config
    parser.add_argument("--eps", default=6, type=float, help="norm of virtual adversarial noise")
    parser.add_argument("--xi", default=1e-6, type=float, help="perturbation for finite difference method")
    parser.add_argument("--vat_iter", default=1, type=int, help="number of iteration for power iteration")
    #evaluation config
    parser.add_argument("--weight_average", action="store_true", help="evaluation with weight-averaged model")
    parser.add_argument("--wa_ema_factor", default=0.999, type=float, help="exponential mean avarage factor for weight-averaged model")
    parser.add_argument("--wa_apply_wd", action="store_true", help="apply weight decay to weight-averaged model")
    parser.add_argument("--checkpoint", default=10000, type=int, help="checkpoint every N samples")
    #subset selection arguments
    parser.add_argument("--dss_strategy", default='GradMatchPB', type=str, help="Data Subset Selection Strategy")
    parser.add_argument("--fraction", default=0.1, type=float, help="Unlabeled dataset fraction")
    parser.add_argument("--select_every", default=20, type=int, help="subset selection every N epochs")
    parser.add_argument("--kappa", default=0.5, type=float, help="Kappa value for Warm Variants")
    parser.add_argument("--valid", action="store_true", help="Use Validation Set for Gradient Matching")
    parser.add_argument("--max_iter", default=-1, type=int, help="Use max iterations if not -1")
    # training from checkpoint
    parser.add_argument("--checkpoint_model", default=None, type=str, help="path to checkpoint model")
    parser.add_argument("--checkpoint_optimizer", default=None, type=str, help="path to checkpoint optimizer")
    parser.add_argument("--start_iter", default=None, type=int, help="start iteration")
    # misc
    parser.add_argument("--out_dir", default="log", type=str, help="output directory")
    parser.add_argument("--seed", default=96, type=int, help="random seed")
    parser.add_argument("--disp", default=256, type=int, help="display loss every N")
    return parser.parse_args()

File Path: cords/__init__.py
Content:
# __init__.py
__version__ = "v0.0.2"


File Path: cords/selectionstrategies/SL/__init__.py
Content:
from .craigstrategy import CRAIGStrategy
from .dataselectionstrategy import DataSelectionStrategy
from .glisterstrategy import GLISTERStrategy
from .randomstrategy import RandomStrategy
from .submodularselectionstrategy import SubmodularSelectionStrategy
from .gradmatchstrategy import GradMatchStrategy
from .fixedweightstrategy import FixedWeightStrategy
from .selconstrategy import SELCONstrategy
from .adapweightsstrategy import AdapWeightsStrategy
from .stochasticgreedyexplorationstrategy import StochasticGreedyExplorationStrategy
from .weightedrandomexplorationstrategy import WeightedRandomExplorationStrategy

File Path: cords/selectionstrategies/SL/adapweightsstrategy.py
Content:
import math
import time
import torch
import numpy as np
from .dataselectionstrategy import DataSelectionStrategy
# from ..helpers import OrthogonalMP_REG_Parallel, OrthogonalMP_REG, OrthogonalMP_REG_Parallel_V1
from torch.utils.data import Subset, DataLoader
from scipy.optimize import nnls
from sklearn.linear_model import LinearRegression

class AdapWeightsStrategy(DataSelectionStrategy):
    """
    Implementation of GradMatch Strategy from the paper :footcite:`sivasubramanian2020gradmatch` for supervised learning frameworks.

    GradMatch strategy tries to solve the optimization problem given below:

    .. math::
        \\min_{\\mathbf{w}, S: |S| \\leq k} \\Vert \\sum_{i \\in S} w_i \\nabla_{\\theta}L_T^i(\\theta) -  \\nabla_{\\theta}L(\\theta)\\Vert

    In the above equation, :math:`\\mathbf{w}` denotes the weight vector that contains the weights for each data instance, :math:`\mathcal{U}` training set where :math:`(x^i, y^i)` denotes the :math:`i^{th}` training data point and label respectively,
    :math:`L_T` denotes the training loss, :math:`L` denotes either training loss or validation loss depending on the parameter valid,
    :math:`S` denotes the data subset selected at each round, and :math:`k` is the budget for the subset.

    The above optimization problem is solved using the Orthogonal Matching Pursuit(OMP) algorithm.

    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    loss: class
        PyTorch loss function for training
    eta: float
        Learning rate. Step size for the one step gradient update
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        Apply linear transformation to the data
    selection_type: str
        Type of selection -
        - 'PerClass': PerClass method is where OMP algorithm is applied on each class data points seperately.
        - 'PerBatch': PerBatch method is where OMP algorithm is applied on each minibatch data points.
        - 'PerClassPerGradient': PerClassPerGradient method is same as PerClass but we use the gradient corresponding to classification layer of that class only.
    logger : class
        - logger object for logging the information
    valid : bool
        If valid==True, we use validation dataset gradient sum in OMP otherwise we use training dataset (default: False)
    """

    def __init__(self, trainloader, valloader, model, loss,
                 eta, device, num_classes, linear_layer,
                 selection_type, logger, ss_indices, valid=False):
        """
        Constructor method
        """
        super().__init__(trainloader, valloader, model, num_classes, linear_layer, loss, device, logger)
        self.eta = eta  # step size for the one step gradient update
        self.device = device
        self.init_out = list()
        self.init_l1 = list()
        self.selection_type = selection_type
        self.valid = valid

        self.ss_indices = ss_indices

    def select(self, budget, model_params):
        """
        Apply OMP Algorithm for data selection

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing models parameters

        Returns
        ----------
        idxs: list
            List containing indices of the best datapoints,
        gammas: weights tensors
            Tensor containing weights of each instance
        """
        omp_start_time = time.time()
        self.update_model(model_params)

        self.compute_gradients(self.valid, perBatch=False, perClass=False)

        idxs = self.ss_indices
        trn_gradients = self.grads_per_elem

        ss_grad = trn_gradients[idxs].clone().detach()
        ss_grad = torch.transpose(ss_grad, 0, 1)
        b_ = trn_gradients.sum(dim = 0)
        if self.valid:
                b_ = torch.sum(self.val_grads_per_elem, dim=0)
        print("ss_grad shape in adapweightsstrategy:", ss_grad.shape)
        
        if ss_grad.shape[0] != b_.shape[0]:
            print("Shapes mismatch, error in adapweightstrategy, exiting")
            exit(1)
        elif ss_grad.shape[1] > 0 and b_.shape[0] > 0:
            # reg_nnls = LinearRegression(positive=True)
            # gammas = reg_nnls.fit(np.nan_to_num(ss_grad.detach().cpu().numpy()), np.nan_to_num(b_.detach().cpu().numpy())).coef_
            gammas, _ = nnls(np.nan_to_num(ss_grad.detach().cpu().numpy()), np.nan_to_num(b_.detach().cpu().numpy())\
                , maxiter = int(30*ss_grad.shape[1]))
        else:
            gammas = list(np.random.ranint(1,10,ss_grad.shape[1]))

        omp_end_time = time.time()
        self.logger.debug("AdapWeights algorithm Subset Selection time is: %.4f", omp_end_time - omp_start_time)
        return idxs, torch.FloatTensor(gammas)

File Path: cords/selectionstrategies/SL/craigstrategy.py
Content:
import math
import apricot
import numpy as np
import time
import torch
from scipy.sparse import csr_matrix
from torch.utils.data.sampler import SubsetRandomSampler
from .dataselectionstrategy import DataSelectionStrategy


class CRAIGStrategy(DataSelectionStrategy):
    """
    Implementation of CRAIG Strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a` for supervised learning frameworks.

    CRAIG strategy tries to solve the optimization problem given below for convex loss functions:

    .. math::
        \\sum_{i\\in \\mathcal{U}} \\min_{j \\in S, |S| \\leq k} \\| x^i - x^j \\|

    In the above equation, :math:`\\mathcal{U}` denotes the training set where :math:`(x^i, y^i)` denotes the :math:`i^{th}` training data point and label respectively,
    :math:`L_T` denotes the training loss, :math:`S` denotes the data subset selected at each round, and :math:`k` is the budget for the subset.

    Since, the above optimization problem is not dependent on model parameters, we run the subset selection only once right before the start of the training.

    CRAIG strategy tries to solve the optimization problem given below for non-convex loss functions:

    .. math::
        \\sum_{i\\in \\mathcal{U}} \\min_{j \\in S, |S| \\leq k} \\| \\nabla_{\\theta} {L_T}^i(\\theta) - \\nabla_{\\theta} {L_T}^j(\\theta) \\|

    In the above equation, :math:`\\mathcal{U}` denotes the training set, :math:`L_T` denotes the training loss, :math:`S` denotes the data subset selected at each round,
    and :math:`k` is the budget for the subset. In this case, CRAIG acts an adaptive subset selection strategy that selects a new subset every epoch.

    Both the optimization problems given above are an instance of facility location problems which is a submodular function. Hence, it can be optimally solved using greedy selection methods.

    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    loss_type: class
        PyTorch Loss Function
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        Apply linear transformation to the data
    if_convex: bool
        If convex or not
    selection_type: str
        Type of selection:
         - 'PerClass': PerClass Implementation where the facility location problem is solved for each class seperately for speed ups.
         - 'Supervised':  Supervised Implementation where the facility location problem is solved using a sparse similarity matrix by 
                          assigning the similarity of a point with other points of different class to zero.
         - 'PerBatch': PerBatch Implementation where the facility location problem tries to select subset of mini-batches.
    logger : class
        - logger object for logging the information
    optimizer: str
        Type of Greedy Algorithm
    """

    def __init__(self, trainloader, valloader, model, loss,
                 device, num_classes, linear_layer, if_convex,
                 selection_type, logger, optimizer='lazy'):
        """
        Constructer method
        """
        super().__init__(trainloader, valloader, model, num_classes, linear_layer, loss, device, logger)
        self.if_convex = if_convex
        self.selection_type = selection_type
        self.logger = logger
        self.optimizer = optimizer

    def distance(self, x, y, exp=2):
        """
        Compute the distance.

        Parameters
        ----------
        x: Tensor
            First input tensor
        y: Tensor
            Second input tensor
        exp: float, optional
            The exponent value (default: 2)

        Returns
        ----------
        dist: Tensor
            Output tensor
        """

        n = x.size(0)
        m = y.size(0)
        d = x.size(1)
        x = x.unsqueeze(1).expand(n, m, d)
        y = y.unsqueeze(0).expand(n, m, d)
        dist = torch.pow(x - y, exp).sum(2)
        # dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))
        return dist

    def compute_score(self, model_params, idxs):
        """
        Compute the score of the indices.

        Parameters
        ----------
        model_params: OrderedDict
            Python dictionary object containing models parameters
        idxs: list
            The indices
        """

        trainset = self.trainloader.sampler.data_source
        subset_loader = torch.utils.data.DataLoader(trainset, batch_size=self.trainloader.batch_size, shuffle=False,
                                                    sampler=SubsetRandomSampler(idxs),
                                                    pin_memory=True, collate_fn=self.trainloader.collate_fn)
        self.model.load_state_dict(model_params)
        self.N = 0
        g_is = []

        if self.if_convex:
            for batch_idx, (inputs, targets) in enumerate(subset_loader):
                inputs, targets = inputs, targets
                if self.selection_type == 'PerBatch':
                    self.N += 1
                    g_is.append(inputs.view(inputs.size()[0], -1).mean(dim=0).view(1, -1))
                else:
                    self.N += inputs.size()[0]
                    g_is.append(inputs.view(inputs.size()[0], -1))
        else:
            embDim = self.model.get_embedding_dim()
            for batch_idx, (inputs, targets) in enumerate(subset_loader):
                inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)
                if self.selection_type == 'PerBatch':
                    self.N += 1
                else:
                    self.N += inputs.size()[0]
                out, l1 = self.model(inputs, freeze=True, last=True)
                loss = self.loss(out, targets).sum()
                l0_grads = torch.autograd.grad(loss, out)[0]
                if self.linear_layer:
                    l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                    l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                    if self.selection_type == 'PerBatch':
                        g_is.append(torch.cat((l0_grads, l1_grads), dim=1).mean(dim=0).view(1, -1))
                    else:
                        g_is.append(torch.cat((l0_grads, l1_grads), dim=1))
                else:
                    if self.selection_type == 'PerBatch':
                        g_is.append(l0_grads.mean(dim=0).view(1, -1))
                    else:
                        g_is.append(l0_grads)

        self.dist_mat = torch.zeros([self.N, self.N], dtype=torch.float32)
        first_i = True
        if self.selection_type == 'PerBatch':
            g_is = torch.cat(g_is, dim=0)
            self.dist_mat = self.distance(g_is, g_is).cpu()
        else:
            for i, g_i in enumerate(g_is, 0):
                if first_i:
                    size_b = g_i.size(0)
                    first_i = False
                for j, g_j in enumerate(g_is, 0):
                    self.dist_mat[i * size_b: i * size_b + g_i.size(0),
                    j * size_b: j * size_b + g_j.size(0)] = self.distance(g_i, g_j).cpu()
        self.const = torch.max(self.dist_mat).item()
        self.dist_mat = (self.const - self.dist_mat).numpy()

    def compute_gamma(self, idxs):
        """
        Compute the gamma values for the indices.

        Parameters
        ----------
        idxs: list
            The indices

        Returns
        ----------
        gamma: list
            Gradient values of the input indices
        """

        if self.selection_type in ['PerClass', 'PerBatch']:
            gamma = [0 for i in range(len(idxs))]
            best = self.dist_mat[idxs]  # .to(self.device)
            rep = np.argmax(best, axis=0)
            for i in rep:
                gamma[i] += 1
        elif self.selection_type == 'Supervised':
            gamma = [0 for i in range(len(idxs))]
            best = self.dist_mat[idxs]  # .to(self.device)
            rep = np.argmax(best, axis=0)
            for i in range(rep.shape[1]):
                gamma[rep[0, i]] += 1
        return gamma

    def get_similarity_kernel(self):
        """
        Obtain the similarity kernel.

        Returns
        ----------
        kernel: ndarray
            Array of kernel values
        """
        for batch_idx, (inputs, targets) in enumerate(self.trainloader):
            if batch_idx == 0:
                labels = targets
            else:
                tmp_target_i = targets
                labels = torch.cat((labels, tmp_target_i), dim=0)
        kernel = np.zeros((labels.shape[0], labels.shape[0]))
        for target in np.unique(labels):
            x = np.where(labels == target)[0]
            # prod = np.transpose([np.tile(x, len(x)), np.repeat(x, len(x))])
            for i in x:
                kernel[i, x] = 1
        return kernel

    def select(self, budget, model_params):
        """
        Data selection method using different submodular optimization
        functions.

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing models parameters
        optimizer: str
            The optimization approach for data selection. Must be one of
            'random', 'modular', 'naive', 'lazy', 'approximate-lazy', 'two-stage',
            'stochastic', 'sample', 'greedi', 'bidirectional'

        Returns
        ----------
        total_greedy_list: list
            List containing indices of the best datapoints
        gammas: list
            List containing gradients of datapoints present in greedySet
        """
        start_time = time.time()
        for batch_idx, (inputs, targets) in enumerate(self.trainloader):
            if batch_idx == 0:
                labels = targets
            else:
                tmp_target_i = targets
                labels = torch.cat((labels, tmp_target_i), dim=0)
        # per_class_bud = int(budget / self.num_classes)
        total_greedy_list = []
        gammas = []
        if self.selection_type == 'PerClass':
            for i in range(self.num_classes):
                idxs = torch.where(labels == i)[0]
                self.compute_score(model_params, idxs)
                fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0, metric='precomputed',
                                                                                  n_samples=math.ceil(
                                                                                      budget * len(idxs) / self.N_trn),
                                                                                  optimizer=self.optimizer)
                sim_sub = fl.fit_transform(self.dist_mat)
                greedyList = list(np.argmax(sim_sub, axis=1))
                gamma = self.compute_gamma(greedyList)
                total_greedy_list.extend(idxs[greedyList])
                gammas.extend(gamma)
            rand_indices = np.random.permutation(len(total_greedy_list))
            total_greedy_list = list(np.array(total_greedy_list)[rand_indices])
            gammas = list(np.array(gammas)[rand_indices])
        elif self.selection_type == 'Supervised':
            idxs = torch.arange(0, self.N_trn).long()
            N = len(idxs)
            self.compute_score(model_params, idxs)
            row = idxs.repeat_interleave(N)
            col = idxs.repeat(N)
            data = self.dist_mat.flatten()
            sparse_simmat = csr_matrix((data, (row.numpy(), col.numpy())), shape=(self.N_trn, self.N_trn))
            self.dist_mat = sparse_simmat
            fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0, metric='precomputed',
                                                                              n_samples=budget,
                                                                              optimizer=self.optimizer)
            sim_sub = fl.fit_transform(sparse_simmat)
            total_greedy_list = list(np.array(np.argmax(sim_sub, axis=1)).reshape(-1))
            gammas = self.compute_gamma(total_greedy_list)
        elif self.selection_type == 'PerBatch':
            idxs = torch.arange(self.N_trn)
            N = len(idxs)
            self.compute_score(model_params, idxs)
            fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0, metric='precomputed',
                                                                              n_samples=math.ceil(
                                                                                  budget / self.trainloader.batch_size),
                                                                              optimizer=self.optimizer)
            sim_sub = fl.fit_transform(self.dist_mat)
            temp_list = list(np.array(np.argmax(sim_sub, axis=1)).reshape(-1))
            gammas_temp = self.compute_gamma(temp_list)
            batch_wise_indices = list(self.trainloader.batch_sampler)
            for i in range(len(temp_list)):
                tmp = batch_wise_indices[temp_list[i]]
                total_greedy_list.extend(tmp)
                gammas.extend([gammas_temp[i]] * len(tmp))
        end_time = time.time()
        total_greedy_list = [int(x) for x in total_greedy_list]
        self.logger.debug("CRAIG strategy data selection time is: %.4f", end_time-start_time)
        return total_greedy_list, torch.FloatTensor(gammas)

File Path: cords/selectionstrategies/SL/dataselectionstrategy.py
Content:
import torch


class DataSelectionStrategy(object):
    """
    Implementation of Data Selection Strategy class which serves as base class for other
    dataselectionstrategies for general learning frameworks.
    Parameters
    ----------
        trainloader: class
            Loading the training data using pytorch dataloader
        valloader: class
            Loading the validation data using pytorch dataloader
        model: class
            Model architecture used for training
        num_classes: int
            Number of target classes in the dataset
        linear_layer: bool
            If True, we use the last fc layer weights and biases gradients
            If False, we use the last fc layer biases gradients
        loss: class
            PyTorch Loss function
        device: str
            The device being utilized - cpu | cuda
        logger: class
            logger object for logging the information
    """

    def __init__(self, trainloader, valloader, model, num_classes, linear_layer, loss, device, logger):
        """
        Constructor method
        """
        self.trainloader = trainloader  # assume its a sequential loader.
        self.valloader = valloader
        self.model = model
        self.N_trn = len(trainloader.sampler)
        self.N_val = len(valloader.sampler)
        self.grads_per_elem = None
        self.val_grads_per_elem = None
        self.numSelected = 0
        self.linear_layer = linear_layer
        self.num_classes = num_classes
        self.trn_lbls = None
        self.val_lbls = None
        self.loss = loss
        self.device = device
        self.logger = logger

    def select(self, budget, model_params):
        pass

    def get_labels(self, valid=False):
        if isinstance(self.trainloader.dataset[0], dict):
            for batch_idx, batch in enumerate(self.trainloader):
                if batch_idx == 0:
                    self.trn_lbls = batch['labels'].view(-1, 1)
                else:
                    self.trn_lbls = torch.cat((self.trn_lbls, batch['labels'].view(-1, 1)), dim=0)
        else:
            for batch_idx, (_, targets) in enumerate(self.trainloader):
                if batch_idx == 0:
                    self.trn_lbls = targets.view(-1, 1)
                else:
                    self.trn_lbls = torch.cat((self.trn_lbls, targets.view(-1, 1)), dim=0)
        self.trn_lbls = self.trn_lbls.view(-1)

        if valid:
            if isinstance(self.valloader.dataset[0], dict):
                for batch_idx, batch in enumerate(self.valloader):
                    if batch_idx == 0:
                        self.val_lbls = batch['labels'].view(-1, 1)
                    else:
                        self.val_lbls = torch.cat((self.val_lbls, batch['labels'].view(-1, 1)), dim=0)
            else:
                for batch_idx, (_, targets) in enumerate(self.valloader):
                    if batch_idx == 0:
                        self.val_lbls = targets.view(-1, 1)
                    else:
                        self.val_lbls = torch.cat((self.val_lbls, targets.view(-1, 1)), dim=0)
            self.val_lbls = self.val_lbls.view(-1)

    def compute_gradients(self, valid=False, perBatch=False, perClass=False):
        """
        Computes the gradient of each element.

        Here, the gradients are computed in a closed form using CrossEntropyLoss with reduction set to 'none'.
        This is done by calculating the gradients in last layer through addition of softmax layer.

        Using different loss functions, the way we calculate the gradients will change.

        For LogisticLoss we measure the Mean Absolute Error(MAE) between the pairs of observations.
        With reduction set to 'none', the loss is formulated as:

        .. math::
            \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad
            l_n = \\left| x_n - y_n \\right|,

        where :math:`N` is the batch size.


        For MSELoss, we measure the Mean Square Error(MSE) between the pairs of observations.
        With reduction set to 'none', the loss is formulated as:

        .. math::
            \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad
            l_n = \\left( x_n - y_n \\right)^2,

        where :math:`N` is the batch size.
        Parameters
        ----------
        valid: bool
            if True, the function also computes the validation gradients
        perBatch: bool
            if True, the function computes the gradients of each mini-batch
        perClass: bool
            if True, the function computes the gradients using perclass dataloaders
        """
        if (perBatch and perClass):
            raise ValueError("batch and perClass are mutually exclusive. Only one of them can be true at a time")

        embDim = self.model.get_embedding_dim()
        if perClass:
            trainloader = self.pctrainloader
            if valid:
                valloader = self.pcvalloader
        else:
            trainloader = self.trainloader
            if valid:
                valloader = self.valloader
            
        if isinstance(trainloader.dataset[0], dict):
            for batch_idx, batch in enumerate(trainloader):
                batch = {k: v.to(self.device) for k, v in batch.items()}        
                if batch_idx == 0:                
                    out, l1 = self.model(**batch, last=True, freeze=True)
                    loss = self.loss(out, batch['labels'].view(-1)).sum()
                    l0_grads = torch.autograd.grad(loss, out)[0]                    
                    if self.linear_layer:
                        l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                        l1_grads = l0_expand * l1.repeat(1, self.num_classes)                    
                    if perBatch:
                        l0_grads = l0_grads.mean(dim=0).view(1, -1)
                        if self.linear_layer:
                            l1_grads = l1_grads.mean(dim=0).view(1, -1)                
                else:                    
                    out, l1 = self.model(**batch, last=True, freeze=True)
                    loss = self.loss(out, batch['labels'].view(-1)).sum()
                    batch_l0_grads = torch.autograd.grad(loss, out)[0]                    
                    if self.linear_layer:
                        batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                        batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)
                    if perBatch:
                        batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                        if self.linear_layer:
                            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)            
                    l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)                    
                    if self.linear_layer:
                        l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)
        else:    
            for batch_idx, (inputs, targets) in enumerate(trainloader):
                inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)
                if batch_idx == 0:
                    out, l1 = self.model(inputs, last=True, freeze=True)
                    loss = self.loss(out, targets).sum()
                    l0_grads = torch.autograd.grad(loss, out)[0]
                    if self.linear_layer:
                        l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                        l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                    if perBatch:
                        l0_grads = l0_grads.mean(dim=0).view(1, -1)
                        if self.linear_layer:
                            l1_grads = l1_grads.mean(dim=0).view(1, -1)
                else:
                    out, l1 = self.model(inputs, last=True, freeze=True)
                    loss = self.loss(out, targets).sum()
                    batch_l0_grads = torch.autograd.grad(loss, out)[0]
                    if self.linear_layer:
                        batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                        batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)

                    if perBatch:
                        batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                        if self.linear_layer:
                            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)
                    l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                    if self.linear_layer:
                        l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)

        torch.cuda.empty_cache()

        if self.linear_layer:
            self.grads_per_elem = torch.cat((l0_grads, l1_grads), dim=1)
        else:
            self.grads_per_elem = l0_grads

        if valid:
            if isinstance(valloader.dataset[0], dict):
                for batch_idx, batch in enumerate(valloader):
                    batch = {k: v.to(self.device) for k, v in batch.items()}        
                    if batch_idx == 0:                
                        out, l1 = self.model(**batch, last=True, freeze=True)
                        loss = self.loss(out, batch['labels'].view(-1)).sum()
                        l0_grads = torch.autograd.grad(loss, out)[0]                    
                        if self.linear_layer:
                            l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                            l1_grads = l0_expand * l1.repeat(1, self.num_classes)                    
                        if perBatch:
                            l0_grads = l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                l1_grads = l1_grads.mean(dim=0).view(1, -1)                
                    else:                    
                        out, l1 = self.model(**batch, last=True, freeze=True)
                        loss = self.loss(out, batch['labels'].view(-1)).sum()
                        batch_l0_grads = torch.autograd.grad(loss, out)[0]                    
                        if self.linear_layer:
                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                            batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)
                        if perBatch:
                            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)            
                        l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)                    
                        if self.linear_layer:
                            l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)
            else:    
                for batch_idx, (inputs, targets) in enumerate(valloader):
                    inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)
                    if batch_idx == 0:
                        out, l1 = self.model(inputs, last=True, freeze=True)
                        loss = self.loss(out, targets).sum()
                        l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                            l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                        if perBatch:
                            l0_grads = l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                l1_grads = l1_grads.mean(dim=0).view(1, -1)
                    else:
                        out, l1 = self.model(inputs, last=True, freeze=True)
                        loss = self.loss(out, targets).sum()
                        batch_l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                            batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)

                        if perBatch:
                            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)
                        l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                        if self.linear_layer:
                            l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)

            torch.cuda.empty_cache()
            if self.linear_layer:
                self.val_grads_per_elem = torch.cat((l0_grads, l1_grads), dim=1)
            else:
                self.val_grads_per_elem = l0_grads

    def update_model(self, model_params):
        """
        Update the models parameters

        Parameters
        ----------
        model_params: OrderedDict
            Python dictionary object containing models parameters
        """
        self.model.load_state_dict(model_params)
File Path: cords/selectionstrategies/SL/fixedweightstrategy.py
Content:
import math
import time
import torch
import numpy as np
from .dataselectionstrategy import DataSelectionStrategy
from ..helpers import OrthogonalMP_REG_Parallel, OrthogonalMP_REG, OptimalWeights
from torch.utils.data import Subset, DataLoader


class FixedWeightStrategy(DataSelectionStrategy):
    """
    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    loss_type: class
        The type of loss criterion
    eta: float
        Learning rate. Step size for the one step gradient update
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        Apply linear transformation to the data
    selection_type: str
        Type of selection -
        - 'PerClass': PerClass method is where OMP algorithm is applied on each class data points seperately.
        - 'PerBatch': PerBatch method is where OMP algorithm is applied on each minibatch data points.
        - 'PerClassPerGradient': PerClassPerGradient method is same as PerClass but we use the gradient corresponding to classification layer of that class only.
    valid : bool, optional
        If valid==True we use validation dataset gradient sum in OMP otherwise we use training dataset (default: False)
    lam : float
        Regularization constant of OMP solver
    eps : float
        Epsilon parameter to which the above optimization problem is solved using OMP algorithm
    """

    def __init__(self, trainloader, valloader, model, loss,
                 eta, device, num_classes, linear_layer, selection_type, valid=True, lam=0, eps=1e-4, r=1):
        """
        Constructor method
        """
        super().__init__(trainloader, valloader, model, num_classes, linear_layer, loss, device)
        self.eta = eta  # step size for the one step gradient update
        self.init_out = list()
        self.init_l1 = list()
        self.selection_type = selection_type
        self.valid = valid
        self.lam = lam
        self.eps = eps

    def optimalWeightsWrapper(self, X, Y, bud):

        if self.device == "cpu":
            ind, weights = OptimalWeights(X.cpu().numpy(), Y.cpu().numpy(), nnz=bud)

        else:
            ind, weights = OptimalWeights(X, Y, nnz=bud, device=self.device)

        return ind, weights

    def select(self, budget, model_params):
        """
        Apply OMP Algorithm for data selection

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing models parameters

        Returns
        ----------
        idxs: list
            List containing indices of the best datapoints,
        gammas: weights tensors
            Tensor containing weights of each instance
        """
        omp_start_time = time.time()
        self.update_model(model_params)

        if self.selection_type == 'PerClass':
            self.get_labels(valid=self.valid)
            idxs = []
            gammas = []
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                                shuffle=False, pin_memory=True)
                if self.valid:
                    val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                    val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                    self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                                  shuffle=False, pin_memory=True)

                self.compute_gradients(self.valid, perBatch=False, perClass=True)
                trn_gradients = self.grads_per_elem
                if self.valid:
                    sum_val_grad = torch.sum(self.val_grads_per_elem, dim=0)
                else:
                    sum_val_grad = torch.sum(trn_gradients, dim=0)
                idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                                         sum_val_grad,
                                                         math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)

        elif self.selection_type == 'PerBatch':
            self.compute_gradients(self.valid, perBatch=True, perClass=False)
            idxs = []
            gammas = []
            trn_gradients = self.grads_per_elem
            if self.valid:
                sum_val_grad = torch.sum(self.val_grads_per_elem, dim=0)
            else:
                sum_val_grad = torch.sum(trn_gradients, dim=0)
            idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                                     sum_val_grad, math.ceil(budget / self.trainloader.batch_size))
            batch_wise_indices = list(self.trainloader.batch_sampler)
            for i in range(len(idxs_temp)):
                tmp = batch_wise_indices[idxs_temp[i]]
                idxs.extend(tmp)
                gammas.extend(list(gammas_temp[i] * np.ones(len(tmp))))

        elif self.selection_type == 'PerClassPerGradient':
            self.get_labels(valid=self.valid)
            idxs = []
            gammas = []
            embDim = self.model.get_embedding_dim()
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                                shuffle=False, pin_memory=True)
                if self.valid:
                    val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                    val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                    self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                                  shuffle=False, pin_memory=True)
                self.compute_gradients(self.valid, perBatch=False, perClass=True)
                trn_gradients = self.grads_per_elem
                tmp_gradients = trn_gradients[:, i].view(-1, 1)
                tmp1_gradients = trn_gradients[:,
                                 self.num_classes + (embDim * i): self.num_classes + (embDim * (i + 1))]
                trn_gradients = torch.cat((tmp_gradients, tmp1_gradients), dim=1)

                if self.valid:
                    val_gradients = self.val_grads_per_elem
                    tmp_gradients = val_gradients[:, i].view(-1, 1)
                    tmp1_gradients = val_gradients[:,
                                     self.num_classes + (embDim * i): self.num_classes + (embDim * (i + 1))]
                    val_gradients = torch.cat((tmp_gradients, tmp1_gradients), dim=1)
                    sum_val_grad = torch.sum(val_gradients, dim=0)
                else:
                    sum_val_grad = torch.sum(trn_gradients, dim=0)

                idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                                         sum_val_grad,
                                                         math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)

        omp_end_time = time.time()
        diff = budget - len(idxs)

        if diff > 0:
            remainList = set(np.arange(self.N_trn)).difference(set(idxs))
            new_idxs = np.random.choice(list(remainList), size=diff, replace=False)
            idxs.extend(new_idxs)
            gammas.extend([1 for _ in range(diff)])
            idxs = np.array(idxs)
            gammas = np.array(gammas)

        if self.selection_type in ["PerClass", "PerClassPerGradient"]:
            rand_indices = np.random.permutation(len(idxs))
            idxs = list(np.array(idxs)[rand_indices])
            gammas = list(np.array(gammas)[rand_indices])

        print("Fixed Weight algorithm Subset Selection time is: ", omp_end_time - omp_start_time)
        return idxs, torch.FloatTensor(gammas)
File Path: cords/selectionstrategies/SL/glisterstrategy.py
Content:
import math
import random
import time
import torch
import torch.nn.functional as F
from .dataselectionstrategy import DataSelectionStrategy
from torch.utils.data import Subset, DataLoader
import numpy as np


class GLISTERStrategy(DataSelectionStrategy):
    """
    Implementation of GLISTER-ONLINE Strategy from the paper :footcite:`killamsetty2021glister`  for supervised learning frameworks.
    GLISTER-ONLINE methods tries to solve the  bi-level optimization problem given below:

    .. math::
        \\overbrace{\\underset{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\operatorname{argmin\\hspace{0.7mm}}} L_V(\\underbrace{\\underset{\\theta}{\\operatorname{argmin\\hspace{0.7mm}}} L_T( \\theta, S)}_{inner-level}, {\\mathcal V})}^{outer-level}

    In the above equation, :math:`\\mathcal{U}` denotes the training set, :math:`\\mathcal{V}` denotes the validation set that guides the subset selection process, :math:`L_T` denotes the
    training loss, :math:`L_V` denotes the validation loss, :math:`S` denotes the data subset selected at each round,  and :math:`k` is the budget for the subset.

    Since, solving the complete inner-optimization is expensive, GLISTER-ONLINE adopts a online one-step meta approximation where we approximate the solution to inner problem
    by taking a single gradient step.

    The optimization problem after the approximation is as follows:

    .. math::
        \\overbrace{\\underset{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\operatorname{argmin\\hspace{0.7mm}}} L_V(\\underbrace{\\theta - \\eta \\nabla_{\\theta}L_T(\\theta, S)}_{inner-level}, {\\mathcal V})}^{outer-level}

    In the above equation, :math:`\\eta` denotes the step-size used for one-step gradient update.

    GLISTER-ONLINE also makes an additional approximation called Taylor-Series approximation to easily solve the outer problem using a greedy selection algorithm.
    The Taylor series approximation is as follows:

    .. math::
        L_V(\\theta - \\eta \\nabla_{\\theta}L_T(\\theta, S), {\\mathcal V}) \\approx L_V(\\theta) - \\eta {\\nabla_{\\theta}L_T(\\theta, S)}^T \\nabla_{\\theta}L_V(\\theta, {\\mathcal V})

    The Optimization problem after the Taylor series approximation is as follows:

    .. math::
        \\underset{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\operatorname{argmin\\hspace{0.7mm}}}L_V(\\theta - \\eta \\nabla_{\\theta}L_T(\\theta, S), {\\mathcal V}) \\approx L_V(\\theta) - \\eta {\\nabla_{\\theta}L_T(\\theta, S)}^T \\nabla_{\\theta}L_V(\\theta, {\\mathcal V})

    Taylor's series approximation reduces the time complexity by reducing the need of calculating the validation loss for each element during greedy selection step which
    means reducing the number of forward passes required.

    GLISTER-ONLINE is an adaptive subset selection algorithm that tries to select a subset every :math:`L` epochs and the parameter `L` can be set in the original training loop.

    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    loss_func: object
        Loss function object
    eta: float
        Learning rate. Step size for the one step gradient update
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        If True, we use the last fc layer weights and biases gradients
        If False, we use the last fc layer biases gradients
    selection_type: str
        Type of selection algorithm -
        - 'PerBatch' : PerBatch method is where GLISTER algorithm is applied on each minibatch data points.
        - 'PerClass' : PerClass method is where GLISTER algorithm is applied on each class data points seperately.
        - 'Supervised' : Supervised method is where GLISTER algorithm is applied on entire training data.
    greedy: str
        Type of greedy selection algorithm -
        - 'RGreedy' : RGreedy Selection method is a variant of naive greedy where we just perform r rounds of greedy selection by choosing k/r points in each round.
        - 'Stochastic' : Stochastic greedy selection method is based on the algorithm presented in this paper :footcite:`mirzasoleiman2014lazier`
        - 'Naive' : Normal naive greedy selection method that selects a single best element every step until the budget is fulfilled
    logger: class
        logger class for logging the information
    r : int, optional
        Number of greedy selection rounds when selection method is RGreedy (default: 15)
    """

    def __init__(self, trainloader, valloader, model, 
                loss_func, eta, device, num_classes, 
                linear_layer, selection_type, greedy,
                logger, r=15):
        """
        Constructor method
        """
        super().__init__(trainloader, valloader, model, num_classes, linear_layer, loss_func, device, logger)
        self.eta = eta  # step size for the one step gradient update
        self.init_out = list()
        self.init_l1 = list()
        self.selection_type = selection_type
        self.greedy = greedy
        self.r = r

    def _update_grads_val(self, grads_curr=None, first_init=False):
        """
        Update the gradient values
        Parameters
        ----------
        grad_currX: OrderedDict, optional
            Gradients of the current element (default: None)
        first_init: bool, optional
            Gradient initialization (default: False)
        perClass: bool
            if True, the function computes the validation gradients using perclass dataloaders
        perBatch: bool
            if True, the function computes the validation gradients of each mini-batch
        """
        # if (perBatch and perClass):
        #     raise ValueError("perBatch and perClass are mutually exclusive. Only one of them can be true at a time")
        self.model.zero_grad()
        embDim = self.model.get_embedding_dim()
        
        if self.selection_type == 'PerClass':
            valloader = self.pcvalloader
        else:
            valloader = self.valloader
        
        if first_init:
            if isinstance(valloader.dataset[0], dict):
                for batch_idx, batch in enumerate(valloader):
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    if batch_idx == 0:
                        out, l1 = self.model(**batch, last=True, freeze=True)
                        loss = self.loss(out, batch['labels'].view(-1)).sum()
                        l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                            l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                        self.init_out = out
                        self.init_l1 = l1
                        self.y_val = batch['labels'].view(-1, 1)
                        if self.selection_type == 'PerBatch':
                            l0_grads = l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                l1_grads = l1_grads.mean(dim=0).view(1, -1)
                    else:
                        out, l1 = self.model(**batch, last=True, freeze=True)
                        loss = self.loss(out, batch['labels'].view(-1)).sum()
                        batch_l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                            batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)
                        if self.selection_type == 'PerBatch':
                            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)
                        l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                        if self.linear_layer:
                            l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)
                        self.init_out = torch.cat((self.init_out, out), dim=0)
                        self.init_l1 = torch.cat((self.init_l1, l1), dim=0)
                        self.y_val = torch.cat((self.y_val, batch['labels'].view(-1, 1)), dim=0)

            else:    
                for batch_idx, (inputs, targets) in enumerate(valloader):
                    inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)
                    if batch_idx == 0:
                        out, l1 = self.model(inputs, last=True, freeze=True)
                        loss = self.loss(out, targets).sum()
                        l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                            l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                        self.init_out = out
                        self.init_l1 = l1
                        self.y_val = targets.view(-1, 1)
                        if self.selection_type == 'PerBatch':
                            l0_grads = l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                l1_grads = l1_grads.mean(dim=0).view(1, -1)
                    else:
                        out, l1 = self.model(inputs, last=True, freeze=True)
                        loss = self.loss(out, targets).sum()
                        batch_l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                            batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)
                        if self.selection_type == 'PerBatch':
                            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)
                        l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                        if self.linear_layer:
                            l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)
                        self.init_out = torch.cat((self.init_out, out), dim=0)
                        self.init_l1 = torch.cat((self.init_l1, l1), dim=0)
                        self.y_val = torch.cat((self.y_val, targets.view(-1, 1)), dim=0)

        elif grads_curr is not None:
            out_vec = self.init_out - (
                    self.eta * grads_curr[0][0:self.num_classes].view(1, -1).expand(self.init_out.shape[0], -1))

            if self.linear_layer:
                out_vec = out_vec - (self.eta * torch.matmul(self.init_l1, grads_curr[0][self.num_classes:].view(
                    self.num_classes, -1).transpose(0, 1)))

            loss = self.loss(out_vec, self.y_val.view(-1)).sum()
            l0_grads = torch.autograd.grad(loss, out_vec)[0]
            if self.linear_layer:
                l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                l1_grads = l0_expand * self.init_l1.repeat(1, self.num_classes)
            if self.selection_type == 'PerBatch':
                b = int(self.y_val.shape[0]/self.valloader.batch_size)
                l0_grads = torch.chunk(l0_grads, b, dim=0)
                new_t = []
                for i in range(len(l0_grads)):
                    new_t.append(torch.mean(l0_grads[i], dim=0).view(1, -1))
                l0_grads = torch.cat(new_t, dim=0)
                if self.linear_layer:
                    l1_grads = torch.chunk(l1_grads, b, dim=0)
                    new_t = []
                    for i in range(len(l1_grads)):
                        new_t.append(torch.mean(l1_grads[i], dim=0).view(1, -1))
                    l1_grads = torch.cat(new_t, dim=0)
        torch.cuda.empty_cache()
        if self.linear_layer:
            self.grads_val_curr = torch.mean(torch.cat((l0_grads, l1_grads), dim=1), dim=0).view(-1, 1)
        else:
            self.grads_val_curr = torch.mean(l0_grads, dim=0).view(-1, 1)

    def eval_taylor_modular(self, grads):
        """
        Evaluate gradients

        Parameters
        ----------
        grads: Tensor
            Gradients

        Returns
        ----------
        gains: Tensor
            Matrix product of two tensors
        """

        grads_val = self.grads_val_curr
        with torch.no_grad():
            gains = torch.matmul(grads, grads_val)
        return gains

    def _update_gradients_subset(self, grads, element):
        """
        Update gradients of set X + element (basically adding element to X)
        Note that it modifies the input vector! Also grads is a list! grad_e is a tuple!

        Parameters
        ----------
        grads: list
            Gradients
        element: int
            Element that need to be added to the gradients
        """
        # if isinstance(element, list):
        grads += self.grads_per_elem[element].sum(dim=0)

    def greedy_algo(self, budget):
        greedySet = list()
        N = self.grads_per_elem.shape[0]
        remainSet = list(range(N))
        t_ng_start = time.time()  # naive greedy start time
        numSelected = 0
        if self.greedy == 'RGreedy':
            # subset_size = int((len(self.grads_per_elem) / r))
            selection_size = int(budget / self.r)
            while (numSelected < budget):
                # Try Using a List comprehension here!
                rem_grads = self.grads_per_elem[remainSet]
                gains = self.eval_taylor_modular(rem_grads)
                # Update the greedy set and remaining set
                sorted_gains, indices = torch.sort(gains.view(-1), descending=True)
                selected_indices = [remainSet[index.item()] for index in indices[0:selection_size]]
                greedySet.extend(selected_indices)
                [remainSet.remove(idx) for idx in selected_indices]
                if numSelected == 0:
                    grads_curr = self.grads_per_elem[selected_indices].sum(dim=0).view(1, -1)
                else:  # If 1st selection, then just set it to bestId grads
                    self._update_gradients_subset(grads_curr, selected_indices)
                # Update the grads_val_current using current greedySet grads
                self._update_grads_val(grads_curr)
                numSelected += selection_size
            self.logger.debug("R greedy GLISTER total time: %.4f", time.time() - t_ng_start)

        # Stochastic Greedy Selection Algorithm
        elif self.greedy == 'Stochastic':
            subset_size = int((len(self.grads_per_elem) / budget) * math.log(100))
            while (numSelected < budget):
                # Try Using a List comprehension here!
                subset_selected = random.sample(remainSet, k=subset_size)
                rem_grads = self.grads_per_elem[subset_selected]
                gains = self.eval_taylor_modular(rem_grads)
                # Update the greedy set and remaining set
                _, indices = torch.sort(gains.view(-1), descending=True)
                bestId = [subset_selected[indices[0].item()]]
                greedySet.append(bestId[0])
                remainSet.remove(bestId[0])
                numSelected += 1
                # Update info in grads_currX using element=bestId
                if numSelected > 1:
                    self._update_gradients_subset(grads_curr, bestId)
                else:  # If 1st selection, then just set it to bestId grads
                    grads_curr = self.grads_per_elem[bestId].view(1, -1)  # Making it a list so that is mutable!
                # Update the grads_val_current using current greedySet grads
                self._update_grads_val(grads_curr)
            self.logger.debug("Stochastic Greedy GLISTER total time: %.4f", time.time() - t_ng_start)

        elif self.greedy == 'Naive':
            while (numSelected < budget):
                # Try Using a List comprehension here!
                rem_grads = self.grads_per_elem[remainSet]
                gains = self.eval_taylor_modular(rem_grads)
                # Update the greedy set and remaining set
                # _, maxid = torch.max(gains, dim=0)
                _, indices = torch.sort(gains.view(-1), descending=True)
                bestId = [remainSet[indices[0].item()]]
                greedySet.append(bestId[0])
                remainSet.remove(bestId[0])
                numSelected += 1
                # Update info in grads_currX using element=bestId
                if numSelected == 1:
                    grads_curr = self.grads_per_elem[bestId[0]].view(1, -1)
                else:  # If 1st selection, then just set it to bestId grads
                    self._update_gradients_subset(grads_curr, bestId)
                # Update the grads_val_current using current greedySet grads
                self._update_grads_val(grads_curr)
            self.logger.debug("Naive Greedy GLISTER total time: %.4f", time.time() - t_ng_start)
        return list(greedySet), [1] * budget


    def select(self, budget, model_params):
        """
        Apply naive greedy method for data selection

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing models parameters

        Returns
        ----------
        greedySet: list
            List containing indices of the best datapoints,
        budget: Tensor
            Tensor containing gradients of datapoints present in greedySet
        """
        glister_start_time = time.time()
        self.update_model(model_params)
        if self.selection_type == 'PerClass':
            self.get_labels(valid=True)
            idxs = []
            gammas = []
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                                shuffle=False, pin_memory=True)
                val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                                shuffle=False, pin_memory=True)
                self.compute_gradients(perClass=True)
                self._update_grads_val(first_init=True)
                idxs_temp, gammas_temp = self.greedy_algo(math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)
        elif self.selection_type == 'PerBatch':
            idxs = []
            gammas = []
            self.compute_gradients(perBatch=True)
            self._update_grads_val(first_init=True)
            idxs_temp, gammas_temp = self.greedy_algo(math.ceil(budget/self.trainloader.batch_size))
            batch_wise_indices = list(self.trainloader.batch_sampler)
            for i in range(len(idxs_temp)):
                tmp = batch_wise_indices[idxs_temp[i]]
                idxs.extend(tmp)
                gammas.extend([gammas_temp[i]] * len(tmp))
        else:
            self.compute_gradients()
            self._update_grads_val(first_init=True)
            idxs, gammas = self.greedy_algo(budget)
        glister_end_time = time.time()
        idxs = [int(x) for x in idxs]
        self.logger.debug("GLISTER algorithm Subset Selection time is: %.4f", glister_end_time - glister_start_time)
        return idxs, torch.FloatTensor(gammas)
File Path: cords/selectionstrategies/SL/gradmatchstrategy.py
Content:
import math
import time
import torch
import numpy as np
from .dataselectionstrategy import DataSelectionStrategy
from ..helpers import OrthogonalMP_REG_Parallel, OrthogonalMP_REG, OrthogonalMP_REG_Parallel_V1
from torch.utils.data import Subset, DataLoader


class GradMatchStrategy(DataSelectionStrategy):
    """
    Implementation of GradMatch Strategy from the paper :footcite:`pmlr-v139-killamsetty21a` for supervised learning frameworks.

    GradMatch strategy tries to solve the optimization problem given below:

    .. math::
        \\min_{\\mathbf{w}, S: |S| \\leq k} \\Vert \\sum_{i \\in S} w_i \\nabla_{\\theta}L_T^i(\\theta) -  \\nabla_{\\theta}L(\\theta)\\Vert

    In the above equation, :math:`\\mathbf{w}` denotes the weight vector that contains the weights for each data instance, :math:`\mathcal{U}` training set where :math:`(x^i, y^i)` denotes the :math:`i^{th}` training data point and label respectively,
    :math:`L_T` denotes the training loss, :math:`L` denotes either training loss or validation loss depending on the parameter valid,
    :math:`S` denotes the data subset selected at each round, and :math:`k` is the budget for the subset.

    The above optimization problem is solved using the Orthogonal Matching Pursuit(OMP) algorithm.

    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    loss: class
        PyTorch loss function for training
    eta: float
        Learning rate. Step size for the one step gradient update
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        Apply linear transformation to the data
    selection_type: str
        Type of selection -
        - 'PerClass': PerClass method is where OMP algorithm is applied on each class data points seperately.
        - 'PerBatch': PerBatch method is where OMP algorithm is applied on each minibatch data points.
        - 'PerClassPerGradient': PerClassPerGradient method is same as PerClass but we use the gradient corresponding to classification layer of that class only.
    logger : class
        - logger object for logging the information
    valid : bool
        If valid==True, we use validation dataset gradient sum in OMP otherwise we use training dataset (default: False)
    v1 : bool
        If v1==True, we use newer version of OMP solver that is more accurate
    lam : float
        Regularization constant of OMP solver
    eps : float
        Epsilon parameter to which the above optimization problem is solved using OMP algorithm
    """

    def __init__(self, trainloader, valloader, model, loss,
                 eta, device, num_classes, linear_layer,
                 selection_type, logger, valid=False, v1=True, lam=0, eps=1e-4):
        """
        Constructor method
        """
        super().__init__(trainloader, valloader, model, num_classes, linear_layer, loss, device, logger)
        self.eta = eta  # step size for the one step gradient update
        self.device = device
        self.init_out = list()
        self.init_l1 = list()
        self.selection_type = selection_type
        self.valid = valid
        self.lam = lam
        self.eps = eps
        self.v1 = v1

    def ompwrapper(self, X, Y, bud):
        if self.device == "cpu":
            reg = OrthogonalMP_REG(X.numpy(), Y.numpy(), nnz=bud, positive=True, lam=0)
            ind = np.nonzero(reg)[0]
        else:
            if self.v1:
                reg = OrthogonalMP_REG_Parallel_V1(X, Y, nnz=bud,
                                                 positive=True, lam=self.lam,
                                                 tol=self.eps, device=self.device)
            else:
                reg = OrthogonalMP_REG_Parallel(X, Y, nnz=bud,
                                                positive=True, lam=self.lam,
                                                tol=self.eps, device=self.device)
            ind = torch.nonzero(reg).view(-1)
        return ind.tolist(), reg[ind].tolist()

    def select(self, budget, model_params):
        """
        Apply OMP Algorithm for data selection

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing models parameters

        Returns
        ----------
        idxs: list
            List containing indices of the best datapoints,
        gammas: weights tensors
            Tensor containing weights of each instance
        """
        omp_start_time = time.time()
        self.update_model(model_params)

        if self.selection_type == 'PerClass':
            self.get_labels(valid=self.valid)
            idxs = []
            gammas = []
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                                shuffle=False, pin_memory=True, collate_fn=self.trainloader.collate_fn)
                if self.valid:
                    val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                    val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                    self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                                  shuffle=False, pin_memory=True, collate_fn=self.trainloader.collate_fn)

                self.compute_gradients(self.valid, perBatch=False, perClass=True)
                trn_gradients = self.grads_per_elem
                if self.valid:
                    sum_val_grad = torch.sum(self.val_grads_per_elem, dim=0)
                else:
                    sum_val_grad = torch.sum(trn_gradients, dim=0)
                idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                                         sum_val_grad,
                                                         math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)

        elif self.selection_type == 'PerBatch':
            self.compute_gradients(self.valid, perBatch=True, perClass=False)
            idxs = []
            gammas = []
            trn_gradients = self.grads_per_elem
            if self.valid:
                sum_val_grad = torch.sum(self.val_grads_per_elem, dim=0)
            else:
                sum_val_grad = torch.sum(trn_gradients, dim=0)
            idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                                     sum_val_grad, math.ceil(budget / self.trainloader.batch_size))
            batch_wise_indices = list(self.trainloader.batch_sampler)
            for i in range(len(idxs_temp)):
                tmp = batch_wise_indices[idxs_temp[i]]
                idxs.extend(tmp)
                gammas.extend(list(gammas_temp[i] * np.ones(len(tmp))))

        elif self.selection_type == 'PerClassPerGradient':
            self.get_labels(valid=self.valid)
            idxs = []
            gammas = []
            embDim = self.model.get_embedding_dim()
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                                shuffle=False, pin_memory=True, collate_fn=self.trainloader.collate_fn)
                if self.valid:
                    val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                    val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                    self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                                  shuffle=False, pin_memory=True, collate_fn=self.trainloader.collate_fn)
                self.compute_gradients(self.valid, perBatch=False, perClass=True)
                trn_gradients = self.grads_per_elem
                tmp_gradients = trn_gradients[:, i].view(-1, 1)
                tmp1_gradients = trn_gradients[:,
                                 self.num_classes + (embDim * i): self.num_classes + (embDim * (i + 1))]
                trn_gradients = torch.cat((tmp_gradients, tmp1_gradients), dim=1)

                if self.valid:
                    val_gradients = self.val_grads_per_elem
                    tmp_gradients = val_gradients[:, i].view(-1, 1)
                    tmp1_gradients = val_gradients[:,
                                     self.num_classes + (embDim * i): self.num_classes + (embDim * (i + 1))]
                    val_gradients = torch.cat((tmp_gradients, tmp1_gradients), dim=1)
                    sum_val_grad = torch.sum(val_gradients, dim=0)
                else:
                    sum_val_grad = torch.sum(trn_gradients, dim=0)

                idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                                         sum_val_grad,
                                                         math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)
        diff = budget - len(idxs)
        self.logger.debug("Random points added: %d ", diff)

        if diff > 0:
            remainList = set(np.arange(self.N_trn)).difference(set(idxs))
            new_idxs = np.random.choice(list(remainList), size=diff, replace=False)
            idxs.extend(new_idxs)
            gammas.extend([1 for _ in range(diff)])
            idxs = np.array(idxs)
            gammas = np.array(gammas)

        if self.selection_type in ["PerClass", "PerClassPerGradient"]:
            rand_indices = np.random.permutation(len(idxs))
            idxs = list(np.array(idxs)[rand_indices])
            gammas = list(np.array(gammas)[rand_indices])
        
        idxs = [int(x) for x in idxs]
        omp_end_time = time.time()
        self.logger.debug("GradMatch algorithm Subset Selection time is: %.4f", omp_end_time - omp_start_time)
        return idxs, torch.FloatTensor(gammas)
File Path: cords/selectionstrategies/SL/randomstrategy.py
Content:
import numpy as np
import torch


class RandomStrategy(object):
    """
    This is the Random Selection Strategy class where we select a set of random points as a datasubset
    and often acts as baselines to compare other selection strategies.

    Parameters
    ----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    """

    def __init__(self, trainloader, online=False):
        """
        Constructor method
        """

        self.trainloader = trainloader
        self.N_trn = len(trainloader.sampler.data_source)
        self.online = online
        self.indices = None
        self.gammas = None

    def select(self, budget):
        """
        Perform random sampling of indices of size budget.

        Parameters
        ----------
        budget: int
            The number of data points to be selected

        Returns
        ----------
        indices: ndarray
            Array of indices of size budget selected randomly
        gammas: Tensor
            Gradient weight values of selected indices
        """
        if self.online or (self.indices is None):
            np.random.seed()
            self.indices = np.random.choice(self.N_trn, size=budget, replace=False)
            self.gammas = torch.ones(budget)
        self.indices = [int(x) for x in self.indices]
        return self.indices, self.gammas

File Path: cords/selectionstrategies/SL/selconstrategy.py
Content:
import math
import torch
import copy
from .dataselectionstrategy import DataSelectionStrategy
from cords.utils.data.datasets.SL.custom_dataset_selcon import CustomDataset_SELCON, CustomDataset_WithId_SELCON, SubsetDataset_WithId_SELCON
import numpy as np


class SELCONstrategy(DataSelectionStrategy):
    def __init__(self, trainset, validset, trainloader, valloader, model, 
                loss_func, device, num_classes, delta, num_epochs,
                linear_layer, lam, lr, logger, optimizer, 
                batch_size, criterion):
        """
        Constructor method
        """
        super().__init__(trainloader, valloader, model, num_classes, linear_layer, loss_func, device, logger)
        self.delta = delta
        self.lam = lam
        self.lr = lr
        self.optimizer = optimizer
        self.batch_size = batch_size
        self.criterion = criterion
        self.trainset = trainset
        self.validset = validset
        self.num_epochs = num_epochs
        self.logger = logger
        self.sub_epoch = num_epochs // 20  # doubt : what to take as sub epoch? a param?
        self.__precompute(self.num_epochs//4, self.sub_epoch, torch.randn_like(self.delta))

    def __precompute(self, f_pi_epoch, p_epoch, alphas): # TODO: alphas?
        main_optimizer = torch.optim.Adam([
                {'params': self.model.parameters()}], lr=self.lr)
                
        dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.lr)

        self.logger.info("SELCON: starting pre compute")

        # loader_val = torch.utils.data.DataLoader(CustomDataset(self.x_val, self.y_val,transform=None),\
        #     shuffle=False,batch_size=self.batch_size, pin_memory=False)
        loader_val = self.valloader
        loader_tr = self.trainloader
        # todo: update len(loader_val)

        prev_loss = 1000
        stop_count = 0
        i = 0

        while(True):
            main_optimizer.zero_grad()
            constraint = 0.

            # for batch_idx in list(loader_val.batch_sampler):
            for batch_idx, (inputs, targets, _) in enumerate(loader_val):
                # inputs, targets = loader_val.dataset[batch_idx]
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                val_out = self.model(inputs)
                constraint += self.criterion(val_out, targets.view(-1)) # to discuss this

            constraint /= len(loader_val)
            constraint = constraint - self.delta
            multiplier = alphas * constraint # todo: try torch.dot(alphas, constraint)

            loss = multiplier
            self.F_phi = loss.item()
            loss.backward()
            main_optimizer.step()

            dual_optimizer.zero_grad()
            constraint = 0.

            # for batch_idx in list(loader_val.batch_sampler):
            for batch_idx, (inputs, targets, _) in enumerate(loader_val):
                # inputs, targets = loader_val.dataset[batch_idx]
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                val_out = self.model(inputs)
                constraint += self.criterion(val_out, targets.view(-1))
            
            constraint /= len(loader_val)
            constraint = constraint - self.delta
            multiplier = -1. * alphas * constraint # todo: try -1.*torch.dot(alphas, constraint)

            multiplier.backward()
            dual_optimizer.step()

            alphas.requires_grad = False
            alphas.clamp_(min=0.)
            alphas.requires_grad = True

            if loss.item() <= 0.:
                break

            if prev_loss - loss.item() < 1e-3 and stop_count >= 5:
                if stop_count >= 5:
                    break
                else:
                    stop_count += 1
            else:
                stop_count = 0
            
            prev_loss = loss.item()
            i += 1
        
        self.logger.info("SELCON: Finishing F phi")

        if loss.item() <= 0.:
            alphas = torch.zeros_like(alphas)
        
        l = [torch.flatten(p) for p in self.model.state_dict().values()]
        flat = torch.cat(l).detach().clone()

        self.F_values = torch.zeros(len(loader_tr.dataset), device=self.device) # change len(x_trn) to x_trn.shape[0]

        beta1, beta2 = main_optimizer.param_groups[0]['betas']
        # loader_tr = torch.utils.data.DataLoader(CustomDataset_WithId(self.x_trn, self.y_trn,\
        #     transform=None), device = self.device, shuffle=False,batch_size=self.batch_size*20)
        loader_tr = self.trainloader

        # loader_val = torch.utils.data.DataLoader(CustomDataset(self.x_val, self.y_val,device = self.device,transform=None),\
        #     shuffle=False,batch_size=self.batch_size*20)    
        loader_val = self.valloader

        # for batch_idx in list(loader_tr.batch_sampler):
        for _, (inputs, targets, idxs) in enumerate(loader_tr):
            # inputs, targets, idxs = loader_tr.dataset[batch_idx]
            inputs, targets = inputs.to(self.device), targets.to(self.device)

            ele_delta = self.delta.repeat(targets.shape[0]).to(self.device)

            weights = flat.view(1, -1).repeat(targets.shape[0], 1)
            ele_alphas = alphas.detach().repeat(targets.shape[0]).to(self.device)

            exp_avg_w = torch.zeros_like(weights)
            exp_avg_sq_w = torch.zeros_like(weights)

            exten_inp = torch.cat((inputs, torch.ones(inputs.shape[0], \
                device=self.device).view(-1,1)), dim=1)

            bias_correction1 = 1.
            bias_correction2 = 1.

            for i in range(p_epoch):
                trn_loss_g = torch.sum(exten_inp*weights, dim=1) - targets
                fin_trn_loss_g = 2 * exten_inp * trn_loss_g[:, None]

                weight_grad = fin_trn_loss_g +2*self.lam * torch.cat((weights[:,:-1],\
                            torch.zeros((weights.shape[0],1),device=self.device)),dim=1)
                    
                exp_avg_w.mul_(beta1).add_(1. - beta1, weight_grad)
                exp_avg_sq_w.mul_(beta2).addcmul_(1. - beta2, weight_grad, weight_grad)
                denom = exp_avg_sq_w.sqrt().add_(main_optimizer.param_groups[0]['eps'])

                bias_correction1 *= beta1
                bias_correction2 *= beta2
                step_size = self.lr * math.sqrt(1. - bias_correction2) / (1. - bias_correction1) # doubt: why sqrt only on numerator?
                weights.addcdiv_(-step_size, exp_avg_w, denom)
            
            val_losses = 0.
            # for batch_idx_val in list(loader_val.batch_sampler):
            for batch_idx_val, (inputs_val, targets_val, _) in enumerate(loader_val):
                # inputs_val, targets_val = loader_val.dataset[batch_idx_val]
                inputs_val, targets_val = inputs_val.to(self.device), targets_val.to(self.device)

                exten_val = torch.cat((inputs_val, torch.ones(inputs_val.shape[0], device=self.device).view(-1,1)), dim=1)
                exten_val_y = torch.mean(targets_val).repeat(min(self.batch_size*20, targets_val.shape[0]))

                # print(weights.shape, exten_val.shape, exten_val_y.shape)

                val_loss = torch.sum(weights*torch.mean(exten_val,dim=0),dim=1) - exten_val_y

                val_losses+= val_loss*val_loss #torch.mean(val_loss*val_loss,dim=0)

            reg = torch.sum(weights[:,:-1]*weights[:,:-1], dim=1)
            trn_loss = torch.sum(exten_inp*weights, dim=1) - targets

            add_term = torch.max( torch.zeros_like(ele_alphas), (val_losses/len(loader_val)-ele_delta)*ele_alphas )
            F_new = torch.square(trn_loss) + self.lam*reg + add_term
            self.F_values[idxs] = F_new
            
        self.logger.info("SELCON: Finishing element wise F")

    def __return_subset(self, theta_init, p_epoch, curr_subset, budget, 
                        batch, step, w_exp_avg, w_exp_avg_sq):

        m_values = self.F_values.detach().clone()
        self.model.load_state_dict(theta_init) # todo: use this, update theta_init before calling this function

        # loader_tr = torch.utils.data.DataLoader(CustomDataset_WithId(self.x_trn[curr_subset], self.y_trn[curr_subset],\
        #     transform=None),shuffle=False,batch_size=batch)
        loader_tr = torch.utils.data.DataLoader(SubsetDataset_WithId_SELCON(self.trainset, curr_subset), shuffle=False, batch_size=batch)
        # loader_tr = self.trainloader

        sum_error = torch.nn.MSELoss(reduction='sum') # doubt: why not use self.criterion here, also check the reduction here and nored

        with torch.no_grad():
            F_curr = 0.
            # for batch_idx in list(loader_tr.batch_sampler):
            for batch_idx, (inputs, targets, _) in enumerate(loader_tr):
                # inputs, targets, _ = loader_tr.dataset[batch_idx]
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                scores = self.model(inputs)
                F_curr += sum_error(scores, targets).item() 

            l = [torch.flatten(p) for p in self.model.parameters()]
            flatt = torch.cat(l)
            l2_reg = torch.sum(flatt[:-1]*flatt[:-1])

            F_curr += (self.lam*l2_reg*len(curr_subset)).item() #+ multiplier).item()

        main_optimizer = torch.optim.Adam([{'params': self.model.parameters()}], lr=self.lr)

        l = [torch.flatten(p) for p in self.model.state_dict().values()]
        flat = torch.cat(l).detach()

        # loader_tr = torch.utils.data.DataLoader(CustomDataset_WithId(self.x_trn[curr_subset], self.y_trn[curr_subset],\
        #     transform=None),shuffle=False,batch_size=self.batch_size)
        loader_tr = torch.utils.data.DataLoader(SubsetDataset_WithId_SELCON(self.trainset, curr_subset), shuffle=False, batch_size=self.batch_size)
        # loader_tr = self.trainloader

        beta1,beta2 = main_optimizer.param_groups[0]['betas']
        rem_len = (len(curr_subset)-1)
        b_idxs = 0
        device_new = self.device


        # for batch_idx in list(loader_tr.batch_sampler):
        for batch_idx, (inputs, targets, _) in enumerate(loader_tr):
            # inputs, targets, _ = loader_tr.dataset[batch_idx]
            inputs, targets = inputs.to(self.device), targets.to(self.device)
        
            weights = flat.repeat(targets.shape[0], 1)

            exp_avg_w = w_exp_avg.repeat(targets.shape[0], 1)
            exp_avg_sq_w = w_exp_avg_sq.repeat(targets.shape[0], 1)

            exten_inp = torch.cat((inputs,torch.ones(inputs.shape[0],device=self.device).view(-1,1)),dim=1)

            bias_correction1 = beta1**step#1.0 
            bias_correction2 = beta2**step#1.0 

            for _ in range(p_epoch):

                sum_fin_trn_loss_g = torch.zeros_like(weights).to(device_new)
                # for batch_idx_trn in list(loader_tr.batch_sampler):
                for batch_idx_trn, (inputs_trn, targets_trn, _) in enumerate(loader_tr):
                    # inputs_trn, targets_trn,_ = loader_tr.dataset[batch_idx_trn]
                    inputs_trn, targets_trn = inputs_trn.to(self.device), targets_trn.to(self.device)

                    exten_trn = torch.cat((inputs_trn,torch.ones(inputs_trn.shape[0]\
                        ,device=self.device).view(-1,1)),dim=1).to(device_new)
                    exten_trn_y = targets_trn.view(-1,1).repeat(1,min(self.batch_size,\
                        targets.shape[0])).to(device_new)
                
                    sum_trn_loss_p = 2*(torch.matmul(exten_trn,torch.transpose(weights, 0, 1)\
                        .to(device_new)) - exten_trn_y)
            
                    sum_fin_trn_loss_g += torch.sum(sum_trn_loss_p[:,:,None]*exten_trn[:,None,:],dim=0)

                    del exten_trn,exten_trn_y,sum_trn_loss_p,inputs_trn, targets_trn #mod_trn,sum_trn_loss_g,
                    torch.cuda.empty_cache()

                sum_fin_trn_loss_g = sum_fin_trn_loss_g.to(self.device)

                trn_loss_g = torch.sum(exten_inp*weights,dim=1) - targets
                fin_trn_loss_g = exten_inp*2*trn_loss_g[:,None]

                fin_trn_loss_g = (sum_fin_trn_loss_g - fin_trn_loss_g)/rem_len

                weight_grad = fin_trn_loss_g+ 2*rem_len*\
                    torch.cat((weights[:,:-1], torch.zeros((weights.shape[0],1),device=self.device)),dim=1)#+\

                exp_avg_w.mul_(beta1).add_(1.0 - beta1, weight_grad)
                exp_avg_sq_w.mul_(beta2).addcmul_(1.0 - beta2, weight_grad, weight_grad)
                denom = exp_avg_sq_w.sqrt().add_(main_optimizer.param_groups[0]['eps'])

                bias_correction1 *= beta1
                bias_correction2 *= beta2
                step_size = (self.lr)* math.sqrt(1.0-bias_correction2) / (1.0-bias_correction1)
                weights.addcdiv_(-step_size, exp_avg_w, denom)
            
            reg = torch.sum(weights[:,:-1]*weights[:,:-1],dim=1)

            trn_losses = 0.
            # for batch_idx_trn in list(loader_tr.batch_sampler):
            for batch_idx_trn, (inputs_trn, targets_trn, _) in enumerate(loader_tr):
                # inputs_trn, targets_trn,_ = loader_tr.dataset[batch_idx_trn]
                inputs_trn, targets_trn = inputs_trn.to(self.device), targets_trn.to(self.device)

                exten_trn = torch.cat((inputs_trn,torch.ones(inputs_trn.shape[0],device=self.device).view(-1,1)),dim=1)
                exten_trn_y = targets_trn.view(-1,1).repeat(1,min(self.batch_size,targets.shape[0]))
            
                trn_loss = torch.matmul(exten_trn,torch.transpose(weights, 0, 1)) - exten_trn_y
                
                trn_losses+= torch.sum(trn_loss*trn_loss,dim=0)

            trn_loss_ind = torch.sum(exten_inp*weights,dim=1) - targets
            trn_losses -= trn_loss_ind*trn_loss_ind
            abs_value = F_curr - (trn_losses + self.lam*reg*rem_len) #\
            neg_ind = ((abs_value ) < 0).nonzero().view(-1)
            abs_value [neg_ind] = torch.max(self.F_values)
            m_values[torch.tensor(curr_subset)[b_idxs*self.batch_size:(b_idxs+1)*self.batch_size]] = abs_value
            b_idxs +=1

        values,indices = m_values.topk(budget,largest=False)

        return list(indices.cpu().numpy()), list(values.cpu().numpy())

    def select(self, budget, model_params):
        N = len(self.trainloader)
        current_idx = list(np.random.choice(N, budget, replace=False)) # take this from prev train loop
        state_values = list(self.optimizer.state.values())
        step = state_values[0]['step']
        w_exp_avg = torch.cat((state_values[0]['exp_avg'].view(-1),state_values[1]['exp_avg']))
        w_exp_avg_sq = torch.cat((state_values[0]['exp_avg_sq'].view(-1),state_values[1]['exp_avg_sq']))
        sub_epoch = 3

        # doubt: where to get batch_size and sub_epoch from?
        return self.__return_subset(
            theta_init=model_params,
            p_epoch=sub_epoch,
            curr_subset=current_idx,
            budget=budget,
            batch=self.batch_size, # assert this is train batch size
            step=step,
            w_exp_avg=w_exp_avg,
            w_exp_avg_sq=w_exp_avg_sq
        )
        
File Path: cords/selectionstrategies/SL/stochasticgreedyexplorationstrategy.py
Content:
import numpy as np
import torch
import pickle
import random


def pickle2dict(file_name, key):
    """
    Load dictionary from pickle file
    """
    with open(file_name, "rb") as fIn:
        stored_data = pickle.load(fIn)
        value = stored_data[key]
    return value


class StochasticGreedyExplorationStrategy(object):
    """
    This is the Stochastic Greedy Exploration Strategy class defined in the paper :footcite:`killamsetty2023milo`, where we select multiple subsets using stochastic greedy algorithm.
    Stochastic subsets has to be provided in prior for selection. We provide a way to compute stochastic subsets for text and image datasets
    using various submodular functions as a util function.
   
    Parameters
    ----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    """

    def __init__(self, trainloader, stochastic_subsets_file):
        """
        Constructor method
        """
        self.trainloader = trainloader
        self.N_trn = len(trainloader.sampler.data_source)
        self.indices = None
        self.gammas = None
        stochasticsubsets = pickle2dict(stochastic_subsets_file, 'stochastic_subsets')
        self.stochastic_subsets = []
        for subset in stochasticsubsets:
            self.stochastic_subsets.append([x[0] for x in subset])
        random.shuffle(self.stochastic_subsets)
        self.sel_idx = 0
        

    def select(self, budget):
        """
        Utilizes already selected stochastic subsets of indices of size budget.

        Parameters
        ----------
        budget: int
            The number of data points to be selected

        Returns
        ----------
        indices: ndarray
            Array of indices of size budget selected randomly
        gammas: Tensor
            Gradient weight values of selected indices
        """
        self.indices = self.stochastic_subsets[self.sel_idx]
        self.indices = [int(x) for x in self.indices]
        self.sel_idx  = (self.sel_idx+1) % len(self.stochastic_subsets)
        self.gammas = torch.ones(len(self.indices))
        return self.indices, self.gammas
File Path: cords/selectionstrategies/SL/submodularselectionstrategy.py
Content:
import apricot
import numpy as np
import torch
import torch.nn.functional as F
from scipy.sparse import csr_matrix
from .dataselectionstrategy import DataSelectionStrategy
from torch.utils.data.sampler import SubsetRandomSampler


class SubmodularSelectionStrategy(DataSelectionStrategy):
    """
    This class extends :class:`selectionstrategies.supervisedlearning.dataselectionstrategy.DataSelectionStrategy`
    to include submodular optmization functions using apricot for data selection.

    Parameters
    ----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    loss_type: class
        The type of loss criterion
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        Apply linear transformation to the data
    if_convex: bool
        If convex or not
    selection_type: str
        PerClass or Supervised
    submod_func_type: str
        The type of submodular optimization function. Must be one of
        'facility-location', 'graph-cut', 'sum-redundancy', 'saturated-coverage'
    """

    def __init__(self, trainloader, valloader, model, loss,
                 device, num_classes, linear_layer, if_convex, selection_type, submod_func_type, optimizer):
        """
        Constructer method
        """
        super().__init__(trainloader, valloader, model, num_classes, linear_layer, loss, device)
        self.if_convex = if_convex
        self.selection_type = selection_type
        self.submod_func_type = submod_func_type
        self.optimizer = optimizer

    def distance(self, x, y, exp=2):
        """
        Compute the distance.

        Parameters
        ----------
        x: Tensor
            First input tensor
        y: Tensor
            Second input tensor
        exp: float, optional
            The exponent value (default: 2)

        Returns
        ----------
        dist: Tensor
            Output tensor
        """

        n = x.size(0)
        m = y.size(0)
        d = x.size(1)
        x = x.unsqueeze(1).expand(n, m, d)
        y = y.unsqueeze(0).expand(n, m, d)
        dist = torch.pow(x - y, exp).sum(2)
        # dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))
        return dist

    def compute_score(self, model_params, idxs):
        """
        Compute the score of the indices.

        Parameters
        ----------
        model_params: OrderedDict
            Python dictionary object containing models parameters
        idxs: list
            The indices
        """

        trainset = self.trainloader.sampler.data_source
        subset_loader = torch.utils.data.DataLoader(trainset, batch_size=self.trainloader.batch_size, shuffle=False,
                                                    sampler=SubsetRandomSampler(idxs),
                                                    pin_memory=True)
        self.model.load_state_dict(model_params)
        self.N = 0
        g_is = []

        if self.if_convex:
            for batch_idx, (inputs, targets) in enumerate(subset_loader):
                inputs, targets = inputs, targets
                if self.selection_type == 'PerBatch':
                    self.N += 1
                    g_is.append(inputs.view(inputs.size()[0], -1).mean(dim=0).view(1, -1))
                else:
                    self.N += inputs.size()[0]
                    g_is.append(inputs.view(inputs.size()[0], -1))
        else:
            embDim = self.model.get_embedding_dim()
            for batch_idx, (inputs, targets) in enumerate(subset_loader):
                inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)
                if self.selection_type == 'PerBatch':
                    self.N += 1
                else:
                    self.N += inputs.size()[0]
                out, l1 = self.model(inputs, freeze=True, last=True)
                loss = self.loss(out, targets).sum()
                l0_grads = torch.autograd.grad(loss, out)[0]
                if self.linear_layer:
                    l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                    l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                    if self.selection_type == 'PerBatch':
                        g_is.append(torch.cat((l0_grads, l1_grads), dim=1).mean(dim=0).view(1, -1))
                    else:
                        g_is.append(torch.cat((l0_grads, l1_grads), dim=1))
                else:
                    if self.selection_type == 'PerBatch':
                        g_is.append(l0_grads.mean(dim=0).view(1, -1))
                    else:
                        g_is.append(l0_grads)

        self.dist_mat = torch.zeros([self.N, self.N], dtype=torch.float32)
        first_i = True
        if self.selection_type == 'PerBatch':
            g_is = torch.cat(g_is, dim=0)
            self.dist_mat = self.distance(g_is, g_is).cpu()
        else:
            for i, g_i in enumerate(g_is, 0):
                if first_i:
                    size_b = g_i.size(0)
                    first_i = False
                for j, g_j in enumerate(g_is, 0):
                    self.dist_mat[i * size_b: i * size_b + g_i.size(0),
                    j * size_b: j * size_b + g_j.size(0)] = self.distance(g_i, g_j).cpu()
        self.const = torch.max(self.dist_mat).item()
        self.dist_mat = (self.const - self.dist_mat).numpy()

    def compute_gamma(self, idxs):
        """
        Compute the gamma values for the indices.

        Parameters
        ----------
        idxs: list
            The indices

        Returns
        ----------
        gamma: list
            Gradient values of the input indices
        """

        if self.selection_type == 'PerClass':
            gamma = [0 for i in range(len(idxs))]
            best = self.dist_mat[idxs]  # .to(self.device)
            rep = np.argmax(best, axis=0)
            for i in rep:
                gamma[i] += 1
        elif self.selection_type == 'Supervised':
            gamma = [0 for i in range(len(idxs))]
            best = self.dist_mat[idxs]  # .to(self.device)
            rep = np.argmax(best, axis=0)
            for i in range(rep.shape[1]):
                gamma[rep[0, i]] += 1
        return gamma

    def get_similarity_kernel(self):
        """
        Obtain the similarity kernel.

        Returns
        ----------
        kernel: ndarray
            Array of kernel values
        """

        for batch_idx, (inputs, targets) in enumerate(self.trainloader):
            if batch_idx == 0:
                labels = targets
            else:
                tmp_target_i = targets
                labels = torch.cat((labels, tmp_target_i), dim=0)
        kernel = np.zeros((labels.shape[0], labels.shape[0]))
        for target in np.unique(labels):
            x = np.where(labels == target)[0]
            # prod = np.transpose([np.tile(x, len(x)), np.repeat(x, len(x))])
            for i in x:
                kernel[i, x] = 1
        return kernel

    def select(self, budget, model_params):
        """
        Data selection method using different submodular optimization
        functions.

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing models parameters
        optimizer: str
            The optimization approach for data selection. Must be one of
            'random', 'modular', 'naive', 'lazy', 'approximate-lazy', 'two-stage',
            'stochastic', 'sample', 'greedi', 'bidirectional'

        Returns
        ----------
        total_greedy_list: list
            List containing indices of the best datapoints
        gammas: list
            List containing gradients of datapoints present in greedySet
        """

        for batch_idx, (inputs, targets) in enumerate(self.trainloader):
            if batch_idx == 0:
                x_trn, labels = inputs, targets
            else:
                tmp_inputs, tmp_target_i = inputs, targets
                labels = torch.cat((labels, tmp_target_i), dim=0)
        per_class_bud = int(budget / self.num_classes)
        total_greedy_list = []
        gammas = []
        if self.selection_type == 'PerClass':
            for i in range(self.num_classes):
                idxs = torch.where(labels == i)[0]
                self.compute_score(model_params, idxs)
                if self.submod_func_type == 'facility-location':
                    fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0,
                                                                                      metric='precomputed',
                                                                                      n_samples=per_class_bud,
                                                                                      optimizer=self.optimizer)
                elif self.submod_func_type == 'graph-cut':
                    fl = apricot.functions.graphCut.GraphCutSelection(random_state=0, metric='precomputed',
                                                                      n_samples=per_class_bud, optimizer=self.optimizer)
                elif self.submod_func_type == 'sum-redundancy':
                    fl = apricot.functions.sumRedundancy.SumRedundancySelection(random_state=0, metric='precomputed',
                                                                                n_samples=per_class_bud,
                                                                                optimizer=self.optimizer)
                elif self.submod_func_type == 'saturated-coverage':
                    fl = apricot.functions.saturatedCoverage.SaturatedCoverageSelection(random_state=0,
                                                                                        metric='precomputed',
                                                                                        n_samples=per_class_bud,
                                                                                        optimizer=self.optimizer)

                sim_sub = fl.fit_transform(self.dist_mat)
                greedyList = list(np.argmax(sim_sub, axis=1))
                gamma = self.compute_gamma(greedyList)
                total_greedy_list.extend(idxs[greedyList])
                gammas.extend(gamma)

        elif self.selection_type == 'Supervised':
            for i in range(self.num_classes):
                if i == 0:
                    idxs = torch.where(labels == i)[0]
                    N = len(idxs)
                    self.compute_score(model_params, idxs)
                    row = idxs.repeat_interleave(N)
                    col = idxs.repeat(N)
                    data = self.dist_mat.flatten()
                else:
                    idxs = torch.where(labels == i)[0]
                    N = len(idxs)
                    self.compute_score(model_params, idxs)
                    row = torch.cat((row, idxs.repeat_interleave(N)), dim=0)
                    col = torch.cat((col, idxs.repeat(N)), dim=0)
                    data = np.concatenate([data, self.dist_mat.flatten()], axis=0)
            sparse_simmat = csr_matrix((data, (row.numpy(), col.numpy())), shape=(self.N_trn, self.N_trn))
            self.dist_mat = sparse_simmat
            if self.submod_func_type == 'facility-location':
                fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0, metric='precomputed',
                                                                                  n_samples=per_class_bud,
                                                                                  optimizer=self.optimizer)
            elif self.submod_func_type == 'graph-cut':
                fl = apricot.functions.graphCut.GraphCutSelection(random_state=0, metric='precomputed',
                                                                  n_samples=per_class_bud, optimizer=self.optimizer)
            elif self.submod_func_type == 'sum-redundancy':
                fl = apricot.functions.sumRedundancy.SumRedundancySelection(random_state=0, metric='precomputed',
                                                                            n_samples=per_class_bud,
                                                                            optimizer=self.optimizer)
            elif self.submod_func_type == 'saturated-coverage':
                fl = apricot.functions.saturatedCoverage.SaturatedCoverageSelection(random_state=0,
                                                                                    metric='precomputed',
                                                                                    n_samples=per_class_bud,
                                                                                    optimizer=self.optimizer)

            sim_sub = fl.fit_transform(sparse_simmat)
            total_greedy_list = list(np.array(np.argmax(sim_sub, axis=1)).reshape(-1))
            gammas = self.compute_gamma(total_greedy_list)
        return total_greedy_list, gammas

File Path: cords/selectionstrategies/SL/weightedrandomexplorationstrategy.py
Content:
import numpy as np
import torch, time
import pickle
from torch.nn import Softmax
import math


def pickle2dict(file_name, key):
    """
    Load dictionary from pickle file
    """
    with open(file_name, "rb") as fIn:
        stored_data = pickle.load(fIn)
        value = stored_data[key]
    return value


def taylor_softmax_v1(x, dim=1, n=2, use_log=False):
    assert n % 2 == 0 and n > 0
    fn = torch.ones_like(x)
    denor = 1.
    for i in range(1, n + 1):
        denor *= i
        fn = fn + x.pow(i) / denor
    out = fn / fn.sum(dim=dim, keepdims=True)
    if use_log: out = out.log()
    return out


class WeightedRandomExplorationStrategy(object):
    """
    Implementation of the Weighted Random Exploration Strategy class defined in the paper :footcite:`killamsetty2023milo`, where we select a set of points based on a global ordering of the dataset.
    Global Ordering has to be provided in prior for selection. We provide a way to compute global ordering for text and image datasets
    using various submodular functions as a util function.
   
    Parameters
    ----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    """

    def __init__(self, trainloader, global_order_file, online=False, temperature=1, per_class=False):
        """
        Constructor method
        """
        self.trainloader = trainloader
        self.N_trn = len(trainloader.sampler.data_source)
        self.online = online
        self.indices = None
        self.gammas = None
        globalorder = pickle2dict(global_order_file, 'globalorder')
        self.global_idxs = np.array([x[0] for x in globalorder])
        self.global_gains = np.array([x[1] for x in globalorder])
        self.global_gains = self.global_gains - self.global_gains.min()
        self.global_gains = np.maximum(self.global_gains, 1e-10)
        self.temperature = temperature 
        self.probs = taylor_softmax_v1(torch.from_numpy(np.array([self.global_gains])/self.temperature)).numpy()[0]
        self.cluster_idxs = pickle2dict(global_order_file, 'cluster_idxs')
        self.per_class = per_class
        self.num_classes = len(list(self.cluster_idxs.keys()))
        #self.probs = softmax(torch.from_numpy(np.array([self.global_gains])/self.temperature)).numpy()[0]

    def select(self, budget):
        """
        Samples subset of size budget from the generated probability distribution.

        Parameters
        ----------
        budget: int
            The number of data points to be selected

        Returns
        ----------
        indices: ndarray
            Array of indices of size budget selected randomly
        gammas: Tensor
            Gradient weight values of selected indices
        """
        if self.per_class:
            per_cls_cnt = [len(self.cluster_idxs[key]) for key in self.cluster_idxs.keys()]
            min_cls_cnt = min(per_cls_cnt)
            total_sample_cnt = sum(per_cls_cnt)
            if min_cls_cnt < math.ceil(budget/self.num_classes):
                per_cls_budget = [min_cls_cnt]*self.num_classes
                while sum(per_cls_budget) < budget:
                    for cls in range(self.num_classes):
                        if per_cls_budget[cls] < per_cls_cnt[cls]:
                            per_cls_budget[cls] += 1
            else:
                per_cls_budget = [math.ceil(budget/self.num_classes) for _ in per_cls_cnt]

            
            cluster_labels = list(self.cluster_idxs.keys())
            if self.online:
                self.indices = []
                for i in range(len(cluster_labels)):
                    per_cls_idxs = self.cluster_idxs[cluster_labels[i]]
                    rng = np.random.default_rng(int(time.time()))
                    sel_idxs = rng.choice(per_cls_idxs, size=per_cls_budget[i], replace=False, p=self.probs[per_cls_idxs]/self.probs[per_cls_idxs].sum())
                    sel_idxs = [int(x) for x in sel_idxs]
                    self.indices.extend(sel_idxs)
            elif self.indices is None:
                self.indices = []
                for i in range(len(cluster_labels)):
                    per_cls_idxs = self.cluster_idxs[cluster_labels[i]]
                    sel_idxs = per_cls_idxs[:per_cls_budget[i]]
                    # sel_idxs = [x.item() for x in sel_idxs]
                    self.indices.extend(sel_idxs)
        else:
            if self.online:
                rng = np.random.default_rng(int(time.time()))
                self.indices = rng.choice(self.global_idxs, size=budget, replace=False, p=self.probs)
                self.indices = [int(x) for x in self.indices]
                #self.gammas = torch.ones(budget)
            elif self.indices is None:
                self.indices = self.global_idxs[:budget]
        self.gammas = torch.ones(len(self.indices))
        return self.indices, self.gammas
File Path: cords/selectionstrategies/SSL/__init__.py
Content:
from .craigstrategy import CRAIGStrategy
from .dataselectionstrategy import DataSelectionStrategy
from .retrievestrategy import RETRIEVEStrategy
from .randomstrategy import RandomStrategy
from .gradmatchstrategy import GradMatchStrategy

File Path: cords/selectionstrategies/SSL/craigstrategy.py
Content:
import numpy as np
import torch, time, apricot, math
from scipy.sparse import csr_matrix
from .dataselectionstrategy import DataSelectionStrategy
from torch.utils.data.sampler import SubsetRandomSampler


class CRAIGStrategy(DataSelectionStrategy):
    """
    Adapted Implementation of CRAIG Strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a` for semi-supervised learning setting.

    CRAIG strategy tries to solve the optimization problem given below for convex loss functions:

    .. math::
        \\sum_{i\\in \\mathcal{U}} \\min_{j \\in S, |S| \\leq k} \\| x^i - x^j \\|

    In the above equation, :math:`\\mathcal{U}` denotes the training set where :math:`(x^i, y^i)` denotes the :math:`i^{th}` training data point and label respectively,
    :math:`L_T` denotes the training loss, :math:`S` denotes the data subset selected at each round, and :math:`k` is the budget for the subset.

    Since, the above optimization problem is not dependent on model parameters, we run the subset selection only once right before the start of the training.

    CRAIG strategy tries to solve the optimization problem given below for non-convex loss functions:

    .. math::
        \\underset{\\mathcal{S} \\subseteq \\mathcal{U}:|\\mathcal{S}| \\leq k}{\\operatorname{argmin\\hspace{0.7cm}}}\\underset{i \\in \\mathcal{U}}{\\sum} \\underset{j \\in \\mathcal{S}}{\\min} \\left \\Vert \\mathbf{m}_i \\nabla_{\\theta}l_u(x_i, \\theta) - \\mathbf{m}_j \\nabla_{\\theta}l_u(x_j, \\theta) \\right \Vert

    In the above equation, :math:`\\mathcal{U}` denotes the unlabeled set, :math:`l_u` denotes the unlabeled loss, :math:`\\mathcal{S}` denotes the data subset selected at each round,
    and :math:`k` is the budget for the subset. In this case, CRAIG acts an adaptive subset selection strategy that selects a new subset every epoch.

    Both the optimization problems given above are an instance of facility location problems which is a submodular function. Hence, it can be optimally solved using greedy selection methods.

    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    tea_model: class
        Teacher model architecture used for training
    ssl_alg: class
        SSL algorithm class
    loss: class
        Consistency loss function for unlabeled data with no reduction
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        Apply linear transformation to the data
    if_convex: bool
        If convex or not
    selection_type: str
        Type of selection:
         - 'PerClass': PerClass Implementation where the facility location problem is solved for each class seperately for speed ups.
         - 'Supervised': Supervised Implementation where the facility location problem is solved using a sparse similarity matrix by assigning the similarity of a point with other points of different class to zero.
         - 'PerBatch': PerBatch Implementation where the facility location problem tries to select subset of mini-batches.
    logger: class
        Logger class for logging the information
    optimizer: str
        Type of Greedy Algorithm
    """

    def __init__(self, trainloader, valloader, model, tea_model, ssl_alg, loss,
                 device, num_classes, linear_layer, if_convex, selection_type, 
                 logger, optimizer='lazy'):
        """
        Constructor method
        """
        super().__init__(trainloader, valloader, model, tea_model, ssl_alg, num_classes, linear_layer, loss, device, logger)
        self.if_convex = if_convex
        self.selection_type = selection_type
        self.optimizer = optimizer
        self.dist_mat = None

    def distance(self, x, y, exp=2):
        """
        Compute the distance.

        Parameters
        ----------
        x: Tensor
            First input tensor
        y: Tensor
            Second input tensor
        exp: float, optional
            The exponent value (default: 2)

        Returns
        ----------
        dist: Tensor
            Output tensor
        """

        n = x.size(0)
        m = y.size(0)
        d = x.size(1)
        x = x.unsqueeze(1).expand(n, m, d)
        y = y.unsqueeze(0).expand(n, m, d)
        dist = torch.pow(x - y, exp).sum(2)
        # dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))
        return dist

    def compute_score(self, model_params, tea_model_params, idxs):
        """
        Compute the score of the indices.

        Parameters
        ----------
        model_params: OrderedDict
            Python dictionary object containing model's parameters
        tea_model_params: OrderedDict
            Python dictionary object containing teacher model's parameters
        idxs: list
            The indices
        """

        trainset = self.trainloader.sampler.data_source
        subset_loader = torch.utils.data.DataLoader(trainset, batch_size=self.trainloader.batch_size, shuffle=False,
                                                    sampler=SubsetRandomSampler(idxs),
                                                    pin_memory=True)
        self.model.load_state_dict(model_params)
        if self.tea_model is not None:
            self.tea_model.load_state_dict(tea_model_params)

        self.N = 0
        g_is = []

        if self.if_convex:
            for batch_idx, (ul_weak_aug, ul_strong_aug, _) in enumerate(subset_loader):
                if self.selection_type == 'PerBatch':
                    self.N += 1
                    g_is.append(ul_strong_aug.view(ul_strong_aug.size()[0], -1).mean(dim=0).view(1, -1))
                else:
                    self.N += ul_strong_aug.size()[0]
                    g_is.append(ul_strong_aug.view(ul_strong_aug.size()[0], -1))
        else:
            embDim = self.model.get_embedding_dim()
            for batch_idx, (ul_weak_aug, ul_strong_aug, _) in enumerate(subset_loader):
                ul_weak_aug, ul_strong_aug = ul_weak_aug.to(self.device), ul_strong_aug.to(self.device)
                if self.selection_type == 'PerBatch':
                    self.N += 1
                else:
                    self.N += ul_strong_aug.size()[0]
                loss, out, l1, _, _ = self.ssl_loss(ul_weak_data=ul_weak_aug, ul_strong_data=ul_strong_aug)
                loss = loss.sum()
                l0_grads = torch.autograd.grad(loss, out)[0]
                if self.linear_layer:
                    l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                    l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                    if self.selection_type == 'PerBatch':
                        g_is.append(torch.cat((l0_grads, l1_grads), dim=1).mean(dim=0).view(1, -1))
                    else:
                        g_is.append(torch.cat((l0_grads, l1_grads), dim=1))
                else:
                    if self.selection_type == 'PerBatch':
                        g_is.append(l0_grads.mean(dim=0).view(1, -1))
                    else:
                        g_is.append(l0_grads)

        self.dist_mat = torch.zeros([self.N, self.N], dtype=torch.float32)
        first_i = True
        if self.selection_type == 'PerBatch':
            g_is = torch.cat(g_is, dim=0)
            self.dist_mat = self.distance(g_is, g_is).cpu()
        else:
            for i, g_i in enumerate(g_is, 0):
                if first_i:
                    size_b = g_i.size(0)
                    first_i = False
                for j, g_j in enumerate(g_is, 0):
                    self.dist_mat[i * size_b: i * size_b + g_i.size(0),
                    j * size_b: j * size_b + g_j.size(0)] = self.distance(g_i, g_j).cpu()
        self.const = torch.max(self.dist_mat).item()
        self.dist_mat = (self.const - self.dist_mat).numpy()

    def compute_gamma(self, idxs):
        """
        Compute the gamma values for the indices.

        Parameters
        ----------
        idxs: list
            The indices

        Returns
        ----------
        gamma: list
            Gradient values of the input indices
        """

        if self.selection_type in ['PerClass', 'PerBatch']:
            gamma = [0 for i in range(len(idxs))]
            best = self.dist_mat[idxs]  # .to(self.device)
            rep = np.argmax(best, axis=0)
            for i in rep:
                gamma[i] += 1
        elif self.selection_type == 'Supervised':
            gamma = [0 for i in range(len(idxs))]
            best = self.dist_mat[idxs]  # .to(self.device)
            rep = np.argmax(best, axis=0)
            for i in range(rep.shape[1]):
                gamma[rep[0, i]] += 1
        return gamma

    def get_similarity_kernel(self):
        """
        Obtain the similarity kernel.

        Returns
        ----------
        kernel: ndarray
            Array of kernel values
        """
        for batch_idx, (inputs, targets) in enumerate(self.trainloader):
            if batch_idx == 0:
                labels = targets
            else:
                tmp_target_i = targets
                labels = torch.cat((labels, tmp_target_i), dim=0)
        kernel = np.zeros((labels.shape[0], labels.shape[0]))
        for target in np.unique(labels):
            x = np.where(labels == target)[0]
            # prod = np.transpose([np.tile(x, len(x)), np.repeat(x, len(x))])
            for i in x:
                kernel[i, x] = 1
        return kernel

    def select(self, budget, model_params, tea_model_params):
        """
        Data selection method using different submodular optimization
        functions.

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing models parameters
        optimizer: str
            The optimization approach for data selection. Must be one of
            'random', 'modular', 'naive', 'lazy', 'approximate-lazy', 'two-stage',
            'stochastic', 'sample', 'greedi', 'bidirectional'

        Returns
        ----------
        total_greedy_list: list
            List containing indices of the best datapoints
        gammas: list
            List containing gradients of datapoints present in greedySet
        """
        # per_class_bud = int(budget / self.num_classes)
        total_greedy_list = []
        gammas = []
        start_time = time.time()
        if self.selection_type == 'PerClass':
            self.get_labels(valid=False)
            for i in range(self.num_classes):
                idxs = torch.where(self.trn_lbls == i)[0]
                self.compute_score(model_params, tea_model_params, idxs)
                fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0, metric='precomputed',
                                                                                  n_samples=math.ceil(
                                                                                      budget * len(idxs) / self.N_trn),
                                                                                  optimizer=self.optimizer)
                sim_sub = fl.fit_transform(self.dist_mat)
                greedyList = list(np.argmax(sim_sub, axis=1))
                gamma = self.compute_gamma(greedyList)
                total_greedy_list.extend(idxs[greedyList])
                gammas.extend(gamma)
            rand_indices = np.random.permutation(len(total_greedy_list))
            total_greedy_list = list(np.array(total_greedy_list)[rand_indices])
            gammas = list(np.array(gammas)[rand_indices])
        elif self.selection_type == 'Supervised':
            self.get_labels(valid=False)
            for i in range(self.num_classes):
                if i == 0:
                    idxs = torch.where(self.trn_lbls == i)[0]
                    N = len(idxs)
                    self.compute_score(model_params, tea_model_params, idxs)
                    row = idxs.repeat_interleave(N)
                    col = idxs.repeat(N)
                    data = self.dist_mat.flatten()
                else:
                    idxs = torch.where(self.trn_lbls == i)[0]
                    N = len(idxs)
                    self.compute_score(model_params, tea_model_params, idxs)
                    row = torch.cat((row, idxs.repeat_interleave(N)), dim=0)
                    col = torch.cat((col, idxs.repeat(N)), dim=0)
                    data = np.concatenate([data, self.dist_mat.flatten()], axis=0)
            sparse_simmat = csr_matrix((data, (row.numpy(), col.numpy())), shape=(self.N_trn, self.N_trn))
            self.dist_mat = sparse_simmat
            fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0, metric='precomputed',
                                                                              n_samples=budget, optimizer=self.optimizer)
            sim_sub = fl.fit_transform(sparse_simmat)
            total_greedy_list = list(np.array(np.argmax(sim_sub, axis=1)).reshape(-1))
            gammas = self.compute_gamma(total_greedy_list)
        elif self.selection_type == 'PerBatch':
            idxs = torch.arange(self.N_trn)
            self.compute_score(model_params, tea_model_params, idxs)
            fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0, metric='precomputed',
                                                                              n_samples=math.ceil(
                                                                                  budget / self.trainloader.batch_size),
                                                                              optimizer=self.optimizer)
            sim_sub = fl.fit_transform(self.dist_mat)
            temp_list = list(np.array(np.argmax(sim_sub, axis=1)).reshape(-1))
            gammas_temp = self.compute_gamma(temp_list)
            batch_wise_indices = list(self.trainloader.batch_sampler)
            for i in range(len(temp_list)):
                tmp = batch_wise_indices[temp_list[i]]
                total_greedy_list.extend(tmp)
                gammas.extend(list(gammas_temp[i] * np.ones(len(tmp))))
        end_time = time.time()
        self.logger.debug("CRAIG subset selection time is: %f", end_time-start_time)
        return total_greedy_list, gammas

File Path: cords/selectionstrategies/SSL/dataselectionstrategy.py
Content:
import numpy as np
import torch
from torch.nn.functional import cross_entropy


class DataSelectionStrategy(object):
    """
    Implementation of Data Selection Strategy class which serves as base class for other
    dataselectionstrategies for semi-supervised learning frameworks.
    Parameters
    ----------
    trainloader: class
        Loading the training data using pytorch dataloader
    valloader: class
        Loading the validation data using pytorch dataloader
    model: class
        Model architecture used for training
    tea_model: class
        Teacher model architecture used for training
    ssl_alg: class
        SSL algorithm class
    num_classes: int
        Number of target classes in the dataset
    linear_layer: bool
        If True, we use the last fc layer weights and biases gradients
        If False, we use the last fc layer biases gradients
    loss: class
        Consistency loss function for unlabeled data with no reduction
    device: str
        The device being utilized - cpu | cuda
    logger : class
        logger file for printing the info
    """

    def __init__(self, trainloader, valloader, model, tea_model, ssl_alg, num_classes, linear_layer, loss, device, logger):
        """
        Constructor method
        """
        self.trainloader = trainloader  # assume its a sequential loader.
        self.valloader = valloader
        self.model = model
        self.N_trn = len(trainloader.sampler)
        self.N_val = len(valloader.sampler)
        self.grads_per_elem = None
        self.val_grads_per_elem = None
        self.numSelected = 0
        self.linear_layer = linear_layer
        self.num_classes = num_classes
        self.trn_lbls = None
        self.val_lbls = None
        self.loss = loss
        self.device = device
        self.tea_model = tea_model
        self.ssl_alg = ssl_alg
        self.logger = logger

    def select(self, budget, model_params, tea_model_params):
        """
        Abstract select function that is overloaded by the child classes
        """
        pass

    def ssl_loss(self, ul_weak_data, ul_strong_data, labels=False):
        """
        Function that computes contrastive semi-supervised loss

        Parameters
        -----------
        ul_weak_data: 
            Weak agumented version of unlabeled data
        ul_strong_data:
            Strong agumented version of unlabeled data
        labels: bool
            if labels, just return hypothesized labels of the unlabeled data
        
        Returns
        --------
        L_consistency: Consistency loss
        y: Actual labels(Not used anywhere)
        l1_strong: Penultimate layer outputs for strongly augmented version of unlabeled data
        targets: Hypothesized labels
        mask: mask vector of the unlabeled data

        """
        self.logger.debug("SSL loss computation initiated")
        all_data = torch.cat([ul_weak_data, ul_strong_data], 0)
        forward_func = self.model.forward
        stu_logits, l1 = forward_func(all_data, last=True, freeze=True)
        stu_unlabeled_weak_logits, stu_unlabeled_strong_logits = torch.chunk(stu_logits, 2, dim=0)
        _, l1_strong = torch.chunk(l1, 2, dim=0)

        if self.tea_model is not None: # get target values from teacher model
            t_forward_func = self.tea_model.forward
            tea_logits = t_forward_func(all_data)
            tea_unlabeled_weak_logits, _ = torch.chunk(tea_logits, 2, dim=0)
        else:
            t_forward_func = forward_func
            tea_unlabeled_weak_logits = stu_unlabeled_weak_logits

        self.model.update_batch_stats(False)
        if self.ssl_alg.__class__.__name__ in ['PseudoLabel', 'ConsistencyRegularization']:
            y, targets, mask = self.ssl_alg(
                stu_preds=stu_unlabeled_strong_logits,
                tea_logits=tea_unlabeled_weak_logits.detach(),
                w_data=ul_strong_data,
                stu_forward=forward_func,
                tea_forward=t_forward_func
            )
        else:
            y, l1_strong, targets, mask = self.ssl_alg(
                stu_preds=stu_unlabeled_strong_logits,
                tea_logits=tea_unlabeled_weak_logits.detach(),
                w_data=ul_strong_data,
                subset=True,
                stu_forward=forward_func,
                tea_forward=t_forward_func
            )
        self.logger.debug("SSL loss computation finished")
        if labels:
            if targets.ndim == 1:
                return targets
            else:
                return targets.argmax(dim=1)
        else:
            self.model.update_batch_stats(True)
            #mask = torch.ones(len(mask), device=self.device)
            L_consistency = self.loss(y, targets, mask, weak_prediction=stu_unlabeled_weak_logits.softmax(1))
            return L_consistency, y, l1_strong, targets, mask

    def get_labels(self, valid=False):
        """
        Function that iterates over labeled or unlabeled data and returns target or hypothesized labels.

        Parameters
        -----------
        valid: bool
            If True, iterate over the labeled set
        """
        self.logger.debug("Get labels function Initiated")
        for batch_idx, (ul_weak_aug, ul_strong_aug, _) in enumerate(self.trainloader):
            ul_weak_aug, ul_strong_aug = ul_weak_aug.to(self.device), ul_strong_aug.to(self.device)
            targets = self.ssl_loss(ul_weak_data=ul_weak_aug, ul_strong_data=ul_strong_aug, labels=True)
            if batch_idx == 0:
                self.trn_lbls = targets.view(-1, 1)
            else:
                self.trn_lbls = torch.cat((self.trn_lbls, targets.view(-1, 1)), dim=0)
        self.trn_lbls = self.trn_lbls.view(-1)

        if valid:
            for batch_idx, (inputs, targets) in enumerate(self.valloader):
                if batch_idx == 0:
                    self.val_lbls = targets.view(-1, 1)
                else:
                    self.val_lbls = torch.cat((self.val_lbls, targets.view(-1, 1)), dim=0)
            self.val_lbls = self.val_lbls.view(-1)
        self.logger.debug("Get labels function finished")

    def compute_gradients(self, valid=False, perBatch=False, perClass=False, store_t=False):
        """
        Computes the gradient of each element.

        Here, the gradients are computed in a closed form using CrossEntropyLoss with reduction set to 'none'.
        This is done by calculating the gradients in last layer through addition of softmax layer.

        Using different loss functions, the way we calculate the gradients will change.

        For LogisticLoss we measure the Mean Absolute Error(MAE) between the pairs of observations.
        With reduction set to 'none', the loss is formulated as:

        .. math::
            \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad
            l_n = \\left| x_n - y_n \\right|,

        where :math:`N` is the batch size.


        For MSELoss, we measure the Mean Square Error(MSE) between the pairs of observations.
        With reduction set to 'none', the loss is formulated as:

        .. math::
            \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad
            l_n = \\left( x_n - y_n \\right)^2,

        where :math:`N` is the batch size.
        Parameters
        ----------
        valid: bool
            if True, the function also computes the validation gradients
        batch: bool
            if True, the function computes the gradients of each mini-batch
        perClass: bool
            if True, the function computes the gradients using perclass dataloaders
        store_t: bool
            if True, the function stores the hypothesized weak augmentation targets and masks for unlabeled set.
        """
        if (perBatch and perClass):
            raise ValueError("batch and perClass are mutually exclusive. Only one of them can be true at a time")

        self.logger.debug("Per-sampele gradient computation Initiated")
        embDim = self.model.get_embedding_dim()
        if perClass:
            trainloader = self.pctrainloader
            if valid:
                valloader = self.pcvalloader
        else:
            trainloader = self.trainloader
            if valid:
                valloader = self.valloader
        

        if store_t:
            targets = []
            masks = []

        for batch_idx, (ul_weak_aug, ul_strong_aug, _) in enumerate(trainloader):
            ul_weak_aug, ul_strong_aug = ul_weak_aug.to(self.device), ul_strong_aug.to(self.device)
            if store_t:
                loss, out, l1, t, m = self.ssl_loss(ul_weak_data=ul_weak_aug, ul_strong_data=ul_strong_aug)
                targets.append(t)
                masks.append(m)
            else:
                loss, out, l1, _, _ = self.ssl_loss(ul_weak_data=ul_weak_aug, ul_strong_data=ul_strong_aug)
            loss = loss.sum()
            if batch_idx == 0:
                l0_grads = torch.autograd.grad(loss, out)[0]
                if self.linear_layer:
                    l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                    l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                if perBatch:
                    l0_grads = l0_grads.mean(dim=0).view(1, -1)
                    if self.linear_layer:
                        l1_grads = l1_grads.mean(dim=0).view(1, -1)
            else:
                batch_l0_grads = torch.autograd.grad(loss, out)[0]
                if self.linear_layer:
                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                    batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)
                if perBatch:
                    batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                    if self.linear_layer:
                        batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)
                l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                if self.linear_layer:
                    l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)

        torch.cuda.empty_cache()
        if store_t:
            self.weak_targets = targets
            self.weak_masks = masks
        
        if self.linear_layer:
            self.grads_per_elem = torch.cat((l0_grads, l1_grads), dim=1)
        else:
            self.grads_per_elem = l0_grads
            
        if valid:
            for batch_idx, (inputs, targets) in enumerate(valloader):
                inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)
                if batch_idx == 0:
                    out, l1 = self.model(inputs, last=True, freeze=True)
                    loss = cross_entropy(out, targets, reduction='none').sum()
                    l0_grads = torch.autograd.grad(loss, out)[0]
                    if self.linear_layer:
                        l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                        l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                    if perBatch:
                        l0_grads = l0_grads.mean(dim=0).view(1, -1)
                        if self.linear_layer:
                            l1_grads = l1_grads.mean(dim=0).view(1, -1)
                else:
                    out, l1 = self.model(inputs, last=True, freeze=True)
                    loss = cross_entropy(out, targets, reduction='none').sum()
                    batch_l0_grads = torch.autograd.grad(loss, out)[0]
                    if self.linear_layer:
                        batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                        batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)

                    if perBatch:
                        batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                        if self.linear_layer:
                            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)
                    l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                    if self.linear_layer:
                        l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)
            torch.cuda.empty_cache()
            if self.linear_layer:
                self.val_grads_per_elem = torch.cat((l0_grads, l1_grads), dim=1)
            else:
                self.val_grads_per_elem = l0_grads
        self.logger.debug("Per-sample gradient computation Finished")

        
    def update_model(self, model_params, tea_model_params):
        """
        Update the models parameters

        Parameters
        ----------
        model_params: OrderedDict
            Python dictionary object containing model's parameters
        tea_model_params: OrderedDict
            Python dictionary object containing teacher model's parameters
        """
        self.model.load_state_dict(model_params)
        if self.tea_model is not None:
            self.tea_model.load_state_dict(tea_model_params)

File Path: cords/selectionstrategies/SSL/gradmatchstrategy.py
Content:
import math
import time
import torch
import numpy as np
from .dataselectionstrategy import DataSelectionStrategy
from ..helpers import OrthogonalMP_REG_Parallel, OrthogonalMP_REG, OrthogonalMP_REG_Parallel_V1
from torch.utils.data import Subset, DataLoader


class GradMatchStrategy(DataSelectionStrategy):
    """
    Implementation of OMPGradMatch Strategy from the paper :footcite:`pmlr-v139-killamsetty21a` for supervised learning frameworks.

    OMPGradMatch strategy tries to solve the optimization problem given below:

    .. math::
        \\underset{\\mathcal{S} \\subseteq \\mathcal{U}:|\\mathcal{S}| \\leq k, \{\\mathbf{w}_j\}_{j \\in [1, |\\mathcal{S}|]}:\\forall_{j} \\mathbf{w}_j \\geq 0}{\\operatorname{argmin\\hspace{0.7mm}}} \\left \\Vert \\underset{i \\in \\mathcal{U}}{\\sum} \\mathbf{m}_i \\nabla_{\\theta}l_u(x_i, \\theta) - \\underset{j \\in \\mathcal{S}}{\\sum} \\mathbf{m}_j \\mathbf{w}_j \\nabla_{\\theta} l_u(x_j, \\theta)\\right \\Vert

    In the above equation, :math:`\\mathbf{w}` denotes the weight vector that contains the weights for each data instance, :math:`\\mathcal{U}` denotes the unlabeled set 
    where :math:`(x^i, y^i)` denotes the :math:`i^{th}` training data point and label respectively, :math:`l_u` denotes the unlabeled loss, :math:`\\mathcal{S}` denotes the
    data subset selected at each round, and :math:`k` is the budget for the subset.

    The above optimization problem is solved using the Orthogonal Matching Pursuit(OMP) algorithm.

    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    tea_model: class
        Teacher model architecture used for training
    ssl_alg: class
        SSL algorithm class
    loss: class
        Consistency loss function for unlabeled data with no reduction
    eta: float
        Learning rate. Step size for the one step gradient update
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        Apply linear transformation to the data
    selection_type: str
        Type of selection -
        - 'PerClass': PerClass method is where OMP algorithm is applied on each class data points seperately.
        - 'PerBatch': PerBatch method is where OMP algorithm is applied on each minibatch data points.
        - 'PerClassPerGradient': PerClassPerGradient method is same as PerClass but we use the gradient corresponding to classification layer of that class only.
    logger : class
        logger file for printing the info
    valid : bool, optional
        If valid==True we use validation dataset gradient sum in OMP otherwise we use training dataset (default: False)
    v1 : bool
        If v1==True, we use newer version of OMP solver that is more accurate
    lam : float
        Regularization constant of OMP solver
    eps : float
        Epsilon parameter to which the above optimization problem is solved using OMP algorithm
    """

    def __init__(self, trainloader, valloader, model, tea_model, ssl_alg, loss,
                 eta, device, num_classes, linear_layer, selection_type, logger, 
                 valid=False, v1=True, lam=0, eps=1e-4):
        """
        Constructor method
        """

        super().__init__(trainloader, valloader, model, tea_model, ssl_alg, num_classes, linear_layer, loss, device, logger)
        self.eta = eta  # step size for the one step gradient update
        self.device = device
        self.selection_type = selection_type
        self.valid = valid
        self.lam = lam
        self.eps = eps
        self.v1 = v1

    def ompwrapper(self, X, Y, bud):
        """
        Wrapper function that instantiates the OMP algorithm 

        Parameters
	    ----------
        X: 
            Individual datapoint gradients 
        Y: 
            Gradient sum that needs to be matched to.
        bud:
            Budget of datapoints that needs to be sampled from the unlabeled set

        Returns
        ----------
        idxs: list
            List containing indices of the best datapoints,
        gammas: weights tensors
            Tensor containing weights of each instance
        """

        if self.device == "cpu":
            reg = OrthogonalMP_REG(X.numpy(), Y.numpy(), nnz=bud, positive=True, lam=0)
            ind = np.nonzero(reg)[0]
        else:
            if self.v1:
                reg = OrthogonalMP_REG_Parallel_V1(X, Y, nnz=bud,
                                                 positive=True, lam=self.lam,
                                                 tol=self.eps, device=self.device)
            else:
                reg = OrthogonalMP_REG_Parallel(X, Y, nnz=bud,
                                                positive=True, lam=self.lam,
                                                tol=self.eps, device=self.device)
            ind = torch.nonzero(reg).view(-1)
        return ind.tolist(), reg[ind].tolist()

    def select(self, budget, model_params, tea_model_params):
        """
        Apply OMP Algorithm for data selection

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing model's parameters
        tea_model_params: OrderedDict
            Python dictionary object containing teacher model's parameters

        Returns
        --------
        idxs: list
            List containing indices of the best datapoints,
        gammas: weights tensors
            Tensor containing weights of each instance
        """
        omp_start_time = time.time()
        self.update_model(model_params, tea_model_params)
        if self.selection_type == 'PerClass':
            self.get_labels(valid=self.valid)
            idxs = []
            gammas = []
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                          shuffle=False, pin_memory=True)
                if self.valid:
                    val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                    val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                    self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                                    shuffle=False, pin_memory=True)

                self.compute_gradients(self.valid, perBatch=False, perClass=True)
                trn_gradients = self.grads_per_elem
                if self.valid:
                    sum_val_grad = torch.sum(self.val_grads_per_elem, dim=0)
                else:
                    sum_val_grad = torch.sum(trn_gradients, dim=0)
                idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                          sum_val_grad, math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)

        elif self.selection_type == 'PerBatch':
            self.compute_gradients(self.valid, perBatch=True, perClass=False)
            idxs = []
            gammas = []
            trn_gradients = self.grads_per_elem
            if self.valid:
                sum_val_grad = torch.sum(self.val_grads_per_elem, dim=0)
            else:
                sum_val_grad = torch.sum(trn_gradients, dim=0)
            idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                                     sum_val_grad, math.ceil(budget/self.trainloader.batch_size))
            batch_wise_indices = list(self.trainloader.batch_sampler)
            for i in range(len(idxs_temp)):
                tmp = batch_wise_indices[idxs_temp[i]]
                idxs.extend(tmp)
                gammas.extend(list(gammas_temp[i] * np.ones(len(tmp))))

        elif self.selection_type == 'PerClassPerGradient':
            self.get_labels(valid=self.valid)
            idxs = []
            gammas = []
            embDim = self.model.get_embedding_dim()
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                          shuffle=False, pin_memory=True)
                if self.valid:
                    val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                    val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                    self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                                    shuffle=False, pin_memory=True)
                self.compute_gradients(self.valid, perBatch=False, perClass=True)
                trn_gradients = self.grads_per_elem
                tmp_gradients = trn_gradients[:, i].view(-1, 1)
                tmp1_gradients = trn_gradients[:,
                                 self.num_classes + (embDim * i): self.num_classes + (embDim * (i + 1))]
                trn_gradients = torch.cat((tmp_gradients, tmp1_gradients), dim=1)

                if self.valid:
                    val_gradients = self.val_grads_per_elem
                    tmp_gradients = val_gradients[:, i].view(-1, 1)
                    tmp1_gradients = val_gradients[:,
                                     self.num_classes + (embDim * i): self.num_classes + (embDim * (i + 1))]
                    val_gradients = torch.cat((tmp_gradients, tmp1_gradients), dim=1)
                    sum_val_grad = torch.sum(val_gradients, dim=0)
                else:
                    sum_val_grad = torch.sum(trn_gradients, dim=0)

                idxs_temp, gammas_temp = self.ompwrapper(torch.transpose(trn_gradients, 0, 1),
                                          sum_val_grad, math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)

        omp_end_time = time.time()
        diff = budget - len(idxs)

        if diff > 0:
            remainList = set(np.arange(self.N_trn)).difference(set(idxs))
            new_idxs = np.random.choice(list(remainList), size=diff, replace=False)
            idxs.extend(new_idxs)
            gammas.extend([1 for _ in range(diff)])
            idxs = np.array(idxs)
            gammas = np.array(gammas)

        if self.selection_type in ["PerClass", "PerClassPerGradient"]:
            rand_indices = np.random.permutation(len(idxs))
            idxs = list(np.array(idxs)[rand_indices])
            gammas = list(np.array(gammas)[rand_indices])

        self.logger.debug("OMP algorithm Subset Selection time is: %f", omp_end_time - omp_start_time)
        return idxs, gammas
File Path: cords/selectionstrategies/SSL/randomstrategy.py
Content:
import numpy as np
import torch


class RandomStrategy(object):
    """
    This is the Random Selection Strategy class where we select a set of random points as a datasubset
    and often acts as baselines to compare other subset selection strategies.

    Parameters
    ----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    """

    def __init__(self, trainloader, online=False):
        """
        Constructor method
        """

        self.trainloader = trainloader
        self.N_trn = len(trainloader.sampler.data_source)
        self.online = online
        self.indices = None
        self.gammas = None

    def select(self, budget):
        """
        Perform random sampling of indices of size budget.

        Parameters
        ----------
        budget: int
            The number of data points to be selected

        Returns
        ----------
        indices: ndarray
            Array of indices of size budget selected randomly
        gammas: Tensor
            Gradient weight values of selected indices
        """
        if self.online or (self.indices is None):
            self.indices = np.random.choice(self.N_trn, size=budget, replace=False)
            self.gammas = torch.ones(budget)
        return self.indices, self.gammas
File Path: cords/selectionstrategies/SSL/retrievestrategy.py
Content:
import math
import random
import time
import torch
import torch.nn.functional as F
from .dataselectionstrategy import DataSelectionStrategy
from torch.utils.data import Subset, DataLoader
import numpy as np


class RETRIEVEStrategy(DataSelectionStrategy):
    """
    Implementation of RETRIEVE Strategy from the paper :footcite:`killamsetty2021retrieve`  for efficient and robust semi-supervised learning frameworks.
    RETRIEVE method tries to solve the  bi-level optimization problem given below:

    .. math::
        \\overbrace{\\mathcal{S}_{t} = \\underset{\\mathcal{S} \\subseteq \\mathcal{U}:|\\mathcal{S}| \\leq k}{\\operatorname{argmin\\hspace{0.7mm}}}L_S\\Big(\\mathcal{D}, \\underbrace{\\underset{\\theta}{\\operatorname{argmin\hspace{0.7mm}}}\\big(L_S(\\mathcal{D}, \\theta_t) + \\lambda_t \\underset{j \\in \\mathcal{S}}{\\sum} \\mathbf{m}_{jt} l_u(x_j, \\theta_t) \\big)}_{inner-level}\\Big)}^{outer-level}

    Notation: Denote :math: `\\mathcal{D} = \{x_i, y_i\}_{i=1}^n`  to be the labeled set with :math: `n` labeled data points, and :math: `\\mathcal{U} = \{x_j\}_{j=1}^m` 
    to be the unlabeled set with :math: `m` data points. Let :math: `\\theta` be the classifier model parameters, :math: `l_s` be the labeled set loss function 
    (such as cross-entropy loss) and :math: `l_u` be the unlabeled set loss, e.g. consistency-regularization loss, entropy loss, etc. Denote :math: `L_S(\\mathcal{D}, \\theta) = 
    \\underset{i \\in \\mathcal{D}}{\\sum}l_{s}(\\theta, x_i, y_i)` and :math: `L_U(\\mathcal{U}, \\theta, \\mathbf{m}) = \\underset{j \\in \\mathcal{U}}{\\sum} \\mathbf{m}_i 
    l_u(x_j, \\theta)` where :math: `\mathbf{m} \\in \{0, 1\}^m` is the binary mask vector for unlabeled set. For notational convenience, we denote :math: `l_{si}(\\theta) = l_s(x_i, y_i, \\theta)`
    and denote :math: `$l_{uj}(\\theta) = l_u(x_j, \\theta)$`.
    
    Since, solving the complete inner-optimization is expensive, RETRIEVE adopts a online one-step meta approximation where we approximate the solution to inner problem
    by taking a single gradient step.

    The optimization problem after the approximation is as follows:

    .. math::
        \\mathcal{S}_{t} = \\underset{\\mathcal{S} \\subseteq \\mathcal{U}:|\\mathcal{S}| \\leq k}{\\operatorname{argmin\hspace{0.7mm}}}L_S(\\mathcal{D}, \\theta_t - \\alpha_t \\nabla_{\\theta}L_S(\\mathcal{D}, \\theta_t) - \\alpha_t \\lambda_t \\underset{j \\in \\mathcal{S}}{\\sum} \\mathbf{m}_{jt} \\nabla_{\\theta}l_u(x_j, \\theta_t))\\text{\\hspace{1.7cm}}

    In the above equation, :math:`\\alpha_t` denotes the step-size used for one-step gradient update.

    RETRIEVE-ONLINE also makes an additional approximation called Taylor-Series approximation to easily solve the outer problem using a greedy selection algorithm.
    The Taylor series approximation is as follows:

    .. math::
        L_S(\\mathcal{D}, \\theta_t - \\alpha_t \\nabla_{\\theta}L_S(\\mathcal{D}, \\theta_t) - \\alpha_t \\lambda_t \\underset{j \\in \\mathcal{S}}{\\sum} \\mathbf{m}_{jt} \\nabla_{\\theta}l_u(x_j, \\theta_t)) \\approx L_S(\\mathcal{D}, \\theta^{S}) - \\alpha_t \\lambda_t  {\\nabla_{\\theta}L_S(\\mathcal{D}, \\theta^S)}^T  \\mathbf{m}_{et} \\nabla_{\\theta}l_u(x_e, \\theta_t)

    Taylor's series approximation reduces the time complexity by reducing the need of calculating the labeled set loss for each element during greedy selection step which
    means reducing the number of forward passes required.

    RETRIEVE-ONLINE is an adaptive subset selection algorithm that tries to select a subset every :math:`L` epochs and the parameter `L` can be set in the original training loop.

    Parameters
	----------
    trainloader: class
        Loading the training data using pytorch DataLoader
    valloader: class
        Loading the validation data using pytorch DataLoader
    model: class
        Model architecture used for training
    tea_model: class
        Teacher model architecture used for training
    ssl_alg: class
        SSL algorithm class
    loss: class
        Consistency loss function for unlabeled data with no reduction
    eta: float
        Learning rate. Step size for the one step gradient update
    device: str
        The device being utilized - cpu | cuda
    num_classes: int
        The number of target classes in the dataset
    linear_layer: bool
        If True, we use the last fc layer weights and biases gradients
        If False, we use the last fc layer biases gradients
    selection_type: str
        Type of selection algorithm -
        - 'PerBatch' : PerBatch method is where RETRIEVE algorithm is applied on each minibatch data points.
        - 'PerClass' : PerClass method is where RETRIEVE algorithm is applied on each class data points seperately.
        - 'Supervised' : Supervised method is where RETRIEVE algorithm is applied on entire training data.
    greedy: str
        Type of greedy selection algorithm -
        - 'RGreedy' : RGreedy Selection method is a variant of naive greedy where we just perform r rounds of greedy selection by choosing k/r points in each round.
        - 'Stochastic' : Stochastic greedy selection method is based on the algorithm presented in this paper :footcite:`mirzasoleiman2014lazier`
        - 'Naive' : Normal naive greedy selection method that selects a single best element every step until the budget is fulfilled
    logger: class
        Logger class for logging the information
    r : int, optional
        Number of greedy selection rounds when selection method is RGreedy (default: 15)
    valid: bool
        - If True, we select subset that maximizes the performance on the labeled set.
        - If False, we select subset that maximizes the performance on the unlabeled set.
    """

    def __init__(self, trainloader, valloader, model, tea_model, ssl_alg, loss,
                 eta, device, num_classes, linear_layer, selection_type, greedy, 
                 logger, r=15, valid=True):
        """
        Constructor method
        """
        super().__init__(trainloader, valloader, model, tea_model, ssl_alg, num_classes, linear_layer, loss, device, logger)
        self.eta = eta  # step size for the one step gradient update
        self.init_out = list()
        self.init_l1 = list()
        self.selection_type = selection_type
        self.r = r
        self.valid = valid
        self.greedy = greedy

    def _update_grads_val(self, grads_currX=None, first_init=False):
        """
        Update the gradient values

        Parameters
        ----------
        grad_currX: OrderedDict, optional
            Gradients of the current element (default: None)
        first_init: bool, optional
            Gradient initialization (default: False)
        """
        self.model.zero_grad()
        if self.selection_type == 'PerClass':
            valloader = self.pcvalloader
        else:
            valloader = self.valloader

        if self.selection_type == 'PerClass':
            trainloader = self.pctrainloader
        else:
            trainloader = self.trainloader
            
        embDim = self.model.get_embedding_dim()
        loss_name = self.loss.__class__.__name__
        if self.valid:
            if first_init:
                for batch_idx, (inputs, targets) in enumerate(valloader):
                    inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)
                    if loss_name == 'MeanSquared':
                        tmp_targets = torch.zeros(len(inputs), self.num_classes, device=self.device)
                        tmp_targets[torch.arange(len(inputs)), targets] = 1
                        targets = tmp_targets
                    if batch_idx == 0:
                        out, l1 = self.model(inputs, last=True, freeze=True)
                        if loss_name == 'MeanSquared':
                            temp_out = F.softmax(out, dim=1)
                            loss = F.mse_loss(temp_out, targets, reduction='none').sum()
                        else:
                            loss = F.cross_entropy(out, targets, reduction='none').sum()
                        l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                            l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                        self.init_out = out
                        self.init_l1 = l1                        
                        if self.selection_type == 'PerBatch':
                            l0_grads = l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                l1_grads = l1_grads.mean(dim=0).view(1, -1)                        
                        if loss_name == 'MeanSquared':
                            self.y_val = targets
                        else:
                            self.y_val = targets.view(-1, 1)
                    else:
                        out, l1 = self.model(inputs, last=True, freeze=True)
                        
                        if loss_name == 'MeanSquared':
                            temp_out = F.softmax(out, dim=1)
                            loss = F.mse_loss(temp_out, targets, reduction='none').sum()
                        else:
                            loss = F.cross_entropy(out, targets, reduction='none').sum()
                        
                        batch_l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                            batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)
                        
                        if self.selection_type == 'PerBatch':
                            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)
                        
                        l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                        if self.linear_layer:
                            l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)
                        self.init_out = torch.cat((self.init_out, out), dim=0)
                        self.init_l1 = torch.cat((self.init_l1, l1), dim=0)
                        if loss_name == 'MeanSquared':
                            self.y_val = torch.cat((self.y_val, targets), dim=0)
                        else:
                            self.y_val = torch.cat((self.y_val, targets.view(-1, 1)), dim=0)
            elif grads_currX is not None:
                out_vec = self.init_out - (
                        self.eta * grads_currX[0][0:self.num_classes].view(1, -1).expand(self.init_out.shape[0], -1))
                if self.linear_layer:
                    out_vec = out_vec - (self.eta * torch.matmul(self.init_l1, grads_currX[0][self.num_classes:].view(
                        self.num_classes, -1).transpose(0, 1)))
                if loss_name == 'MeanSquared':
                    temp_out_vec = F.softmax(out_vec, dim=1)
                    loss = self.loss(temp_out_vec, self.y_val, torch.ones(len(temp_out_vec), device=self.device)).sum()
                else:
                    loss = self.loss(out_vec, self.y_val, torch.ones(len(out_vec), device=self.device)).sum()
                l0_grads = torch.autograd.grad(loss, out_vec)[0]
                if self.linear_layer:
                    l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                    l1_grads = l0_expand * self.init_l1.repeat(1, self.num_classes)
                if self.selection_type == 'PerBatch':
                    b = int(l0_grads.shape[0]/self.valloader.batch_size)
                    l0_grads = torch.chunk(l0_grads, b, dim=0)
                    new_t = []
                    for i in range(len(l0_grads)):
                        new_t.append(torch.mean(l0_grads[i], dim=0).view(1, -1))
                    l0_grads = torch.cat(new_t, dim=0)
                    if self.linear_layer:
                        l1_grads = torch.chunk(l1_grads, b, dim=0)
                        new_t = []
                        for i in range(len(l1_grads)):
                            new_t.append(torch.mean(l1_grads[i], dim=0).view(1, -1))
                        l1_grads = torch.cat(new_t, dim=0)
            torch.cuda.empty_cache()
            if self.linear_layer:
                self.grads_val_curr = torch.mean(torch.cat((l0_grads, l1_grads), dim=1), dim=0).view(-1, 1)
            else:
                self.grads_val_curr = torch.mean(l0_grads, dim=0).view(-1, 1)
        else:
            if first_init:
                self.y_val = torch.cat(self.weak_targets, dim=0)
                for batch_idx, (ul_weak_aug, ul_strong_aug, _) in enumerate(trainloader):
                    ul_weak_aug, ul_strong_aug = ul_weak_aug.to(self.device), ul_strong_aug.to(self.device)
                    if batch_idx == 0:
                        out, l1 = self.model(ul_strong_aug, last=True, freeze=True)
                        if loss_name == 'MeanSquared':
                            temp_out = F.softmax(out, dim=1)
                            loss = self.loss(temp_out, self.weak_targets[batch_idx], self.weak_masks[batch_idx]).sum()
                        else:
                            loss = self.loss(out, self.weak_targets[batch_idx], self.weak_masks[batch_idx]).sum()
                        l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                            l1_grads = l0_expand * l1.repeat(1, self.num_classes)
                        self.init_out = out
                        self.init_l1 = l1
                        if self.selection_type == 'PerBatch':
                            l0_grads = l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                l1_grads = l1_grads.mean(dim=0).view(1, -1)                        
                        
                    else:
                        out, l1 = self.model(ul_strong_aug, last=True, freeze=True)
                        if loss_name == 'MeanSquared':
                            temp_out = F.softmax(out, dim=1)
                            loss = self.loss(temp_out, self.weak_targets[batch_idx], self.weak_masks[batch_idx]).sum()
                        else:
                            loss = self.loss(out, self.weak_targets[batch_idx], self.weak_masks[batch_idx]).sum()
                        batch_l0_grads = torch.autograd.grad(loss, out)[0]
                        if self.linear_layer:
                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, embDim, dim=1)
                            batch_l1_grads = batch_l0_expand * l1.repeat(1, self.num_classes)
                        
                        if self.selection_type == 'PerBatch':
                            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)
                            if self.linear_layer:
                                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)

                        l0_grads = torch.cat((l0_grads, batch_l0_grads), dim=0)
                        if self.linear_layer:
                            l1_grads = torch.cat((l1_grads, batch_l1_grads), dim=0)
                        self.init_out = torch.cat((self.init_out, out), dim=0)
                        self.init_l1 = torch.cat((self.init_l1, l1), dim=0)
            elif grads_currX is not None:
                out_vec = self.init_out - (
                            self.eta * grads_currX[0][0:self.num_classes].view(1, -1).expand(self.init_out.shape[0],-1))

                if self.linear_layer:
                    out_vec = out_vec - (self.eta * torch.matmul(self.init_l1, grads_currX[0][self.num_classes:].view(
                        self.num_classes, -1).transpose(0, 1)))

                if loss_name == 'MeanSquared':
                    temp_out_vec = F.softmax(out_vec, dim=1)
                    loss = self.loss(temp_out_vec, torch.cat(self.weak_targets, dim=0), torch.cat(self.weak_masks, dim=0)).sum()
                else:
                    loss = self.loss(out_vec, torch.cat(self.weak_targets, dim=0),
                                     torch.cat(self.weak_masks, dim=0)).sum()
                l0_grads = torch.autograd.grad(loss, out_vec)[0]
                if self.linear_layer:
                    l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)
                    l1_grads = l0_expand * self.init_l1.repeat(1, self.num_classes)
                
                if self.selection_type == 'PerBatch':
                    b = int(l0_grads.shape[0]/self.valloader.batch_size)
                    l0_grads = torch.chunk(l0_grads, b, dim=0)
                    new_t = []
                    for i in range(len(l0_grads)):
                        new_t.append(torch.mean(l0_grads[i], dim=0).view(1, -1))
                    l0_grads = torch.cat(new_t, dim=0)
                    if self.linear_layer:
                        l1_grads = torch.chunk(l1_grads, b, dim=0)
                        new_t = []
                        for i in range(len(l1_grads)):
                            new_t.append(torch.mean(l1_grads[i], dim=0).view(1, -1))
                        l1_grads = torch.cat(new_t, dim=0)
            torch.cuda.empty_cache()
            if self.linear_layer:
                self.grads_val_curr = torch.mean(torch.cat((l0_grads, l1_grads), dim=1), dim=0).view(-1, 1)
            else:
                self.grads_val_curr = torch.mean(l0_grads, dim=0).view(-1, 1)

    def eval_taylor_modular(self, grads):
        """
        Evaluate gradients

        Parameters
        ----------
        grads: Tensor
            Gradients

        Returns
        ----------
        gains: Tensor
            Matrix product of two tensors
        """

        grads_val = self.grads_val_curr
        with torch.no_grad():
            gains = torch.matmul(grads, grads_val)
        return gains

    def _update_gradients_subset(self, grads_X, element):
        """
        Update gradients of set X + element (basically adding element to X)
        Note that it modifies the inpute vector! Also grads_X is a list! grad_e is a tuple!

        Parameters
        ----------
        grads_X: list
            Gradients
        element: int
            Element that need to be added to the gradients
        """
        # if isinstance(element, list):
        grads_X += self.grads_per_elem[element].sum(dim=0)

    def greedy_algo(self, budget):
        """
        Implement various greedy algorithms for data subset selection.

        Parameters
        ----------
        budget: int
            Budget of data points that needs to be sampled
        """
        greedySet = list()
        N = self.grads_per_elem.shape[0]
        remainSet = list(range(N))
        t_ng_start = time.time()  # naive greedy start time
        numSelected = 0
        if self.greedy == 'RGreedy':
            # subset_size = int((len(self.grads_per_elem) / r))
            selection_size = int(budget / self.r)
            while (numSelected < budget):
                # Try Using a List comprehension here!
                rem_grads = self.grads_per_elem[remainSet]
                gains = self.eval_taylor_modular(rem_grads)
                # Update the greedy set and remaining set
                sorted_gains, indices = torch.sort(gains.view(-1), descending=True)
                selected_indices = [remainSet[index.item()] for index in indices[0:selection_size]]
                greedySet.extend(selected_indices)
                [remainSet.remove(idx) for idx in selected_indices]
                if numSelected == 0:
                    grads_curr = self.grads_per_elem[selected_indices].sum(dim=0).view(1, -1)
                else:  # If 1st selection, then just set it to bestId grads
                    self._update_gradients_subset(grads_curr, selected_indices)
                # Update the grads_val_current using current greedySet grads
                self._update_grads_val(grads_curr)
                numSelected += selection_size
            self.logger.debug("RETRIEVE's R-greedy selection time: %f", time.time() - t_ng_start)

        # Stochastic Greedy Selection Algorithm
        elif self.greedy == 'Stochastic':
            subset_size = int((len(self.grads_per_elem) / budget) * math.log(100))
            while (numSelected < budget):
                # Try Using a List comprehension here!
                subset_selected = random.sample(remainSet, k=subset_size)
                rem_grads = self.grads_per_elem[subset_selected]
                gains = self.eval_taylor_modular(rem_grads)
                # Update the greedy set and remaining set
                _, indices = torch.sort(gains.view(-1), descending=True)
                bestId = [subset_selected[indices[0].item()]]
                greedySet.append(bestId[0])
                remainSet.remove(bestId[0])
                numSelected += 1
                # Update debug in grads_currX using element=bestId
                if numSelected > 1:
                    self._update_gradients_subset(grads_curr, bestId)
                else:  # If 1st selection, then just set it to bestId grads
                    grads_curr = self.grads_per_elem[bestId].view(1, -1)  # Making it a list so that is mutable!
                # Update the grads_val_current using current greedySet grads
                self._update_grads_val(grads_curr)
            self.logger.debug("RETRIEVE's Stochastic Greedy selection time: %f", time.time() - t_ng_start)

        elif self.greedy == 'Naive':
            while (numSelected < budget):
                # Try Using a List comprehension here!
                rem_grads = self.grads_per_elem[remainSet]
                gains = self.eval_taylor_modular(rem_grads)
                # Update the greedy set and remaining set
                # _, maxid = torch.max(gains, dim=0)
                _, indices = torch.sort(gains.view(-1), descending=True)
                bestId = [remainSet[indices[0].item()]]
                greedySet.append(bestId[0])
                remainSet.remove(bestId[0])
                numSelected += 1
                # Update debug in grads_currX using element=bestId
                if numSelected == 1:
                    grads_curr = self.grads_per_elem[bestId[0]].view(1, -1)
                else:  # If 1st selection, then just set it to bestId grads
                    self._update_gradients_subset(grads_curr, bestId)
                # Update the grads_val_current using current greedySet grads
                self._update_grads_val(grads_curr)
            self.logger.debug("RETRIEVE's Naive Greedy selection time: %f", time.time() - t_ng_start)
        return list(greedySet), [1] * budget

    def select(self, budget, model_params, tea_model_params):
        """
        Apply naive greedy method for data selection

        Parameters
        ----------
        budget: int
            The number of data points to be selected
        model_params: OrderedDict
            Python dictionary object containing model's parameters
        tea_model_params: OrderedDict
            Python dictionary object containing teacher model's parameters

        Returns
        ----------
        greedySet: list
            List containing indices of the best datapoints,
        budget: Tensor
            Tensor containing gradients of datapoints present in greedySet
        """
        glister_start_time = time.time() # naive greedy start time
        self.update_model(model_params, tea_model_params)
        if self.selection_type == 'PerClass':
            self.get_labels(valid=True)
            idxs = []
            gammas = []
            for i in range(self.num_classes):
                trn_subset_idx = torch.where(self.trn_lbls == i)[0].tolist()
                trn_data_sub = Subset(self.trainloader.dataset, trn_subset_idx)
                self.pctrainloader = DataLoader(trn_data_sub, batch_size=self.trainloader.batch_size,
                                                shuffle=False, pin_memory=True)
                
                val_subset_idx = torch.where(self.val_lbls == i)[0].tolist()
                val_data_sub = Subset(self.valloader.dataset, val_subset_idx)
                self.pcvalloader = DataLoader(val_data_sub, batch_size=self.trainloader.batch_size,
                                            shuffle=False, pin_memory=True)
                if self.valid:
                    self.compute_gradients(store_t=False, perClass=True)
                else:
                    self.compute_gradients(store_t=True, perClass=True)
                
                self._update_grads_val(first_init=True)
                idxs_temp, gammas_temp = self.greedy_algo(math.ceil(budget * len(trn_subset_idx) / self.N_trn))
                idxs.extend(list(np.array(trn_subset_idx)[idxs_temp]))
                gammas.extend(gammas_temp)
        elif self.selection_type == 'PerBatch':
            idxs = []
            gammas = []
            if self.valid:
                self.compute_gradients(store_t=False, perBatch=True)
            else:
                self.compute_gradients(store_t=True, perBatch=True)
            self._update_grads_val(first_init=True)
            idxs_temp, gammas_temp = self.greedy_algo(math.ceil(budget/self.trainloader.batch_size))
            batch_wise_indices = list(self.trainloader.batch_sampler)
            for i in range(len(idxs_temp)):
                tmp = batch_wise_indices[idxs_temp[i]]
                idxs.extend(tmp)
                gammas.extend([gammas_temp[i]] * len(tmp))
        else:
            if self.valid:
                self.compute_gradients(store_t=False)
            else:
                self.compute_gradients(store_t=True)
            self._update_grads_val(first_init=True)
            idxs, gammas = self.greedy_algo(budget)
        glister_end_time = time.time()
        self.logger.debug("RETRIEVE algorithm Subset Selection time is: %f", glister_end_time - glister_start_time)
        return idxs, torch.FloatTensor(gammas)


File Path: cords/selectionstrategies/__init__.py
Content:

File Path: cords/selectionstrategies/helpers/__init__.py
Content:
from .omp_solvers import OrthogonalMP_REG_Parallel
from .omp_solvers import OrthogonalMP_REG_Parallel_V1
from .omp_solvers import OrthogonalMP_REG
from .omp_solvers import OrthogonalMP_REG_NNLS_Parallel
from .omp_solvers import OrthogonalMP_REG_NNLS
from .optimalWeights import OptimalWeights

File Path: cords/selectionstrategies/helpers/omp_solvers.py
Content:
import numpy as np

np.seterr(all='raise')
from numpy.linalg import cond
from numpy.linalg import inv
from numpy.linalg import norm
from scipy import sparse as sp
from scipy.linalg import lstsq
from scipy.linalg import solve
from scipy.optimize import nnls
import torch


# NOTE: Textbook Primal-Dual IPM: Boyd & Vandenberghe, ``Chapter 11: Interior-point Methods," Convex Optimization, 2004.
# NOTE: Works on toy problems but struggles in word embedding recovery setting (n>10000).
def NonnegativeBP(A, b, x0=None, tol=1E-4, niter=100, biter=32):
    '''solves min |x|_1 s.t. Ax=b,x>=0 using a Primal-Dual Interior Point Method
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      x0: starting point; if None sets x0=A^T(AA^T)^(-1)b
      tol: solver tolerance
      niter: maximum length of central path
      biter: maximum number of steps in backtracking line search
    Returns:
      vector of length n
    '''

    AT = A.T
    d, n = A.shape
    alpha = 0.01
    beta = 0.5
    mu = 20
    e = np.ones(n)

    if x0 is None:
        x = tol / np.sqrt(n) * e
        x += nnls(A, b - A.dot(x))[0]
    else:
        x = np.copy(x0)
    lam = 1.0 / x
    v = -A.dot(lam)
    t = mu * d
    rp = A.dot(x) - b
    rd = 1.0 - lam + AT.dot(v)

    for i in range(niter):

        oot = 1.0 / t
        rc = lam * x - oot
        resnorm = np.sqrt(norm(rd) ** 2 + norm(rc) ** 2 + norm(rp) ** 2)

        try:
            dv = solve(A.dot(AT * (x / lam)[:, np.newaxis]), rp - A.dot((rc + x * rd) / lam), assume_a='pos')
        except np.linalg.linalg.LinAlgError:
            return x
        dlam = AT.dot(dv) + rd
        dx = -(rc + x * dlam) / lam

        ind = np.less(dlam, 0.0)
        s = 0.99 * min(1.0, min(-lam[ind] / dlam[ind])) if np.any(ind) else 0.99
        for j in range(biter):
            xp = x + s * dx
            lamp = lam + s * dlam
            vp = v + s * dv
            s *= beta
            if all(xp > 0.0) and np.sqrt(
                    norm(1.0 - lamp + AT.dot(vp)) ** 2 + norm(lamp * xp - oot) ** 2 + norm(A.dot(xp) - b) ** 2) <= (
                    1.0 - alpha * s) * resnorm:
                break
        else:
            break

        eta = np.inner(lam, x)
        rp = A.dot(xp) - b
        rd = 1.0 - lamp + AT.dot(vp)
        if max(eta, norm(rp), norm(rd)) < tol:
            return x
        x = xp
        lam = lamp
        v = vp
        t = mu * d / eta

    return x


# NOTE: Ported to Python from l1-MAGIC: Cand\'es & Romberg, ``l_1-MAGIC: Recovery of Sparse Signals via Convex Programming," Technical Report, 2005.
def BasisPursuit(A, b, x0=None, ATinvAAT=None, positive=False, tol=1E-4, niter=100, biter=32):
    '''solves min |x|_1 s.t. Ax=b using a Primal-Dual Interior Point Method
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      x0: starting point; if None sets x0=A^T(AA^T)^(-1)b
      ATinvAAT: precomputed matrix A^T(AA^T)^(-1); computed if None; ignored if not x0 is None
      positive: only allow positive nonzero coefficients
      tol: solver tolerance
      niter: maximum length of central path
      biter: maximum number of steps in backtracking line search
    Returns:
      vector of length n
    '''

    if positive:
        return NonnegativeBP(A, b, x0=x0, tol=tol, niter=niter, biter=biter)

    AT = A.T
    d, n = A.shape
    alpha = 0.01
    beta = 0.5
    mu = 10
    e = np.ones(n)
    gradf0 = np.hstack([np.zeros(n), e])

    if x0 is None:
        if ATinvAAT is None:
            ATinvAAT = AT.dot(inv(A.dot(AT)))
        x = ATinvAAT.dot(b)
    else:
        x = np.copy(x0)
    absx = np.abs(x)
    u = 0.95 * absx + 0.1 * max(absx)

    fu1 = x - u
    fu2 = -x - u
    lamu1 = -1.0 / fu1
    lamu2 = -1.0 / fu2
    v = A.dot(lamu2 - lamu1)
    ATv = AT.dot(v)
    sdg = -(np.inner(fu1, lamu1) + np.inner(fu2, lamu2))
    tau = 2.0 * n * mu / sdg
    ootau = 1.0 / tau

    rcent = np.hstack([-lamu1 * fu1, -lamu2 * fu2]) - ootau
    rdual = gradf0 + np.hstack([lamu1 - lamu2 + ATv, -lamu1 - lamu2])
    rpri = A.dot(x) - b
    resnorm = np.sqrt(norm(rdual) ** 2 + norm(rcent) ** 2 + norm(rpri) ** 2)
    rdp = np.empty(2 * n)
    rcp = np.empty(2 * n)

    for i in range(niter):

        oofu1 = 1.0 / fu1
        oofu2 = 1.0 / fu2
        w1 = -ootau * (oofu2 - oofu1) - ATv
        w2 = -1.0 - ootau * (oofu1 + oofu2)
        w3 = -rpri

        lamu1xoofu1 = lamu1 * oofu1
        lamu2xoofu2 = lamu2 * oofu2
        sig1 = -lamu1xoofu1 - lamu2xoofu2
        sig2 = lamu1xoofu1 - lamu2xoofu2
        sigx = sig1 - sig2 ** 2 / sig1
        if min(np.abs(sigx)) == 0.0:
            break

        w1p = -(w3 - A.dot(w1 / sigx - w2 * sig2 / (sigx * sig1)))
        H11p = A.dot(AT * (e / sigx)[:, np.newaxis])
        if min(sigx) > 0.0:
            dv = solve(H11p, w1p, assume_a='pos')
        else:
            dv = solve(H11p, w1p, assume_a='sym')
        dx = (w1 - w2 * sig2 / sig1 - AT.dot(dv)) / sigx
        Adx = A.dot(dx)
        ATdv = AT.dot(dv)

        du = (w2 - sig2 * dx) / sig1
        dlamu1 = lamu1xoofu1 * (du - dx) - lamu1 - ootau * oofu1
        dlamu2 = lamu2xoofu2 * (dx + du) - lamu2 - ootau * oofu2

        s = 1.0
        indp = np.less(dlamu1, 0.0)
        indn = np.less(dlamu2, 0.0)
        if np.any(indp):
            s = min(s, min(-lamu1[indp] / dlamu1[indp]))
        if np.any(indn):
            s = min(s, min(-lamu2[indn] / dlamu2[indn]))
        indp = np.greater(dx - du, 0.0)
        indn = np.greater(-dx - du, 0.0)
        if np.any(indp):
            s = min(s, min(-fu1[indp] / (dx[indp] - du[indp])))
        if np.any(indn):
            s = min(s, min(-fu2[indn] / (-dx[indn] - du[indn])))
        s = 0.99 * s

        for j in range(biter):
            xp = x + s * dx
            up = u + s * du
            vp = v + s * dv
            ATvp = ATv + s * ATdv
            lamu1p = lamu1 + s * dlamu1
            lamu2p = lamu2 + s * dlamu2
            fu1p = xp - up
            fu2p = -xp - up
            rdp[:n] = lamu1p - lamu2p + ATvp
            rdp[n:] = -lamu1p - lamu2p
            rdp += gradf0
            rcp[:n] = -lamu1p * fu1p
            rcp[n:] = lamu2p * fu2p
            rcp -= ootau
            rpp = rpri + s * Adx
            s *= beta
            if np.sqrt(norm(rdp) ** 2 + norm(rcp) ** 2 + norm(rpp) ** 2) <= (1 - alpha * s) * resnorm:
                break
        else:
            break

        x = xp
        lamu1 = lamu1p
        lamu2 = lamu2p
        fu1 = fu1p
        fu2 = fu2p
        sdg = -(np.inner(fu1, lamu1) + np.inner(fu2, lamu2))
        if sdg < tol:
            return x

        u = up
        v = vp
        ATv = ATvp
        tau = 2.0 * n * mu / sdg
        rpri = rpp
        rcent[:n] = lamu1 * fu1
        rcent[n:] = lamu2 * fu2
        ootau = 1.0 / tau
        rcent -= ootau
        rdual[:n] = lamu1 - lamu2 + ATv
        rdual[n:] = -lamu1 + lamu2
        rdual += gradf0
        resnorm = np.sqrt(norm(rdual) ** 2 + norm(rcent) ** 2 + norm(rpri) ** 2)

    return x


BP = BasisPursuit


# NOTE: Standard Algorithm, e.g. Tropp, ``Greed is Good: Algorithmic Results for Sparse Approximation," IEEE Trans. Info. Theory, 2004.
def OrthogonalMP(A, b, tol=1E-4, nnz=None, positive=False):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Orthogonal Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
    Returns:
       vector of length n
    '''

    AT = A.T
    d, n = A.shape
    if nnz is None:
        nnz = n
    x = np.zeros(n)
    resid = np.copy(b)
    normb = norm(b)
    indices = []

    for i in range(nnz):
        if norm(resid) / normb < tol:
            break
        projections = AT.dot(resid)
        if positive:
            index = np.argmax(projections)
        else:
            index = np.argmax(abs(projections))
        if index in indices:
            break
        indices.append(index)
        if len(indices) == 1:
            A_i = A[:, index]
            x_i = projections[index] / A_i.T.dot(A_i)
        else:
            A_i = np.vstack([A_i, A[:, index]])
            x_i = solve(A_i.dot(A_i.T), A_i.dot(b), assume_a='sym')
            if positive:
                while min(x_i) < 0.0:
                    argmin = np.argmin(x_i)
                    indices = indices[:argmin] + indices[argmin + 1:]
                    A_i = np.vstack([A_i[:argmin], A_i[argmin + 1:]])
                    x_i = solve(A_i.dot(A_i.T), A_i.dot(b), assume_a='sym')
        resid = b - A_i.T.dot(x_i)

    for i, index in enumerate(indices):
        try:
            x[index] += x_i[i]
        except IndexError:
            x[index] += x_i
    return x


# NOTE: Standard Algorithm, e.g. Tropp, ``Greed is Good: Algorithmic Results for Sparse Approximation," IEEE Trans. Info. Theory, 2004.
def OrthogonalMP_REG_NNLS(A, b, tol=1E-4, nnz=None, positive=False, lam=1):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Orthogonal Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
    Returns:
       vector of length n
    '''

    AT = A.T
    d, n = A.shape
    if nnz is None:
        nnz = n
    x = np.zeros(n)
    resid = np.copy(b)
    normb = norm(b)
    indices = []

    for i in range(nnz):
        if norm(resid) / normb < tol:
            break
        projections = AT.dot(resid)
        if positive:
            index = np.argmax(projections)
        else:
            index = np.argmax(abs(projections))
        if index in indices:
            break
        indices.append(index)
        if len(indices) == 1:
            A_i = A[:, index]
            x_i = projections[index] / A_i.T.dot(A_i)
        else:
            A_i = np.vstack([A_i, A[:, index]])
            if positive:
                x_i = nnls(A_i.dot(A_i.T) + lam * np.identity(A_i.shape[0]), A_i.dot(b))[0]
            else:
                x_i = lstsq(A_i.dot(A_i.T) + lam * np.identity(A_i.shape[0]), A_i.dot(b))[0]
                # print(x_i)
        # print(b.shape,A_i.T.shape,x_i.shape)
        resid = b - A_i.T.dot(x_i)

    for i, index in enumerate(indices):
        try:
            x[index] += x_i[i]
        except IndexError:
            x[index] += x_i
    return x


# NOTE: Standard Algorithm, e.g. Tropp, ``Greed is Good: Algorithmic Results for Sparse Approximation," IEEE Trans. Info. Theory, 2004.
def OrthogonalMP_REG(A, b, tol=1E-4, nnz=None, positive=False, lam=1):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Orthogonal Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
    Returns:
       vector of length n
    '''
    AT = A.T
    d, n = A.shape
    if nnz is None:
        nnz = n
    x = np.zeros(n)
    resid = np.copy(b)
    normb = norm(b)
    indices = []

    for i in range(nnz):
        if norm(resid) / normb < tol:
            break
        projections = AT.dot(resid)
        if positive:
            index = np.argmax(projections)
        else:
            index = np.argmax(abs(projections))
        if index in indices:
            break
        indices.append(index)
        if len(indices) == 1:
            A_i = A[:, index]
            x_i = projections[index] / A_i.T.dot(A_i)
        else:
            A_i = np.vstack([A_i, A[:, index]])
            x_i = lstsq(A_i.dot(A_i.T) + lam * np.identity(A_i.shape[0]), A_i.dot(b))[0]
            # print(x_i.shape)
            if positive:
                while min(x_i) < 0.0:
                    # print("Negative",b.shape,A_i.T.shape,x_i.shape)
                    argmin = np.argmin(x_i)
                    indices = indices[:argmin] + indices[argmin + 1:]
                    A_i = np.vstack([A_i[:argmin], A_i[argmin + 1:]])
                    x_i = lstsq(A_i.dot(A_i.T) + lam * np.identity(A_i.shape[0]), A_i.dot(b))[0]
        resid = b - A_i.T.dot(x_i)
    for i, index in enumerate(indices):
        try:
            x[index] += x_i[i]
        except IndexError:
            x[index] += x_i
    return x


# NOTE: Standard Algorithm, e.g. Tropp, ``Greed is Good: Algorithmic Results for Sparse Approximation," IEEE Trans. Info. Theory, 2004.
def OrthogonalMP_REG_Parallel_V1(A, b, tol=1E-4, nnz=None, positive=False, lam=1, device="cpu"):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Orthogonal Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
    Returns:
       vector of length n
    '''
    AT = torch.transpose(A, 0, 1)
    d, n = A.shape
    if nnz is None:
        nnz = n
    x = torch.zeros(n, device=device)  # ,dtype=torch.float64)
    resid = b.detach().clone()
    normb = b.norm().item()
    indices = []

    argmin = torch.tensor([-1])
    for i in range(nnz):
        if resid.norm().item() / normb < tol:
            break
        projections = torch.matmul(AT, resid)  # AT.dot(resid)
        # print("Projections",projections.shape)

        if positive:
            index = torch.argmax(projections)
        else:
            index = torch.argmax(torch.abs(projections))

        if index not in indices:
            indices.append(index)

        if len(indices) == 1:
            A_i = A[:, index]
            x_i = projections[index] / torch.dot(A_i, A_i).view(-1)  # A_i.T.dot(A_i)
            A_i = A[:, index].view(1, -1)
        else:
            # print(indices)
            A_i = torch.cat((A_i, A[:, index].view(1, -1)), dim=0)  # np.vstack([A_i, A[:,index]])
            temp = torch.matmul(A_i, torch.transpose(A_i, 0, 1)) + lam * torch.eye(A_i.shape[0], device=device)
            x_i, _, _, _ = torch.linalg.lstsq(temp, torch.matmul(A_i, b).view(-1, 1))
            # print(x_i.shape)
            if positive:
                while min(x_i) < 0.0:
                    # print("Negative",b.shape,torch.transpose(A_i, 0, 1).shape,x_i.shape)
                    argmin = torch.argmin(x_i)
                    indices = indices[:argmin] + indices[argmin + 1:]
                    A_i = torch.cat((A_i[:argmin], A_i[argmin + 1:]),
                                    dim=0)  # np.vstack([A_i[:argmin], A_i[argmin+1:]])
                    temp = torch.matmul(A_i, torch.transpose(A_i, 0, 1)) + lam * torch.eye(A_i.shape[0], device=device)
                    x_i, _, _, _ = torch.linalg.lstsq(temp, torch.matmul(A_i, b).view(-1, 1))
        resid = b - torch.matmul(torch.transpose(A_i, 0, 1), x_i).view(-1)  # A_i.T.dot(x_i)
    x_i = x_i.view(-1)
    for i, index in enumerate(indices):
        try:
            x[index] += x_i[i]
        except IndexError:
            x[index] += x_i
    return x


# NOTE: Standard Algorithm, e.g. Tropp, ``Greed is Good: Algorithmic Results for Sparse Approximation," IEEE Trans. Info. Theory, 2004.
def OrthogonalMP_REG_Parallel(A, b, tol=1E-4, nnz=None, positive=False, lam=1, device="cpu"):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Orthogonal Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
    Returns:
       vector of length n
    '''
    AT = torch.transpose(A, 0, 1)
    d, n = A.shape
    if nnz is None:
        nnz = n
    x = torch.zeros(n, device=device)  # ,dtype=torch.float64)
    resid = b.detach().clone()
    normb = b.norm().item()
    indices = []
    argmin = torch.tensor([-1])
    for i in range(nnz):
        if resid.norm().item() / normb < tol:
            break
        projections = torch.matmul(AT, resid)  # AT.dot(resid)
        # print("Projections",projections.shape)
        if positive:
            index = torch.argmax(projections)
        else:
            index = torch.argmax(torch.abs(projections))
        if index in indices:
            break
        indices.append(index)
        if len(indices) == 1:
            A_i = A[:, index]
            x_i = projections[index] / torch.dot(A_i, A_i).view(-1)  # A_i.T.dot(A_i)
            A_i = A[:, index].view(1, -1)
        else:
            # print(indices)
            A_i = torch.cat((A_i, A[:, index].view(1, -1)), dim=0)  # np.vstack([A_i, A[:,index]])
            temp = torch.matmul(A_i, torch.transpose(A_i, 0, 1)) + lam * torch.eye(A_i.shape[0], device=device)
            x_i, _, _, _ = torch.linalg.lstsq(temp, torch.matmul(A_i, b).view(-1, 1))
            # print(x_i.shape)
            if positive:

                while min(x_i) < 0.0:
                    # print("Negative",b.shape,torch.transpose(A_i, 0, 1).shape,x_i.shape)
                    argmin = torch.argmin(x_i)
                    indices = indices[:argmin] + indices[argmin + 1:]
                    A_i = torch.cat((A_i[:argmin], A_i[argmin + 1:]),
                                    dim=0)  # np.vstack([A_i[:argmin], A_i[argmin+1:]])
                    if argmin.item() == A_i.shape[0]:
                        break
                    # print(argmin.item(),A_i.shape[0],index.item())
                    temp = torch.matmul(A_i, torch.transpose(A_i, 0, 1)) + lam * torch.eye(A_i.shape[0], device=device)
                    x_i, _, _, _ = torch.linalg.lstsq(temp, torch.matmul(A_i, b).view(-1, 1))
        if argmin.item() == A_i.shape[0]:
            break
        # print(b.shape,torch.transpose(A_i, 0, 1).shape,x_i.shape,\
        #  torch.matmul(torch.transpose(A_i, 0, 1), x_i).shape)
        resid = b - torch.matmul(torch.transpose(A_i, 0, 1), x_i).view(-1)  # A_i.T.dot(x_i)
        # print("REsID",resid.shape)

    x_i = x_i.view(-1)
    # print(x_i.shape)
    # print(len(indices))
    for i, index in enumerate(indices):
        # print(i,index,end="\t")
        try:
            x[index] += x_i[i]
        except IndexError:
            x[index] += x_i
    # print(x[indices])
    return x


# NOTE: Standard Algorithm, e.g. Tropp, ``Greed is Good: Algorithmic Results for Sparse Approximation," IEEE Trans. Info. Theory, 2004.
def OrthogonalMP_REG_NNLS_Parallel(A, b, tol=1E-4, nnz=None, positive=False, lam=1, device="cpu"):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Orthogonal Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
    Returns:
       vector of length n
    '''
    AT = torch.transpose(A, 0, 1)
    d, n = A.shape
    if nnz is None:
        nnz = n
    x = torch.zeros(n, device=device)  # ,dtype=torch.float64)
    resid = b.detach().clone()
    normb = b.norm().item()
    indices = []
    argmin = torch.tensor([-1])
    for i in range(nnz):
        # if resid.norm().item() / normb < tol:
        #     break
        projections = torch.matmul(AT, resid)  # AT.dot(resid)
        # print("Projections",projections.shape)
        if positive:
            index = torch.argmax(projections)
        else:
            index = torch.argmax(torch.abs(projections))
        if index in indices:
            break
        indices.append(index)
            #break
        if len(indices) == 1:
            A_i = A[:, index]
            x_i = projections[index] / torch.dot(A_i, A_i).view(-1)  # A_i.T.dot(A_i)
            A_i = A[:, index].view(1, -1)
        else:
            # print(indices)
            A_i = torch.cat((A_i, A[:, index].view(1, -1)), dim=0)  # np.vstack([A_i, A[:,index]])
            temp = torch.matmul(A_i, torch.transpose(A_i, 0, 1)) + lam * torch.eye(A_i.shape[0], device=device)
            if positive:
                x_i, _ = nnls(temp.cpu().numpy(), torch.matmul(A_i, b).view(-1).cpu().numpy())
                x_i = torch.from_numpy(x_i).float().to(device=device)
            else:
                x_i, _, _, _ = torch.linalg.lstsq(temp, torch.matmul(A_i, b).view(-1, 1))
        resid = b - torch.matmul(torch.transpose(A_i, 0, 1), x_i).view(-1)  # A_i.T.dot(x_i)
    x_i = x_i.view(-1)
    for i, index in enumerate(indices):
        try:
            x[index] += x_i[i]
        except IndexError:
            x[index] += x_i
    # print(x[indices])
    return x


# NOTE: Standard Algorithm, e.g. Tropp, ``Greed is Good: Algorithmic Results for Sparse Approximation," IEEE Trans. Info. Theory, 2004.
def MatchingPursuit(A, b, tol=1E-4, nnz=None, positive=False, orthogonal=False):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
      orthogonal: use Orthogonal Matching Pursuit (OMP)
    Returns:
       vector of length n
    '''

    if orthogonal:
        return OrthogonalMP(A, b, tol=tol, nnz=nnz, positive=positive)

    AT = A.T
    d, n = A.shape
    if nnz is None:
        nnz = n
    x = np.zeros(n)
    resid = np.copy(b)
    normb = norm(b)
    selected = np.zeros(n, dtype=np.bool)

    for i in range(nnz):
        if norm(resid) / normb < tol:
            break
        projections = AT.dot(resid)
        projections[selected] = 0.0
        if positive:
            index = np.argmax(projections)
        else:
            index = np.argmax(abs(projections))
        atom = AT[index]
        coef = projections[index] / norm(A[:, index])
        if positive and coef <= 0.0:
            break
        resid -= coef * atom
        x[index] = coef
        selected[index] = True
    return x


MP = MatchingPursuit


def outer_product_cache(X, limit=float('inf')):
    '''cache method for computing and storing outer products
    Args:
      X: matrix of row vectors
      limit: stops storing outer products after cache contains this many elements
    Returns:
      function that computes outer product of row with itself given its index
    '''

    cache = {}

    def outer_product(i):
        output = cache.get(i)
        if output is None:
            output = np.outer(X[i], X[i])
            if len(cache) < limit:
                cache[i] = output
        return output

    return outer_product


def binary_line_search(x, dx, f, nsplit=16):
    '''computes update coefficient using binary line search
    Args:
      x: current position
      dx: full step
      f: objective function
      nsplit: how many binary splits to perform when doing line search
    Returns:
      (coefficient, whether any coefficient was found to improve objective)
    '''

    obj = f(x)
    alpha = 0.0
    failed = True
    increment = True
    while increment:
        alpha += 0.5
        for i in range(2, nsplit + 1):
            step = x + alpha * dx
            objstep = f(step)
            if objstep < obj:
                alpha += 2.0 ** -i
                obj = objstep
                failed = False
            else:
                alpha -= 2.0 ** -i
                increment = False
    return alpha, failed


# NOTE: Hybrid (1st & 2nd Order) Method Based on Boyd & Vandenberghe, ``Chapter 10: Equality-Contrained Minimization," Convex Optimization, 2004.
def SupportingHyperplaneProperty(x, A, niter=None, eps=1.0, nsplit=16):
    '''checks SHP property by solving min_h sum(max{Ch+eps,0}^2) s.t. Sh=0, where C=(A_{supp(x)^C}^T 1) and S=(A_supp(x)^T 1)
    Args:
      x: nonnegative vector of length n
      A: matrix of size (d, n)
      niter: give up after this many iterations; if None sets niter=n
      eps: separation of non-support vertices from support supporting hyperplane
      nsplit: how many binary splits to perform when doing line search
    Returns:
      hyperplane (d+1-dimensional vector, last dimension the negative intercept) supporting columns of A in support of x, if one exists; otherwise False
    '''

    assert not (x < 0).sum(), "signal (x) must be nonnegative"
    assert eps > 0.0, "separation (eps) must be positive"
    d, n = A.shape
    A = np.append(A, np.zeros((d, 1)), axis=1)
    n += 1
    if niter is None:
        niter = d
    if type(x) != np.ndarray:
        x = x.toarray()[0]
    x = np.append(x, 0)
    nz = np.where(x > 0)[0]
    z = np.where(x == 0)[0]
    nnz = nz.shape[0]
    C = np.hstack([A[:, z].T, np.ones((n - nnz, 1))])
    AST = A[:, nz].T
    h, ssr, _, _ = lstsq(AST, np.ones(nnz))
    if ssr:
        return False
    h = np.append(h, -1.0)
    Ch = C.dot(h)
    if all(Ch < 0.0):
        return h
    S = np.hstack([AST, np.ones((nnz, 1))])
    ST = S.T
    correction = (ST.dot(inv(S.dot(ST)).dot(S)) - np.eye(d + 1))
    objective = lambda Chpeps: sum(Chpeps[Chpeps > 0.0] ** 3)
    b = np.zeros(d + 1 + nnz)
    outer_product = outer_product_cache(C, 1E10 / d ** 2)

    for i in range(niter):

        Chpeps = Ch + eps
        v = np.where(Ch > - eps)
        gradient = (3.0 * Chpeps[v] ** 2).dot(C[v])
        if i:
            if i == 1:
                M = np.zeros((d + 1 + nnz, d + 1 + nnz))
                M[d + 1:, :d + 1] = S
                M[:d + 1, d + 1:] = ST
            M[:d + 1, :d + 1] = 6.0 * sum(c * outer_product(j) for c, j in zip(Chpeps[v], v[0]))
            if 1.0 / cond(M) < 1E-16:
                step = correction.dot(gradient)
            else:
                b[:d + 1] = -gradient
                step = solve(M, b, assume_a='sym')[:d + 1]
        else:
            step = correction.dot(gradient)

        Cstep = C.dot(step)
        alpha, failed = binary_line_search(Chpeps, Cstep, objective, nsplit=nsplit)
        if failed:
            break
        h += alpha * step
        Ch = C.dot(h)
        if all(Ch < 0.0):
            return h

    return False

File Path: cords/selectionstrategies/helpers/optimalWeights.py
Content:
import numpy as np

np.seterr(all='raise')
from numpy.linalg import cond
from numpy.linalg import inv
from numpy.linalg import norm
from scipy import sparse as sp
from scipy.linalg import lstsq
from scipy.linalg import solve
from scipy.optimize import nnls

import torch



def OptimalWeights(A, b, tol=1E-4, nnz=None, device="cpu"):
    '''approximately solves min_x |x|_0 s.t. Ax=b using Orthogonal Matching Pursuit
    Args:
      A: design matrix of size (d, n)
      b: measurement vector of length d
      tol: solver tolerance
      nnz = maximum number of nonzero coefficients (if None set to n)
      positive: only allow positive nonzero coefficients
    Returns:
       vector of length n
    '''

    sum_sel_grad = torch.zeros_like(b,device= device)
    w = 1.0

    final_indices = []
    remainList = [i for i in range(A.shape[0])]

    b_norm = b.norm()

    for i in range(nnz):

        projection = (A + sum_sel_grad - w*b).norm(dim=1)
        index = torch.argmin(projection).item()

        sum_sel_grad += A[index]
        w = torch.dot(A[index],sum_sel_grad)/b_norm

        actual_idx = remainList[index]
        final_indices.append(actual_idx)

        remainList.remove(actual_idx)        
        A = torch.cat((A[:index], A[index + 1:]), dim=0)

    return final_indices, [w for _ in range(nnz)]


File Path: cords/selectionstrategies/helpers/ssl_lib/__init__.py
Content:

File Path: cords/selectionstrategies/helpers/ssl_lib/algs/__init__.py
Content:

File Path: cords/selectionstrategies/helpers/ssl_lib/algs/builder.py
Content:
from .ict import ICT
from .consistency import ConsistencyRegularization
from .pseudo_label import PseudoLabel
from .vat import VAT


def gen_ssl_alg(name, cfg):
    if name == "ict": # mixed target <-> mixed input
        return ICT(
            cfg.ssl_args.consis,
            cfg.ssl_args.threshold,
            cfg.ssl_args.sharpen,
            cfg.ssl_args.temp_softmax,
            cfg.ssl_args.alpha
        )
    elif name == "cr": # base augment <-> another augment
        return ConsistencyRegularization(
            cfg.ssl_args.consis,
            cfg.ssl_args.threshold,
            cfg.ssl_args.sharpen,
            cfg.ssl_args.temp_softmax
        )
    elif name == "pl": # hard label <-> strong augment
        return PseudoLabel(
            cfg.ssl_args.consis,
            cfg.ssl_args.threshold,
            cfg.ssl_args.sharpen,
            cfg.ssl_args.temp_softmax
        )
    elif name == "vat": # base augment <-> adversarial
        from ..consistency import builder
        return VAT(
            cfg.ssl_args.consis,
            cfg.ssl_args.threshold,
            cfg.ssl_args.sharpen,
            cfg.ssl_args.temp_softmax,
            builder.gen_consistency(cfg.ssl_args.consis, cfg),
            cfg.ssl_args.eps,
            cfg.ssl_args.xi,
            cfg.ssl_args.vat_iter
        )
    else:
        raise NotImplementedError

File Path: cords/selectionstrategies/helpers/ssl_lib/algs/consistency.py
Content:
import torch
from .utils import sharpening, tempereture_softmax

class ConsistencyRegularization:
    """
    Basis Consistency Regularization

    Parameters
    --------
    consistency: str
        consistency objective name
    threshold: float
        threshold to make mask
    sharpen: float
        sharpening temperature for target value
    temp_softmax: float
        temperature for temperature softmax
    """
    def __init__(
        self,
        consistency,
        threshold: float = None,
        sharpen: float = None,
        temp_softmax: float = None
    ):
        self.consistency = consistency
        self.threshold = threshold
        self.sharpen = sharpen
        self.tau = temp_softmax

    def __call__(
        self,
        stu_preds,
        tea_logits,
        *args,
        **kwargs
    ):
        mask = self.gen_mask(tea_logits)
        targets = self.adjust_target(tea_logits)
        return stu_preds, targets, mask

    def adjust_target(self, targets):
        if self.sharpen is not None:
            targets = targets.softmax(1)
            targets = sharpening(targets, self.sharpen)
        elif self.tau is not None:
            targets = tempereture_softmax(targets, self.tau)
        else:
            targets = targets.softmax(1)
        return targets

    def gen_mask(self, targets):
        targets = targets.softmax(1)
        if self.threshold is None or self.threshold == 0:
            return torch.ones_like(targets.max(1)[0])
        return (targets.max(1)[0] >= self.threshold).float()

    def __repr__(self):
        return f"Consistency(threshold={self.threshold}, sharpen={self.sharpen}, tau={self.tau})"

File Path: cords/selectionstrategies/helpers/ssl_lib/algs/ict.py
Content:
import torch
from .consistency import ConsistencyRegularization
from .utils import mixup

class ICT(ConsistencyRegularization):
    """
    Interpolation Consistency Training https://arxiv.org/abs/1903.03825

    Parameters
    --------
    consistency: str
        consistency objective name
    threshold: float
        threshold to make mask
    sharpen: float
        sharpening temperature for target value
    temp_softmax: float
        temperature for temperature softmax
    alpha: float
        beta distribution parameter
    """
    def __init__(
        self,
        consistency,
        threshold: float = 1.,
        sharpen: float = None,
        temp_softmax: float = None,
        alpha: float = 0.1
    ):
        super().__init__(
            consistency,
            threshold,
            sharpen,
            temp_softmax
        )
        self.alpha = alpha

    def __call__(self, tea_logits, w_data, stu_forward, subset=False, *args, **kwargs):
        mask = self.gen_mask(tea_logits)
        targets = self.adjust_target(tea_logits)
        mixed_x, mixed_targets = mixup(w_data, targets, self.alpha)
        if subset:
            y, l1 = stu_forward(mixed_x, last=True, freeze=True)
            return y, l1, mixed_targets, mask
        else:
            y = stu_forward(mixed_x)
            return y, mixed_targets, mask

    def __repr__(self):
        return f"ICT(threshold={self.threshold}, sharpen={self.sharpen}, tau={self.tau}, alpha={self.alpha})"


File Path: cords/selectionstrategies/helpers/ssl_lib/algs/pseudo_label.py
Content:
import torch
import torch.nn.functional as F
from .consistency import ConsistencyRegularization
from ..consistency.cross_entropy import CrossEntropy
from .utils import make_pseudo_label, sharpening

class PseudoLabel(ConsistencyRegularization):
    """
    PseudoLabel

    Parameters
    --------
    consistency: str
        consistency objective name
    threshold: float
        threshold to make mask
    sharpen: float
        sharpening temperature for target value
    temp_softmax: float
        temperature for temperature softmax
    """
    def __init__(
        self,
        consistency,
        threshold = 0.95,
        sharpen: float = None,
        temp_softmax: float = None
    ):
        super().__init__(
            consistency,
            threshold,
            sharpen,
            temp_softmax
        )

    def __call__(self, stu_preds, tea_logits, *args, **kwargs):
        hard_label, mask = make_pseudo_label(tea_logits, self.threshold)
        return stu_preds, hard_label, mask

    def __repr__(self):
        return f"PseudoLabel(threshold={self.threshold}, sharpen={self.sharpen}, tau={self.tau})"


File Path: cords/selectionstrategies/helpers/ssl_lib/algs/utils.py
Content:
import torch
import torch.nn as nn


def make_pseudo_label(logits, threshold):
    max_value, hard_label = logits.softmax(1).max(1)
    mask = (max_value >= threshold)
    return hard_label, mask


def sharpening(soft_labels, temp):
    soft_labels = soft_labels.pow(temp)
    return soft_labels / soft_labels.abs().sum(1, keepdim=True)


def tempereture_softmax(logits, tau):
    return (logits/tau).softmax(1)


def mixup(x, y, alpha):
    device = x.device
    b = x.shape[0]
    permute = torch.randperm(b)
    perm_x = x[permute]
    perm_y = y[permute]
    factor = torch.distributions.beta.Beta(alpha, alpha).sample((b,1)).to(device)
    if x.ndim == 4:
        x_factor = factor[...,None,None]
    else:
        x_factor = factor
    mixed_x = x_factor * x + (1-x_factor) * perm_x
    mixed_y = factor * y + (1-factor) * perm_y
    return mixed_x, mixed_y


def anneal_loss(logits, labels, loss, global_step, max_iter, num_classes, schedule):
    tsa_start = 1 / num_classes
    threshold = get_tsa_threshold(
        schedule, global_step, max_iter,
        tsa_start, end=1
    )
    with torch.no_grad():
        probs = logits.softmax(1)
        correct_label_probs = probs.gather(1, labels[:,None]).squeeze()
        mask = correct_label_probs < threshold
    return (loss * mask).mean()


def get_tsa_threshold(schedule, global_step, max_iter, start, end):
    step_ratio = global_step / max_iter
    if schedule == "linear":
        coef = step_ratio
    elif schedule == "exp":
        scale = 5
        coef = ((step_ratio - 1) * scale).exp()
    elif schedule == "log":
        scale = 5
        coef = 1 - (-step_ratio * scale).exp()
    else:
        raise NotImplementedError
    return coef * (end - start) + start


class InfiniteSampler(object):
    pass
File Path: cords/selectionstrategies/helpers/ssl_lib/algs/vat.py
Content:
import torch
from .consistency import ConsistencyRegularization

class VAT(ConsistencyRegularization):
    """
    Virtual Adversarial Training https://arxiv.org/abs/1704.03976

    Parameters
    --------
    consistency: str
        consistency objective name
    threshold: float
        threshold to make mask
    sharpen: float
        sharpening temperature for target value
    temp_softmax: float
        temperature for temperature softmax
    objective: function
        objective function
    eps: float
        virtual adversarial noise norm
    xi: float
        perturbation for finite differential method
    n_iter: int
        number of iterations for power method
    """
    def __init__(
        self,
        consistency,
        threshold: float = 1.,
        sharpen: float = None,
        temp_softmax: float = None,
        objective = None,
        eps = 1.0,
        xi = 1e-6,
        n_iter = 1
    ):
        super().__init__(
            consistency,
            threshold,
            sharpen,
            temp_softmax
        )
        self.eps = eps
        self.xi  = xi
        self.n_iter = n_iter
        self.obj_func = objective

    def __call__(self, tea_logits, stu_forward, w_data, subset=False, *args, **kwargs):
        mask = self.gen_mask(tea_logits)
        targets = self.adjust_target(tea_logits)
        d = torch.randn_like(w_data)
        d = self.__normalize(d)
        for _ in range(self.n_iter):
            d.requires_grad = True
            x_hat = w_data + self.xi * d
            y = stu_forward(x_hat)
            loss = self.obj_func(y, targets)
            d = torch.autograd.grad(loss, d)[0]
            d = self.__normalize(d).detach()
        x_hat = w_data + self.eps * d
        if subset:
            y, l1 = stu_forward(x_hat, last=True, freeze=True)
            return y, l1, targets, mask
        else:
            y = stu_forward(x_hat)
            return y, targets, mask

    def __normalize(self, v):
        v = v / (1e-12 + self.__reduce_max(v.abs(), range(1, len(v.shape)))) # to avoid overflow by v.pow(2)
        v = v / (1e-6 + v.pow(2).sum(list(range(1, len(v.shape))), keepdim=True)).sqrt()
        return v

    def __reduce_max(self, v, idx_list):
        for i in idx_list:
            v = v.max(i, keepdim=True)[0]
        return v

    def __repr__(self):
        return f"VAT(threshold={self.threshold}, \
            sharpen={self.sharpen}, \
            tau={self.tau}, \
            eps={self.eps}), \
            xi={self.xi}"

File Path: cords/selectionstrategies/helpers/ssl_lib/consistency/__init__.py
Content:

File Path: cords/selectionstrategies/helpers/ssl_lib/consistency/builder.py
Content:
from .cross_entropy import CrossEntropy
from .mean_squared import MeanSquared
from .kl_divergence import KLDivergence

def gen_consistency(type, cfg):
    # doubts: what is this doing?
    if type == "ce":
        return CrossEntropy(True)
    elif type == "ce_red":
        return CrossEntropy(False)
    elif type == "ms":
        return MeanSquared(True)
    elif type == "ms_red":
        return MeanSquared(False)
    elif type == "kld":
        return KLDivergence(True)
    elif type == "kld_red":
        return KLDivergence(False)
    else:
        return None

File Path: cords/selectionstrategies/helpers/ssl_lib/consistency/cross_entropy.py
Content:
import torch.nn as nn
import torch.nn.functional as F

def cross_entropy(y, target, mask=None, reduce=True):
    if target.ndim == 1: # for hard label
        loss = F.cross_entropy(y, target, reduction="none")
    else:
        loss = -(target * F.log_softmax(y, 1)).sum(1)
    if mask is not None:
        loss = mask * loss
    if reduce:
        return loss.mean()
    else:
        return loss


class CrossEntropy(nn.Module):

    def __init__(self, reduce):
        super().__init__()
        self.reduce = reduce

    def forward(self, y, target, mask=None, *args, **kwargs):
        return cross_entropy(y, target.detach(), mask, self.reduce)

File Path: cords/selectionstrategies/helpers/ssl_lib/consistency/kl_divergence.py
Content:
import torch.nn as nn
import torch.nn.functional as F
import torch

def kl_divergence(y, target, mask=None, reduce=True):
    loss = ((target * torch.log(target)) - (target * F.log_softmax(y, 1))).sum(1)
    if mask is not None:
        loss = mask * loss
    if reduce:
        return loss.mean()
    else:
        return loss


class KLDivergence(nn.Module):

    def __init__(self, reduce):
        super().__init__()
        self.reduce = reduce

    def forward(self, y, target, mask=None, *args, **kwargs):
        return kl_divergence(y, target.detach(), mask, self.reduce)

File Path: cords/selectionstrategies/helpers/ssl_lib/consistency/mean_squared.py
Content:
import torch.nn as nn
import torch.nn.functional as F

def mean_squared(y, target, mask=None, reduce=True):
    y = y.softmax(1)
    loss = F.mse_loss(y, target, reduction="none").mean(1)
    if mask is not None:
        loss = mask * loss
    if reduce:
        return loss.mean()
    else:
        return loss

class MeanSquared(nn.Module):

    def __init__(self, reduce):
        super().__init__()
        self.reduce = reduce

    def forward(self, y, target, mask=None, *args, **kwargs):
        return mean_squared(y, target.detach(), mask, self.reduce)
File Path: cords/selectionstrategies/helpers/ssl_lib/misc/__init__.py
Content:

File Path: cords/selectionstrategies/helpers/ssl_lib/misc/meter.py
Content:
class Meter:
    def __init__(self, ema_coef=0.9):
        self.ema_coef = ema_coef
        self.params = {}

    def add(self, params:dict, ignores:list = []):
        for k, v in params.items():
            if k in ignores:
                continue
            if not k in self.params.keys():
                self.params[k] = v
            else:
                self.params[k] -= (1 - self.ema_coef) * (self.params[k] - v)

    def state(self, header="", footer=""):
        state = header
        for k, v in self.params.items():
            state += f" {k} {v:.6g} |"
        return state + " " + footer

    def reset(self):
        self.params = {}

File Path: cords/selectionstrategies/helpers/ssl_lib/param_scheduler/__init__.py
Content:

File Path: cords/selectionstrategies/helpers/ssl_lib/param_scheduler/scheduler.py
Content:
import torch
import warnings
import math
import torch.optim as optim


def exp_warmup(base_value, max_warmup_iter, cur_step):
    """exponential warmup proposed in mean teacher

    calcurate
    base_value * exp(-5(1 - t)^2), t = cur_step / max_warmup_iter

    Parameters
    -----
    base_value: float
        maximum value
    max_warmup_iter: int
        maximum warmup iteration
    cur_step: int
        current iteration
    """
    if max_warmup_iter <= cur_step:
        return base_value
    return base_value * math.exp(-5 * (1 - cur_step/max_warmup_iter)**2)


def linear_warmup(base_value, max_warmup_iter, cur_step):
    """linear warmup

    calcurate
    base_value * (cur_step / max_warmup_iter)
    
    Parameters
    -----
    base_value: float
        maximum value
    max_warmup_iter: int
        maximum warmup iteration
    cur_step: int
        current iteration
    """
    if max_warmup_iter <= cur_step:
        return base_value
    return base_value * cur_step / max_warmup_iter


def cosine_decay(base_lr, max_iteration, cur_step):
    """cosine learning rate decay
    
    cosine learning rate decay with parameters proposed FixMatch
    base_lr * cos( (7\pi cur_step) / (16 max_warmup_iter) )

    Parameters
    -----
    base_lr: float
        maximum learning rate
    max_warmup_iter: int
        maximum warmup iteration
    cur_step: int
        current iteration
    """
    return base_lr * (math.cos( (7*math.pi*cur_step) / (16*max_iteration) ))


def CosineAnnealingLR(optimizer, max_iteration):
    """
    generate cosine annealing learning rate scheduler as LambdaLR
    """
    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda cur_step : math.cos((7*math.pi*(cur_step % max_iteration)) / (16*max_iteration)))

File Path: cords/test.py
Content:
import torch

t = torch.rand(50, 100)
chunked_t = torch.chunk(t, 5, dim=0)
new_t = []
for i in range(len(chunked_t)):
    new_t.append(torch.mean(chunked_t[i], dim=0).view(1, -1))
new_t = torch.cat(new_t, dim=0)
print()
File Path: cords/utils/__init__.py
Content:
# __init__.py
# Author: Krishnateja Killamsetty <krishnatejakillamsetty@gmail.com>
from .utils import generate_cumulative_timing
from .utils import sllogstoxl
from .utils import sllogstodfs
from .config_utils import load_config_data

File Path: cords/utils/config_utils.py
Content:
import os
import os.path as osp
from pathlib import Path
import ast
import yaml
import importlib.util
import copy
import os
from dotmap import DotMap


def is_str(x):
    """Whether the input is an string instance.
    Note: This method is deprecated since python 2 is no longer supported.
    """
    return isinstance(x, str)


def is_filepath(x):
    return is_str(x) or isinstance(x, Path)


def fopen(filepath, *args, **kwargs):
    if is_str(filepath):
        return open(filepath, *args, **kwargs)
    elif isinstance(filepath, Path):
        return filepath.open(*args, **kwargs)
    raise ValueError('`filepath` should be a string or a Path')


def check_file_exist(filename, msg_tmpl='file "{}" does not exist'):
    if not osp.isfile(filename):
        raise FileNotFoundError(msg_tmpl.format(filename))


def check_dir_exist(dirname, msg_tmpl='Directory "{}" does not exist'):
    if not osp.isdir(dirname):
        raise FileNotFoundError(msg_tmpl.format(dirname))


def mkdir_or_exist(dir_name, mode=0o777):
    if dir_name == '':
        return
    else:
        dir_name = osp.expanduser(dir_name)
        os.makedirs(dir_name, mode=mode, exist_ok=True)


def _validate_py_syntax(filename):
    with open(filename, 'r') as f:
        content = f.read()
    try:
        ast.parse(content)
    except SyntaxError as e:
        raise SyntaxError('There are syntax errors in config '
                          f'file {filename}: {e}')


def load_config_data(filepath):
    filename = osp.abspath(osp.expanduser(filepath))
    check_file_exist(filename)
    fileExtname = osp.splitext(filename)[1]
    if fileExtname not in ['.py', '.yaml', '.yml']:
        raise IOError('Only py/yml/yaml type are supported now!')
    """
    Parsing Config file
    """
    if filename.endswith('.yaml'):
        with open(filename, 'r') as config_file:
            configdata = yaml.load(config_file, Loader=yaml.FullLoader)
    elif filename.endswith('.py'):
        spec = importlib.util.spec_from_file_location("config", filename)
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
        configdata = copy.deepcopy(mod.config)
    return DotMap(configdata)
File Path: cords/utils/data/__init__.py
Content:

File Path: cords/utils/data/data_utils/__init__.py
Content:
from operator import imod
from .create_slices import get_slices
from .regression_data_utils import *
from .weightedsubset import WeightedSubset
from .collate import collate_fn_pad_batch
from .generate_global_order import generate_text_global_order
from .generate_global_order import generate_image_global_order
from .generate_global_order import generate_image_stochastic_subsets
from .generate_global_order import generate_text_stochastic_subsets
File Path: cords/utils/data/data_utils/collate.py
Content:
import torch

def collate_fn_pad_batch(data):
    """Pad data in a batch.
    Parameters
    ----------
    data : list((tensor, int), )
        data and label in a batch
    Returns
    -------
    tuple(tensor, tensor)
    """
    num_items = len(data[0])
    max_len = max([i[0].shape[0] for i in data])
    labels = torch.tensor([i[1] for i in data], dtype=torch.long)
    padded = torch.zeros((len(data), max_len), dtype=torch.long)
    if num_items == 3:
        weights = torch.tensor([i[2] for i in data], dtype=torch.float)
    # randomizing might be better
    for i, _ in enumerate(padded):
        padded[i][:data[i][0].shape[0]] = data[i][0]
    if num_items == 3:    
        return padded, labels, weights
    else:
        return padded, labels


def max_len_pad(data):
    """Pad data globally.
    Parameters
    ----------
    data : list((tensor, int), )
        data and label in a batch
    Returns
    -------
    tuple(tensor, tensor)
    """
    max_len = -1
    for sample in data:
        print(sample[0])
    num_items = len(data[0])
    labels = torch.tensor([i[1] for i in data], dtype=torch.long)
    padded = torch.zeros((len(data), max_len), dtype=torch.long)
    if num_items == 3:
        weights = torch.tensor([i[2] for i in data], dtype=torch.float)
    # randomizing might be better
    for i, _ in enumerate(padded):
        padded[i][:data[i][0].shape[0]] = data[i][0]
    if num_items == 3:    
        return padded, labels, weights
    else:
        return padded, labels
File Path: cords/utils/data/data_utils/create_slices.py
Content:
import numpy as np
import sys
import torch
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn import preprocessing

# todo : update in better format

'''def gen_rand_prior_indices(curr_size,num_cls,x_trn,y_trn,remainList=None):

    per_sample_budget = int(curr_size/num_cls)
    if remainList is None:
        per_sample_count = [len(torch.where(y_trn == x)[0]) for x in np.arange(num_cls)]
        total_set = list(np.arange(N))
    else:
        per_sample_count = [len(torch.where(y_trn[remainList] == x)[0]) for x in np.arange(num_cls)]
        total_set = remainList
    indices = []
    count = 0
    for i in range(num_cls):
        if remainList is None:
            label_idxs = torch.where(y_trn == i)[0].cpu().numpy()
        else:
            label_idxs = torch.where(y_trn[remainList] == i)[0].cpu().numpy()
            label_idxs = np.array(remainList)[label_idxs]

        if per_sample_count[i] > per_sample_budget:
            indices.extend(list(np.random.choice(label_idxs, size=per_sample_budget, replace=False)))
        else:
            indices.extend(label_idxs)
            count += (per_sample_budget - per_sample_count[i])
    for i in indices:
        total_set.remove(i)
    indices.extend(list(np.random.choice(total_set, size= count, replace=False)))
    return indices'''

def get_slices(data_name, data,labels,device,buckets=None,clean=True):

    #data_slices = []
    #abel_slices =[]
    
    val_data_slices = []
    val_label_slices =[]

    tst_data_slices = []
    tst_label_slices =[]

    '''sc = StandardScaler()
    x_trn = sc.fit_transform(x_trn)
    x_val = sc.transform(x_val)
    x_tst = sc.transform(x_tst)'''

    if data_name == 'Community_Crime_old':
        protect_feature =[2,3,4,5]

        data_class =[]

        N = int(0.1*len(data)/(buckets*len(protect_feature)))

        total_set = set(list(np.arange(len(data))))
        
        for i in protect_feature:            

            digit = np.ones(data.shape[0],dtype=np.int8)*(i-1)
            low = np.min(data[:,i])
            high = np.max(data[:,i])
            bins = np.linspace(low, high, buckets)
            digitized = np.digitize(data[:,i], bins)
            digit =  digit*10 + digitized

            classes,times = np.unique(digit,return_counts=True) 
            times, classes = zip(*sorted(zip(times, classes)))
            data_class.append(classes)
            
            count = 0
            for cl in classes[:-1]:

                indices=[]
                indices_tst=[]
                
                idx = (digit == cl).nonzero()[0].flatten()
                idx.tolist()
                idxs = set(idx)
                idxs.intersection_update(total_set)
                idx = list(idxs)
                #print(cl,len(idx))

                curr_N = int(len(idx)/3)

                #print(curr_N,N)
                 
                indices.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))
                total_set.difference(indices)
                idxs.difference(indices)
                idx = list(idxs)
                indices_tst.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))
                total_set.difference(indices_tst)
                
                if curr_N < N:
                    count += (N - curr_N)

                val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))
                val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))

                tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))
                tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))

            indices=[]
            indices_tst=[]
            
            idx = (digit == classes[-1]).nonzero()[0].flatten()
            idx.tolist()
            idxs = set(idx)
            idxs.intersection_update(total_set)
            idx = list(idxs)

            indices.extend(list(np.random.choice(idx, size=N+count, replace=False)))
            total_set.difference(indices)
            idxs.difference(indices)
            idx = list(idxs)
            indices_tst.extend(list(np.random.choice(idx, size=N+count, replace=False)))
            total_set.difference(indices_tst)

            val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))
            val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))

            tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))
            tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))

        final_lables = [j for sub in data_class for j in sub]
        left = list(total_set)    
        data_left = data[left]
        label_left = labels[left]

    elif data_name == 'OnlineNewsPopularity':

        protect_feature = [11,12,13,14,15,16]

        final_lables = [ 'Lifestyle','Entertainment','Business','Social Media','Tech','World']

        total_set = set(list(np.arange(len(data))))

        N = int(0.1*len(data)/len(protect_feature))

        max_times = 0
        
        for pf in protect_feature:            
        
            classes,times = np.unique(data[:,pf],return_counts=True) 

            one_id = (classes == 1.0).nonzero()[0].flatten()[0]

            if max_times < times[one_id]:
                max_times = times[one_id]
                max_id = pf

        most = final_lables[max_id-protect_feature[0]]
        final_lables.remove(most)
        final_lables.append(most)

        count = 0
        
        for pf in protect_feature:

            if pf == max_id:
                continue
            
            idx = (data[:,pf] == 1.0).nonzero()[0].flatten()
            idx.tolist()
            idxs = set(idx)
            idxs.intersection_update(total_set)
            idx = list(idxs)
            #print(cl,len(idx))

            curr_N = int(len(idx)/3)

            #print(curr_N,N)
            
            indices = list(np.random.choice(idx, size=min(N,curr_N), replace=False))
            total_set.difference(indices)
            idxs.difference(indices)
            idx = list(idxs)
            indices_tst = list(np.random.choice(idx, size=min(N,curr_N), replace=False))
            total_set.difference(indices_tst)
            
            if curr_N < N:
                count += (N - curr_N)

            '''val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))
            val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))

            tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))
            tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))'''

            val_data_slices.append(data[indices])
            val_label_slices.append(labels[indices])

            tst_data_slices.append(data[indices_tst])
            tst_label_slices.append(labels[indices_tst])

            
        idx = (data[:,max_id] == 1.0).nonzero()[0].flatten()
        idx.tolist()
        idxs = set(idx)
        idxs.intersection_update(total_set)
        idx = list(idxs)

        indices = list(np.random.choice(idx, size=N+count, replace=False))
        total_set.difference(indices)
        idxs.difference(indices)
        idx = list(idxs)
        indices_tst = list(np.random.choice(idx, size=N+count, replace=False)) 
        total_set.difference(indices_tst)

        '''val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))
        val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))

        tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))
        tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))'''

        val_data_slices.append(data[indices])
        val_label_slices.append(labels[indices])

        tst_data_slices.append(data[indices_tst])
        tst_label_slices.append(labels[indices_tst])

        left = list(total_set)
        sc = MinMaxScaler() #StandardScaler()
        sc_l = MinMaxScaler()

        #print(data[left][0])
        data_left = sc.fit_transform(data[left])
        label_left = np.reshape(sc_l.fit_transform(np.reshape(labels[left],(-1,1))),(-1))
        #print(data_left[0])
        #preprocessing.normalize(data[left])

        for j in range(len(val_data_slices)):
            
            val_data_slices[j] = torch.from_numpy(sc.transform(val_data_slices[j])).float().to(device)
            tst_data_slices[j] = torch.from_numpy(sc.transform(tst_data_slices[j])).float().to(device)

            val_label_slices[j] = torch.from_numpy(np.reshape(\
                sc_l.transform(np.reshape(val_label_slices[j],(-1,1))),(-1))).float().to(device)
            tst_label_slices[j] = torch.from_numpy(np.reshape(\
                sc_l.transform(np.reshape(tst_label_slices[j],(-1,1))),(-1))).float().to(device)
    
    elif data_name in ['census','LawSchool_selcon','German_credit','Community_Crime']:
        
        if data_name == 'census':
            protect_feature = 8 #9
        elif data_name == 'LawSchool_selcon':
            protect_feature = 0
        elif data_name == 'German_credit':
            protect_feature = 8
        elif data_name == 'Community_Crime':
            protect_feature = -1

        total_set = set(list(np.arange(len(data))))
        
        classes,times = np.unique(data[:,protect_feature],return_counts=True) 
        times, classes = zip(*sorted(zip(times, classes)))

        #print(times)
        #print(classes)

        N = int(0.1*len(data)/len(classes))
        
        count = 0
        for cl in classes[:-1]:

            indices=[]
            indices_tst=[]
            
            idx = (data[:,protect_feature] == cl).nonzero()[0].flatten()
            idx.tolist()

            curr_N = int(len(idx)/3)

            #print(curr_N,N)
                
            indices.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))
            total_set.difference(indices)
            idxs = set(idx)
            idxs.difference(indices)
            idx = list(idxs)
            indices_tst.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))
            total_set.difference(indices_tst)
            
            if curr_N < N:
                count += (N - curr_N)

            #val_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices])).float().to(device))
            val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))
            val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))

            #tst_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices_tst])).float().to(device))
            tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))
            tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))

        indices=[]
        indices_tst=[]
        
        idx = (data[:,protect_feature] == classes[-1]).nonzero()[0].flatten()
        idx.tolist()

        indices.extend(list(np.random.choice(idx, size=N+count, replace=False)))
        total_set.difference(indices)
        idxs = set(idx)
        idxs.difference(indices)
        idx = list(idxs)
        indices_tst.extend(list(np.random.choice(idx, size=N+count, replace=False)))
        total_set.difference(indices_tst)

        #val_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices])).float().to(device))
        val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))
        val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))

        #tst_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices_tst])).float().to(device))
        tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))
        tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))

        final_lables = classes
        left = list(total_set)
        data_left = data[left] #preprocessing.normalize(data[left]) 
        label_left = labels[left]

        if not clean:

            noise_size = int(len(label_left) * 0.5)
            noise_indices = np.random.choice(np.arange(len(label_left)), size=noise_size, replace=False)
            
            sigma = 40
            label_left[noise_indices] = label_left[noise_indices] + np.random.normal(0, sigma, noise_size)
    
        
    return data_left, label_left, val_data_slices, val_label_slices, final_lables, tst_data_slices,\
        tst_label_slices,final_lables

File Path: cords/utils/data/data_utils/generate_global_order.py
Content:
import datasets
import torchvision
from sentence_transformers import SentenceTransformer, util
from matplotlib import pyplot as plt 
from ..datasets.__utils import TinyImageNet
from transformers import ViTFeatureExtractor, ViTModel
from scipy.spatial.distance import cdist
from sklearn.metrics.pairwise import euclidean_distances
from numba import jit, config
from torch.utils.data import random_split, BatchSampler, SequentialSampler
import torch
import pickle
import math
import os
import submodlib
import argparse
import h5py
import numpy as np
import time


LABEL_MAPPINGS = {'glue_sst2':'label', 
                  'trec6':'coarse_label', 
                  'imdb':'label',
                  'rotten_tomatoes': 'label',
                  'tweet_eval': 'label'}

SENTENCE_MAPPINGS = {'glue_sst2': 'sentence', 
                    'trec6':'text',  
                    'imdb':'text',
                    'rotten_tomatoes': 'text',
                    'tweet_eval': 'text'}

IMAGE_MAPPINGS = {'cifar10': 'images'}


def parse_args():
    parser = argparse.ArgumentParser(description="Compute Global ordering of a dataset using pretrained LMs.")
    parser.add_argument(
                        "--dataset",
                        type=str,
                        default="glue_sst2",
                        help="Only supports datasets for hugging face currently."
                        )
    parser.add_argument(
                        "--model",
                        type=str,
                        default="all-distilroberta-v1",
                        help="Transformer model used for computing the embeddings."
                        )
    
    parser.add_argument(
                        "--data_dir",
                        type=str,
                        required=False,
                        help="Directory in which data downloads happen.",
                        default="/home/krishnateja/data"
                        ) 
    parser.add_argument(
                        "--submod_function",
                        type=str,
                        default="logdet",
                        help="Submdular function used for finding the global order."
                        )
    parser.add_argument(
                        "--seed",
                        type=int,
                        default=42,
                        help="Seed value for reproducibility of the experiments."
                        )
    parser.add_argument(
                        "--device",
                        type=str,
                        default='cuda:1',
                        help= "Device used for computing the embeddings"
                        )
    args=parser.parse_args()
    return args


def dict2pickle(file_name, dict_object):
    """
    Store dictionary to pickle file
    """    
    with open(file_name, "wb") as fOut:
        pickle.dump(dict_object, fOut, protocol=pickle.HIGHEST_PROTOCOL)


def pickle2dict(file_name, key):
    """
    Load dictionary from pickle file
    """
    with open(file_name, "rb") as fIn:
        stored_data = pickle.load(fIn)
        value = stored_data[key]
    return value


def store_embeddings(pickle_name, embeddings):
    """
    Store embeddings to disc
    """
    with open(pickle_name, "wb") as fOut:
        pickle.dump({'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)


def load_embeddings(pickle_name):
    """
    Load embeddings from disc
    """
    with open(pickle_name, "rb") as fIn:
        stored_data = pickle.load(fIn)
        #stored_sentences = stored_data['sentences']
        stored_embeddings = stored_data['embeddings']
    return stored_embeddings    


def get_cdist(V):
	ct = time.time()
	dist_mat = euclidean_distances(V)
	print("Distance Matrix construction time ", time.time()-ct)
	return get_square(dist_mat)


#@torch.jit.script
@jit(nopython=True, parallel=True)
def get_square(mat):
	return mat**2


@jit(nopython=True, parallel=True)
def get_rbf_kernel(dist_mat, kw=0.1):
	sim = np.exp(-dist_mat/(kw*dist_mat.mean()))
	return sim


# @jit(nopython=True, parallel=True)
def get_dot_product(mat):
	sim = np.matmul(mat, np.transpose(mat))
	return sim


def compute_text_embeddings(model_name, sentences, device, return_tensor=False):
    """
    Compute sentence embeddings using a transformer model and return in numpy or tensor format
    """
    model = SentenceTransformer(model_name, device=device)
    if return_tensor:
        embeddings = model.encode(sentences, device=device, convert_to_tensor=True).cpu()
    else:
        embeddings = model.encode(sentences, device=device, convert_to_numpy=True)
    return embeddings


def compute_image_embeddings(model_name, images, device, return_tensor=False):
    """
    Compute image embeddings using CLIP based model and return in numpy or tensor format
    """
    model = SentenceTransformer(model_name, device=device)
    if return_tensor:
        embeddings = model.encode(images, device=device, convert_to_tensor=True).cpu()
    else:
        embeddings = model.encode(images, device=device, convert_to_numpy=True)
    return embeddings


def compute_vit_image_embeddings(images, device, return_tensor=False):
    feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-large-patch16-224-in21k")
    model = ViTModel.from_pretrained("google/vit-large-patch16-224-in21k")
    model= model.to(device)
    #inputs = feature_extractor(images, return_tensors="pt")
    sampler = BatchSampler(SequentialSampler(range(len(images))),
                           20, 
                           drop_last=False)

    inputs = []
    for indices in sampler:
        if images[0].mode == 'L':
            images_batch = [images[x].convert('RGB') for x in indices]
        else:
            images_batch = [images[x] for x in indices]
        inputs.append(feature_extractor(images_batch, return_tensors="pt"))

    img_features = []
    for batch_inputs in inputs:
        tmp_feat_dict = {}
        for key in batch_inputs.keys():
            tmp_feat_dict[key] = batch_inputs[key].to(device=device)
        with torch.no_grad():
            batch_outputs = model(**tmp_feat_dict)
        batch_img_features = batch_outputs.last_hidden_state.mean(dim=1).cpu()
        img_features.append(batch_img_features)
        del tmp_feat_dict
    
    img_features = torch.cat(img_features, dim=0)
    if return_tensor == False:
        return img_features.numpy()
    else:
        return img_features
    

def compute_vit_cls_image_embeddings(images, device, return_tensor=False):
    feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-large-patch16-224-in21k")
    model = ViTModel.from_pretrained("google/vit-large-patch16-224-in21k")
    model= model.to(device)
    #inputs = feature_extractor(images, return_tensors="pt")
    sampler = BatchSampler(SequentialSampler(range(len(images))),
                           20, 
                           drop_last=False)

    inputs = []
    for indices in sampler:
        if images[0].mode == 'L':
            images_batch = [images[x].convert('RGB') for x in indices]
        else:
            images_batch = [images[x] for x in indices]
        inputs.append(feature_extractor(images_batch, return_tensors="pt"))

    img_features = []
    for batch_inputs in inputs:
        tmp_feat_dict = {}
        for key in batch_inputs.keys():
            tmp_feat_dict[key] = batch_inputs[key].to(device=device)
        with torch.no_grad():
            batch_outputs = model(**tmp_feat_dict)
        batch_img_features = batch_outputs.last_hidden_state[:, 0, :].cpu()
        img_features.append(batch_img_features)
        del tmp_feat_dict
    
    img_features = torch.cat(img_features, dim=0)
    if return_tensor == False:
        return img_features.numpy()
    else:
        return img_features


def compute_dino_image_embeddings(images, device, return_tensor=False):
    feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')
    model = ViTModel.from_pretrained('facebook/dino-vitb16')
    model= model.to(device)
    #inputs = feature_extractor(images, return_tensors="pt")
    sampler = BatchSampler(SequentialSampler(range(len(images))),
                           20, 
                           drop_last=False)

    inputs = []
    for indices in sampler:
        if images[0].mode == 'L':
            images_batch = [images[x].convert('RGB') for x in indices]
        else:
            images_batch = [images[x] for x in indices]
        inputs.append(feature_extractor(images_batch, return_tensors="pt"))

    img_features = []
    for batch_inputs in inputs:
        tmp_feat_dict = {}
        for key in batch_inputs.keys():
            tmp_feat_dict[key] = batch_inputs[key].to(device=device)
        with torch.no_grad():
            batch_outputs = model(**tmp_feat_dict)
        batch_img_features = batch_outputs.last_hidden_state.mean(dim=1).cpu()
        img_features.append(batch_img_features)
        del tmp_feat_dict
    
    img_features = torch.cat(img_features, dim=0)
    if return_tensor == False:
        return img_features.numpy()
    else:
        return img_features
    

def compute_dino_cls_image_embeddings(images, device, return_tensor=False):
    feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')
    model = ViTModel.from_pretrained('facebook/dino-vitb16')
    model= model.to(device)
    #inputs = feature_extractor(images, return_tensors="pt")
    sampler = BatchSampler(SequentialSampler(range(len(images))),
                           20, 
                           drop_last=False)

    inputs = []
    for indices in sampler:
        if images[0].mode == 'L':
            images_batch = [images[x].convert('RGB') for x in indices]
        else:
            images_batch = [images[x] for x in indices]
        inputs.append(feature_extractor(images_batch, return_tensors="pt"))

    img_features = []
    for batch_inputs in inputs:
        tmp_feat_dict = {}
        for key in batch_inputs.keys():
            tmp_feat_dict[key] = batch_inputs[key].to(device=device)
        with torch.no_grad():
            batch_outputs = model(**tmp_feat_dict)
        batch_img_features = batch_outputs.last_hidden_state[:, 0, :].cpu()
        img_features.append(batch_img_features)
        del tmp_feat_dict
    
    img_features = torch.cat(img_features, dim=0)
    if return_tensor == False:
        return img_features.numpy()
    else:
        return img_features


def compute_global_ordering(embeddings, submod_function, train_labels, kw, r2_coefficient, knn, metric):
    """
    Return greedy ordering and gains with different submodular functions as the global order.
    """
    if submod_function not in ["supfl", "gc_pc", "logdet_pc", "disp_min_pc", "disp_sum_pc"]:
        data_dist = get_cdist(embeddings)
        if metric == 'rbf_kernel':
            data_sijs = get_rbf_kernel(data_dist, kw)
        elif metric == 'dot':
            data_sijs = get_dot_product(embeddings)
            if submod_function in ["disp_min", "disp_sum"]:
                data_sijs = (data_sijs - np.min(data_sijs))/(np.max(data_sijs) - np.min(data_sijs))
            else:
                if np.min(data_sijs) < 0:
                    data_sijs = data_sijs - np.min(data_sijs)
        elif metric == 'cossim':
            normalized_embeddings = embeddings/np.linalg.norm(embeddings, axis=1, keepdims=True)
            data_sijs = get_dot_product(normalized_embeddings)
            if submod_function in ["disp_min", "disp_sum"]:
                data_sijs = (data_sijs - np.min(data_sijs))/(np.max(data_sijs) - np.min(data_sijs))
            else:
                data_sijs = (data_sijs + 1)/2
        else:
            raise ValueError('Please enter a valid metric')

        data_knn = np.argsort(data_dist, axis=1)[:, :knn].tolist()
        data_r2 = np.nonzero(data_dist <= max(1e-5, data_dist.mean() - r2_coefficient*data_dist.std()))
        data_r2 = zip(data_r2[0].tolist(), data_r2[1].tolist())
        data_r2_dict = {}
        for x in data_r2:
            if x[0] in data_r2_dict.keys():
                data_r2_dict[x[0]].append(x[1])
            else:
                data_r2_dict[x[0]] = [x[1]]


    if submod_function == 'fl':
        obj = submodlib.FacilityLocationFunction(n = embeddings.shape[0],
                                                separate_rep=False,
                                                mode = 'dense',
                                                sijs = data_sijs)

    elif submod_function == 'logdet':
        obj = submodlib.LogDeterminantFunction(n = embeddings.shape[0],
                                                mode = 'dense',
                                                lambdaVal = 1,
                                                sijs = data_sijs)
    
    elif submod_function == 'gc':
        obj = submodlib.GraphCutFunction(n = embeddings.shape[0],
                                        mode = 'dense',
                                        lambdaVal = 1,
                                        separate_rep=False,
                                        ggsijs = data_sijs)

    elif submod_function == 'disp_min':
        obj = submodlib.DisparityMinFunction(n = embeddings.shape[0], 
                                            mode = 'dense', 
                                            sijs= data_sijs)

    elif submod_function == 'disp_sum':
        obj = submodlib.DisparitySumFunction(n = embeddings.shape[0], 
                                            mode = 'dense', 
                                            sijs= data_sijs)
    
    if submod_function in ['gc', 'fl', 'logdet', 'disp_min', 'disp_sum']:
        if submod_function == 'disp_min':
            greedyList = obj.maximize(budget=embeddings.shape[0]-1, optimizer='NaiveGreedy', stopIfZeroGain=False,
                            stopIfNegativeGain=False, verbose=False)
        else:
            greedyList = obj.maximize(budget=embeddings.shape[0]-1, optimizer='LazyGreedy', stopIfZeroGain=False,
                            stopIfNegativeGain=False, verbose=False)
        rem_elem = list(set(range(embeddings.shape[0])).difference(set([x[0] for x in greedyList])))[0]
        rem_gain = greedyList[-1][1]
        greedyList.append((rem_elem, rem_gain))
    else:
        clusters = set(train_labels)
        data_knn  = [[] for _ in range(len(train_labels))]
        data_r2_dict = {}
        greedyList = []
        #Label-wise Partition
        cluster_idxs = {}
        for i in range(len(train_labels)):
            if train_labels[i] in cluster_idxs.keys():
                cluster_idxs[train_labels[i]].append(i)
            else:
                cluster_idxs[train_labels[i]] = [i]
        for cluster in clusters:
            idxs = cluster_idxs[cluster]
            cluster_embeddings = embeddings[idxs, :]

            print(cluster_embeddings.shape)
            clustered_dist = get_cdist(cluster_embeddings)
            if metric == 'rbf_kernel':
                clustered_sijs = get_rbf_kernel(clustered_dist, kw)
            elif metric == 'dot':
                clustered_sijs = get_dot_product(cluster_embeddings)
                if submod_function in ["disp_min_pc", "disp_sum_pc"]:
                    clustered_sijs = (clustered_sijs - np.min(clustered_sijs))/(np.max(clustered_sijs) - np.min(clustered_sijs))
                else:
                    if np.min(clustered_sijs) < 0:
                        clustered_sijs = clustered_sijs - np.min(clustered_sijs)
            elif metric == 'cossim':
                normalized_embeddings = cluster_embeddings/np.linalg.norm(cluster_embeddings, axis=1, keepdims=True)
                clustered_sijs = get_dot_product(normalized_embeddings)
                if submod_function in ["disp_min_pc", "disp_sum_pc"]:
                    clustered_sijs = (clustered_sijs - np.min(clustered_sijs))/(np.max(clustered_sijs) - np.min(clustered_sijs))
                else:
                    clustered_sijs = (1 + clustered_sijs)/2
            else:
                raise ValueError('Please enter a valid metric')
            
            if submod_function in ['supfl']:
                obj = submodlib.FacilityLocationFunction(n = cluster_embeddings.shape[0],
                                                separate_rep=False,
                                                mode = 'dense',
                                                sijs = clustered_sijs)
            elif submod_function in ['gc_pc']:
                obj = submodlib.GraphCutFunction(n = cluster_embeddings.shape[0],
                                                mode = 'dense',
                                                lambdaVal = 0.4,
                                                separate_rep=False,
                                                ggsijs = clustered_sijs)
            elif submod_function in ['logdet_pc']:
                obj = submodlib.LogDeterminantFunction(n = cluster_embeddings.shape[0],
                                                mode = 'dense',
                                                lambdaVal = 1,
                                                sijs = clustered_sijs)
            
            elif submod_function == 'disp_min_pc':
                obj = submodlib.DisparityMinFunction(n = cluster_embeddings.shape[0], 
                                                    mode = 'dense', 
                                                    sijs= clustered_sijs)

            elif submod_function == 'disp_sum_pc':
                obj = submodlib.DisparitySumFunction(n = cluster_embeddings.shape[0], 
                                                    mode = 'dense', 
                                                    sijs= clustered_sijs)

            if submod_function == 'disp_min_pc':
                clustergreedyList = obj.maximize(budget=cluster_embeddings.shape[0]-1, optimizer='NaiveGreedy', stopIfZeroGain=False,
                            stopIfNegativeGain=False, verbose=False)
            else:
                clustergreedyList = obj.maximize(budget=cluster_embeddings.shape[0]-1, optimizer='LazyGreedy', stopIfZeroGain=False,
                            stopIfNegativeGain=False, verbose=False)
            rem_elem = list(set(range(cluster_embeddings.shape[0])).difference(set([x[0] for x in clustergreedyList])))[0]
            rem_gain = clustergreedyList[-1][1]
            clustergreedyList.append((rem_elem, rem_gain))
            clusteredgreedylist_with_orig_idxs = [(idxs[x[0]], x[1]) for x in clustergreedyList]
            greedyList.extend(clusteredgreedylist_with_orig_idxs)
            del obj
            clustered_knn = np.argsort(clustered_dist, axis=1)[:, :knn].tolist()
            for i in range(len(idxs)):
                data_knn[idxs[i]] = [idxs[j] for j in clustered_knn[i]]
            clustered_r2 = np.nonzero(clustered_dist <= max(1e-5, clustered_dist.mean() - r2_coefficient*clustered_dist.std()))
            clustered_r2 = zip(clustered_r2[0].tolist(), clustered_r2[1].tolist())
            for x in clustered_r2:
                if idxs[x[0]] in data_r2_dict.keys():
                    data_r2_dict[idxs[x[0]]].append(idxs[x[1]])
                else:
                    data_r2_dict[idxs[x[0]]] = [idxs[x[1]]]
        greedyList.sort(key=lambda x: x[1], reverse=True)

    knn_list = []
    r2_list = []
    for x in greedyList:
        knn_list.append(data_knn[x[0]])
        r2_list.append(data_r2_dict[x[0]])
    #Sorted Label-wise Partition
    cluster_idxs = {}
    greedy_idxs = [x[0] for x in greedyList]
    for i in greedy_idxs:
        if train_labels[i] in cluster_idxs.keys():
            cluster_idxs[train_labels[i]].append(i)
        else:
            cluster_idxs[train_labels[i]] = [i]
    return greedyList, knn_list, r2_list, cluster_idxs


def compute_stochastic_greedy_subsets(embeddings, submod_function, train_labels, kw, metric, fraction, n_subsets=300):
    """
    Return greedy ordering and gains with different submodular functions as the global order.
    """
    budget = int(fraction * embeddings.shape[0])
    if submod_function not in ["supfl", "gc_pc", "logdet_pc", "disp_min", "disp_sum"]:
        data_dist = get_cdist(embeddings)
        if metric == 'rbf_kernel':
            data_sijs = get_rbf_kernel(data_dist, kw)
        elif metric == 'dot':
            data_sijs = get_dot_product(embeddings)
            if submod_function in ["disp_min", "disp_sum"]:
                data_sijs = (data_sijs - np.min(data_sijs))/(np.max(data_sijs) - np.min(data_sijs))
            else:
                if np.min(data_sijs) < 0:
                    data_sijs = data_sijs - np.min(data_sijs)
        elif metric == 'cossim':
            normalized_embeddings = embeddings/np.linalg.norm(embeddings, axis=1, keepdims=True)
            data_sijs = get_dot_product(normalized_embeddings)
            if submod_function in ["disp_min", "disp_sum"]:
                data_sijs = (data_sijs - np.min(data_sijs))/(np.max(data_sijs) - np.min(data_sijs))
            else:
                data_sijs = (data_sijs + 1)/2
        else:
            raise ValueError('Please enter a valid metric')

    if submod_function == 'fl':
        obj = submodlib.FacilityLocationFunction(n = embeddings.shape[0],
                                                separate_rep=False,
                                                mode = 'dense',
                                                sijs = data_sijs)

    elif submod_function == 'logdet':
        obj = submodlib.LogDeterminantFunction(n = embeddings.shape[0],
                                                mode = 'dense',
                                                lambdaVal = 1,
                                                sijs = data_sijs)
    
    elif submod_function == 'gc':
        obj = submodlib.GraphCutFunction(n = embeddings.shape[0],
                                        mode = 'dense',
                                        lambdaVal = 1,
                                        separate_rep=False,
                                        ggsijs = data_sijs)
    
    elif submod_function == 'disp_min':
        obj = submodlib.DisparityMinFunction(n = embeddings.shape[0], 
                                            mode = 'dense', 
                                            sijs= data_sijs)

    elif submod_function == 'disp_sum':
        obj = submodlib.DisparitySumFunction(n = embeddings.shape[0], 
                                            mode = 'dense', 
                                            sijs= data_sijs)
    
    subsets = []
    total_time = 0
    if submod_function not in ['supfl', 'gc_pc', 'logdet_pc', "disp_min_pc", "disp_sum_pc"]:
        for _ in range(n_subsets):
            st_time = time.time()
            if submod_function == 'disp_min':
                subset = obj.maximize(budget=budget, optimizer='StochasticGreedy', epsilon=0.001, stopIfZeroGain=False,
                            stopIfNegativeGain=False, verbose=False)
            else:
                subset = obj.maximize(budget=budget, optimizer='LazierThanLazyGreedy', epsilon=0.001, stopIfZeroGain=False,
                            stopIfNegativeGain=False, verbose=False)
            subsets.append(subset)
            total_time += (time.time() - st_time)
    else:
        clusters = set(train_labels)
        #Label-wise Partition
        cluster_idxs = {}
        #print(train_labels)
        for i in range(len(train_labels)):
            if train_labels[i] in cluster_idxs.keys():
                cluster_idxs[train_labels[i]].append(i)
            else:
                cluster_idxs[train_labels[i]] = [i]

        per_cls_cnt = [len(cluster_idxs[key]) for key in cluster_idxs.keys()]
        min_cls_cnt = min(per_cls_cnt)
        if min_cls_cnt < math.ceil(budget/len(clusters)):
            per_cls_budget = [min_cls_cnt]*len(clusters)
            while sum(per_cls_budget) < budget:
                for cls in range(len(clusters)):
                    if per_cls_budget[cls] < per_cls_cnt[cls]:
                        per_cls_budget[cls] += 1
        else:
            per_cls_budget = [math.ceil(budget/len(clusters)) for _ in per_cls_cnt]
        
        for _ in range(n_subsets):
            st_time = time.time()
            subset = []
            cluster_idx = 0
            for cluster in cluster_idxs.keys():
                idxs = cluster_idxs[cluster]
                cluster_embeddings = embeddings[idxs, :]
                clustered_dist = get_cdist(cluster_embeddings)
                if metric == 'rbf_kernel':
                    clustered_sijs = get_rbf_kernel(clustered_dist, kw)
                elif metric == 'dot':
                    clustered_sijs = get_dot_product(cluster_embeddings)
                    if submod_function in ["disp_min_pc", "disp_sum_pc"]:
                        clustered_sijs = (clustered_sijs - np.min(clustered_sijs))/(np.max(clustered_sijs) - np.min(clustered_sijs))
                    else:
                        if np.min(clustered_sijs) < 0:
                            clustered_sijs = clustered_sijs - np.min(clustered_sijs)
                elif metric == 'cossim':
                    normalized_embeddings = cluster_embeddings/np.linalg.norm(cluster_embeddings, axis=1, keepdims=True)
                    clustered_sijs = get_dot_product(normalized_embeddings)
                    if submod_function in ["disp_min_pc", "disp_sum_pc"]:
                        clustered_sijs = (clustered_sijs - np.min(clustered_sijs))/(np.max(clustered_sijs) - np.min(clustered_sijs))
                    else:
                        clustered_sijs = (1 + clustered_sijs)/2
                else:
                    raise ValueError('Please enter a valid metric')
                
                if submod_function in ['supfl']:
                    obj = submodlib.FacilityLocationFunction(n = cluster_embeddings.shape[0],
                                                    separate_rep=False,
                                                    mode = 'dense',
                                                    sijs = clustered_sijs)
                elif submod_function in ['gc_pc']:
                    obj = submodlib.GraphCutFunction(n = cluster_embeddings.shape[0],
                                                    mode = 'dense',
                                                    lambdaVal = 0.4,
                                                    separate_rep=False,
                                                    ggsijs = clustered_sijs)
                elif submod_function in ['logdet_pc']:
                    obj = submodlib.LogDeterminantFunction(n = cluster_embeddings.shape[0],
                                                    mode = 'dense',
                                                    lambdaVal = 1,
                                                    sijs = clustered_sijs)

                elif submod_function == 'disp_min_pc':
                    obj = submodlib.DisparityMinFunction(n = cluster_embeddings.shape[0], 
                                                        mode = 'dense', 
                                                        sijs= clustered_sijs)

                elif submod_function == 'disp_sum_pc':
                    obj = submodlib.DisparitySumFunction(n = cluster_embeddings.shape[0], 
                                                        mode = 'dense', 
                                                        sijs= clustered_sijs)
                #print(budget, per_cls_budget, per_cls_cnt)
                if submod_function in ['disp_min_pc', 'gc_pc']:
                    #print(cluster_idx, per_cls_budget[cluster_idx], cluster_embeddings.shape[0])
                    if per_cls_budget[cluster_idx] == cluster_embeddings.shape[0]:
                        clustergreedyList = obj.maximize(budget=per_cls_budget[cluster_idx]-1, optimizer='StochasticGreedy',
                        stopIfZeroGain=False, stopIfNegativeGain=False, epsilon = 0.1, verbose=False, show_progress=True, costs=None, costSensitiveGreedy=False)
                        rem_elem = list(set(range(cluster_embeddings.shape[0])).difference(set([x[0] for x in clustergreedyList])))[0]
                        rem_gain = clustergreedyList[-1][1]
                        clustergreedyList.append((rem_elem, rem_gain))
                    else:
                        clustergreedyList = obj.maximize(budget=per_cls_budget[cluster_idx], optimizer='StochasticGreedy',
                        stopIfZeroGain=False, stopIfNegativeGain=False, epsilon = 0.1, verbose=False, show_progress=True, costs=None, costSensitiveGreedy=False)
                else:
                    #print(cluster_idx, per_cls_budget[cluster_idx], cluster_embeddings.shape[0])
                    if per_cls_budget[cluster_idx] == cluster_embeddings.shape[0]:
                        clustergreedyList = obj.maximize(budget=per_cls_budget[cluster_idx]-1, optimizer='LazierThanLazyGreedy',
                        stopIfZeroGain=False, stopIfNegativeGain=False, epsilon = 0.1, verbose=False, show_progress=True, costs=None, costSensitiveGreedy=False)
                        rem_elem = list(set(range(cluster_embeddings.shape[0])).difference(set([x[0] for x in clustergreedyList])))[0]
                        rem_gain = clustergreedyList[-1][1]
                        clustergreedyList.append((rem_elem, rem_gain))
                    else:
                        clustergreedyList = obj.maximize(budget=per_cls_budget[cluster_idx], optimizer='LazierThanLazyGreedy',
                        stopIfZeroGain=False, stopIfNegativeGain=False, epsilon = 0.1, verbose=False, show_progress=True, costs=None, costSensitiveGreedy=False)
                cluster_idx += 1
                clusteredgreedylist_with_orig_idxs = [(idxs[x[0]], x[1]) for x in clustergreedyList]
                subset.extend(clusteredgreedylist_with_orig_idxs)
                del obj
            subset.sort(key=lambda x: x[1], reverse=True)
            subsets.append(subset)
            total_time += (time.time() - st_time)

    print("Average Time for Stochastic Greedy Subset Selection is :", total_time)
    #Sorted Label-wise Partition
    # cluster_idxs = {}
    # greedy_idxs = [x[0] for x in subset]
    # for i in greedy_idxs:
    #     if train_labels[i] in cluster_idxs.keys():
    #         cluster_idxs[train_labels[i]].append(i)
    #     else:
    #         cluster_idxs[train_labels[i]] = [i]
    return subsets


def load_dataset(dataset_name, data_dir, seed, return_valid=False, return_test=False):
    if dataset_name == "glue_sst2":
        """
        Load GLUE SST2 dataset. We are only using train and validation splits since the test split doesn't come with gold labels. For testing purposes, we use 5% of train
        dataset as test dataset.
        """
        glue_dataset = datasets.load_dataset("glue", "sst2", cache_dir=data_dir)
        fullset = glue_dataset['train']
        valset = glue_dataset['validation']
        test_set_fraction = 0.05
        seed = 42
        num_fulltrn = len(fullset)
        num_test = int(num_fulltrn * test_set_fraction)
        num_trn = num_fulltrn - num_test
        trainset, testset = random_split(fullset, [num_trn, num_test], generator=torch.Generator().manual_seed(seed))

    elif dataset_name == 'trec6':
        trec6_dataset = datasets.load_dataset("trec", cache_dir=data_dir)
        fullset = trec6_dataset["train"]
        testset = trec6_dataset['test']
        validation_set_fraction = 0.1
        seed = 42
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

    elif dataset_name == 'imdb':
        trec6_dataset = datasets.load_dataset("imdb", cache_dir=data_dir)
        fullset = trec6_dataset["train"]
        testset = trec6_dataset['test']
        validation_set_fraction = 0.1
        seed = 42
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

    elif dataset_name == 'rotten_tomatoes':
        dataset = datasets.load_dataset("rotten_tomatoes", cache_dir=data_dir)
        trainset = dataset["train"]
        valset = dataset['validation']
        testset = dataset['test']
        
    elif dataset_name == 'tweet_eval':
        dataset = datasets.load_dataset("tweet_eval", "emoji", cache_dir=data_dir)
        trainset = dataset["train"]
        valset = dataset['validation']
        testset = dataset['test']

    elif dataset_name == 'cifar10':
        fullset = torchvision.datasets.CIFAR10(root=data_dir, train=True, 
                                              download=True, transform=None)
        testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, 
                                              download=True, transform=None)
        
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

    elif dataset_name == 'cifar100':
        fullset = torchvision.datasets.CIFAR100(root=data_dir, train=True, 
                                              download=True, transform=None)
        testset = torchvision.datasets.CIFAR100(root=data_dir, train=False, 
                                              download=True, transform=None)
        
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))
    
    elif dataset_name == 'tinyimagenet':
        fullset = TinyImageNet(root=data_dir, split='train', download=True, transform=None)
        testset = TinyImageNet(root=data_dir, split='val', download=True, transform=None)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

    elif dataset_name == 'mnist':
        fullset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=None)
        testset = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=None)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))
    else:
        return None

    if not (return_valid and return_test):
        if return_valid:
            return trainset, valset
        elif return_test:
            return trainset, testset
        else:
            return trainset
    else:
        return trainset, valset, testset


def generate_text_similarity_kernel(dataset, model, stats=True, seed=42, data_dir='../data', device='cpu'):    
    #Assertion Check:
    assert (dataset in list(SENTENCE_MAPPINGS.keys())) and (dataset in list(LABEL_MAPPINGS.keys())), \
    "Please add the SENTENCE and LABEL column names to the SENTENCE_MAPPING and LABEL_MAPPINGS dictionaries in generate_global_order.py file."
        
    #Load Dataset
    train_dataset = load_dataset(dataset, data_dir, seed)

    if train_dataset.__class__.__name__ == 'Subset':        
        train_sentences = [x[SENTENCE_MAPPINGS[dataset]] for x in train_dataset] 
        train_labels = [x[LABEL_MAPPINGS[dataset]] for x in train_dataset]
    else:
        train_sentences = train_dataset[SENTENCE_MAPPINGS[dataset]]
        train_labels = train_dataset[LABEL_MAPPINGS[dataset]]            

    # Load embeddings from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model +  '_train_embeddings.pkl')):
        train_embeddings = compute_text_embeddings(model, train_sentences, device)
        store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_' + model +  '_train_embeddings.pkl'), train_embeddings)
    else:
        # Load the embeddings from disc
        train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl'))

    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_dist_kernel.h5')):
        data_dist = get_cdist(train_embeddings)
        with h5py.File(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_dist_kernel.h5'), 'w') as hf:
            hf.create_dataset("dist_kernel",  data=data_dist)
        
        if stats:
            plt.hist(data_dist, bins = 'auto') 
            plt.savefig(dataset + '_' + model + '_dist_hist.png')


def generate_image_similarity_kernel(dataset, model, stats=True, seed=42, data_dir='../data', device='cpu'):    
    #Load Dataset
    train_dataset = load_dataset(dataset, data_dir, seed)

    train_images = [x[0] for x in train_dataset] 
    train_labels = [x[1] for x in train_dataset]

    # Load embeddings from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl')):
        if model[:3] == 'ViT':
            train_embeddings = compute_vit_image_embeddings(train_images, device)
        else:
            train_embeddings = compute_image_embeddings(model, train_images, device)
        store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_' + model + '_train_embeddings.pkl'), train_embeddings)
    else:
        # Load the embeddings from disc
        train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl'))

    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_dist_kernel.h5')):
        data_dist = get_cdist(train_embeddings)
        with h5py.File(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_dist_kernel.h5'), 'w') as hf:
            hf.create_dataset("dist_kernel",  data=data_dist)
        
        if stats:
            plt.hist(data_dist, bins = 'auto') 
            plt.savefig(dataset + '_' + model + '_dist_hist.png')
    

def generate_image_global_order(dataset, model, submod_function, metric, kw, r2_coefficient, knn, seed=42, data_dir='../data', device='cpu'):    
    #Load Dataset
    train_dataset = load_dataset(dataset, data_dir, seed)

    train_images = [x[0] for x in train_dataset] 
    train_labels = [x[1] for x in train_dataset]

    # Load embeddings from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl')):
        if model == 'ViT':
            train_embeddings = compute_vit_image_embeddings(train_images, device)
        elif model == 'ViT_cls':
            train_embeddings = compute_vit_cls_image_embeddings(train_images, device)
        elif model == 'dino':
            train_embeddings = compute_dino_image_embeddings(train_images, device)
        elif model == 'dino_cls':
            train_embeddings = compute_dino_cls_image_embeddings(train_images, device)
        else:
            train_embeddings = compute_image_embeddings(model, train_images, device)
        store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_' + model + '_train_embeddings.pkl'), train_embeddings)
    else:
        # Load the embeddings from disc
        train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl'))

    # Load global order from pickle file if it exists otherwise compute them and store them.
    if not (os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl')) and
            os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(r2_coefficient) + '_global_r2.pkl')) and
            os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(knn) + '_global_knn.pkl'))):
        global_order, global_knn, global_r2, cluster_idxs = compute_global_ordering(train_embeddings, submod_function=submod_function, kw=kw, r2_coefficient=r2_coefficient, knn=knn, train_labels=train_labels, metric=metric)
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl'), {'globalorder': global_order, 'cluster_idxs': cluster_idxs}) 
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(r2_coefficient) + '_global_r2.pkl'), {'globalr2': global_r2, 'cluster_idxs': cluster_idxs}) 
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(knn) + '_global_knn.pkl'), {'globalknn': global_knn, 'cluster_idxs': cluster_idxs}) 
    else:
        global_order = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl'), 'globalorder')
        cluster_idxs = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl'), 'cluster_idxs')
        global_r2 = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(r2_coefficient) + '_global_r2.pkl'), 'globalr2')
        global_knn = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(knn) + '_global_knn.pkl'), 'globalknn')
    return global_order, global_knn, global_r2, cluster_idxs

    
def generate_text_global_order(dataset, model, submod_function, metric, kw, r2_coefficient, knn, seed=42, data_dir='../data', device='cpu'):    
    #Assertion Check:
    assert (dataset in list(SENTENCE_MAPPINGS.keys())) and (dataset in list(LABEL_MAPPINGS.keys())), \
    "Please add the SENTENCE and LABEL column names to the SENTENCE_MAPPING and LABEL_MAPPINGS dictionaries in generate_global_order.py file."
        
    #Load Dataset
    train_dataset = load_dataset(dataset, data_dir, seed)

    if train_dataset.__class__.__name__ == 'Subset':        
        train_sentences = [x[SENTENCE_MAPPINGS[dataset]] for x in train_dataset] 
        train_labels = [x[LABEL_MAPPINGS[dataset]] for x in train_dataset]
    else:
        train_sentences = train_dataset[SENTENCE_MAPPINGS[dataset]]
        train_labels = train_dataset[LABEL_MAPPINGS[dataset]]            

    # Load embeddings from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model +  '_train_embeddings.pkl')):
        train_embeddings = compute_text_embeddings(model, train_sentences, device)
        store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_' + model +  '_train_embeddings.pkl'), train_embeddings)
    else:
        # Load the embeddings from disc
        train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl'))

    # Load global order from pickle file if it exists otherwise compute them and store them.
    if not (os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl')) and
            os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(r2_coefficient) + '_global_r2.pkl')) and
            os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(knn) + '_global_knn.pkl'))):
        global_order, global_knn, global_r2, cluster_idxs = compute_global_ordering(train_embeddings, submod_function=submod_function, kw=kw, r2_coefficient=r2_coefficient, knn=knn, train_labels=train_labels, metric=metric)
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl'), {'globalorder': global_order, 'cluster_idxs': cluster_idxs}) 
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(r2_coefficient) + '_global_r2.pkl'), {'globalr2': global_r2, 'cluster_idxs': cluster_idxs}) 
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(knn) + '_global_knn.pkl'), {'globalknn': global_knn, 'cluster_idxs': cluster_idxs}) 
    else:
        global_order = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl'), 'globalorder')
        cluster_idxs = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl'), 'cluster_idxs')
        global_r2 = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(r2_coefficient) + '_global_r2.pkl'), 'globalr2')
        global_knn = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(knn) + '_global_knn.pkl'), 'globalknn')
    return global_order, global_knn, global_r2, cluster_idxs


def generate_image_stochastic_subsets(dataset, model, submod_function, metric, kw, fraction, n_subsets, seed=42, data_dir='../data', device='cpu'):    
    #Load Dataset
    train_dataset = load_dataset(dataset, data_dir, seed)

    train_images = [x[0] for x in train_dataset] 
    train_labels = [x[1] for x in train_dataset]

    # Load embeddings from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl')):
        if model == 'ViT':
            train_embeddings = compute_vit_image_embeddings(train_images, device)
        elif model == 'ViT_cls':
            train_embeddings = compute_vit_cls_image_embeddings(train_images, device)
        elif model == 'dino':
            train_embeddings = compute_dino_image_embeddings(train_images, device)
        elif model == 'dino_cls':
            train_embeddings = compute_dino_cls_image_embeddings(train_images, device)
        else:
            train_embeddings = compute_image_embeddings(model, train_images, device)
        store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_' + model + '_train_embeddings.pkl'), train_embeddings)
    else:
        # Load the embeddings from disc
        train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl'))

    # Load stochastic subsets from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_' + str(fraction) + '_stochastic_subsets.pkl')):
        stochastic_subsets = compute_stochastic_greedy_subsets(train_embeddings, submod_function, train_labels, kw, metric, fraction, n_subsets=n_subsets)
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_' + str(fraction) + '_stochastic_subsets.pkl'), {'stochastic_subsets': stochastic_subsets}) 
    else:
        stochastic_subsets = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_' + str(fraction) + '_stochastic_subsets.pkl'), 'stochastic_subsets')
    return stochastic_subsets


def generate_text_stochastic_subsets(dataset, model, submod_function, metric, kw, fraction, n_subsets, seed=42, data_dir='../data', device='cpu'):    
    #Assertion Check:
    assert (dataset in list(SENTENCE_MAPPINGS.keys())) and (dataset in list(LABEL_MAPPINGS.keys())), \
    "Please add the SENTENCE and LABEL column names to the SENTENCE_MAPPING and LABEL_MAPPINGS dictionaries in generate_global_order.py file."
        
    #Load Dataset
    train_dataset = load_dataset(dataset, data_dir, seed)

    if train_dataset.__class__.__name__ == 'Subset':        
        train_sentences = [x[SENTENCE_MAPPINGS[dataset]] for x in train_dataset] 
        train_labels = [x[LABEL_MAPPINGS[dataset]] for x in train_dataset]
    else:
        train_sentences = train_dataset[SENTENCE_MAPPINGS[dataset]]
        train_labels = train_dataset[LABEL_MAPPINGS[dataset]]            

    # Load embeddings from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model +  '_train_embeddings.pkl')):
        train_embeddings = compute_text_embeddings(model, train_sentences, device)
        store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_' + model +  '_train_embeddings.pkl'), train_embeddings)
    else:
        # Load the embeddings from disc
        train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_train_embeddings.pkl'))

    # Load stochastic subsets from pickle file if it exists otherwise compute them and store them.
    if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_' + str(fraction) + '_stochastic_subsets.pkl')):
        stochastic_subsets = compute_stochastic_greedy_subsets(train_embeddings, submod_function, train_labels, kw, metric, fraction, n_subsets=n_subsets)
        dict2pickle(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_' + str(fraction) + '_stochastic_subsets.pkl'), {'stochastic_subsets': stochastic_subsets}) 
    else:
        stochastic_subsets = pickle2dict(os.path.join(os.path.abspath(data_dir), dataset + '_' + model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_' + str(fraction) + '_stochastic_subsets.pkl'), 'stochastic_subsets')
    return stochastic_subsets


# def analyze_go_wt_diff_init(dataset, model, submod_function, data_dir='../data', device='cpu', seed=42):

#     #Load Arguments
#     #args = parse_args()

#     #Assertion Check:
#     assert (dataset in list(SENTENCE_MAPPINGS.keys())) and (dataset in list(LABEL_MAPPINGS.keys())), \
#     "Please add the SENTENCE and LABEL column names to the SENTENCE_MAPPING and LABEL_MAPPINGS dictionaries in generate_global_order.py file."
        
#     #Load Dataset
#     train_dataset = load_dataset(dataset, data_dir, seed)

#     if train_dataset.__class__.__name__ == 'Subset':        
#         train_sentences = [x[SENTENCE_MAPPINGS[dataset]] for x in train_dataset] 
#         train_labels = [x[LABEL_MAPPINGS[dataset]] for x in train_dataset]
#     else:
#         train_sentences = train_dataset[SENTENCE_MAPPINGS[dataset]]
#         train_labels = train_dataset[LABEL_MAPPINGS[dataset]]            

#     # Load embeddings from pickle file if it exists otherwise compute them and store them.
#     if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_train_embeddings.pkl')):
#         train_embeddings = compute_text_embeddings(model, train_sentences, device)
#         store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_train_embeddings.pkl'), train_embeddings)
#     else:
#         # Load the embeddings from disc
#         train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_train_embeddings.pkl'))

#     groundset = list(range(train_embeddings.shape[0]))
#     random_inits = random.sample(groundset, 10)
#     subsets = []

#     for random_init in random_inits:
#         remset = [x for x in groundset if x != random_init]
#         remdata = train_embeddings[remset]
#         privatedata = train_embeddings[random_init].reshape(1, -1)
#         global_order = compute_global_ordering(remdata, submod_function= submod_function, train_labels=train_labels, private_embeddings=privatedata)
#         subset = [random_init]
#         rem_subset = [remset[x[0]] for x in global_order]
#         subset.extend(rem_subset)
#         subsets.append(subset)

#     budget = int(0.3 * train_embeddings.shape[0])
    
#     common_fraction = np.zeros((len(subsets), len(subsets)))
#     for i in range(len(subsets)):
#         for j in range(len(subsets)):
#            common_fraction[i][j] = len(set(subsets[i][:budget]).intersection(set(subsets[j][:budget])))/len(set(subsets[i][:budget]))
#     return common_fraction


# def analyze_go_label_dists(dataset, model, submod_function, data_dir='../data', device='cpu', seed=42):

#     #Load Arguments
#     #args = parse_args()
#     fractions = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1]
#     label_dists_dict = {}
#     #Assertion Check:
#     assert (dataset in list(SENTENCE_MAPPINGS.keys())) and (dataset in list(LABEL_MAPPINGS.keys())), \
#     "Please add the SENTENCE and LABEL column names to the SENTENCE_MAPPING and LABEL_MAPPINGS dictionaries in generate_global_order.py file."
        
#     #Load Dataset
#     train_dataset = load_dataset(dataset, data_dir, seed)

#     if train_dataset.__class__.__name__ == 'Subset':        
#         train_sentences = [x[SENTENCE_MAPPINGS[dataset]] for x in train_dataset] 
#         train_labels = [x[LABEL_MAPPINGS[dataset]] for x in train_dataset]
#     else:
#         train_sentences = train_dataset[SENTENCE_MAPPINGS[dataset]]
#         train_labels = train_dataset[LABEL_MAPPINGS[dataset]]            

#     # Load embeddings from pickle file if it exists otherwise compute them and store them.
#     if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_train_embeddings.pkl')):
#         train_embeddings = compute_text_embeddings(model, train_sentences, device)
#         store_embeddings(os.path.join(os.path.abspath(data_dir), dataset  + '_train_embeddings.pkl'), train_embeddings)
#     else:
#         # Load the embeddings from disc
#         train_embeddings = load_embeddings(os.path.join(os.path.abspath(data_dir), dataset + '_train_embeddings.pkl'))

#     if not os.path.exists(os.path.join(os.path.abspath(data_dir), dataset + '_' + submod_function + '_global_order.pkl')):
#         global_order, global_knn, global_r2 = compute_global_ordering(train_embeddings, submod_function= submod_function, train_labels=train_labels)
#         store_globalorder(os.path.join(os.path.abspath(data_dir), dataset + '_' + submod_function + '_global_order.pkl'), global_order, global_knn, global_r2)
#     else:
#         global_order, global_knn, global_r2 = load_globalorder(os.path.join(os.path.abspath(data_dir), dataset + '_' + submod_function + '_global_order.pkl'))
    
#     global_order_idxs = [x[0] for x in global_order]
#     num_labels = len(set(train_labels))

#     for fraction in fractions:
#         budget = int(fraction * train_embeddings.shape[0])
#         label_dist = np.array([0]*num_labels)
#         for i in range(budget):
#             label = train_labels[global_order_idxs[i]]
#             label_dist[label] += 1 
#         label_dist = label_dist/budget
#         label_dists_dict[fraction] = label_dist
#     return label_dists_dict


if __name__ == "__main__":
    #args = parse_args()
    #submod_functions = ['fl', 'supfl', 'gc', 'logdet']
    #label_dists_dict = analyze_go_label_dists(args.dataset, args.model, args.submod_function, data_dir=args.data_dir, device=args.device, seed=args.seed)
    train_dataset = load_dataset('cifar10', '../data', 42)

    train_images = [x[0] for x in train_dataset] 
    train_labels = [x[1] for x in train_dataset]
    compute_vit_image_embeddings(train_images, 'cuda')

    print()
File Path: cords/utils/data/data_utils/regression_data_utils.py
Content:
import numpy as np
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


## Utility function to load datasets from libsvm datasets
def csv_file_load(path,dim,skip=False,save_data=False):
    data = []
    target = []
    with open(path) as fp:
       if skip:
           line = fp.readline()
       line = fp.readline()
       while line:
        temp = [i for i in line.strip().split(",")]
        target.append(int(float(temp[-1]))) # Class Number. # Not assumed to be in (0, K-1)
        temp_data = [0]*dim
        count = 0
        for i in temp[:-1]:
            #ind, val = i.split(':')
            temp_data[count] = float(i)
            count += 1
        data.append(temp_data)
        line = fp.readline()
    X_data = np.array(data, dtype=np.float32)
    Y_label = np.array(target)
    if save_data:
        # Save the numpy files to the folder where they come from
        data_np_path = path + '.data.npy'
        target_np_path = path + '.label.npy'
        np.save(data_np_path, X_data)
        np.save(target_np_path, Y_label)
    return (X_data, Y_label)

def libsvm_file_load(path,dim, save_data=False):
    data = []
    target = []
    with open(path) as fp:
       line = fp.readline()
       while line:
        temp = [i for i in line.strip().split(" ")]
        target.append(int(float(temp[0]))) # Class Number. # Not assumed to be in (0, K-1)
        temp_data = [0]*dim
        
        for i in temp[1:]:
            if len(i) > 1: 
                ind,val = i.split(':')
                temp_data[int(ind)-1] = float(val)
        data.append(temp_data)
        line = fp.readline()
    X_data = np.array(data,dtype=np.float32)
    Y_label = np.array(target)
    if save_data:
        # Save the numpy files to the folder where they come from
        data_np_path = path + '.data.npy'
        target_np_path = path + '.label.npy'
        np.save(data_np_path, X_data)
        np.save(target_np_path, Y_label)
    return (X_data, Y_label)

def clean_lawschool_full(path):
   
    df = pd.read_csv(path)
    df = df.dropna()
    # remove y from df
    y = df['ugpa']
    y = y / 4
    df = df.drop('ugpa', axis=1)
    # convert gender variables to 0,1
    df['gender'] = df['gender'].map({'male': 1, 'female': 0})
    # add bar1 back to the feature set
    df_bar = df['bar1']
    df = df.drop('bar1', axis=1)
    df['bar1'] = [int(grade == 'P') for grade in df_bar]
    #df['race'] = [int(race == 7.0) for race in df['race']]
    #a = df['race']
    return df.to_numpy(), y.to_numpy()

def majority_pop(a):
    """
    Identify the main ethnicity group of each community
    """
    B = "racepctblack"
    W = "racePctWhite"
    A = "racePctAsian"
    H = "racePctHisp"
    races = [B, W, A, H]
    maj = a.apply(pd.Series.idxmax, axis=1)
    return maj

def clean_communities_full(path):
    """
    Extract black and white dominant communities; 
    sub_size : number of communities for each group
    """
    df = pd.read_csv(path)
    df = df.fillna(0)
    B = "racepctblack"
    W = "racePctWhite"
    A = "racePctAsian"
    H = "racePctHisp"
    sens_features = [2, 3, 4, 5]
    df_sens = df.iloc[:, sens_features]

    # creating labels using crime rate
    Y = df['ViolentCrimesPerPop']
    df = df.drop('ViolentCrimesPerPop', axis=1)

    maj = majority_pop(df_sens)

    # remap the values of maj
    a = maj.map({B : 0, W : 1, A : 2, H : 3})
   
    df['race'] = a
    df = df.drop(H, axis=1)
    df = df.drop(B, axis=1)
    df = df.drop(W, axis=1)
    df = df.drop(A, axis=1)

    #print(df.head())

    return df.to_numpy(), Y.to_numpy()

def community_crime_load(path,dim, save_data=False):

    data = []
    target = []
    with open(path) as fp:
        line = fp.readline()
        while line:
            temp = [i.strip() for i in line.strip().split(",")][5:]

            target.append(float(temp[-1]))
            
            temp_data = [0.0]*dim
            
            #print(temp)

            for i in range(len(temp[:-1])):

                if temp[i] != '?':
                    temp_data[i] = float(temp[i])
            
            data.append(temp_data)
            line = fp.readline()
    
    X_data = np.array(data, dtype=np.float32)
    Y_label = np.array(target)
    
    if save_data:
        # Save the numpy files to the folder where they come from
        data_np_path = path + '.data.npy'
        target_np_path = path + '.label.npy'
        np.save(data_np_path, X_data)
        np.save(target_np_path, Y_label)
    return (X_data, Y_label)

File Path: cords/utils/data/data_utils/weightedsubset.py
Content:
from typing import TypeVar, Sequence
from torch.utils.data import Dataset

T_co = TypeVar('T_co', covariant=True)
T = TypeVar('T')

class WeightedSubset(Dataset[T_co]):
    r"""
    Subset of a dataset with weights at specified indices.

    Args:
        dataset (Dataset): The whole Dataset
        indices (sequence): Indices in the whole set selected for subset
        weights (sequence): Weights of the subset
    """
    dataset: Dataset[T_co]
    indices: Sequence[int]
    weights: Sequence[float]

    def __init__(self, dataset: Dataset[T_co], indices: Sequence[int], weights: Sequence[float]) -> None:
        self.dataset = dataset
        self.indices = indices
        self.weights = weights

    def __getitem__(self, idx):
        tmp_list = list(self.dataset[self.indices[idx]])
        tmp_list.append(self.weights[idx])
        return tuple(tmp_list)

    def __len__(self):
        return len(self.indices)
File Path: cords/utils/data/dataloader/SL/__init__.py
Content:
from .dssdataloader import DSSDataLoader

File Path: cords/utils/data/dataloader/SL/adaptive/__init__.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from .glisterdataloader import GLISTERDataLoader
from .gradmatchdataloader import GradMatchDataLoader
from .craigdataloader import CRAIGDataLoader
from .adaptiverandomdataloader import AdaptiveRandomDataLoader
from .randomdataloader import RandomDataLoader
from .selcondataloader import SELCONDataLoader
from .adapweightsdataloader import AdapWeightsDataLoader
from .weightedrandomdataloader import WeightedRandomDataLoader
from .stochasticgreedydataloader import StochasticGreedyDataLoader
from .milodataloader import MILODataLoader
File Path: cords/utils/data/dataloader/SL/adaptive/adaptivedataloader.py
Content:
import logging
from abc import abstractmethod
from torch.utils.data import DataLoader
from ..dssdataloader import DSSDataLoader
from math import ceil


class AdaptiveDSSDataLoader(DSSDataLoader):
    """
    Implementation of AdaptiveDSSDataLoader class which serves as base class for dataloaders of other
    adaptive subset selection strategies for supervised learning framework.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        
        """
        Constructor function
        """
        super(AdaptiveDSSDataLoader, self).__init__(train_loader.dataset, dss_args,
                                                    logger, *args, **kwargs)
        self.train_loader = train_loader
        self.val_loader = val_loader
        
        # Arguments assertion check
        assert "select_every" in dss_args.keys(), "'select_every' is a compulsory argument. Include it as a key in dss_args"
        assert "device" in dss_args.keys(), "'device' is a compulsory argument. Include it as a key in dss_args"
        assert "kappa" in dss_args.keys(), "'kappa' is a compulsory argument. Include it as a key in dss_args"
        self.select_every = dss_args.select_every
        self.device = dss_args.device
        self.kappa = dss_args.kappa
        if dss_args.kappa > 0:
            assert "num_epochs" in dss_args.keys(), "'num_epochs' is a compulsory argument when warm starting the model(i.e., kappa > 0). Include it as a key in dss_args"
            self.select_after =  int(dss_args.kappa * dss_args.num_epochs)
            self.warmup_epochs = ceil(self.select_after * dss_args.fraction)
        else:
            self.select_after = 0
            self.warmup_epochs = 0
        self.initialized = False
        
    
    def __iter__(self):
        """
        Iter function that returns the iterator of full data loader or data subset loader or empty loader based on the 
        warmstart kappa value.
        """
        self.initialized = True
        if self.warmup_epochs < self.cur_epoch < self.select_after:
            self.logger.debug(
                "Skipping epoch {0:d} due to warm-start option. ".format(self.cur_epoch, self.warmup_epochs))
            loader = DataLoader([])
            
        elif self.cur_epoch < self.warmup_epochs:
            self.logger.debug('Epoch: {0:d}, reading dataloader... '.format(self.cur_epoch))
            loader = self.wtdataloader
            self.logger.debug('Epoch: {0:d}, finished reading dataloader. '.format(self.cur_epoch))
        else:
            self.logger.debug('Epoch: {0:d}, reading dataloader... '.format(self.cur_epoch))
            if ((self.cur_epoch) % self.select_every == 0) and (self.cur_epoch > 1):
                self.resample()
            loader = self.subset_loader
            self.logger.debug('Epoch: {0:d}, finished reading dataloader. '.format(self.cur_epoch))
            
        self.cur_epoch += 1
        return loader.__iter__()

    def __len__(self) -> int:
        """
        Returns the length of the current data loader
        """
        if self.warmup_epochs < self.cur_epoch <= self.select_after:
            self.logger.debug(
                "Skipping epoch {0:d} due to warm-start option. ".format(self.cur_epoch, self.warmup_epochs))
            loader = DataLoader([])
            return len(loader)

        elif self.cur_epoch <= self.warmup_epochs:
            self.logger.debug('Epoch: {0:d}, reading dataloader... '.format(self.cur_epoch))
            loader = self.wtdataloader
            #self.logger.debug('Epoch: {0:d}, finished reading dataloader. '.format(self.cur_epoch))
            return len(loader)
        else:
            self.logger.debug('Epoch: {0:d}, reading dataloader... '.format(self.cur_epoch))
            loader = self.subset_loader
            return len(loader)
            
    def resample(self):
        """
        Function that resamples the subset indices and recalculates the subset weights
        """
        self.subset_indices, self.subset_weights = self._resample_subset_indices()
        self.logger.debug("Subset indices length: %d", len(self.subset_indices))
        self._refresh_subset_loader()
        self.logger.debug("Subset loader initiated, args: %s, kwargs: %s", self.loader_args, self.loader_kwargs)
        self.logger.debug('Subset selection finished, Training data size: %d, Subset size: %d',
                     self.len_full, len(self.subset_loader.dataset))

    @abstractmethod
    def _resample_subset_indices(self):
        """
        Abstract function that needs to be implemented in the child classes. 
        Needs implementation of subset selection implemented in child classes.
        """
        raise Exception('Not implemented.')

File Path: cords/utils/data/dataloader/SL/adaptive/adaptiverandomdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import RandomStrategy
import time


class AdaptiveRandomDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of AdaptiveRandomDataLoader that serves as the dataloader for the adaptive Random subset selection strategy.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for Random subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        """
        Constructor function
        """
        super(AdaptiveRandomDataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                                 logger, *args, **kwargs)
        self.strategy = RandomStrategy(train_loader, online=True)
        self.logger.debug('AdaptiveRandom dataloader initialized.')

    def _resample_subset_indices(self):
        """
        Function that calls the Random subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        print("Epoch: {0:d}, requires subset selection. ".format(self.cur_epoch))
        self.logger.debug("Random budget: %d", self.budget)
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info("Epoch: {0:d}, AdaptiveRandom subset selection finished, takes {1:.4f}. ".format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights
File Path: cords/utils/data/dataloader/SL/adaptive/adapweightsdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import AdapWeightsStrategy
import time, copy, torch


class AdapWeightsDataLoader(AdaptiveDSSDataLoader):

    def __init__(self, train_loader, val_loader, dss_args, logger, ss_indices, *args, **kwargs):
        
        """
         Arguments assertion check
        """
        assert "model" in dss_args.keys(), "'model' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        if dss_args.loss.reduction != "none":
            raise ValueError("Please set 'reduction' of loss function to 'none' for adaptive subset selection strategies")
        assert "eta" in dss_args.keys(), "'eta' is a compulsory argument. Include it as a key in dss_args"
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "valid" in dss_args.keys(), "'valid' is a compulsory argument for GradMatch. Include it as a key in dss_args"

        super(AdapWeightsDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                                  logger, *args, **kwargs)
        self.strategy = AdapWeightsStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), dss_args.loss, dss_args.eta,
                                          dss_args.device, dss_args.num_classes, dss_args.linear_layer, dss_args.selection_type,
                                          logger, ss_indices, dss_args.valid)
        self.train_model = dss_args.model
        self.logger.debug('AdapWeights dataloader initialized. ')

    def _resample_subset_indices(self):
        start = time.time()
        self.logger.debug("Epoch: {0:d}, requires subset selection. ".format(self.cur_epoch))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        end = time.time()
        self.logger.info("Epoch: {0:d}, AdapWeights subset selection finished, takes {1:.4f}. ".format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/adaptive/craigdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import CRAIGStrategy
import time, copy


# CRAIG
class CRAIGDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of CRAIGDataLoader that serves as the dataloader for the adaptive CRAIG subset selection strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        """
         Constructor function
        """
        # Arguments assertion check
        assert "model" in dss_args.keys(), "'model' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduction != "none":
            raise ValueError("Please set 'reduction' of loss function to 'none' for adaptive subset selection strategies")
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "optimizer" in dss_args.keys(), "'optimizer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "if_convex" in dss_args.keys(), "'if_convex' is a compulsory argument for CRAIG. Include it as a key in dss_args"

        super(CRAIGDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                                logger, *args, **kwargs)
        
        self.strategy = CRAIGStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), dss_args.loss, 
                                     dss_args.device, dss_args.num_classes, dss_args.linear_layer,  
                                     dss_args.if_convex, dss_args.selection_type, logger, dss_args.optimizer)
        self.train_model = dss_args.model        
        self.logger.info('CRAIG dataloader initialized. ')

    def _resample_subset_indices(self):
        """
        Function that calls the CRAIG subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        
        start = time.time()
        self.logger.info('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        end = time.time()
        self.logger.info('Epoch: {0:d}, subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/adaptive/glisterdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import GLISTERStrategy
import time, copy


# GLISTER
class GLISTERDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of GLISTERDataLoader that serves as the dataloader for the adaptive GLISTER subset selection strategy from the paper 
    :footcite:`killamsetty2021glister`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for GLISTER subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        """
         Constructor function
        """

        # Arguments assertion check
        assert "model" in dss_args.keys(), "'model' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduction != "none":
            raise ValueError("Please set 'reduction' of loss function to 'none' for adaptive subset selection strategies")
        assert "eta" in dss_args.keys(), "'eta' is a compulsory argument. Include it as a key in dss_args"
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for GLISTER. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for GLISTER. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for GLISTER. Include it as a key in dss_args"
        assert "greedy" in dss_args.keys(), "'greedy' is a compulsory argument for GLISTER. Include it as a key in dss_args"
        if dss_args.greedy == 'RGreedy':
            assert "r" in dss_args.keys(), "'r' is a compulsory argument for RGreedy version of GLISTER. Include it as a key in dss_args"
        else:
            dss_args.r = 0
        
        super(GLISTERDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                                logger, *args, **kwargs)
        
        self.strategy = GLISTERStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), dss_args.loss, dss_args.eta, dss_args.device,
                                        dss_args.num_classes, dss_args.linear_layer, dss_args.selection_type, dss_args.greedy, logger, r=dss_args.r)
        self.train_model = dss_args.model    
        self.logger.debug('Glister dataloader initialized. ')

    def _resample_subset_indices(self):
        """
        Function that calls the GLISTER subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        end = time.time()
        self.logger.info('Epoch: {0:d}, GLISTER dataloader subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/adaptive/gradmatchdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import GradMatchStrategy
import time, copy, torch


class GradMatchDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of GradMatchDataLoader that serves as the dataloader for the adaptive GradMatch subset selection strategy from the paper 
    :footcite:`pmlr-v139-killamsetty21a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for GradMatch subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        
        """
         Constructor function
        """
        # Arguments assertion check
        assert "model" in dss_args.keys(), "'model' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        if dss_args.loss.reduction != "none":
            raise ValueError("Please set 'reduction' of loss function to 'none' for adaptive subset selection strategies")
        assert "eta" in dss_args.keys(), "'eta' is a compulsory argument. Include it as a key in dss_args"
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "valid" in dss_args.keys(), "'valid' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "v1" in dss_args.keys(), "'v1' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "lam" in dss_args.keys(), "'lam' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "eps" in dss_args.keys(), "'eps' is a compulsory argument for GradMatch. Include it as a key in dss_args"

        super(GradMatchDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                                  logger, *args, **kwargs)
        self.strategy = GradMatchStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), dss_args.loss, dss_args.eta,
                                          dss_args.device, dss_args.num_classes, dss_args.linear_layer, dss_args.selection_type,
                                          logger, dss_args.valid, dss_args.v1, dss_args.lam, dss_args.eps)
        self.train_model = dss_args.model
        self.logger.debug('Grad-match dataloader initialized. ')

    def _resample_subset_indices(self):
        """
        Function that calls the GradMatch subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug("Epoch: {0:d}, requires subset selection. ".format(self.cur_epoch))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        end = time.time()
        self.logger.info("Epoch: {0:d}, GradMatch subset selection finished, takes {1:.4f}. ".format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/adaptive/milodataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import WeightedRandomExplorationStrategy
from torch.utils.data import DataLoader
from .weightedrandomdataloader import WeightedRandomDataLoader
from .stochasticgreedydataloader import StochasticGreedyDataLoader
import time, math


class MILODataLoader(AdaptiveDSSDataLoader):
    """
    Implements of MILODataLoader that serves as the dataloader for the adaptive CRAIG subset selection strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        self.strategy = WeightedRandomExplorationStrategy(train_loader, 
                                            dss_args.global_order_file, 
                                            online=True, 
                                            temperature=dss_args.temperature, 
                                            per_class=dss_args.per_class)
        super(MILODataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                                 logger, *args, **kwargs)
        """
         Arguments assertion check
        """
        assert "global_order_file" in dss_args.keys(), "'global_order_file' is a compulsory argument. Include it as a key in dss_args"
        #assert "facloc_stochastic_subsets_file" in dss_args.keys(), "'facloc_stochastic_subsets_file' is a compulsory argument. Include it as a key in dss_args"
        assert "gc_stochastic_subsets_file" in dss_args.keys(), "'gc_stochastic_subsets_file' is a compulsory argument. Include it as a key in dss_args"
        assert "temperature" in dss_args.keys(), "'temperature' is a compulsory argument. Include it as a key in dss_args"
        assert "per_class" in dss_args.keys(), "'per_class' is a compulsory argument. Include it as a key in dss_args"
        assert "num_epochs" in dss_args.keys(), "'num_epochs' is a compulsory argument when warm starting the model(i.e., kappa > 0). Include it as a key in dss_args"
        assert "gc_ratio" in dss_args.keys(), "'gc_ratio' is a compulsory argument when warm starting the model(i.e., kappa > 0). Include it as a key in dss_args"
        
        self.logger.debug('Hybrid Global order dataloader initialized.')
        self.num_epochs = dss_args.num_epochs
        self.olgo_loader = WeightedRandomDataLoader(train_loader, dss_args, logger, *args, **kwargs)
        self.gc_ratio = dss_args.gc_ratio
        dss_args.stochastic_subsets_file = dss_args.gc_stochastic_subsets_file
        self.gc_stochasticgo_loader = StochasticGreedyDataLoader(train_loader, dss_args, logger, *args, **kwargs)

    def __iter__(self):
        """
        Iter function that returns the curriculum of easy to hard subsets using stochastic greedy exploration and weighted random exploration based on the value of gc_ratio.
        """
        self.initialized = True
        if self.cur_epoch < math.ceil(self.gc_ratio * self.num_epochs):
            self.logger.debug(
                "Using GC Stochastic Data Loader from epoch {0:d}".format(self.cur_epoch, math.ceil((1/12) * self.num_epochs)))
            loader = self.gc_stochasticgo_loader
        else:
            self.logger.debug('Epoch: {0:d}, reading dataloader... '.format(self.cur_epoch))
            loader = self.olgo_loader
            self.logger.debug('Epoch: {0:d}, finished reading dataloader. '.format(self.cur_epoch))
        self.cur_epoch += 1
        return loader.__iter__()

    # Over-riding initial random subset selection
    def _init_subset_loader(self):
        """
        Function that initializes the initial subset loader
        """
        self.subset_indices, self.subset_weights = self._init_subset_indices()
        self._refresh_subset_loader()

    def _init_subset_indices(self):
        """
        Function that initializes the initial subset indices
        """
        start = time.time()
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info('Epoch: {0:d}, GlobalOrder based subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

    def _resample_subset_indices(self):
        pass
File Path: cords/utils/data/dataloader/SL/adaptive/randomdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import RandomStrategy
import time


class RandomDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of RandomDataLoader that serves as the dataloader for the non-adaptive Random subset selection strategy.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for Random subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        """
        Constructor function
        """
        self.strategy = RandomStrategy(train_loader, online=False)
        super(RandomDataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                                    logger, *args, **kwargs)
        
        self.logger.debug('Random dataloader initialized.')

    # Over-riding initial random subset selection
    def _init_subset_loader(self):
        # All strategies start with random selection
        self.subset_indices, self.subset_weights = self._init_subset_indices()
        self._refresh_subset_loader()

    def _init_subset_indices(self):
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        start = time.time()
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info('Epoch: {0:d}, Random subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

    def _resample_subset_indices(self):
        """
        Function that calls the Random subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug("Epoch: {0:d}, requires subset selection. ".format(self.cur_epoch))
        self.logger.debug("Random budget: %d", self.budget)
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info("Epoch: {0:d}, Random subset selection finished, takes {1:.4f}. ".format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/adaptive/selcondataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import SELCONstrategy
import time, copy


# SELCONstrategy
class SELCONDataLoader(AdaptiveDSSDataLoader):
    def __init__(self, trainset, validset, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        """
         Arguments assertion check
        """
        assert "model" in dss_args.keys(), "'model' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduction != "none":
            raise ValueError("Please set 'reduction' of loss function to 'none' for adaptive subset selection strategies")
        assert "device" in dss_args.keys(), "'device' is a compulsory argument. Include it as a key in dss_args"
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for SELCON. Include it as a key in dss_args"
        assert "delta" in dss_args.keys(), "'delta' is a compulsory argument for SELCON. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for SELCON. Include it as a key in dss_args"
        
        '''
        self, trainloader, valloader, model, 
        loss_func, device, num_classes, delta, 
        linear_layer, lam, lr, logger, optimizer, 
        batch_size, criterion
        '''
        
        super(SELCONDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                                logger, *args, **kwargs)
        
        self.strategy = SELCONstrategy(trainset, validset, train_loader, val_loader, copy.deepcopy(dss_args.model), dss_args.loss, dss_args.device,
                                        dss_args.num_classes, dss_args.delta, dss_args.num_epochs, dss_args.linear_layer, dss_args.lam, dss_args.lr, 
                                        logger, dss_args.optimizer, dss_args.batch_size, dss_args.criterion)

        self.train_model = dss_args.model
        self.logger.debug('SELCON dataloader initialized. ')

    def _resample_subset_indices(self):
        start = time.time()
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        end = time.time()
        self.logger.info('Epoch: {0:d}, SELCON dataloader subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/adaptive/stochasticgreedydataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import StochasticGreedyExplorationStrategy
import time


class StochasticGreedyDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of StochasticGreedyDataLoader that serves as the dataloader for the adaptive CRAIG subset selection strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        self.strategy = StochasticGreedyExplorationStrategy(train_loader, 
                                            dss_args.stochastic_subsets_file, 
                                            )
        super(StochasticGreedyDataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                                 logger, *args, **kwargs)
        """
         Arguments assertion check
        """
        assert "stochastic_subsets_file" in dss_args.keys(), "'stochastic_subsets_file' is a compulsory argument. Include it as a key in dss_args"
        self.logger.debug('Global order stochastic dataloader initialized.')

    # Over-riding initial random subset selection
    def _init_subset_loader(self):
        """
        Function that initializes the initial subset loader
        """
        self.subset_indices, self.subset_weights = self._init_subset_indices()
        self._refresh_subset_loader()

    def _init_subset_indices(self):
        """
        Function that initializes the initial subset indices
        """
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        start = time.time()
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info('Epoch: {0:d}, GlobalOrder stochastic subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

    def _resample_subset_indices(self):
        """
        Function that calls the Stochastic Greedy subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        print("Epoch: {0:d}, requires subset selection. ".format(self.cur_epoch))
        self.logger.debug("Stochastic GlobalOrder budget: %d", self.budget)
        start = time.time()
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info("Epoch: {0:d}, GlobalOrder stochastic subset selection finished, takes {1:.4f}. ".format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/adaptive/weightedrandomdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import WeightedRandomExplorationStrategy
import time


class WeightedRandomDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of WeightedRandomDataLoader that serves as the dataloader for the adaptive CRAIG subset selection strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        self.strategy = WeightedRandomExplorationStrategy(train_loader, 
                                            dss_args.global_order_file, 
                                            online=True, 
                                            temperature=dss_args.temperature, 
                                            per_class=dss_args.per_class)
        super(WeightedRandomDataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                                 logger, *args, **kwargs)
        """
         Arguments assertion check
        """
        assert "global_order_file" in dss_args.keys(), "'global_order_file' is a compulsory argument. Include it as a key in dss_args"
        assert "temperature" in dss_args.keys(), "'temperature' is a compulsory argument. Include it as a key in dss_args"
        assert "per_class" in dss_args.keys(), "'per_class' is a compulsory argument. Include it as a key in dss_args"

        self.logger.debug('Global order dataloader initialized.')

    # Over-riding initial random subset selection
    def _init_subset_loader(self):
        """
        Function that initializes the initial subset loader
        """
        self.subset_indices, self.subset_weights = self._init_subset_indices()
        self._refresh_subset_loader()

    def _init_subset_indices(self):
        """
        Function that initializes the initial subset indices
        """
        start = time.time()
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info('Epoch: {0:d}, GlobalOrder based subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

    def _resample_subset_indices(self):
        """
        Function that calls the Weighted Random subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        print("Epoch: {0:d}, requires subset selection. ".format(self.cur_epoch))
        self.logger.debug("OLGlobalOrder budget: %d", self.budget)
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info("Epoch: {0:d}, OLGlobalOrder based subset selection finished, takes {1:.4f}. ".format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SL/dssdataloader.py
Content:
from abc import abstractmethod
from cords.utils.data.data_utils import WeightedSubset
from torch.utils.data.dataloader import DataLoader
import torch
import numpy as np


# Base objects
class DSSDataLoader:
    """
    Implementation of DSSDataLoader class which serves as base class for dataloaders of other
    selection strategies for supervised learning framework.

    Parameters
    -----------
    full_data: torch.utils.data.Dataset Class
        Full dataset from which data subset needs to be selected.
    dss_args: dict 
        Data subset selection arguments dictionary
    logger: class
        Logger class for logging the information
    """
    def __init__(self, full_data, dss_args, logger, *args, **kwargs):
        """
        Constructor Method
        """
        super(DSSDataLoader, self).__init__()
        # TODO: Integrate verbose in logging
        self.len_full = len(full_data)
        """
         Arguments assertion check
        """
        assert "fraction" in dss_args.keys(), "'fraction' is a compulsory argument. Include it as a key in dss_args"
        if (dss_args.fraction > 1) or (dss_args.fraction<0):
             raise ValueError("'fraction' should lie between 0 and 1")

        self.fraction = dss_args.fraction
        self.budget = int(self.len_full * self.fraction)
        self.logger = logger
        self.dataset = full_data
        self.loader_args = args
        self.loader_kwargs = kwargs
        self.subset_indices = None
        self.subset_weights = None
        self.subset_loader = None
        self.batch_wise_indices = None
        self.selection_count = 0
        self.selected_idxs = {}
        # self.strategy = None
        self.cur_epoch = 0
        wt_trainset = WeightedSubset(full_data, list(range(len(full_data))), [1]*len(full_data))
        self.wtdataloader = torch.utils.data.DataLoader(wt_trainset, *self.loader_args, **self.loader_kwargs)
        self._init_subset_loader()

    def __getattr__(self, item):
        return object.__getattribute__(self, "subset_loader").__getattribute__(item)

    def _init_subset_loader(self):
        """
        Function that initializes the random data subset loader
        """
        # All strategies start with random selection
        self.subset_indices = self._init_subset_indices()
        self.subset_weights = torch.ones(self.budget)
        self._refresh_subset_loader()

    # Default subset indices comes from random selection
    def _init_subset_indices(self):
        """
        Function that initializes the subset indices randomly
        """
        return np.random.choice(self.len_full, size=self.budget, replace=False)

    def _refresh_subset_loader(self):
        """
        Function that regenerates the data subset loader using new subset indices and subset weights
        """
        self.selected_idxs[self.selection_count] = self.subset_indices
        self.selection_count += 1
	
        self.subset_loader = DataLoader(WeightedSubset(self.dataset, self.subset_indices, self.subset_weights), 
                                        *self.loader_args, **self.loader_kwargs)
        self.batch_wise_indices = list(self.subset_loader.batch_sampler)
File Path: cords/utils/data/dataloader/SL/nonadaptive/__init__.py
Content:
from .craigdataloader import CRAIGDataLoader
from .nonadaptivedataloader import NonAdaptiveDSSDataLoader
from .submoddataloader import FacLocDataLoader
from .submoddataloader import GraphCutDataLoader
from .submoddataloader import SaturatedCoverageDataLoader
from .submoddataloader import SumRedundancyDataLoader
from .milofixeddataloader import MILOFixedDataLoader
File Path: cords/utils/data/dataloader/SL/nonadaptive/craigdataloader.py
Content:
from .nonadaptivedataloader import NonAdaptiveDSSDataLoader
from cords.selectionstrategies.SL import CRAIGStrategy
import time, copy


# CRAIG
class CRAIGDataLoader(NonAdaptiveDSSDataLoader):
    """
    Implements of CRAIGDataLoader that serves as the dataloader for the nonadaptive CRAIG subset selection strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        """
         Constructor function
        """
        # Arguments assertion check
        assert "model" in dss_args.keys(), "'model' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduction != "none":
            raise ValueError("Please set 'reduction' of loss function to 'none' for adaptive subset selection strategies")
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "optimizer" in dss_args.keys(), "'optimizer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "if_convex" in dss_args.keys(), "'if_convex' is a compulsory argument for CRAIG. Include it as a key in dss_args"

        super(CRAIGDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                              logger, *args, **kwargs)
        
        self.strategy = CRAIGStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), dss_args.num_classes, 
                                     dss_args.linear_layer, dss_args.loss, dss_args.device, 
                                     dss_args.if_convex, dss_args.selection_type, logger, dss_args.optimizer)
        self.train_model = dss_args.model
        self.eta = dss_args.eta
        self.num_cls = dss_args.num_classes
        self.model = dss_args.model
        self.loss = copy.deepcopy(dss_args.loss)
        self.logger.debug('Non-adaptive CRAIG dataloader loader initialized. ')

    def _init_subset_loader(self):
        """
        Function that initializes the subset loader based on the subset indices and the subset weights.
        """
        # All strategies start with random selection
        self.subset_indices, self.subset_weights = self._init_subset_indices()
        self._refresh_subset_loader()

    def _init_subset_indices(self):
        """
        Function that calls the CRAIG strategy for initial subset selection and calculating the initial subset weights.
        """
        start = time.time()
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        end = time.time()
        self.logger.info('Epoch: {0:d}, CRAIG subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights
File Path: cords/utils/data/dataloader/SL/nonadaptive/milofixeddataloader.py
Content:
from .nonadaptivedataloader import NonAdaptiveDSSDataLoader
from cords.selectionstrategies.SL import WeightedRandomExplorationStrategy
import time, copy


class MILOFixedDataLoader(NonAdaptiveDSSDataLoader):
    """
    Implements of MILOFixedDataLoader that serves as the dataloader for the adaptive CRAIG subset selection strategy from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        self.strategy = WeightedRandomExplorationStrategy(train_loader, dss_args.global_order_file, online=False, per_class=dss_args.per_class)
        super(MILOFixedDataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                                 logger, *args, **kwargs)
        
        """
         Arguments assertion check
        """
        assert "global_order_file" in dss_args.keys(), "'global_order_file' is a compulsory argument. Include it as a key in dss_args"
        assert "per_class" in dss_args.keys(), "'per_class' is a compulsory argument. Include it as a key in dss_args"
        
        self.logger.debug('Global order dataloader initialized.')

    def _init_subset_loader(self):
        """
        Function that initializes the initial subset loader
        """
        self.subset_indices, self.subset_weights = self._init_subset_indices()
        self._refresh_subset_loader()

    def _init_subset_indices(self):
        """
        Function that initializes the initial subset indices
        """
        start = time.time()
        self.logger.debug('Epoch: {0:d}, requires subset selection. '.format(self.cur_epoch))
        subset_indices, subset_weights = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info('Epoch: {0:d}, GlobalOrder based subset selection finished, takes {1:.4f}. '.format(self.cur_epoch, (end - start)))
        return subset_indices, subset_weights
    
    def __len__(self) -> int:
        self.logger.debug('Epoch: {0:d}, reading dataloader... '.format(self.cur_epoch))
        loader = self.subset_loader
        return len(loader)
File Path: cords/utils/data/dataloader/SL/nonadaptive/nonadaptivedataloader.py
Content:
from ..dssdataloader import DSSDataLoader


class NonAdaptiveDSSDataLoader(DSSDataLoader):
    """
    Implementation of NonAdaptiveDSSDataLoader class which serves as base class for dataloaders of other
    nonadaptive subset selection strategies for supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        """
        Constructor function
        """
        # Arguments assertion
        assert "device" in dss_args.keys(), "'device' is a compulsory argument. Include it as a key in dss_args"
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.initialized = False
        self.device = dss_args.device
        super(NonAdaptiveDSSDataLoader, self).__init__(train_loader.dataset, dss_args,
                                                       logger, *args, **kwargs)

    def __iter__(self):
        """
        Iter function that returns the iterator of the data subset loader.
        """
        return self.subset_loader.__iter__()
File Path: cords/utils/data/dataloader/SL/nonadaptive/submoddataloader.py
Content:
import numpy as np
import apricot
import math
from .nonadaptivedataloader import NonAdaptiveDSSDataLoader
import torch
import time

class SubmodDataLoader(NonAdaptiveDSSDataLoader):
    # Currently split dataset with size of |max_chunk| then proportionably select samples in every chunk
    # Otherwise distance matrix will be too large
    """
    Implementation of SubmodDataLoader class for the nonadaptive submodular subset selection strategies for supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        
        """
        Constructor function
        """
        # Arguments assertion
        assert "size_chunk" in dss_args.keys(), "'size_chunk' is a compulsory agument for submodular dataloader"
        self.size_chunk = dss_args.size_chunk
        self.dss_args = dss_args
        super(SubmodDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                               logger, *args, **kwargs)
        self.logger.info("You are using max_chunk: %s", dss_args.size_chunk) 

    def _init_subset_indices(self): 
        """
        Initializes the subset indices and weights by calling the respective submodular function for data subset selection.
        """
        start_time = time.time()
        for i, (x, y) in enumerate(self.train_loader):
            if i == 0:
                if self.dss_args.data_type == 'text':
                    with torch.no_grad():
                        X = self.dss_args.model.embedding(x.to(self.device))
                    X = X.mean(dim = 1)
                    X = X.reshape(X.shape[0], -1)
                else:
                    X = x
                    X = X.reshape(X.shape[0], -1)
            else:
                if self.dss_args.data_type == 'text':
                    with torch.no_grad():
                        X_b = self.dss_args.model.embedding(x.to(self.device))
                    X_b = X_b.mean(dim = 1)
                    X_b = X_b.reshape(X_b.shape[0], -1)
                else:
                    X_b = x
                    X_b = X_b.reshape(X_b.shape[0], -1)
                X = torch.cat((X, X_b), dim=0)
        m = X.shape[0]
        X = X.to(device='cpu').numpy()
        # Chunking dataset to calculate pairwise distance with limited memory
        sample_indices = []
        size_chunk, budget = self.size_chunk, self.budget
        n_chunks = math.ceil(m / self.size_chunk)
        budget_chunk = math.ceil(budget / n_chunks)
        for i_chunk in range(n_chunks):
            l_idx = i_chunk * size_chunk
            r_idx = min(m, (i_chunk + 1) * size_chunk)
            n_samples = min(budget_chunk, budget - len(sample_indices))
            chunk = X[l_idx: r_idx, :]
            _sample_indices = self._chunk_select(chunk, n_samples)
            _sample_indices = [_sample_indice + l_idx for _sample_indice in _sample_indices]
            sample_indices += _sample_indices
        time_taken = time.time() - start_time
        self.logger.info("Submodular subset selection time is %.4f", time_taken)
        return np.array(sample_indices)


# Submodular optimization based
class FacLocDataLoader(SubmodDataLoader):
    """
    Implementation of FacLocDataLoader class for the nonadaptive facility location
    based subset selection strategy for supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        
        super(FacLocDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                               logger, *args, **kwargs)

    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the facility location function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the facility location gain 
        """
        f = apricot.functions.facilityLocation.FacilityLocationSelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)


class GraphCutDataLoader(SubmodDataLoader):
    """
    Implementation of GraphCutDataLoader class for the nonadaptive graph cut function
    based subset selection strategy for supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """

    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        
        super(GraphCutDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                               logger, *args, **kwargs)

    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the graphcut function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the graphcut gain 
        """
        f = apricot.functions.graphCut.GraphCutSelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)


class SumRedundancyDataLoader(SubmodDataLoader):
    """
    Implementation of SumRedundancyDataLoader class for the nonadaptive sum redundancy function
    based subset selection strategy for supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        
        super(SumRedundancyDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                               logger, *args, **kwargs)

    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the sum redundancy function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the sum redundancy gain 
        """
        f = apricot.functions.sumRedundancy.SumRedundancySelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)


class SaturatedCoverageDataLoader(SubmodDataLoader):
    """
    Implementation of SaturatedCoverageDataLoader class for the nonadaptive saturated coverage
    function based subset selection strategy for supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        
        super(SaturatedCoverageDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                               logger, *args, **kwargs)

    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the saturated coverage function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the saturated coverage gain 
        """
        f = apricot.functions.facilityLocation.FacilityLocationSelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)

File Path: cords/utils/data/dataloader/SSL/__init__.py
Content:

File Path: cords/utils/data/dataloader/SSL/adaptive/__init__.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from .retrievedataloader import RETRIEVEDataLoader
from .gradmatchdataloader import GradMatchDataLoader
from .craigdataloader import CRAIGDataLoader
from .olrandomdataloader import OLRandomDataLoader
from .randomdataloader import RandomDataLoader
File Path: cords/utils/data/dataloader/SSL/adaptive/adaptivedataloader.py
Content:
import logging, torch
from abc import abstractmethod
from torch.utils.data import DataLoader
from ..dssdataloader import DSSDataLoader
from cords.utils.data.datasets.SSL.utils import InfiniteSampler
from cords.utils.data.data_utils import WeightedSubset


class AdaptiveDSSDataLoader(DSSDataLoader):
    """
    Implementation of AdaptiveDSSDataLoader class which serves as base class for dataloaders of other
    adaptive subset selection strategies for semi-supervised learning framework.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args,
                 logger, *args, **kwargs):
        """
        Constructor function
        """
        # Arguments assertion check
        assert "select_every" in dss_args.keys(), "'select_every' is a compulsory argument. Include it as a key in dss_args"
        assert "device" in dss_args.keys(), "'device' is a compulsory argument. Include it as a key in dss_args"
        assert "kappa" in dss_args.keys(), "'kappa' is a compulsory argument. Include it as a key in dss_args"
        assert "num_iters" in dss_args.keys(), "'num_iters' is a compulsory argument. Include it as a key in dss_args"
        assert "batch_size" in kwargs.keys(), "'batch_size' is a compulsory argument. Include it as a key in kwargs"
        assert "sampler" not in kwargs.keys(), "'sampler' is a prohibited argument. Do not include it as a key in kwargs"
        assert "shuffle" not in kwargs.keys(), "'shuffle' is a prohibited argument. Do not include it as a key in kwargs"
        
        self.select_every = dss_args.select_every
        self.sel_iteration = int((self.select_every * len(train_loader.dataset) * dss_args.fraction) // (kwargs['batch_size']))  
        self.device = dss_args.device
        self.kappa = dss_args.kappa
        self.num_iters = dss_args.num_iters
        if dss_args.kappa > 0:
            assert "num_iters" in dss_args.keys(), "'num_iters' is a compulsory argument when warm starting the model(i.e., kappa > 0). Include it as a key in dss_args"
            self.select_after = int(self.kappa * self.num_iters)
        else:
            self.select_after = 0
        super(AdaptiveDSSDataLoader, self).__init__(train_loader.dataset, dss_args,
                                                    logger, *args, **kwargs)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.wtdataloader = DataLoader(self.wt_trainset,
                                       sampler=InfiniteSampler(len(self.wt_trainset), self.select_after * kwargs['batch_size']),
                                       *self.loader_args, **self.loader_kwargs)
        self.initialized = False
    
    def _init_subset_loader(self):
        # All strategies start with random selection
        """
        Function that initializes the data subset loader
        """
        self.subset_indices = self._init_subset_indices()
        self.subset_weights = torch.ones(self.budget)
        self._refresh_subset_loader()

    def _refresh_subset_loader(self):
        """
        Function that initializes the subset indices
        """
        data_sub = WeightedSubset(self.dataset, self.subset_indices, self.subset_weights)
        self.subset_loader = DataLoader(data_sub, sampler=InfiniteSampler(len(data_sub), 
                                        self.sel_iteration * self.loader_kwargs['batch_size']),
                                         *self.loader_args, **self.loader_kwargs)
        self.batch_wise_indices = list(self.subset_loader.batch_sampler)
        if self.kappa > 0:
            self.curr_loader = DataLoader(self.wt_trainset, sampler=InfiniteSampler(len(self.wt_trainset), 
                                        self.select_after * self.loader_kwargs['batch_size']),
                                       *self.loader_args, **self.loader_kwargs)
        else:
            self.curr_loader = self.subset_loader


    def __iter__(self):
        """
        Iter function that returns the iterator of full data loader or data subset loader or empty loader based on the 
        warmstart kappa value.
        """
        self.initialized = True
        if self.cur_iter <= self.select_after:
            self.logger.debug('Iteration: {0:d}, reading full dataloader... '.format(self.cur_iter))
            self.curr_loader = self.wtdataloader
            self.logger.debug('Iteration: {0:d}, finished reading full dataloader. '.format(self.cur_iter))
        else:
            self.logger.debug('Iteration: {0:d}, reading subset dataloader... '.format(self.cur_iter))
            if self.cur_iter > 1:
                self.resample()
            self.curr_loader = self.subset_loader
            self.logger.debug('Iteration: {0:d}, finished reading dataloader. '.format(self.cur_iter))
        self.cur_iter += len(list(self.curr_loader.batch_sampler))
        return self.curr_loader.__iter__()

    def __len__(self) -> int:
        """
        Returns the length of the current data loader
        """
        if self.cur_iter <= self.select_after:
            self.logger.debug('Iteration: {0:d}, reading full dataloader... '.format(self.cur_iter))
            loader = self.wtdataloader
            #self.logger.debug('Epoch: {0:d}, finished reading dataloader. '.format(self.cur_epoch))
            return len(loader)
        else:
            self.logger.debug('Iteration: {0:d}, reading subset dataloader... '.format(self.cur_iter))
            loader = self.subset_loader
            return len(loader)
            
    def resample(self):
        """
        Function that resamples the subset indices and recalculates the subset weights
        """
        self.subset_indices, self.subset_weights = self._resample_subset_indices()
        self.logger.debug("Subset indices length: %d", len(self.subset_indices))
        self._refresh_subset_loader()
        self.logger.debug("Subset loader initiated, args: %s, kwargs: %s", self.loader_args, self.loader_kwargs)
        self.logger.debug('Subset selection finished, Training data size: %d, Subset size: %d',
                     self.len_full, len(self.subset_loader.dataset))

    @abstractmethod
    def _resample_subset_indices(self):
        raise Exception('Not implemented. ')
File Path: cords/utils/data/dataloader/SSL/adaptive/craigdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SSL import CRAIGStrategy
import time, copy


# CRAIG
class CRAIGDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of CRAIGDataLoader that serves as the dataloader for the adaptive CRAIG subset selection strategy for semi-supervised learning
    and is an adapted version from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        """
         Constructor function
        """
        # Arguments assertion check
        assert "model" in dss_args.keys(), "'model' is a compulsory argument. Include it as a key in dss_args"
        assert "tea_model" in dss_args.keys(), "'tea_model' is a compulsory argument. Include it as a key in dss_args"
        assert "ssl_alg" in dss_args.keys(), "'ssl_alg' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduce:
            raise ValueError("Please set 'reduce' of loss function to False for adaptive subset selection strategies")
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "optimizer" in dss_args.keys(), "'optimizer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        
        super(CRAIGDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                            logger, *args, **kwargs)
        
        self.strategy = CRAIGStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), copy.deepcopy(dss_args.tea_model), 
                                     dss_args.ssl_alg, dss_args.loss, dss_args.device, dss_args.num_classes, dss_args.linear_layer,  
                                     True, dss_args.selection_type, logger, dss_args.optimizer)
        self.train_model = dss_args.model
        self.teacher_model = dss_args.tea_model
        self.logger.debug('CRAIG dataloader initialized. ')

    def _resample_subset_indices(self):
        """
        Function that calls the CRAIG subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug('Iteration: {0:d}, requires subset selection. '.format(self.cur_iter))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        if self.teacher_model is not None:
            tea_cached_state_dict = copy.deepcopy(self.teacher_model.state_dict())
            tea_clone_dict = copy.deepcopy(self.teacher_model.state_dict())
        else:
            tea_clone_dict = None
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict, tea_clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        if self.teacher_model is not None:
            self.teacher_model.load_state_dict(tea_cached_state_dict)
        end = time.time()
        self.logger.info('Iteration: {0:d}, subset selection finished, takes {1:.2f}. '.format(self.cur_iter, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SSL/adaptive/gradmatchdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SSL import GradMatchStrategy
import time, copy, torch


class GradMatchDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of GradMatchDataLoader that serves as the dataloader for the adaptive GradMatch subset selection strategy for 
    semi-supervised learning and is an adapted version of the one given in the paper :footcite:`pmlr-v139-killamsetty21a`.
    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for GradMatch subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        
        """
         Constructor function
        """
        # Arguments assertion check
        assert "model" in dss_args.keys(), "'model' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "tea_model" in dss_args.keys(), "'tea_model' is a compulsory argument. Include it as a key in dss_args"
        assert "ssl_alg" in dss_args.keys(), "'ssl_alg' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduce:
            raise ValueError("Please set 'reduce' of loss function to False for adaptive subset selection strategies")
        assert "eta" in dss_args.keys(), "'eta' is a compulsory argument. Include it as a key in dss_args"
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "valid" in dss_args.keys(), "'valid' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "v1" in dss_args.keys(), "'v1' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "lam" in dss_args.keys(), "'lam' is a compulsory argument for GradMatch. Include it as a key in dss_args"
        assert "eps" in dss_args.keys(), "'eps' is a compulsory argument for GradMatch. Include it as a key in dss_args"

        super(GradMatchDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                                  logger=logger, *args, **kwargs)
        self.strategy = GradMatchStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), copy.deepcopy(dss_args.tea_model),
                                         dss_args.ssl_alg, dss_args.loss, dss_args.eta, dss_args.device, dss_args.num_classes, 
                                         dss_args.linear_layer, dss_args.selection_type, logger, dss_args.valid, dss_args.v1,
                                         dss_args.lam, dss_args.eps)
        self.train_model = dss_args.model
        self.teacher_model = dss_args.tea_model
        self.logger.info('Grad-match dataloader initialized.')

    def _resample_subset_indices(self):
        """
        Function that calls the GradMatch subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug('Iteration: {0:d}, requires subset selection. '.format(self.cur_iter))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        if self.teacher_model is not None:
            tea_cached_state_dict = copy.deepcopy(self.teacher_model.state_dict())
            tea_clone_dict = copy.deepcopy(self.teacher_model.state_dict())
        else:
            tea_clone_dict = None
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict, tea_clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        if self.teacher_model is not None:
            self.teacher_model.load_state_dict(tea_cached_state_dict)
        end = time.time()
        self.logger.info('Iteration: {0:d}, subset selection finished, takes {1:.2f}. '.format(self.cur_iter, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SSL/adaptive/olrandomdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import RandomStrategy
import time, copy, logging

class OLRandomDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of OLRandomDataLoader that serves as the dataloader for the adaptive Random subset selection strategy.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for Random subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        """
        Constructor function
        """
        super(OLRandomDataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                                logger, *args, **kwargs)
        self.strategy = RandomStrategy(train_loader, online=True)
        self.logger.debug('OLRandom dataloader initialized.')

    def _resample_subset_indices(self):
        """
        Function that calls the Random subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug("Iteration: {0:d}, requires subset selection. ".format(self.cur_iter))
        logging.debug("Random budget: %d", self.budget)
        subset_indices, _ = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info("Iteration: {0:d}, subset selection finished, takes {1:.2f}. ".format(self.cur_iter, (end - start)))
        return subset_indices

File Path: cords/utils/data/dataloader/SSL/adaptive/randomdataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SL import RandomStrategy
import time, copy, logging

class RandomDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of RandomDataLoader that serves as the dataloader for the non-adaptive Random subset selection strategy.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    dss_args: dict
        Data subset selection arguments dictionary required for Random subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, dss_args, logger, *args, **kwargs):
        """
        Constructor function
        """
        super(RandomDataLoader, self).__init__(train_loader, train_loader, dss_args, 
                                               logger, *args, **kwargs)
        self.strategy = RandomStrategy(train_loader, online=False)
        self.logger.debug('Random dataloader initialized. ')

    def _resample_subset_indices(self):
        """
        Function that calls the Random subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug("Iteration: {0:d}, requires subset selection. ".format(self.cur_iter))
        logging.debug("Random budget: %d", self.budget)
        subset_indices, _ = self.strategy.select(self.budget)
        end = time.time()
        self.logger.info("Iteration: {0:d}, subset selection finished, takes {1:.2f}. ".format(self.cur_iter, (end - start)))
        return subset_indices

File Path: cords/utils/data/dataloader/SSL/adaptive/retrievedataloader.py
Content:
from .adaptivedataloader import AdaptiveDSSDataLoader
from cords.selectionstrategies.SSL import RETRIEVEStrategy
import time, copy


# RETRIEVE
class RETRIEVEDataLoader(AdaptiveDSSDataLoader):
    """
    Implements of RETRIEVEDataLoader that serves as the dataloader for the adaptive RETRIEVE subset selection strategy from the paper 
    :footcite:`killamsetty2021retrieve`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for GLISTER subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        """
        Constructor function
        """
        # Arguments assertion check
        
        assert "model" in dss_args.keys(), "'model' is a compulsory argument. Include it as a key in dss_args"
        assert "tea_model" in dss_args.keys(), "'tea_model' is a compulsory argument. Include it as a key in dss_args"
        assert "ssl_alg" in dss_args.keys(), "'ssl_alg' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduce:
            raise ValueError("Please set 'reduce' of loss function to False for adaptive subset selection strategies")
        assert "eta" in dss_args.keys(), "'eta' is a compulsory argument. Include it as a key in dss_args"
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for RETRIEVE. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for RETRIEVE. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for RETRIEVE. Include it as a key in dss_args"
        assert "greedy" in dss_args.keys(), "'greedy' is a compulsory argument for RETRIEVE. Include it as a key in dss_args"
        if dss_args.greedy == 'RGreedy':
            assert "r" in dss_args.keys(), "'r' is a compulsory argument for RGreedy version of RETRIEVE. Include it as a key in dss_args"
        else:
            dss_args.r = 15
        assert "valid" in dss_args.keys(), "'valid' is a compulsory argument for RETRIEVE. Include it as a key in dss_args"
        
        super(RETRIEVEDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                                logger, *args, **kwargs)
        
        self.strategy = RETRIEVEStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model),  copy.deepcopy(dss_args.tea_model), 
                                        dss_args.ssl_alg, dss_args.loss, dss_args.eta, dss_args.device, dss_args.num_classes, 
                                        dss_args.linear_layer, dss_args.selection_type, dss_args.greedy, logger=logger, 
                                        r = dss_args.r, valid = dss_args.valid)
        self.train_model = dss_args.model
        self.teacher_model = dss_args.tea_model
        self.logger.debug('RETRIEVE dataloader initialized.')

    def _resample_subset_indices(self):
        """
        Function that calls the RETRIEVE subset selection strategy to sample new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug('Iteration: {0:d}, requires subset selection. '.format(self.cur_iter))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        if self.teacher_model is not None:
            tea_cached_state_dict = copy.deepcopy(self.teacher_model.state_dict())
            tea_clone_dict = copy.deepcopy(self.teacher_model.state_dict())
        else:
            tea_clone_dict = None
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict, tea_clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        if self.teacher_model is not None:
            self.teacher_model.load_state_dict(tea_cached_state_dict)
        end = time.time()
        self.logger.info('Iteration: {0:d}, subset selection finished, takes {1:.2f}. '.format(self.cur_iter, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SSL/dssdataloader.py
Content:
from abc import abstractmethod
from cords.utils.data.data_utils import WeightedSubset
from torch.utils.data.dataloader import DataLoader
import torch
import numpy as np

# Base objects
class DSSDataLoader:
    """
    Implementation of DSSDataLoader class which serves as base class for dataloaders of other
    selection strategies for semi-supervised learning framework.

    Parameters
    -----------
    full_data: torch.utils.data.Dataset Class
        Full dataset from which data subset needs to be selected.
    dss_args: dict 
        Data subset selection arguments dictionary
    logger: class
        Logger class for logging the information
    """
    def __init__(self, full_data, dss_args, logger, *args, **kwargs):
        """
        Constructor function
        """
        super(DSSDataLoader, self).__init__()
        # TODO: Integrate verbose in logging
        self.len_full = len(full_data)
        # Arguments assertion check
        assert "fraction" in dss_args.keys(), "'fraction' is a compulsory argument. Include it as a key in dss_args"
        if (dss_args.fraction > 1) or (dss_args.fraction<0):
             raise ValueError("'fraction' should lie between 0 and 1")

        self.fraction = dss_args.fraction
        self.budget = int(self.len_full * self.fraction)
        self.logger = logger
        self.dataset = full_data
        self.loader_args = args
        self.loader_kwargs = kwargs
        self.subset_indices = None
        self.subset_weights = None
        self.subset_loader = None
        self.curr_loader = None
        self.batch_wise_indices = None
        self.strategy = None
        self.cur_iter = 1
        self.wt_trainset = WeightedSubset(full_data, list(range(len(full_data))), [1]*len(full_data))
        self._init_subset_loader()

    def __getattr__(self, item):
        return object.__getattribute__(self, "curr_loader").__getattribute__(item)

    def _init_subset_loader(self):
        """
        Function that initializes the random data subset loader
        """
        # All strategies start with random selection
        self.subset_indices = self._init_subset_indices()
        self.logger.debug("Length of the data subset: %d", len(self.subset_indices))
        self.subset_weights = torch.ones(self.budget)
        self.subset_loader = DataLoader(WeightedSubset(self.dataset, self.subset_indices, self.subset_weights), 
                                        *self.loader_args, **self.loader_kwargs)
        self.batch_wise_indices = list(self.subset_loader.batch_sampler)
        self.curr_loader = self.subset_loader

    # Default subset indices comes from random selection
    def _init_subset_indices(self):
        """
        Function that initializes the subset indices randomly
        """
        return np.random.choice(self.len_full, size=self.budget, replace=False)

    def _refresh_subset_loader(self):
        """
        Function that regenerates the data subset loader using new subset indices and subset weights
        """
        self.subset_loader = DataLoader(WeightedSubset(self.dataset, self.subset_indices, self.subset_weights), 
                                        *self.loader_args, **self.loader_kwargs)
        self.logger.debug("Subset Loader Refreshed")
        self.batch_wise_indices = list(self.subset_loader.batch_sampler)


File Path: cords/utils/data/dataloader/SSL/nonadaptive/__init__.py
Content:
from .craigdataloader import CRAIGDataLoader
from .nonadaptivedataloader import NonAdaptiveDSSDataLoader
from .submoddataloader import FacLocDataLoader
from .submoddataloader import GraphCutDataLoader
from .submoddataloader import SaturatedCoverageDataLoader
from .submoddataloader import SumRedundancyDataLoader
File Path: cords/utils/data/dataloader/SSL/nonadaptive/craigdataloader.py
Content:
from .nonadaptivedataloader import NonAdaptiveDSSDataLoader
from cords.selectionstrategies.SSL import CRAIGStrategy
from torch.utils.data import DataLoader
from cords.utils.data.data_utils import WeightedSubset
import time, copy


# CRAIG
class CRAIGDataLoader(NonAdaptiveDSSDataLoader):
    """
    Implements of CRAIGDataLoader that serves as the dataloader for the nonadaptive CRAIG subset selection strategy for semi-supervised learning
    and is an adapted version from the paper :footcite:`pmlr-v119-mirzasoleiman20a`.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary required for CRAIG subset selection strategy
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args, **kwargs):
        """
        Constructor function
        """
        # Arguments assertion check
        
        assert "model" in dss_args.keys(), "'model' is a compulsory argument. Include it as a key in dss_args"
        assert "tea_model" in dss_args.keys(), "'tea_model' is a compulsory argument. Include it as a key in dss_args"
        assert "ssl_alg" in dss_args.keys(), "'ssl_alg' is a compulsory argument. Include it as a key in dss_args"
        assert "loss" in dss_args.keys(), "'loss' is a compulsory argument. Include it as a key in dss_args"
        if dss_args.loss.reduce:
            raise ValueError("Please set 'reduce' of loss function to False for adaptive subset selection strategies")
        assert "num_classes" in dss_args.keys(), "'num_classes' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "linear_layer" in dss_args.keys(), "'linear_layer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "selection_type" in dss_args.keys(), "'selection_type' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        assert "optimizer" in dss_args.keys(), "'optimizer' is a compulsory argument for CRAIG. Include it as a key in dss_args"
        
        super(CRAIGDataLoader, self).__init__(train_loader, val_loader, dss_args,
                                             logger, *args, **kwargs)
        
        self.strategy = CRAIGStrategy(train_loader, val_loader, copy.deepcopy(dss_args.model), copy.deepcopy(dss_args.tea_model), 
                                     dss_args.ssl_alg, dss_args.loss, dss_args.device, dss_args.num_classes, dss_args.linear_layer,  
                                     False, dss_args.selection_type, logger, dss_args.optimizer)
        self.train_model = dss_args.model
        self.eta = dss_args.eta
        self.num_cls = dss_args.num_classes
        self.train_model = dss_args.model
        self.teacher_model = dss_args.tea_model
        if self.verbose:
            print('CRAIG dataloader initialized. ')
        self.loss = copy.deepcopy(dss_args.loss)

    def _init_subset_loader(self):
        """
        Function that initializes the initial subset loader.
        """
        # All strategies start with random selection
        self.subset_indices, self.subset_weights = self._init_subset_indices()
        self.subset_loader = DataLoader(WeightedSubset(self.dataset, self.subset_indices, self.subset_weights), 
                                        *self.loader_args, **self.loader_kwargs)
        self.batch_wise_indices = list(self.subset_loader.batch_sampler)
        self.curr_loader = self.subset_loader

    def _init_subset_indices(self):
        """
        Function that initializes the initial subset indices by calling the CRAIG subset selection strategy to sample 
        new subset indices and the corresponding subset weights.
        """
        start = time.time()
        self.logger.debug('Iteration: {0:d}, requires subset selection. '.format(self.cur_iter))
        cached_state_dict = copy.deepcopy(self.train_model.state_dict())
        clone_dict = copy.deepcopy(self.train_model.state_dict())
        if self.teacher_model is not None:
            tea_cached_state_dict = copy.deepcopy(self.teacher_model.state_dict())
            tea_clone_dict = copy.deepcopy(self.teacher_model.state_dict())
        else:
            tea_clone_dict = None
        subset_indices, subset_weights = self.strategy.select(self.budget, clone_dict, tea_clone_dict)
        self.train_model.load_state_dict(cached_state_dict)
        if self.teacher_model is not None:
            self.teacher_model.load_state_dict(tea_cached_state_dict)
        end = time.time()
        self.logger.info('Iteration: {0:d}, subset selection finished, takes {1:.2f}. '.format(self.cur_iter, (end - start)))
        return subset_indices, subset_weights

File Path: cords/utils/data/dataloader/SSL/nonadaptive/nonadaptivedataloader.py
Content:
from torch.utils.data import DataLoader
from ..dssdataloader import DSSDataLoader
from cords.utils.data.datasets.SSL.utils import InfiniteSampler
from cords.utils.data.data_utils import WeightedSubset


class NonAdaptiveDSSDataLoader(DSSDataLoader):
    """
    Implementation of NonAdaptiveDSSDataLoader class which serves as base class for dataloaders of other
    nonadaptive subset selection strategies for semi-supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, 
                logger, *args, **kwargs):

        """
        Constructor function
        """
        super(NonAdaptiveDSSDataLoader, self).__init__(train_loader.dataset, dss_args,
                                                       logger, *args, **kwargs)
        # Arguments assertion check
        assert "device" in dss_args.keys(), "'device' is a compulsory argument. Include it as a key in dss_args"
        assert "num_iters" in dss_args.keys(), "'num_iters' is a compulsory argument. Include it as a key in dss_args"
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.initialized = False
        self.num_iters = dss_args.num_iters

    def __iter__(self):
        """
        Iter function that returns the iterator of the data subset loader.
        """
        data_sub = WeightedSubset(self.dataset, self.subset_indices, self.subset_weights)
        self.curr_loader = DataLoader(data_sub, sampler=InfiniteSampler(len(data_sub), 
                                        self.num_iters * self.loader_kwargs['batch_size']),
                                         *self.loader_args, **self.loader_kwargs)
        self.batch_wise_indices = list(self.subset_loader.batch_sampler)
        return self.curr_loader.__iter__()




File Path: cords/utils/data/dataloader/SSL/nonadaptive/submoddataloader.py
Content:
import numpy as np
import apricot
import math
from .nonadaptivedataloader import NonAdaptiveDSSDataLoader


class SubmodDataLoader(NonAdaptiveDSSDataLoader):
    # Currently split dataset with size of |max_chunk| then proportionably select samples in every chunk
    # Otherwise distance matrix will be too large
    """
    Implementation of SubmodDataLoader class for the nonadaptive submodular subset selection strategies for 
    semi-supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def __init__(self, train_loader, val_loader, dss_args, logger, *args,
                 **kwargs):
        """
        Constructor function
        """

        super(SubmodDataLoader, self).__init__(train_loader, val_loader, dss_args, 
                                               logger, *args, **kwargs)
        # Arguments assertion check
        assert "size_chunk" in dss_args.keys(), "'size_chunk' is a compulsory agument for submodular dataloader"
        if dss_args.size_chunk:
            self.logger.info("You are using max_chunk: %s" % dss_args.size_chunk)
        self.size_chunk = dss_args.size_chunk
        
    def _init_subset_indices(self):
        """
        Initializes the subset indices and weights by calling the respective submodular function for data subset selection.
        """
        X = np.array([x for (w_x, x, _y) in self.dataset])
        m = X.shape[0]
        # Chunking dataset to calculate pairwise distance with limited memory
        sample_indices = []
        size_chunk, budget = self.size_chunk, self.budget
        n_chunks = math.ceil(m / self.size_chunk)
        budget_chunk = math.ceil(budget / n_chunks)
        for i_chunk in range(n_chunks):
            l_idx = i_chunk * size_chunk
            r_idx = min(m, (i_chunk + 1) * size_chunk)
            n_samples = min(budget_chunk, budget - len(sample_indices))
            chunk = X[l_idx: r_idx, :]
            _sample_indices = self._chunk_select(chunk, n_samples)
            _sample_indices = [_sample_indice + l_idx for _sample_indice in _sample_indices]
            sample_indices += _sample_indices
        return np.array(sample_indices)


# Submodular optimization based

class FacLocDataLoader(SubmodDataLoader):
    """
    Implementation of FacLocDataLoader class for the nonadaptive facility location
    based subset selection strategy for semi-supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the facility location function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the facility location gain 
        """
        f = apricot.functions.facilityLocation.FacilityLocationSelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)


class GraphCutDataLoader(SubmodDataLoader):
    """
    Implementation of GraphCutDataLoader class for the nonadaptive graph cut function
    based subset selection strategy for semi-supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """

    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the graphcut function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the graphcut gain 
        """
        f = apricot.functions.graphCut.GraphCutSelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)


class SumRedundancyDataLoader(SubmodDataLoader):
    """
    Implementation of SumRedundancyDataLoader class for the nonadaptive sum redundancy function
    based subset selection strategy for semi-supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the sum redundancy function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the sum redundancy gain 
        """
        f = apricot.functions.sumRedundancy.SumRedundancySelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)


class SaturatedCoverageDataLoader(SubmodDataLoader):
    """
    Implementation of SaturatedCoverageDataLoader class for the nonadaptive saturated coverage
    function based subset selection strategy for semi-supervised learning setting.

    Parameters
    -----------
    train_loader: torch.utils.data.DataLoader class
        Dataloader of the training dataset
    val_loader: torch.utils.data.DataLoader class
        Dataloader of the validation dataset
    dss_args: dict
        Data subset selection arguments dictionary
    logger: class
        Logger for logging the information
    """
    def _chunk_select(self, chunk, n_samples):
        """
        Function that selects the data samples by calling the saturated coverage function.

        Parameters
        -----------
        chunk: numpy array
            Chunk of the input data from which the subset needs to be selected
        n_samples: int
            Number of samples that needs to be selected from input chunk
        Returns
        --------
        ranking: list
            Ranking of the samples based on the saturated coverage gain 
        """
        f = apricot.functions.facilityLocation.FacilityLocationSelection(n_samples=n_samples)
        m = f.fit(chunk)
        return list(m.ranking)

File Path: cords/utils/data/dataloader/__init__.py
Content:


File Path: cords/utils/data/datasets/SL/__init__.py
Content:
# __init__.py
# Author: Krishnateja Killamsetty <krishnatejakillamsetty@gmail.com>
from .builder import CustomDataset_WithId
from .builder import CustomDataset
from .builder import gen_dataset
from .builder import SSTDataset
from .builder import loadGloveModel

File Path: cords/utils/data/datasets/SL/builder.py
Content:
from cgi import test
import numpy as np
import os
from cords.utils.data.datasets.SL.custom_dataset_selcon import CustomDataset_WithId_SELCON
import torch
import torchvision
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, random_split, TensorDataset
from torchvision import transforms
import PIL.Image as Image
from sklearn.datasets import load_boston
from cords.utils.data.data_utils import *
import re
import pandas as pd
import torch
import torchtext.data
import pickle
from ..__utils import TinyImageNet
from cords.utils.data.data_utils import WeightedSubset
import pandas as pd
from datasets import load_dataset

LABEL_MAPPINGS = {'glue_sst2':'label', 
                  'hf_trec6':'coarse_label', 
		          'imdb':'label',
                  'rotten_tomatoes': 'label',
                  'tweet_eval': 'label'}

SENTENCE_MAPPINGS = {'glue_sst2': 'sentence', 
                    'hf_trec6':'text',  
                    'imdb':'text',
                    'rotten_tomatoes': 'text',
                    'tweet_eval': 'text'}

class standard_scaling:
    def __init__(self):
        self.std = None
        self.mean = None

    def fit_transform(self, data):
        self.std = np.std(data, axis=0)
        self.mean = np.mean(data, axis=0)
        transformed_data = np.subtract(data, self.mean)
        transformed_data = np.divide(transformed_data, self.std)
        return transformed_data

    def transform(self, data):
        transformed_data = np.subtract(data, self.mean)
        transformed_data = np.divide(transformed_data, self.std)
        return transformed_data


def clean_data(sentence, type = 0, TREC=False):
    # From yoonkim: https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
    if type == 0:
        """
        Tokenization for SST
        """
        sentence = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", sentence)
        sentence = re.sub(r"\s{2,}", " ", sentence)
        return sentence.strip().lower()
    elif type == 1:
        """
        Tokenization/string cleaning for all datasets except for SST.
        Every dataset is lower cased except for TREC
        """
        sentence = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", sentence)     
        sentence = re.sub(r"\'s", " \'s", sentence) 
        sentence = re.sub(r"\'ve", " \'ve", sentence) 
        sentence = re.sub(r"n\'t", " n\'t", sentence) 
        sentence = re.sub(r"\'re", " \'re", sentence) 
        sentence = re.sub(r"\'d", " \'d", sentence) 
        sentence = re.sub(r"\'ll", " \'ll", sentence) 
        sentence = re.sub(r",", " , ", sentence) 
        sentence = re.sub(r"!", " ! ", sentence) 
        sentence = re.sub(r"\(", " \( ", sentence) 
        sentence = re.sub(r"\)", " \) ", sentence) 
        sentence = re.sub(r"\?", " \? ", sentence) 
        sentence = re.sub(r"\s{2,}", " ", sentence)    
        return sentence.strip() if TREC else sentence.strip().lower() 
        # if we are using glove uncased, keep TREC = False even for trec6 dataset
    else:
        return sentence

def get_class(sentiment, num_classes):
    # Return a label based on the sentiment value
    return int(sentiment * (num_classes - 0.001))


def loadGloveModel(gloveFile):
    glove = pd.read_csv(gloveFile, sep=' ', header=None, encoding='utf-8', index_col=0, na_values=None, keep_default_na=False, quoting=3)
    return glove  # (word, embedding), 400k*dim


class SSTDataset(Dataset):
    label_tmp = None

    def __init__(self, path_to_dataset, name, num_classes, wordvec_dim, wordvec, device='cpu'):
        """SST dataset
        
        Args:
            path_to_dataset (str): path_to_dataset
            name (str): train, dev or test
            num_classes (int): 2 or 5
            wordvec_dim (int): Dimension of word embedding
            wordvec (array): word embedding
            device (str, optional): torch.device. Defaults to 'cpu'.
        """
        phrase_ids = pd.read_csv(path_to_dataset + 'phrase_ids.' +
                                name + '.txt', header=None, encoding='utf-8', dtype=int)
        phrase_ids = set(np.array(phrase_ids).squeeze())  # phrase_id in this dataset
        self.num_classes = num_classes
        phrase_dict = {}  # {id->phrase} 


        if SSTDataset.label_tmp is None:
            # Read label/sentiment first
            # Share 1 array on train/dev/test set. No need to do this 3 times.
            SSTDataset.label_tmp = pd.read_csv(path_to_dataset + 'sentiment_labels.txt',
                                    sep='|', dtype={'phrase ids': int, 'sentiment values': float})
            SSTDataset.label_tmp = np.array(SSTDataset.label_tmp)[:, 1:]  # sentiment value
        
        with open(path_to_dataset + 'dictionary.txt', 'r', encoding='utf-8') as f:
            i = 0
            for line in f:
                phrase, phrase_id = line.strip().split('|')
                if int(phrase_id) in phrase_ids:  # phrase in this dataset
                    phrase = clean_data(phrase)  # preprocessing
                    phrase_dict[int(phrase_id)] = phrase
                    i += 1
        f.close()

        self.phrase_vec = []  # word index in glove
        # label of each sentence
        self.labels = torch.zeros((len(phrase_dict),), dtype=torch.long)
        missing_count = 0
        for i, (idx, p) in enumerate(phrase_dict.items()):
            tmp1 = []  
            for w in p.split(' '):
                try:
                    tmp1.append(wordvec.index.get_loc(w))  
                except KeyError:
                    missing_count += 1

            self.phrase_vec.append(torch.tensor(tmp1, dtype=torch.long)) 
            self.labels[i] = get_class(SSTDataset.label_tmp[idx], self.num_classes) 

        # print(missing_count)

    def __getitem__(self, index):
        return self.phrase_vec[index], self.labels[index]

    def __len__(self):
        return len(self.phrase_vec)

class Trec6Dataset(Dataset):
    def __init__(self, data_path, cls_to_num, num_classes, wordvec_dim, wordvec, device='cpu'):
        self.phrase_vec = []
        self.labels = []

        missing_count = 0
        with open(data_path, 'r', encoding='latin1') as f:
            for line in f:
                label = cls_to_num[line.split()[0].split(":")[0]]
                sentence = clean_data(" ".join(line.split(":")[1:]), 1, False)
                
                tmp1 = []
                for w in sentence.split(' '):
                    try:
                        tmp1.append(wordvec.index.get_loc(w))  
                    except KeyError:
                        missing_count += 1

                self.phrase_vec.append(torch.tensor(tmp1, dtype=torch.long))
                self.labels.append(label)

    def __getitem__(self, index):
        return self.phrase_vec[index], self.labels[index]

    def __len__(self):
        return len(self.phrase_vec)

class GlueDataset(Dataset):
    def __init__(self, glue_dataset, sentence_str, label_str, clean_type, num_classes, wordvec_dim, wordvec, device='cpu'):
        self.len =  glue_dataset.__len__()       
        self.phrase_vec = []  # word index in glove
        # label of each sentence
        self.labels = torch.zeros((self.len,), dtype=torch.long)
        missing_count = 0
        for i, p in enumerate(glue_dataset):
            tmp1 = []
            for w in clean_data(p[sentence_str], clean_type, False).split(' '): #False since glove used is uncased
                try:
                    tmp1.append(wordvec.index.get_loc(w))  
                except KeyError:
                    missing_count += 1

            self.phrase_vec.append(torch.tensor(tmp1, dtype=torch.long)) 
            self.labels[i] = p[label_str]
        
    def __getitem__(self, index):
        return self.phrase_vec[index], self.labels[index]
    def __len__(self):
        return self.len

## Custom PyTorch Dataset Class wrapper
class CustomDataset(Dataset):
    def __init__(self, data, target, device=None, transform=None, isreg=False):
        self.transform = transform
        self.isreg = isreg
        if device is not None:
            # Push the entire data to given device, eg: cuda:0
            self.data = data.float().to(device)
            if isreg:
                self.targets = target.float().to(device)
            else:
                self.targets = target.long().to(device)

        else:
            self.data = data.float()
            if isreg:
                self.targets = target.float()
            else:
                self.targets = target.long()

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        sample_data = self.data[idx]
        label = self.targets[idx]
        if self.transform is not None:
            sample_data = self.transform(sample_data)
        return (sample_data, label)  # .astype('float32')
        # if self.isreg:
        #     return (sample_data, label, idx)
        # else:


class CustomDataset_WithId(Dataset):
    def __init__(self, data, target, device=None, transform=None, isreg=False):
        self.transform = transform
        if device is not None:
            # Push the entire data to given device, eg: cuda:0
            self.data = data.float().to(device)
            if isreg:
                self.targets = target.float().to(device)
            else:
                self.targets = target.long().to(device)

        else:
            self.data = data.float()
            if isreg:
                self.targets = target.float()
            else:
                self.targets = target.long()
        self.X = self.data
        self.Y = self.targets

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        sample_data = self.data[idx]
        label = self.targets[idx]
        if self.transform is not None:
            sample_data = self.transform(sample_data)
        return sample_data, label, idx  # .astype('float32')


## Utility function to load datasets from libsvm datasets
def csv_file_load(path, dim, save_data=False):
    data = []
    target = []
    with open(path) as fp:
        line = fp.readline()
        while line:
            temp = [i for i in line.strip().split(",")]
            target.append(int(float(temp[-1])))  # Class Number. # Not assumed to be in (0, K-1)
            temp_data = [0] * dim
            count = 0
            for i in temp[:-1]:
                # ind, val = i.split(':')
                temp_data[count] = float(i)
                count += 1
            data.append(temp_data)
            line = fp.readline()
    X_data = np.array(data, dtype=np.float32)
    Y_label = np.array(target)
    if save_data:
        # Save the numpy files to the folder where they come from
        data_np_path = path + '.data.npy'
        target_np_path = path + '.label.npy'
        np.save(data_np_path, X_data)
        np.save(target_np_path, Y_label)
    return (X_data, Y_label)


def libsvm_file_load(path, dim, save_data=False):
    data = []
    target = []
    with open(path) as fp:
        line = fp.readline()
        while line:
            temp = [i for i in line.strip().split(" ")]
            target.append(int(float(temp[0])))  # Class Number. # Not assumed to be in (0, K-1)
            temp_data = [0] * dim

            for i in temp[1:]:
                ind, val = i.split(':')
                temp_data[int(ind) - 1] = float(val)
            data.append(temp_data)
            line = fp.readline()
    X_data = np.array(data, dtype=np.float32)
    Y_label = np.array(target)
    if save_data:
        # Save the numpy files to the folder where they come from
        data_np_path = path + '.data.npy'
        target_np_path = path + '.label.npy'
        np.save(data_np_path, X_data)
        np.save(target_np_path, Y_label)
    return (X_data, Y_label)

def clean_lawschool_full(path):
    df = pd.read_csv(path)
    df = df.dropna()
    # remove y from df
    y = df['ugpa']
    y = y / 4
    df = df.drop('ugpa', axis=1)
    # convert gender variables to 0,1
    df['gender'] = df['gender'].map({'male': 1, 'female': 0})
    # add bar1 back to the feature set
    df_bar = df['bar1']
    df = df.drop('bar1', axis=1)
    df['bar1'] = [int(grade == 'P') for grade in df_bar]
    # df['race'] = [int(race == 7.0) for race in df['race']]
    # a = df['race']
    return df.to_numpy(), y.to_numpy()


def census_load(path, dim, save_data=False):
    enum = enumerate(
        ['Private', 'Self-emp-not-inc', 'Self-emp-inc', 'Federal-gov', 'Local-gov', 'State-gov', 'Without-pay',
         'Never-worked'])
    workclass = dict((j, i) for i, j in enum)

    enum = enumerate(
        ['Bachelors', 'Some-college', '11th', 'HS-grad', 'Prof-school', 'Assoc-acdm', 'Assoc-voc', '9th', '7th-8th',
         '12th', 'Masters', '1st-4th', '10th', 'Doctorate', '5th-6th', 'Preschool'])
    education = dict((j, i) for i, j in enum)

    enum = enumerate(
        ['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed', 'Married-spouse-absent',
         'Married-AF-spouse'])
    marital_status = dict((j, i) for i, j in enum)

    enum = enumerate(['Tech-support', 'Craft-repair', 'Other-service', 'Sales', 'Exec-managerial', 'Prof-specialty',
                      'Handlers-cleaners',
                      'Machine-op-inspct', 'Adm-clerical', 'Farming-fishing', 'Transport-moving', 'Priv-house-serv',
                      'Protective-serv', 'Armed-Forces'])
    occupation = dict((j, i) for i, j in enum)

    enum = enumerate(['Wife', 'Own-child', 'Husband', 'Not-in-family', 'Other-relative', 'Unmarried'])
    relationship = dict((j, i) for i, j in enum)

    enum = enumerate(['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black'])
    race = dict((j, i) for i, j in enum)

    sex = {'Female': 0, 'Male': 1}

    enum = enumerate(
        ['United-States', 'Cambodia', 'England', 'Puerto-Rico', 'Canada', 'Germany', 'Outlying-US(Guam-USVI-etc)',
         'India', 'Japan', 'Greece', 'South', 'China', 'Cuba', 'Iran', 'Honduras', 'Philippines', 'Italy', 'Poland',
         'Jamaica',
         'Vietnam', 'Mexico', 'Portugal', 'Ireland', 'France', 'Dominican-Republic', 'Laos', 'Ecuador', 'Taiwan',
         'Haiti', 'Columbia',
         'Hungary', 'Guatemala', 'Nicaragua', 'Scotland', 'Thailand', 'Yugoslavia', 'El-Salvador', 'Trinadad&Tobago',
         'Peru', 'Hong',
         'Holand-Netherlands'])
    native_country = dict((j, i) for i, j in enum)

    data = []
    target = []
    with open(path) as fp:
        line = fp.readline()
        while line:
            temp = [i.strip() for i in line.strip().split(",")]

            if '?' in temp or len(temp) == 1:
                line = fp.readline()
                continue

            if temp[-1].strip() == "<=50K" or temp[-1].strip() == "<=50K.":
                target.append(0)
            else:
                target.append(1)

            temp_data = [0] * dim
            count = 0
            # print(temp)

            for i in temp[:-1]:

                if count == 1:
                    temp_data[count] = workclass[i.strip()]
                elif count == 3:
                    temp_data[count] = education[i.strip()]
                elif count == 5:
                    temp_data[count] = marital_status[i.strip()]
                elif count == 6:
                    temp_data[count] = occupation[i.strip()]
                elif count == 7:
                    temp_data[count] = relationship[i.strip()]
                elif count == 8:
                    temp_data[count] = race[i.strip()]
                elif count == 9:
                    temp_data[count] = sex[i.strip()]
                elif count == 13:
                    temp_data[count] = native_country[i.strip()]
                else:
                    temp_data[count] = float(i)
                temp_data[count] = float(temp_data[count])
                count += 1

            data.append(temp_data)
            line = fp.readline()
    X_data = np.array(data, dtype=np.float32)
    Y_label = np.array(target)
    if save_data:
        # Save the numpy files to the folder where they come from
        data_np_path = path + '.data.npy'
        target_np_path = path + '.label.npy'
        np.save(data_np_path, X_data)
        np.save(target_np_path, Y_label)
    return (X_data, Y_label)


def create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst, num_cls, ratio, seed=42):
    rng = np.random.default_rng(seed)
    samples_per_class = np.zeros(num_cls)
    val_samples_per_class = np.zeros(num_cls)
    tst_samples_per_class = np.zeros(num_cls)
    for i in range(num_cls):
        samples_per_class[i] = len(np.where(y_trn == i)[0])
        val_samples_per_class[i] = len(np.where(y_val == i)[0])
        tst_samples_per_class[i] = len(np.where(y_tst == i)[0])
    min_samples = int(np.min(samples_per_class) * 0.1)
    selected_classes = rng.choice(np.arange(num_cls), size=int(ratio * num_cls), replace=False)
    for i in range(num_cls):
        if i == 0:
            if i in selected_classes:
                subset_idxs = rng.choice(np.where(y_trn == i)[0], size=min_samples, replace=False)
            else:
                subset_idxs = np.where(y_trn == i)[0]
            x_trn_new = x_trn[subset_idxs]
            y_trn_new = y_trn[subset_idxs].reshape(-1, 1)
        else:
            if i in selected_classes:
                subset_idxs = rng.choice(np.where(y_trn == i)[0], size=min_samples, replace=False)
            else:
                subset_idxs = np.where(y_trn == i)[0]
            x_trn_new = np.row_stack((x_trn_new, x_trn[subset_idxs]))
            y_trn_new = np.row_stack((y_trn_new, y_trn[subset_idxs].reshape(-1, 1)))
    max_samples = int(np.max(val_samples_per_class))
    for i in range(num_cls):
        y_class = np.where(y_val == i)[0]
        if i == 0:
            subset_ids = rng.choice(y_class, size=max_samples - y_class.shape[0], replace=True)
            x_val_new = np.row_stack((x_val, x_val[subset_ids]))
            y_val_new = np.row_stack((y_val.reshape(-1, 1), y_val[subset_ids].reshape(-1, 1)))
        else:
            subset_ids = rng.choice(y_class, size=max_samples - y_class.shape[0], replace=True)
            x_val_new = np.row_stack((x_val, x_val_new, x_val[subset_ids]))
            y_val_new = np.row_stack((y_val.reshape(-1, 1), y_val_new, y_val[subset_ids].reshape(-1, 1)))
    max_samples = int(np.max(tst_samples_per_class))
    for i in range(num_cls):
        y_class = np.where(y_tst == i)[0]
        if i == 0:
            subset_ids = rng.choice(y_class, size=max_samples - y_class.shape[0], replace=True)
            x_tst_new = np.row_stack((x_tst, x_tst[subset_ids]))
            y_tst_new = np.row_stack((y_tst.reshape(-1, 1), y_tst[subset_ids].reshape(-1, 1)))
        else:
            subset_ids = rng.choice(y_class, size=max_samples - y_class.shape[0], replace=True)
            x_tst_new = np.row_stack((x_tst, x_tst_new, x_tst[subset_ids]))
            y_tst_new = np.row_stack((y_tst.reshape(-1, 1), y_tst_new, y_tst[subset_ids].reshape(-1, 1)))

    return x_trn_new, y_trn_new.reshape(-1), x_val_new, y_val_new.reshape(-1), x_tst_new, y_tst_new.reshape(-1)


def create_noisy(y_trn, num_cls, noise_ratio=0.8, seed=42):
    rng = np.random.default_rng(seed)
    noise_size = int(len(y_trn) * noise_ratio)
    noise_indices = rng.choice(np.arange(len(y_trn)), size=noise_size, replace=False)
    y_trn[noise_indices] = rng.choice(np.arange(num_cls), size=noise_size, replace=True)
    return y_trn


def tokenize_function(tokenizer, example, text_column):
    return tokenizer(example[text_column], padding = 'max_length', truncation=True)

def gen_dataset(datadir, dset_name, feature, seed=42, isnumpy=False, **kwargs):
    if feature == 'classimb':
        if 'classimb_ratio' in kwargs:
            pass
        else:
            raise KeyError("Specify a classimbratio value in the config file")

    if dset_name == "dna":
        trn_file = os.path.join(datadir, 'dna.scale.trn')
        val_file = os.path.join(datadir, 'dna.scale.val')
        tst_file = os.path.join(datadir, 'dna.scale.tst')
        data_dims = 180
        num_cls = 3
        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        x_val, y_val = libsvm_file_load(val_file, dim=data_dims)
        x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)

        y_trn -= 1  # First Class should be zero
        y_val -= 1  # First Class should be zero
        y_tst -= 1  # First Class should be zero
        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val,
                                                                        x_tst, y_tst, num_cls, kwargs['classimb_ratio'], seed=seed)
        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))
        return fullset, valset, testset, num_cls

    elif dset_name == "boston":
        num_cls = 1
        x_trn, y_trn = load_boston(return_X_y=True)
        # create train and test indices
        #train, test = train_test_split(list(range(X.shape[0])), test_size=.3)
        x_trn, x_tst, y_trn, y_tst = train_test_split(x_trn, y_trn, test_size=0.2, random_state=seed)
        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)
        scaler = standard_scaling()
        x_trn = scaler.fit_transform(x_trn)
        x_val = scaler.transform(x_val)
        x_tst = scaler.transform(x_tst)
        y_trn = y_trn.reshape((-1, 1))
        y_val = y_val.reshape((-1, 1))
        y_tst = y_tst.reshape((-1, 1))
        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)
        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn), isreg=True)
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val), isreg=True)
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst), isreg=True)
        return fullset, valset, testset, num_cls

    elif dset_name in ["Community_Crime", "LawSchool_selcon"]:
        if dset_name == "Community_Crime":
            x_trn, y_trn = clean_communities_full(os.path.join(datadir, 'communities.scv'))
        elif dset_name == "LawSchool_selcon":
            x_trn, y_trn = clean_lawschool_full(os.path.join(datadir, 'lawschool.csv'))
        else:
            raise NotImplementedError

        fullset = (x_trn, y_trn)
        data_dims = x_trn.shape[1]
        device = 'cpu'

        x_trn, y_trn, x_val_list, y_val_list, val_classes,x_tst_list, y_tst_list, tst_classes\
            = get_slices(dset_name, fullset[0], fullset[1], device, 3)

        assert(val_classes == tst_classes)

        trainset = CustomDataset_WithId_SELCON(torch.from_numpy(x_trn).float().to(device),torch.from_numpy(y_trn).float().to(device))
        valset = CustomDataset_WithId_SELCON(torch.cat(x_val_list,dim=0), torch.cat(y_val_list,dim=0))
        testset = CustomDataset_WithId_SELCON(torch.cat(x_tst_list,dim=0), torch.cat(y_tst_list,dim=0))

        return trainset, valset, testset, val_classes

    elif dset_name in ["cadata","abalone","cpusmall",'LawSchool']:
        if dset_name == "cadata":
            trn_file = os.path.join(datadir, 'cadata.txt')
            x_trn, y_trn = libsvm_file_load(trn_file, dim=8)
        elif dset_name == "abalone":
            trn_file = os.path.join(datadir, 'abalone_scale.txt')
            x_trn, y_trn = libsvm_file_load(trn_file, 8)
        elif dset_name == "cpusmall":
            trn_file = os.path.join(datadir, 'cpusmall_scale.txt')
            x_trn, y_trn = libsvm_file_load(trn_file, 12)
        elif dset_name == 'LawSchool':
            x_trn, y_trn = clean_lawschool_full(os.path.join(datadir, 'lawschool.csv'))

        # create train and test indices
        #train, test = train_test_split(list(range(X.shape[0])), test_size=.3)
        x_trn, x_tst, y_trn, y_tst = train_test_split(x_trn, y_trn, test_size=0.2, random_state=seed)
        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        sc_l = StandardScaler()
        y_trn = np.reshape(sc_l.fit_transform(np.reshape(y_trn, (-1, 1))), (-1))
        y_val = np.reshape(sc_l.fit_transform(np.reshape(y_val, (-1, 1))), (-1))
        y_tst = np.reshape(sc_l.fit_transform(np.reshape(y_tst, (-1, 1))), (-1))

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn),if_reg=True)
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val),if_reg=True)
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst),if_reg=True)

        return fullset, valset, testset, 1

    elif dset_name == 'MSD':

        trn_file = os.path.join(datadir, 'YearPredictionMSD')
        x_trn, y_trn = libsvm_file_load(trn_file, 90)

        tst_file = os.path.join(datadir, 'YearPredictionMSD.t')
        x_tst, y_tst = libsvm_file_load(tst_file, 90)
        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.005, random_state=seed)

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        sc_l = StandardScaler()
        y_trn = np.reshape(sc_l.fit_transform(np.reshape(y_trn, (-1, 1))), (-1))
        y_val = np.reshape(sc_l.fit_transform(np.reshape(y_val, (-1, 1))), (-1))
        y_tst = np.reshape(sc_l.fit_transform(np.reshape(y_tst, (-1, 1))), (-1))

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn),if_reg=True)
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val),if_reg=True)
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst),if_reg=True)

        return fullset, valset, testset, 1
        
    elif dset_name == "adult":
        trn_file = os.path.join(datadir, 'a9a.trn')
        tst_file = os.path.join(datadir, 'a9a.tst')
        data_dims = 123
        num_cls = 2
        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)

        y_trn[y_trn < 0] = 0
        y_tst[y_tst < 0] = 0

        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val,
                                                                        x_tst, y_tst, num_cls, kwargs['classimb_ratio'], seed=seed)
        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name == "connect_4":
        trn_file = os.path.join(datadir, 'connect_4.trn')

        data_dims = 126
        num_cls = 3

        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        # The class labels are (-1,0,1). Make them to (0,1,2)
        y_trn[y_trn < 0] = 2

        x_trn, x_tst, y_trn, y_tst = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)
        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val,
                                                                        x_tst, y_tst, num_cls, kwargs['classimb_ratio'], seed=seed)
        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name == "letter":
        trn_file = os.path.join(datadir, 'letter.scale.trn')
        val_file = os.path.join(datadir, 'letter.scale.val')
        tst_file = os.path.join(datadir, 'letter.scale.tst')
        data_dims = 16
        num_cls = 26
        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        x_val, y_val = libsvm_file_load(val_file, dim=data_dims)
        x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)
        y_trn -= 1  # First Class should be zero
        y_val -= 1
        y_tst -= 1  # First Class should be zero

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name == "satimage":
        trn_file = os.path.join(datadir, 'satimage.scale.trn')
        val_file = os.path.join(datadir, 'satimage.scale.val')
        tst_file = os.path.join(datadir, 'satimage.scale.tst')
        data_dims = 36
        num_cls = 6
        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        x_val, y_val = libsvm_file_load(val_file, dim=data_dims)
        x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)
        y_trn -= 1  # First Class should be zero
        y_val -= 1
        y_tst -= 1  # First Class should be zero
        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)
        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)
        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)
        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name == "svmguide1":
        trn_file = os.path.join(datadir, 'svmguide1.trn_full')
        tst_file = os.path.join(datadir, 'svmguide1.tst')
        data_dims = 4
        num_cls = 2
        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)

        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name == "usps":
        trn_file = os.path.join(datadir, 'usps.trn_full')
        tst_file = os.path.join(datadir, 'usps.tst')
        data_dims = 256
        num_cls = 10
        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)
        y_trn -= 1  # First Class should be zero
        y_tst -= 1  # First Class should be zero

        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)
        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name == "ijcnn1":
        trn_file = os.path.join(datadir, 'ijcnn1.trn')
        val_file = os.path.join(datadir, 'ijcnn1.val')
        tst_file = os.path.join(datadir, 'ijcnn1.tst')
        data_dims = 22
        num_cls = 2
        x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)
        x_val, y_val = libsvm_file_load(val_file, dim=data_dims)
        x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)

        # The class labels are (-1,1). Make them to (0,1)
        y_trn[y_trn < 0] = 0
        y_val[y_val < 0] = 0
        y_tst[y_tst < 0] = 0

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name == "sklearn-digits":
        data, target = datasets.load_digits(return_X_y=True)
        # Test data is 10%
        x_trn, x_tst, y_trn, y_tst = train_test_split(data, target, test_size=0.1, random_state=seed)

        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)
        num_cls = 10
        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(torch.from_numpy(x_trn), torch.from_numpy(y_trn))
            valset = CustomDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
            testset = CustomDataset(torch.from_numpy(x_tst), torch.from_numpy(y_tst))

        return fullset, valset, testset, num_cls

    elif dset_name in ['prior_shift_large_linsep_4', 'conv_shift_large_linsep_4', 'red_large_linsep_4',
                       'expand_large_linsep_4',
                       'shrink_large_linsep_4', 'red_conv_shift_large_linsep_4', "linsep_4", "large_linsep_4"]:
        trn_file = os.path.join(datadir, dset_name + '.trn')
        val_file = os.path.join(datadir, dset_name + '.val')
        tst_file = os.path.join(datadir, dset_name + '.tst')
        data_dims = 2
        num_cls = 4
        x_trn, y_trn = csv_file_load(trn_file, dim=data_dims)
        x_val, y_val = csv_file_load(val_file, dim=data_dims)
        x_tst, y_tst = csv_file_load(tst_file, dim=data_dims)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(x_trn, y_trn)
            valset = CustomDataset(x_val, y_val)
            testset = CustomDataset(x_tst, y_tst)

        return fullset, valset, testset, num_cls

    elif dset_name in ['prior_shift_clf_2', 'prior_shift_gauss_2', 'conv_shift_clf_2', 'conv_shift_gauss_2', "gauss_2",
                       "clf_2", "linsep"]:
        trn_file = os.path.join(datadir, dset_name + '.trn')
        val_file = os.path.join(datadir, dset_name + '.val')
        tst_file = os.path.join(datadir, dset_name + '.tst')
        data_dims = 2
        num_cls = 2
        x_trn, y_trn = csv_file_load(trn_file, dim=data_dims)
        x_val, y_val = csv_file_load(val_file, dim=data_dims)
        x_tst, y_tst = csv_file_load(tst_file, dim=data_dims)

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(x_trn, y_trn)
            valset = CustomDataset(x_val, y_val)
            testset = CustomDataset(x_tst, y_tst)

        return fullset, valset, testset, num_cls

    elif dset_name == "covertype":
        trn_file = os.path.join(datadir, 'covtype.data')
        data_dims = 54
        num_cls = 7
        x_trn, y_trn = csv_file_load(trn_file, dim=data_dims)

        y_trn -= 1  # First Class should be zero

        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)
        x_trn, x_tst, y_trn, y_tst = train_test_split(x_trn, y_trn, test_size=0.2, random_state=seed)

        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)

        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)

        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)

        else:
            fullset = CustomDataset(x_trn, y_trn)
            valset = CustomDataset(x_val, y_val)
            testset = CustomDataset(x_tst, y_tst)

        return fullset, valset, testset, num_cls

    elif dset_name == "census":
        trn_file = os.path.join(datadir, 'adult.data')
        tst_file = os.path.join(datadir, 'adult.test')
        data_dims = 14
        num_cls = 2

        x_trn, y_trn = census_load(trn_file, dim=data_dims)
        x_tst, y_tst = census_load(tst_file, dim=data_dims)

        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)
        sc = StandardScaler()
        x_trn = sc.fit_transform(x_trn)
        x_val = sc.transform(x_val)
        x_tst = sc.transform(x_tst)

        if feature == 'classimb':
            x_trn, y_trn, x_val, y_val, x_tst, y_tst = create_imbalance(x_trn, y_trn, x_val, y_val, x_tst, y_tst,
                                                                        num_cls, kwargs['classimb_ratio'], seed=seed)
        elif feature == 'noise':
            y_trn = create_noisy(y_trn, num_cls, seed=seed)
    
        if isnumpy:
            fullset = (x_trn, y_trn)
            valset = (x_val, y_val)
            testset = (x_tst, y_tst)
        else:
            fullset = CustomDataset(x_trn, y_trn)
            valset = CustomDataset(x_val, y_val)
            testset = CustomDataset(x_tst, y_tst)
        return fullset, valset, testset, num_cls

    elif dset_name == "mnist":
        mnist_transform = transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])

        mnist_tst_transform = transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])
        num_cls = 10
        fullset = torchvision.datasets.MNIST(root=datadir, train=True, download=True, transform=mnist_transform)
        testset = torchvision.datasets.MNIST(root=datadir, train=False, download=True, transform=mnist_tst_transform)

        if feature == 'classimb':
            rng = np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(fullset.targets == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(fullset.targets == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(fullset.targets == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(fullset.targets == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(fullset.targets == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls


    elif dset_name == "fashion-mnist":
        mnist_transform = transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])

        mnist_tst_transform = transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])
        num_cls = 10

        fullset = torchvision.datasets.FashionMNIST(root=datadir, train=True, download=True,
                                                    transform=mnist_transform)
        testset = torchvision.datasets.FashionMNIST(root=datadir, train=False, download=True,
                                                    transform=mnist_tst_transform)

        if feature == 'classimb':
            rng = np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(fullset.targets == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(fullset.targets == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(fullset.targets == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(fullset.targets == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(fullset.targets == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))
        return trainset, valset, testset, num_cls

    elif dset_name == "cifar10":
        cifar_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ])

        cifar_tst_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ])

        num_cls = 10

        fullset = torchvision.datasets.CIFAR10(root=datadir, train=True, download=True, transform=cifar_transform)
        testset = torchvision.datasets.CIFAR10(root=datadir, train=False, download=True,
                                               transform=cifar_tst_transform)

        if feature == 'classimb':
            rng = np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(torch.Tensor(fullset.targets) == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls


    elif dset_name == "tinyimagenet":
        tiny_transform = transforms.Compose([
            transforms.RandomResizedCrop(64),
            transforms.ColorJitter(
                brightness=0.4, contrast=0.4, saturation=0.4),
            transforms.RandomRotation(45),
            transforms.RandomHorizontalFlip(),
            # transforms.RandomVerticalFlip(),
            # transforms.RandomAffine(90),
            # transforms.RandomGrayscale(),
            # transforms.RandomPerspective(),
            transforms.ToTensor(),
            # transforms.RandomErasing(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
        # tiny_transform = transforms.Compose([
        #     transforms.Resize(256), # Resize images to 256 x 256
        #     transforms.CenterCrop(224), # Center crop image
        #     transforms.RandomHorizontalFlip(),
        #     transforms.ToTensor(),
        #     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        # ])
        size = int(64 * 1.15)
        tiny_tst_transform = transforms.Compose([
            transforms.Resize((size, size)),
            transforms.CenterCrop(64),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
        # tiny_tst_transform = transforms.Compose([
        #     transforms.Resize(256), # Resize images to 256 x 256
        #     transforms.CenterCrop(224), # Center crop image
        #     transforms.ToTensor(),
        #     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        # ])
        num_cls = 200
        fullset = TinyImageNet(root=datadir, split='train', download=True, transform=tiny_transform)
        testset = TinyImageNet(root=datadir, split='val', download=True, transform=tiny_tst_transform)

        if feature == 'classimb':
            rng = np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(torch.Tensor(fullset.targets) == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls

    elif dset_name == "cifar100":
        cifar100_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),
        ])

        cifar100_tst_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),
        ])

        num_cls = 100

        fullset = torchvision.datasets.CIFAR100(root=datadir, train=True, download=True, transform=cifar100_transform)
        testset = torchvision.datasets.CIFAR100(root=datadir, train=False, download=True,
                                                transform=cifar100_tst_transform)

        if feature == 'classimb':
            rng = np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(torch.Tensor(fullset.targets) == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls


    elif dset_name == "svhn":
        svhn_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        svhn_tst_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        num_cls = 10
        fullset = torchvision.datasets.SVHN(root=datadir, split='train', download=True, transform=svhn_transform)
        testset = torchvision.datasets.SVHN(root=datadir, split='test', download=True, transform=svhn_tst_transform)

        if feature == 'classimb':
            rng = np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(torch.Tensor(fullset.targets) == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls


    elif dset_name == "kmnist":
        kmnist_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(np.array([0.1904]), np.array([0.3475])),
        ])

        kmnist_tst_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(np.array([0.1904]), np.array([0.3475])),
        ])

        num_cls = 10

        fullset = torchvision.datasets.KMNIST(root=datadir, train=True, download=True, transform=kmnist_transform)
        testset = torchvision.datasets.KMNIST(root=datadir, train=False, download=True,
                                              transform=kmnist_tst_transform)

        if feature == 'classimb':
            rng=np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(torch.Tensor(fullset.targets) == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls


    elif dset_name == "stl10":
        stl10_transform = transforms.Compose([
            transforms.Pad(12),
            transforms.RandomCrop(96),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        stl10_tst_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        num_cls = 10

        fullset = torchvision.datasets.STL10(root=datadir, split='train', download=True, transform=stl10_transform)
        testset = torchvision.datasets.STL10(root=datadir, split='test', download=True, transform=stl10_tst_transform)

        if feature == 'classimb':
            rng=np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(torch.Tensor(fullset.targets) == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy(),
                                             size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls


    elif dset_name == "emnist":
        emnist_transform = transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])

        emnist_tst_transform = transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])

        num_cls = 10

        fullset = torchvision.datasets.EMNIST(root=datadir, split='digits', train=True, download=True,
                                              transform=emnist_transform)
        testset = torchvision.datasets.EMNIST(root=datadir, split='digits', train=False, download=True,
                                              transform=emnist_tst_transform)

        if feature == 'classimb':
            rng=np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(fullset.targets == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(fullset.targets == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(fullset.targets == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(fullset.targets == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(fullset.targets == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            fullset = torch.utils.data.Subset(fullset, subset_idxs)

        # validation dataset is (0.1 * train dataset)
        validation_set_fraction = 0.1
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))
        return trainset, valset, testset, num_cls

    elif dset_name == "celeba":
        crop_size = 108
        re_size = 64
        offset_height = (218 - crop_size) // 2
        offset_width = (178 - crop_size) // 2
        crop = lambda x: x[:, offset_height:offset_height + crop_size, offset_width:offset_width + crop_size]

        celeba_transform = transforms.Compose(
            [transforms.ToTensor(),
             transforms.Lambda(crop),
             transforms.ToPILImage(),
             transforms.Scale(size=(re_size, re_size), interpolation=Image.BICUBIC),
             transforms.ToTensor(),
             transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)])

        num_cls = 10177

        trainset = torchvision.datasets.CelebA(root=datadir, split='train', target_type=['identity'],
                                              transform=celeba_transform, download=True)

        testset = torchvision.datasets.CelebA(root=datadir, split='test', target_type=['identity'],
                                              transform=celeba_transform, download=True)

        valset = torchvision.datasets.CelebA(root=datadir, split='valid', target_type=['identity'],
                                              transform=celeba_transform, download=True)

        trainset.identity.sub_(1)
        valset.identity.sub_(1)
        testset.identity.sub_(1)

        if feature == 'classimb':
            rng = np.random.default_rng(seed)
            samples_per_class = torch.zeros(num_cls)
            for i in range(num_cls):
                samples_per_class[i] = len(torch.where(trainset.identity == i)[0])
            min_samples = int(torch.min(samples_per_class) * 0.1)
            selected_classes = rng.choice(np.arange(num_cls), size=int(kwargs['classimb_ratio'] * num_cls), replace=False)
            for i in range(num_cls):
                if i == 0:
                    if i in selected_classes:
                        subset_idxs = list(
                            rng.choice(torch.where(trainset.identity == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        subset_idxs = list(torch.where(trainset.identity == i)[0].cpu().numpy())
                else:
                    if i in selected_classes:
                        batch_subset_idxs = list(
                            rng.choice(torch.where(trainset.identity == i)[0].cpu().numpy(), size=min_samples,
                                             replace=False))
                    else:
                        batch_subset_idxs = list(torch.where(trainset.identity == i)[0].cpu().numpy())
                    subset_idxs.extend(batch_subset_idxs)
            trainset = torch.utils.data.Subset(trainset, subset_idxs)
        return trainset, valset, testset, num_cls

    elif dset_name == "sst2" or dset_name == "sst2_facloc":
        '''
        download data/SST from https://drive.google.com/file/d/14KU6RQJpP6HKKqVGm0OF3MVxtI0NlEcr/view?usp=sharing
        or get the stanford sst data and make phrase_ids.<dev/test/train>.txt files
        pass datadir arg in dataset in config appropiriately(should look like ......../SST)
        '''
        num_cls = 2
        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)
        trainset = SSTDataset(datadir, 'train', num_cls, wordvec_dim, wordvec)
        testset = SSTDataset(datadir, 'test', num_cls, wordvec_dim, wordvec)
        valset = SSTDataset(datadir, 'dev', num_cls, wordvec_dim, wordvec)
        return trainset, valset, testset, num_cls

    elif dset_name == "glue_sst2":
        num_cls = 2
        raw = load_dataset("glue", "sst2")

        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)

        clean_type = 0
        fullset = GlueDataset(raw['train'], 'sentence', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        # testset = GlueDataset(raw['test'], 'sentence', 'label', clean_type, num_cls, wordvec_dim, wordvec) # doesn't have gold labels
        valset = GlueDataset(raw['validation'], 'sentence', 'label', clean_type, num_cls, wordvec_dim, wordvec)

        test_set_fraction = 0.05
        seed = 42
        num_fulltrn = len(fullset)
        num_test = int(num_fulltrn * test_set_fraction)
        num_trn = num_fulltrn - num_test
        trainset, testset = random_split(fullset, [num_trn, num_test], generator=torch.Generator().manual_seed(seed))
        return trainset, valset, testset, num_cls

    elif  dset_name == "sst5":
        '''
        download data/SST from https://drive.google.com/file/d/14KU6RQJpP6HKKqVGm0OF3MVxtI0NlEcr/view?usp=sharing
        or get the stanford sst data and make phrase_ids.<dev/test/train>.txt files
        pass datadir arg in dataset in config appropiriately(should look like ......../SST)
        '''
        num_cls = 5
        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)
        trainset = SSTDataset(datadir, 'train', num_cls, wordvec_dim, wordvec)
        testset = SSTDataset(datadir, 'test', num_cls, wordvec_dim, wordvec)
        valset = SSTDataset(datadir, 'dev', num_cls, wordvec_dim, wordvec)
        return trainset, valset, testset, num_cls

    elif dset_name == 'trec6':
        num_cls = 6
        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)

        cls_to_num = {"DESC": 0, "ENTY": 1, "HUM": 2, "ABBR": 3, "LOC": 4, "NUM": 5}

        trainset = Trec6Dataset(datadir+'train.txt', cls_to_num, num_cls, wordvec_dim, wordvec)
        testset = Trec6Dataset(datadir+'test.txt', cls_to_num, num_cls, wordvec_dim, wordvec)
        valset = Trec6Dataset(datadir+'valid.txt', cls_to_num, num_cls, wordvec_dim, wordvec)
        return trainset, valset, testset, num_cls

    elif dset_name == "hf_trec6": # hugging face trec6
        num_cls = 6
        raw = load_dataset("trec")

        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        data_dir = kwargs['dataset'].datadir
        if not os.path.exists(os.path.abspath(data_dir)):
            os.makedirs(os.path.abspath(data_dir), exist_ok=True)
        if (os.path.exists(os.path.join(os.path.abspath(data_dir), 'trainset.pkl'))) and (os.path.exists(os.path.join(os.path.abspath(data_dir), 'valset.pkl'))) and (os.path.exists(os.path.join(os.path.abspath(data_dir), 'testset.pkl'))):
            trainset = torch.load(os.path.join(os.path.abspath(data_dir), 'trainset.pkl'))
            valset = torch.load(os.path.join(os.path.abspath(data_dir), 'valset.pkl'))
            testset = torch.load(os.path.join(os.path.abspath(data_dir), 'testset.pkl'))
        else:
            weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
            wordvec = loadGloveModel(weight_full_path)
            clean_type = 1
            fullset = GlueDataset(raw['train'], 'text', 'coarse_label', clean_type, num_cls, wordvec_dim, wordvec)
            testset = GlueDataset(raw['test'], 'text', 'coarse_label', clean_type, num_cls, wordvec_dim, wordvec)
            validation_set_fraction = 0.1
            seed = 42
            num_fulltrn = len(fullset)
            num_val = int(num_fulltrn * validation_set_fraction)
            num_trn = num_fulltrn - num_val    
            trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))
            torch.save(trainset, os.path.join(os.path.abspath(data_dir), 'trainset.pkl'))
            torch.save(valset, os.path.join(os.path.abspath(data_dir), 'valset.pkl'))
            torch.save(testset, os.path.join(os.path.abspath(data_dir), 'testset.pkl'))
        return trainset, valset, testset, num_cls
    
    elif dset_name == "imdb": # hugging face trec6
        num_cls = 2
        raw = load_dataset("imdb")

        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)

        clean_type = 1
        fullset = GlueDataset(raw['train'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        testset = GlueDataset(raw['test'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        # valset = GlueDataset(raw['validation'], num_cls, wordvec_dim, wordvec)
        
        validation_set_fraction = 0.1
        seed = 42
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))

        return trainset, valset, testset, num_cls

    elif dset_name == 'rotten_tomatoes':
        num_cls = 2
        raw = load_dataset("rotten_tomatoes")

        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)

        clean_type = 1
        trainset = GlueDataset(raw['train'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        valset = GlueDataset(raw['validation'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        testset = GlueDataset(raw['test'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        
        return trainset, valset, testset, num_cls

    elif dset_name == 'tweet_eval':
        num_cls = 20
        raw = load_dataset("tweet_eval", "emoji")

        wordvec_dim = kwargs['dataset'].wordvec_dim
        weight_path = kwargs['dataset'].weight_path
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)

        clean_type = 1
        trainset = GlueDataset(raw['train'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        valset = GlueDataset(raw['validation'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        testset = GlueDataset(raw['test'], 'text', 'label', clean_type, num_cls, wordvec_dim, wordvec)
        
        return trainset, valset, testset, num_cls

    elif dset_name == "glue_sst2_transformer":
        """
        Load GLUE SST2 dataset. We are only using train and validation splits since the test split doesn't come with gold labels. For testing purposes, we use 5% of train
        dataset as test dataset.
        """
        num_cls = 2
        glue_dataset = load_dataset("glue", "sst2")
        tokenizer_mapping = lambda example: tokenize_function(kwargs['tokenizer'], example, SENTENCE_MAPPINGS['glue_sst2'])
        glue_dataset = glue_dataset.map(tokenizer_mapping, batched=True) 
        glue_dataset = glue_dataset.remove_columns([SENTENCE_MAPPINGS['glue_sst2'], "idx"])
        glue_dataset = glue_dataset.rename_column(LABEL_MAPPINGS['glue_sst2'], "labels")
        glue_dataset.set_format("torch")

        fullset = glue_dataset['train']
        valset = glue_dataset['validation']
        test_set_fraction = 0.05
        seed = 42
        num_fulltrn = len(fullset)
        num_test = int(num_fulltrn * test_set_fraction)
        num_trn = num_fulltrn - num_test
        trainset, testset = random_split(fullset, [num_trn, num_test], generator=torch.Generator().manual_seed(seed))
        return trainset, valset, testset, num_cls

    elif dset_name == 'hf_trec6_transformer':
        num_cls = 6
        trec6_dataset = load_dataset("trec")
        
        tokenizer_mapping = lambda example: tokenize_function(kwargs['tokenizer'], example, SENTENCE_MAPPINGS['hf_trec6'])
        trec6_dataset = trec6_dataset.map(tokenizer_mapping, batched=True) 
        trec6_dataset = trec6_dataset.remove_columns([SENTENCE_MAPPINGS['hf_trec6'], 'fine_label'])
        trec6_dataset = trec6_dataset.rename_column(LABEL_MAPPINGS['hf_trec6'], "labels")
        trec6_dataset.set_format("torch")

        fullset = trec6_dataset["train"]
        testset = trec6_dataset['test']
        validation_set_fraction = 0.1
        seed = 42
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))
        return trainset, valset, testset, num_cls
        
    elif dset_name == 'imdb_transformer':
        num_cls = 2
        imdb_dataset = load_dataset("imdb")

        tokenizer_mapping = lambda example: tokenize_function(kwargs['tokenizer'], example, SENTENCE_MAPPINGS['imdb'])
        imdb_dataset = imdb_dataset.map(tokenizer_mapping, batched=True) 
        imdb_dataset = imdb_dataset.remove_columns([SENTENCE_MAPPINGS['imdb']])
        imdb_dataset = imdb_dataset.rename_column(LABEL_MAPPINGS['imdb'], "labels")
        imdb_dataset.set_format("torch")

        fullset = imdb_dataset["train"]
        testset = imdb_dataset['test']
        validation_set_fraction = 0.1
        seed = 42
        num_fulltrn = len(fullset)
        num_val = int(num_fulltrn * validation_set_fraction)
        num_trn = num_fulltrn - num_val
        trainset, valset = random_split(fullset, [num_trn, num_val], generator=torch.Generator().manual_seed(seed))
        return trainset, valset, testset, num_cls
    
    elif dset_name == 'rotten_tomatoes_transformer':
        num_cls = 2
        dataset = load_dataset("rotten_tomatoes")
        tokenizer_mapping = lambda example: tokenize_function(kwargs['tokenizer'], example, SENTENCE_MAPPINGS['rotten_tomatoes'])
        dataset = dataset.map(tokenizer_mapping, batched=True) 
        dataset = dataset.remove_columns([SENTENCE_MAPPINGS['rotten_tomatoes']])
        dataset = dataset.rename_column(LABEL_MAPPINGS['rotten_tomatoes'], "labels")
        dataset.set_format("torch")

        trainset = dataset["train"]
        valset = dataset["validation"]
        testset = dataset['test']
        return trainset, valset, testset, num_cls

    elif dset_name == 'tweet_eval_transformer':
        num_cls = 20
        tweet_dataset = load_dataset("tweet_eval", "emoji")

        tokenizer_mapping = lambda example: tokenize_function(kwargs['tokenizer'], example, SENTENCE_MAPPINGS['tweet_eval'])
        tweet_dataset = tweet_dataset.map(tokenizer_mapping, batched=True) 
        tweet_dataset = tweet_dataset.remove_columns([SENTENCE_MAPPINGS['tweet_eval']])
        tweet_dataset = tweet_dataset.rename_column(LABEL_MAPPINGS['tweet_eval'], "labels")
        tweet_dataset.set_format("torch")

        trainset = tweet_dataset["train"]
        valset = tweet_dataset["validation"]
        testset = tweet_dataset['test']
        return trainset, valset, testset, num_cls

    else:
        raise NotImplementedError
File Path: cords/utils/data/datasets/SL/custom_dataset_selcon.py
Content:
import numpy as np
import pandas as pd
import os
import torch
import torchvision
from matplotlib import pyplot as plt
from sklearn import datasets
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset
from torchvision import transforms

from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder


## Custom PyTorch Dataset Class wrapper
class CustomDataset_SELCON(Dataset):
    def __init__(self, data, target, device=None, transform=None):       
        self.transform = transform
        self.data = torch.from_numpy(data.astype('float32'))#.to(device)
        self.targets = torch.from_numpy(target)#.to(device)
        # if device is not None:
        #     # Push the entire data to given device, eg: cuda:0
        #     self.data = torch.from_numpy(data.astype('float32'))#.to(device)
        #     self.targets = torch.from_numpy(target)#.to(device)
        # else:
        #     self.data = data#.astype('float32')
        #     self.targets = target

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()        
        sample_data = self.data[idx]
        label = self.targets[idx]
        if self.transform is not None:
            sample_data = self.transform(sample_data)
        return (sample_data, label) #.astype('float32')

class CustomDataset_WithId_SELCON(Dataset):
    def __init__(self, data, target, transform=None):       
        self.transform = transform
        self.data = data #.astype('float32')
        self.targets = target
        self.X = self.data
        self.Y = self.targets

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()  
        sample_data = self.data[idx]
        label = self.targets[idx]
        if self.transform is not None:
            sample_data = self.transform(sample_data)
        return sample_data, label,idx #.astype('float32')



class SubsetDataset_WithId_SELCON(CustomDataset_WithId_SELCON):
    def __init__(self, dataset, idxs, transform=None):
        super().__init__(dataset.data, dataset.targets, transform=transform)
        self.idxs = idxs
    
    def __len__(self):
        return len(self.idxs)
    
    def __getitem__(self, idx):
        new_idx = self.idxs[idx].tolist()
        data, targets, _ = super().__getitem__(new_idx)
        return (data, targets, idx)

File Path: cords/utils/data/datasets/SSL/__init__.py
Content:
from .builder import gen_dataset
File Path: cords/utils/data/datasets/SSL/augmentation/__init__.py
Content:
from . import augmentation_pool
File Path: cords/utils/data/datasets/SSL/augmentation/augmentation_class.py
Content:
import torch
import torchvision.transforms as tt

from . import augmentation_pool as aug_pool
from .rand_augment import RandAugment


class ReduceChannelwithNormalize:
    """ Reduce alpha channel of RGBA """
    def __init__(self, mean, scale, zca):
        self.mean = mean
        self.scale = scale
        self.zca = zca

    def __call__(self, tch_img):
        rgb = tch_img[:3]
        i1, i2 = torch.where(tch_img[3] == 0)
        if self.zca:
            rgb = aug_pool.GCN()(tch_img)
            rgb = aug_pool.ZCA(self.mean, self.scale)(rgb)
        else:
            rgb = tt.functional.normalize(rgb, self.mean, self.scale, True)
        rgb[:, i1, i2] = 0
        return rgb

    def __repr__(self):
        return f"ReduceChannelwithNormalize(mean={self.mean}, scale={self.scale})"


class RGB2RGBA:
    def __call__(self, x):
        return x.convert("RGBA")

    def __repr__(self):
        return "RGB2RGBA()"


class StrongAugmentation:
    """
    Strong augmentation class
    including RandAugment and Cutout
    """
    def __init__(
        self,
        img_size: int,
        mean: list,
        scale: list,
        flip: bool,
        crop: bool,
        alg: str = "fixmatch",
        zca: bool = False,
        cutout: bool = True,
    ):
        augmentations = [tt.ToPILImage()]
        
        if flip:
            augmentations += [tt.RandomHorizontalFlip(p=0.5)]
        if crop:
            augmentations += [tt.RandomCrop(img_size, int(img_size*0.125), padding_mode="reflect")]

        augmentations += [
            RGB2RGBA(),
            RandAugment(alg=alg),
            tt.ToTensor(),
            ReduceChannelwithNormalize(mean, scale, zca)
        ]
        if cutout:
            augmentations += [aug_pool.TorchCutout(16)]

        self.augmentations = tt.Compose(augmentations)

    def __call__(self, img):
        return self.augmentations(img)

    def __repr__(self):
        return repr(self.augmentations)


class WeakAugmentation:
    """
    Weak augmentation class
    including horizontal flip, random crop, and gaussian noise
    """
    def __init__(
        self,
        img_size: int,
        mean: list,
        scale: list,
        flip=True,
        crop=True,
        noise=True,
        zca=False
    ):
        augmentations = [tt.ToPILImage()]
        if flip:
            augmentations.append(tt.RandomHorizontalFlip())
        if crop:
            augmentations.append(tt.RandomCrop(img_size, int(img_size*0.125), padding_mode="reflect"))
        augmentations += [tt.ToTensor()]
        if zca:
            augmentations += [aug_pool.GCN(), aug_pool.ZCA(mean, scale)]
        else:
            augmentations += [tt.Normalize(mean, scale, True)]
        if noise:
            augmentations.append(aug_pool.GaussianNoise())
        self.augmentations = tt.Compose(augmentations)

    def __call__(self, img):
        return self.augmentations(img)

    def __repr__(self):
        return repr(self.augmentations)

File Path: cords/utils/data/datasets/SSL/augmentation/augmentation_pool.py
Content:
import random
import torch
import torch.nn.functional as F
import numpy as np

from PIL import ImageOps, ImageEnhance, ImageFilter, Image


"""
For PIL.Image
"""

def autocontrast(x, *args, **kwargs):
    return ImageOps.autocontrast(x.convert("RGB")).convert("RGBA")


def brightness(x, level, magnitude=10, max_level=1.8, *args, **kwargs):
    level = (level / magnitude) * max_level + 0.1
    return ImageEnhance.Brightness(x).enhance(level)


def color(x, level, magnitude=10, max_level=1.8, *args, **kwargs):
    level = (level / magnitude) * max_level + 0.1
    return ImageEnhance.Color(x).enhance(level)


def contrast(x, level, magnitude=10, max_level=1.8, *args, **kwargs):
    level = (level / magnitude) * max_level + 0.1
    return ImageEnhance.Contrast(x).enhance(level)


def equalize(x, *args, **kwargs):
    return ImageOps.equalize(x.convert("RGB")).convert("RGBA")


def identity(x, *args, **kwargs):
    return x


def invert(x, *args, **kwargs):
    return ImageOps.invert(x.convert("RGB")).convert("RGBA")


def posterize(x, level, magnitude=10, max_level=4, *args, **kwargs):
    level = int((level / magnitude) * max_level)
    return ImageOps.posterize(x.convert("RGB"), 4 - level).convert("RGBA")


def rotate(x, level, magnitude=10, max_level=30, *args, **kwargs):
    degree = int((level / magnitude) * max_level)
    if random.random() > 0.5:
        degree = -degree
    return x.rotate(degree)


def sharpness(x, level, magnitude=10, max_level=1.8, *args, **kwargs):
    level = (level / magnitude) * max_level + 0.1
    return ImageEnhance.Sharpness(x).enhance(level)


def shear_x(x, level, magnitude=10, max_level=0.3, *args, **kwargs):
    level = (level / magnitude) * max_level
    if random.random() > 0.5:
        level = -level
    return x.transform(x.size, Image.AFFINE, (1, level, 0, 0, 1, 0))


def shear_y(x, level, magnitude=10, max_level=0.3, *args, **kwargs):
    level = (level / magnitude) * max_level
    if random.random() > 0.5:
        level = -level
    return x.transform(x.size, Image.AFFINE, (1, 0, 0, level, 1, 0))


def solarize(x, level, magnitude=10, max_level=256, *args, **kwargs):
    level = int((level / magnitude) * max_level)
    return ImageOps.solarize(x.convert("RGB"), 256 - level).convert("RGBA")


def translate_x(x, level, magnitude=10, max_level=10, *args, **kwargs):
    level = int((level / magnitude) * max_level)
    if random.random() > 0.5:
        level = -level
    return x.transform(x.size, Image.AFFINE, (1, 0, level, 0, 1, 0))


def translate_y(x, level, magnitude=10, max_level=10, *args, **kwargs):
    level = int((level / magnitude) * max_level)
    if random.random() > 0.5:
        level = -level
    return x.transform(x.size, Image.AFFINE, (1, 0, 0, 0, 1, level))


def cutout(x, level, magnitude=10, max_level=20, *args, **kwargs):
    size = int((level / magnitude) * max_level)
    if size <= 0:
        return x
    w, h = x.size
    upper_coord, lower_coord = _gen_cutout_coord(h, w, size)

    pixels = x.load()
    for i in range(upper_coord[0], lower_coord[0]):
        for j in range(upper_coord[1], lower_coord[1]):
            pixels[i, j] = (127, 127, 127, 0)
    return x


def _gen_cutout_coord(height, width, size):
    height_loc = random.randint(0, height - 1)
    width_loc = random.randint(0, width - 1)

    upper_coord = (max(0, height_loc - size // 2),
                    max(0, width_loc - size // 2))
    lower_coord = (min(height, height_loc + size // 2),
                    min(width, width_loc + size // 2))

    return upper_coord, lower_coord

"""
For torch.Tensor
"""

class TorchCutout:
    def __init__(self, size=16):
        self.size = size

    def __call__(self, img):
        h, w = img.shape[-2:]
        upper_coord, lower_coord  = _gen_cutout_coord(h, w, self.size)

        mask_height = lower_coord[0] - upper_coord[0]
        mask_width = lower_coord[1] - upper_coord[1]
        assert mask_height > 0
        assert mask_width > 0

        mask = torch.ones_like(img)
        zeros = torch.zeros((img.shape[0], mask_height, mask_width))
        mask[:, upper_coord[0]:lower_coord[0], upper_coord[1]:lower_coord[1]] = zeros
        return img * mask

    def __repr__(self):
        return f"TorchCutout(size={self.size})"


class GaussianNoise:
    def __init__(self, std=0.15):
        self.std = std

    def __call__(self, x):
        with torch.no_grad():
            return x + torch.randn_like(x) * self.std

    def __repr__(self):
        return f"GaussianNoise(std={self.std})"


class BatchRandomFlip:
    def __init__(self, flip_prob=0.5):
        self.p = flip_prob

    def __call__(self, x):
        with torch.no_grad():
            return torch.stack([
                torch.flip(img, (-1,))
                if random.random() > self.p
                else img
                for img in x
            ], 0)

    def __repr__(self):
        return f"BatchRandomFlip(flip_prob={self.p})"


class RandomFlip:
    def __init__(self, flip_prob=0.5):
        self.p = flip_prob

    def __call__(self, x):
        if random.random() > self.p:
            return torch.flip(x, (-1,))
        return x

    def __repr__(self):
        return f"RandomFlip(flip_prob={self.p})"


class BatchRandomCrop:
    def __init__(self, padding=4):
        self.pad = padding

    def __call__(self, x):
        with torch.no_grad():
            b, _, h, w = x.shape
            x = F.pad(x, [self.pad for _ in range(4)], mode="reflect")
            left, top = torch.randint(0, 1+self.pad*2, (b,)), torch.randint(0, 1+self.pad*2, (b,))
            return torch.stack([
                img[..., t:t+h, l:l+w]
                for img, t, l in zip(x, left, top)
            ], 0)

    def __repr__(self):
        return f"BatchRandomCrop(padding={self.pad})"


class RandomCrop:
    def __init__(self, padding=4):
        self.pad = padding

    def __call__(self, x):
        with torch.no_grad():
            _, h, w = x.shape
            x = F.pad(x[None], [self.pad for _ in range(4)], mode="reflect")
            left, top = random.randint(0, self.pad*2), random.randint(0, self.pad*2)
            return x[0, :, top:top+h, left:left+w]

    def __repr__(self):
        return f"RandomCrop(padding={self.pad})"


class ZCA:
    def __init__(self, mean, scale):
        self.mean = torch.from_numpy(mean).float()
        self.scale = torch.from_numpy(scale).float()

    def __call__(self, x):
        c, h, w = x.shape
        x = x.reshape(-1)
        x = (x - self.mean) @ self.scale
        return x.reshape(c, h, w)

    def __repr__(self):
        return f"ZCA()"


class GCN:
    """global contrast normalization"""
    def __init__(self, multiplier=55, eps=1e-10):
        self.multiplier = multiplier
        self.eps = eps

    def __call__(self, x):
        x -= x.mean()
        norm = x.norm(2)
        norm[norm < self.eps] = 1
        return self.multiplier * x / norm

    def __repr__(self):
        return f"GCN(multiplier={self.multiplier}, eps={self.eps})"


"""
For numpy.array
"""
def numpy_batch_gcn(images, multiplier=55, eps=1e-10):
    # global contrast normalization
    images = images.astype(np.float)
    images -= images.mean(axis=(1,2,3), keepdims=True)
    per_image_norm = np.sqrt(np.square(images).sum((1,2,3), keepdims=True))
    per_image_norm[per_image_norm < eps] = 1
    return multiplier * images / per_image_norm

File Path: cords/utils/data/datasets/SSL/augmentation/builder.py
Content:
from .augmentation_class import WeakAugmentation, StrongAugmentation


def gen_strong_augmentation(img_size, mean, std, flip=True, crop=True, alg="fixmatch", zca=False):
    return StrongAugmentation(img_size, mean, std, flip, crop, alg, zca)


def gen_weak_augmentation(img_size, mean, std, flip=True, crop=True, noise=True, zca=False):
    return WeakAugmentation(img_size, mean, std, flip, crop, noise, zca)

File Path: cords/utils/data/datasets/SSL/augmentation/rand_augment.py
Content:
import numpy as np
from . import augmentation_pool
from . import utils


class RandAugment:
    """
    RandAugment class

    Parameters
    --------
    nops: int
        number of operations per image
    magnitude: int
        maximmum magnitude
    alg: str
        algorithm name
    """
    def __init__(self, nops=2, magnitude=10, prob=0.5, alg="fixmatch"):
        self.nops = nops
        self.magnitude = magnitude
        self.prob = prob
        if alg == "fixmatch":
            self.ops_list = utils.FIXMATCH_RANDAUGMENT_OPS_LIST
        elif alg == "uda":
            self.ops_list = utils.UDA_RANDAUGMENT_OPS_LIST
        else:
            raise NotImplementedError

        self.ops_max_level = utils.RANDAUGMENT_MAX_LEVELS

    def __call__(self, img):
        """
        Apply augmentations to PIL image
        """
        ops = np.random.choice(self.ops_list, self.nops)
        for name in ops:
            if np.random.rand() <= self.prob:
                level = np.random.randint(1, self.magnitude)
                max_level = self.ops_max_level[name]
                transform = getattr(augmentation_pool, name)
                img = transform(img, level, magnitude=self.magnitude, max_level=max_level)
        return img

    def __repr__(self):
        return f"RandAugment(nops={self.nops}, magnitude={self.magnitude})"

File Path: cords/utils/data/datasets/SSL/augmentation/utils.py
Content:
FIXMATCH_RANDAUGMENT_OPS_LIST = [
    'identity',
    'autocontrast',
    'brightness',
    'color',
    'contrast',
    'equalize',
    'posterize',
    'rotate',
    'sharpness',
    'shear_x',
    'shear_y',
    'solarize',
    'translate_x',
    'translate_y'
]


UDA_RANDAUGMENT_OPS_LIST = [
    'invert',
    'autocontrast',
    'brightness',
    'color',
    'contrast',
    'cutout',
    'equalize',
    'posterize',
    'rotate',
    'sharpness',
    'shear_x',
    'shear_y',
    'solarize',
    'translate_x',
    'translate_y'
]


RANDAUGMENT_MAX_LEVELS = {
    'autocontrast': None,
    'brightness': 1.8,
    'color': 1.8,
    'contrast': 1.8,
    'cutout': 20,
    'equalize': None,
    'identity': None,
    'invert': None,
    'posterize': 4,
    'rotate': 30,
    'sharpness': 1.8,
    'shear_x': 0.3,
    'shear_y':0.3,
    'solarize': 256,
    'translate_x': 10,
    'translate_y': 10
}

File Path: cords/utils/data/datasets/SSL/builder.py
Content:
import os
import numpy as np
from torch.utils.data import DataLoader
from torchvision import transforms
from . import utils
from . import dataset_class
from .augmentation.builder import gen_strong_augmentation, gen_weak_augmentation
from .augmentation.augmentation_pool import numpy_batch_gcn, ZCA, GCN


def __val_labeled_unlabeled_split(cfg, train_data, test_data, num_classes, ul_data=None):
    num_validation = int(np.round(len(train_data["images"]) * cfg.dataset.val_ratio))

    np.random.seed(cfg.train_args.seed)

    permutation = np.random.permutation(len(train_data["images"]))
    train_data["images"] = train_data["images"][permutation]
    train_data["labels"] = train_data["labels"][permutation]

    val_data, train_data = utils.dataset_split(train_data, num_validation, num_classes, cfg.dataset.random_split)
    l_train_data, ul_train_data = utils.dataset_split(train_data, cfg.dataset.num_labels, num_classes)

    if ul_data is not None:
        ul_train_data["images"] = np.concatenate([ul_train_data["images"], ul_data["images"]], 0)
        ul_train_data["labels"] = np.concatenate([ul_train_data["labels"], ul_data["labels"]], 0)

    return val_data, l_train_data, ul_train_data


def __labeled_unlabeled_split(cfg, train_data, test_data, num_classes, ul_data=None):
    np.random.seed(cfg.train_args.seed)

    permutation = np.random.permutation(len(train_data["images"]))
    train_data["images"] = train_data["images"][permutation]
    train_data["labels"] = train_data["labels"][permutation]

    l_train_data, ul_train_data = utils.dataset_split(train_data, cfg.dataset.num_labels, num_classes)

    if ul_data is not None:
        ul_train_data["images"] = np.concatenate([ul_train_data["images"], ul_data["images"]], 0)
        ul_train_data["labels"] = np.concatenate([ul_train_data["labels"], ul_data["labels"]], 0)

    return l_train_data, ul_train_data


def gen_dataloader(root, dataset, validation_split, cfg, logger=None):
    """
    generate train, val, and test dataloaders

    Parameters
    --------
    root: str
        root directory
    dataset: str
        dataset name, ['cifar10', 'cifar100', 'svhn', 'stl10']
    validation_split: bool
        if True, return validation loader.
        validation data is made from training data
    cfg: argparse.Namespace or something
    logger: logging.Logger
    """
    ul_train_data = None
    if dataset == "svhn":
        train_data, test_data = utils.get_svhn(root)
        num_classes = 10
        img_size = 32
    elif dataset == "stl10":
        train_data, ul_train_data, test_data = utils.get_stl10(root)
        num_classes = 10
        img_size = 96
    elif dataset == "cifar10":
        train_data, test_data = utils.get_cifar10(root)
        num_classes = 10
        img_size = 32
    elif dataset == "cifar100":
        train_data, test_data = utils.get_cifar100(root)
        num_classes = 100
        img_size = 32
    elif dataset == "cifarOOD":
        train_data, test_data = utils.get_cifarOOD(root, cfg.dataset.ood_ratio)
        num_classes = 6
        img_size = 32
    else:
        raise NotImplementedError

    if validation_split:
        val_data, l_train_data, ul_train_data = __val_labeled_unlabeled_split(
            cfg, train_data, test_data, num_classes, ul_train_data)
    else:
        l_train_data, ul_train_data = __labeled_unlabeled_split(
            cfg, train_data, test_data, num_classes, ul_train_data)
        val_data = None

    ul_train_data["images"] = np.concatenate([ul_train_data["images"], l_train_data["images"]], 0)
    ul_train_data["labels"] = np.concatenate([ul_train_data["labels"], l_train_data["labels"]], 0)

    if logger is not None:
        logger.info("number of :\n \
            training data: %d\n \
            labeled data: %d\n \
            unlabeled data: %d\n \
            validation data: %d\n \
            test data: %d",
            len(train_data["images"]),
            len(l_train_data["images"]),
            len(ul_train_data["images"]),
            0 if val_data is None else len(val_data["images"]),
            len(test_data["images"]))

    labeled_train_data = dataset_class.LabeledDataset(l_train_data)
    unlabeled_train_data = dataset_class.UnlabeledDataset(ul_train_data)

    train_data = np.concatenate([
        labeled_train_data.dataset["images"],
        unlabeled_train_data.dataset["images"]
        ], 0)

    if cfg.dataset.whiten:
        mean = train_data.mean((0, 1, 2)) / 255.
        scale = train_data.std((0, 1, 2)) / 255.
    elif cfg.dataset.zca:
        mean, scale = utils.get_zca_normalization_param(numpy_batch_gcn(train_data))
    else:
        # from [0, 1] to [-1, 1]
        mean = [0.5, 0.5, 0.5]
        scale = [0.5, 0.5, 0.5]

    # set augmentation
    # RA: RandAugment, WA: Weak Augmentation
    randauglist = "fixmatch" if cfg.ssl_args.alg == "pl" else "uda"

    flags = [True if b == "t" else False for b in cfg.dataset.wa.split(".")]

    if cfg.dataset.labeled_aug == "RA":
        labeled_augmentation = gen_strong_augmentation(
            img_size, mean, scale, flags[0], flags[1], randauglist, cfg.dataset.zca)
    elif cfg.dataset.labeled_aug == "WA":
        labeled_augmentation = gen_weak_augmentation(img_size, mean, scale, *flags, cfg.dataset.zca)
    else:
        raise NotImplementedError

    labeled_train_data.transform = labeled_augmentation

    if cfg.dataset.unlabeled_aug == "RA":
        unlabeled_augmentation = gen_strong_augmentation(
            img_size, mean, scale, flags[0], flags[1], randauglist, cfg.dataset.zca)
    elif cfg.dataset.unlabeled_aug == "WA":
        unlabeled_augmentation = gen_weak_augmentation(img_size, mean, scale, *flags, cfg.dataset.zca)
    else:
        raise NotImplementedError

    if logger is not None:
        logger.info("labeled augmentation")
        logger.info(labeled_augmentation)
        logger.info("unlabeled augmentation")
        logger.info(unlabeled_augmentation)

    unlabeled_train_data.weak_augmentation = unlabeled_augmentation

    if cfg.dataset.strong_aug:
        strong_augmentation = gen_strong_augmentation(
            img_size, mean, scale, flags[0], flags[1], randauglist, cfg.dataset.zca)
        unlabeled_train_data.strong_augmentation = strong_augmentation
        if logger is not None:
            logger.info(strong_augmentation)

    if cfg.dataset.zca:
        test_transform = transforms.Compose([GCN(), ZCA(mean, scale)])
    else:
        test_transform = transforms.Compose([transforms.Normalize(mean, scale, True)])

    test_data = dataset_class.LabeledDataset(test_data, test_transform)

    l_train_loader = DataLoader(
        labeled_train_data,
        cfg.dataset.l_batch_size,
        sampler=utils.InfiniteSampler(len(labeled_train_data), cfg.train_args.iteration * cfg.dataloader.l_batch_size),
        num_workers=cfg.dataloader.num_workers
    )
    ul_train_loader = DataLoader(
        unlabeled_train_data,
        cfg.dataloader.ul_batch_size,
        sampler=utils.InfiniteSampler(len(unlabeled_train_data), cfg.train_args.iteration * cfg.dataloader.ul_batch_size),
        num_workers=cfg.dataloader.num_workers
    )
    test_loader = DataLoader(
        test_data,
        1,
        shuffle=False,
        drop_last=False,
        num_workers=cfg.dataloader.num_workers
    )

    if validation_split:
        validation_data = dataset_class.LabeledDataset(val_data, test_transform)
        val_loader = DataLoader(
            validation_data,
            1,
            shuffle=False,
            drop_last=False,
            num_workers=cfg.dataloader.num_workers
        )

        return (
            l_train_loader,
            ul_train_loader,
            val_loader,
            test_loader,
            num_classes,
            img_size
        )

    else:
        return (
            l_train_loader,
            ul_train_loader,
            test_loader,
            num_classes,
            img_size
        )


def gen_dataset(root, dataset, validation_split, cfg, logger=None):
    """
    generate train, val, and test datasets

    Parameters
    -----------
        root: str
            root directory in which data is present or needs to be downloaded
        dataset: str
            dataset name,
            Existing dataset choices: ['cifar10', 'cifar100', 'svhn', 'stl10', 'cifarOOD', 'mnistOOD', 'cifarImbalance']
        validation_split: bool
            if True, return validation loader.
            We use 10% random split of training data as validation data
        cfg: argparse.Namespace or dict
            Dictionary containing necessary arguments for generating the dataset
        logger: logging.Logger
            Logger class for logging the information
    """
    ul_train_data = None
    if dataset == "svhn":
        train_data, test_data = utils.get_svhn(root)
        num_classes = 10
        img_size = 32
    elif dataset == "stl10":
        train_data, ul_train_data, test_data = utils.get_stl10(root)
        num_classes = 10
        img_size = 96
    elif dataset == "cifar10":
        train_data, test_data = utils.get_cifar10(root)
        num_classes = 10
        img_size = 32
    elif dataset == "cifar100":
        train_data, test_data = utils.get_cifar100(root)
        num_classes = 100
        img_size = 32
    elif dataset == "cifarOOD":
        train_data, ul_train_data, test_data = utils.get_cifarOOD(root, cfg.dataset.ood_ratio)
        num_classes = 6
        img_size = 32
    elif dataset == "mnistOOD":
        train_data, ul_train_data, test_data = utils.get_mnistOOD(root, cfg.dataset.ood_ratio)
        num_classes = 6
        img_size = 28
    elif dataset == "cifarImbalance":
        train_data, ul_train_data, test_data = utils.get_cifarClassImb(root, cfg.dataset.ood_ratio)
        num_classes = 10
        img_size = 32
    else:
        raise NotImplementedError

    if validation_split:
        val_data, l_train_data, ul_train_data = __val_labeled_unlabeled_split(
            cfg, train_data, test_data, num_classes, ul_train_data)
    else:
        l_train_data, ul_train_data = __labeled_unlabeled_split(
            cfg, train_data, test_data, num_classes, ul_train_data)
        val_data = None

    ul_train_data["images"] = np.concatenate([ul_train_data["images"], l_train_data["images"]], 0)
    ul_train_data["labels"] = np.concatenate([ul_train_data["labels"], l_train_data["labels"]], 0)

    if logger is not None:
        logger.info("number of :\n \
            training data: %d\n \
            labeled data: %d\n \
            unlabeled data: %d\n \
            validation data: %d\n \
            test data: %d",
            len(train_data["images"]),
            len(l_train_data["images"]),
            len(ul_train_data["images"]),
            0 if val_data is None else len(val_data["images"]),
            len(test_data["images"]))

    labeled_train_data = dataset_class.LabeledDataset(l_train_data)
    unlabeled_train_data = dataset_class.UnlabeledDataset(ul_train_data)

    train_data = np.concatenate([
        labeled_train_data.dataset["images"],
        unlabeled_train_data.dataset["images"]
        ], 0)

    if cfg.dataset.whiten:
        mean = train_data.mean((0, 1, 2)) / 255.
        scale = train_data.std((0, 1, 2)) / 255.
    elif cfg.dataset.zca:
        mean, scale = utils.get_zca_normalization_param(numpy_batch_gcn(train_data))
    elif dataset == 'mnistOOD':
        mean = [0.5]
        scale = [0.5]
    else:
        # from [0, 1] to [-1, 1]
        mean = [0.5, 0.5, 0.5]
        scale = [0.5, 0.5, 0.5]

    # set augmentation
    # RA: RandAugment, WA: Weak Augmentation
    randauglist = "fixmatch" if cfg.ssl_args.alg == "pl" else "uda"

    flags = [True if b == "t" else False for b in cfg.dataset.wa.split(".")]

    if cfg.dataset.labeled_aug == "RA":
        labeled_augmentation = gen_strong_augmentation(
            img_size, mean, scale, flags[0], flags[1], randauglist, cfg.dataset.zca)
    elif cfg.dataset.labeled_aug == "WA":
        labeled_augmentation = gen_weak_augmentation(img_size, mean, scale, *flags, cfg.dataset.zca)
    else:
        raise NotImplementedError

    labeled_train_data.transform = labeled_augmentation

    if cfg.dataset.unlabeled_aug == "RA":
        unlabeled_augmentation = gen_strong_augmentation(
            img_size, mean, scale, flags[0], flags[1], randauglist, cfg.dataset.zca)
    elif cfg.dataset.unlabeled_aug == "WA":
        unlabeled_augmentation = gen_weak_augmentation(img_size, mean, scale, *flags, cfg.dataset.zca)
    else:
        raise NotImplementedError

    if logger is not None:
        logger.info("labeled augmentation")
        logger.info(labeled_augmentation)
        logger.info("unlabeled augmentation")
        logger.info(unlabeled_augmentation)

    unlabeled_train_data.weak_augmentation = unlabeled_augmentation

    if cfg.dataset.strong_aug:
        strong_augmentation = gen_strong_augmentation(
            img_size, mean, scale, flags[0], flags[1], randauglist, cfg.dataset.zca)
        unlabeled_train_data.strong_augmentation = strong_augmentation
        if logger is not None:
            logger.info(strong_augmentation)

    if cfg.dataset.zca:
        test_transform = transforms.Compose([GCN(), ZCA(mean, scale)])
    else:
        test_transform = transforms.Compose([transforms.Normalize(mean, scale, True)])

    test_data = dataset_class.LabeledDataset(test_data, test_transform)

    if validation_split:
        validation_data = dataset_class.LabeledDataset(val_data, test_transform)

        return (
            labeled_train_data,
            unlabeled_train_data,
            validation_data,
            test_data,
            num_classes,
            img_size
        )

    else:
        return (
            labeled_train_data,
            unlabeled_train_data,
            test_data,
            num_classes,
            img_size
        )

File Path: cords/utils/data/datasets/SSL/dataset_class.py
Content:
import torch


class LabeledDataset:
    """
    For labeled dataset
    """
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __getitem__(self, idx):
        image = torch.from_numpy(self.dataset["images"][idx]).float()
        if len(image.shape) == 3:
            image = image.permute(2, 0, 1).contiguous() / 255.
        else:
            image = image.permute(0, 1).contiguous() / 255.
            x = image.shape[0]
            image = image.view(1, x, -1)
        label = int(self.dataset["labels"][idx])
        if self.transform is not None:
            image = self.transform(image)
        return image, label

    def __len__(self):
        return len(self.dataset["images"])


class UnlabeledDataset:
    """
    For unlabeled dataset
    """
    def __init__(self, dataset, weak_augmentation=None, strong_augmentation=None):
        self.dataset = dataset
        self.weak_augmentation = weak_augmentation
        self.strong_augmentation = strong_augmentation

    def __getitem__(self, idx):
        image = torch.from_numpy(self.dataset["images"][idx]).float()
        if len(image.shape) == 3:
            image = image.permute(2, 0, 1).contiguous() / 255.
        else:
            image = image.permute(0, 1).contiguous() / 255.
        label = int(self.dataset["labels"][idx])
        w_aug_image = self.weak_augmentation(image)
        if self.strong_augmentation is not None:
            s_aug_image = self.strong_augmentation(image)
        else:
            s_aug_image = self.weak_augmentation(image)
        return w_aug_image, s_aug_image, label

    def __len__(self):
        return len(self.dataset["images"])


File Path: cords/utils/data/datasets/SSL/load_OOD.py
Content:
import os
import numpy as np
import torch
from torch.utils.data import DataLoader
from skimage.transform import resize
import torch.nn.functional as F
import random

class transform:
    def __init__(self, flip=True, r_crop=True, g_noise=True):
        self.flip = flip
        self.r_crop = r_crop
        self.g_noise = g_noise
        print("holizontal flip : {}, random crop : {}, gaussian noise : {}".format(
            self.flip, self.r_crop, self.g_noise
        ))

    def __call__(self, x):
        if self.flip and random.random() > 0.5:
            x = x.flip(-1)
        if self.r_crop:
            h, w = x.shape[-2:]
            x = F.pad(x, [2,2,2,2], mode="reflect")
            l, t = random.randint(0, 4), random.randint(0,4)
            x = x[:,:,t:t+h,l:l+w]
        if self.g_noise:
            n = torch.randn_like(x) * 0.15
            x = n + x
        return x

class CIFAR10:
    def __init__(self, root, split="l_train"):
        self.dataset = np.load(os.path.join(root, "cifar10_class6", split+".npy"), allow_pickle=True).item()

    def __getitem__(self, idx):
        image = self.dataset["images"][idx]
        label = self.dataset["labels"][idx]
        return image, label, idx

    def __len__(self):
        return len(self.dataset["images"])


class MNIST_idx:
    def __init__(self, root, split="l_train"):
        self.dataset = np.load(os.path.join(root, "mnist_100", split+".npy"), allow_pickle=True).item()

    def __getitem__(self, idx):
        image = self.dataset["images"][idx]
        label = self.dataset["labels"][idx]
        image = (image/255. - 0.5)/0.5
        image = resize(image, (1, 32, 32))
        return image, label, idx

    def __len__(self):
        return len(self.dataset["images"])



class RandomSampler(torch.utils.data.Sampler):
    """ sampling without replacement """

    def __init__(self, num_data, num_sample):
        iterations = num_sample // num_data + 1
        self.indices = torch.cat([torch.randperm(num_data) for _ in range(iterations)]).tolist()[:num_sample]

    def __iter__(self):
        return iter(self.indices)

    def __len__(self):
        return len(self.indices)


def load_minst(dataset, root, ood_ratio, batch_size, iteration):
    if torch.cuda.is_available():
        device = "cuda"
        torch.backends.cudnn.benchmark = True
    else:
        device = "cpu"
    if dataset == 'mnist':
        transform_fn = transform(False, False, False)  # transform function (flip, crop, noise)
        l_train_dataset = MNIST_idx(root, "l_train")
        u_train_dataset = MNIST_idx(root, "u_train_fashion_ood_{}".format(ood_ratio))
        val_dataset = MNIST_idx(root, "val")
        test_dataset = MNIST_idx(root, "test")
    else:
        transform_fn = transform(True, True, True)  # transform function (flip, crop, noise)
        l_train_dataset = CIFAR10(root, "l_train")
        u_train_dataset = CIFAR10(root, "u_train_ood_{}".format(ood_ratio))
        val_dataset = CIFAR10(root, "val")
        test_dataset = CIFAR10(root, "test")

    print("labeled data : {}, unlabeled data : {}, training data : {}, OOD rario : {}".format(
        len(l_train_dataset), len(u_train_dataset), len(l_train_dataset) + len(u_train_dataset), ood_ratio))
    print("validation data : {}, test data : {}".format(len(val_dataset), len(test_dataset)))

    l_loader = DataLoader(
        l_train_dataset, batch_size, drop_last=True,
        sampler=RandomSampler(len(l_train_dataset), iteration * batch_size)
    )
    u_loader = DataLoader(
        u_train_dataset, batch_size, drop_last=True,
        sampler=RandomSampler(len(u_train_dataset), iteration * batch_size)
    )

    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, drop_last=False)
    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, drop_last=False)

    for l_data, u_data in zip(l_loader, u_loader):
        iteration += 1
        l_input, target, _ = l_data
        l_input, target = l_input.to(device).float(), target.to(device).long()

        u_input, dummy_target, idx = u_data
        u_input, dummy_target = u_input.to(device).float(), dummy_target.to(device).long()

    return

if __name__ == '__main__':
    root = "data"
    ood_ratio = 0.5
    batch_size = 64
    iteration = 1000
    dataset = 'mnist' # or cifar10
    load_minst(dataset, root, ood_ratio, batch_size, iteration)



File Path: cords/utils/data/datasets/SSL/utils.py
Content:
import numpy as np
import torch
from torch.utils.data import Sampler
from torchvision.datasets import SVHN, CIFAR10, CIFAR100, STL10, MNIST
import torchvision.transforms as tv_transforms
import os
from skimage.transform import resize


class InfiniteSampler(Sampler):
    """ sampling without replacement """
    def __init__(self, num_data, num_sample):
        epochs = num_sample // num_data + 1
        self.indices = torch.cat([torch.randperm(num_data) for _ in range(epochs)]).tolist()[:num_sample]

    def __iter__(self):
        return iter(self.indices)

    def __len__(self):
        return len(self.indices)


class SequentialSampler(Sampler):
    """ sampling without replacement """
    def __init__(self, num_data, num_sample):
        epochs = num_sample // num_data + 1
        self.indices = torch.cat([torch.range(0, num_data-1).type(torch.int) for _ in range(epochs)]).tolist()[:num_sample]

    def __iter__(self):
        return iter(self.indices)

    def __len__(self):
        return len(self.indices)


def get_svhn(root):
    train_data = SVHN(root, "train", download=True)
    test_data = SVHN(root, "test", download=True)
    train_data = {"images": np.transpose(train_data.data.astype(np.float32), (0, 2, 3, 1)),
                  "labels": train_data.labels.astype(np.int32)}
    test_data = {"images": np.transpose(test_data.data.astype(np.float32), (0, 2, 3, 1)),
                 "labels": test_data.labels.astype(np.int32)}
    return train_data, test_data


def get_cifar10(root):
    train_data = CIFAR10(root, download=True)
    test_data = CIFAR10(root, False)
    train_data = {"images": train_data.data.astype(np.float32),
                  "labels": np.asarray(train_data.targets).astype(np.int32)}
    test_data = {"images": test_data.data.astype(np.float32), 
                 "labels": np.asarray(test_data.targets).astype(np.int32)}
    return train_data, test_data


def get_cifar100(root):
    train_data = CIFAR100(root, download=True)
    test_data = CIFAR100(root, False)
    train_data = {"images": train_data.data.astype(np.float32),
                  "labels": np.asarray(train_data.targets).astype(np.int32)}
    test_data = {"images": test_data.data.astype(np.float32),
                 "labels": np.asarray(test_data.targets).astype(np.int32)}
    return train_data, test_data


def load_mnist(root):
    splits = {}
    #trans = tv_transforms.Compose([tv_transforms.ToPILImage(),tv_transforms.ToTensor(), tv_transforms.Normalize((0.5,), (1.0,))])
    for train in [True, False]:
        dataset = MNIST(root, train, download=True)
        data = {}
        data['images'] = dataset.data
        data['labels'] = np.array(dataset.targets)
        splits['train' if train else 'test'] = data
    return splits.values()


def get_mnistOOD(root, ood_ratio=0.5):
    rng = np.random.RandomState(seed=1)
    train_data, test_data = load_mnist(root)
    # permute index of training set
    indices = rng.permutation(len(train_data['images']))
    train_data['images'] = train_data['images'][indices]
    train_data['labels'] = train_data['labels'][indices]
    test_data = split_test(test_data, tot_class=6)
    train_data, ul_train_data = split_l_u(train_data, n_labels=60, n_unlabels=30000, tot_class=6, ratio=ood_ratio)
    return train_data, ul_train_data, test_data


def split_l_u(train_set, n_labels, n_unlabels, tot_class=6, ratio = 0.5):
    # NOTE: this function assume that train_set is shuffled.
    rng = np.random.RandomState(seed=1)
    images = train_set["images"]
    labels = train_set["labels"]
    classes = np.unique(labels)
    n_labels_per_cls = n_labels // tot_class
    n_unlabels_per_cls = int(n_unlabels*(1.0-ratio)) // tot_class
    if(tot_class < len(classes)):
        n_unlabels_shift = (n_unlabels - (n_unlabels_per_cls * tot_class)) // (len(classes) - tot_class)
    l_images = []
    l_labels = []
    u_images = []
    u_labels = []
    for c in classes[:tot_class]:
        cls_mask = (labels == c)
        c_images = images[cls_mask]
        c_labels = labels[cls_mask]
        l_images += [c_images[:n_labels_per_cls]]
        l_labels += [c_labels[:n_labels_per_cls]]
        u_images += [c_images[n_labels_per_cls:n_labels_per_cls+n_unlabels_per_cls]]
        u_labels += [c_labels[n_labels_per_cls:n_labels_per_cls+n_unlabels_per_cls]]
    for c in classes[tot_class:]:
        cls_mask = (labels == c)
        c_images = images[cls_mask]
        c_labels = labels[cls_mask]
        u_images += [c_images[:n_unlabels_shift]]
        u_labels += [c_labels[:n_unlabels_shift]]

    l_train_set = {"images": np.concatenate(l_images, 0), "labels": np.concatenate(l_labels, 0)}
    u_train_set = {"images": np.concatenate(u_images, 0), "labels": np.concatenate(u_labels, 0)}

    indices = rng.permutation(len(l_train_set["images"]))
    l_train_set["images"] = l_train_set["images"][indices]
    l_train_set["labels"] = l_train_set["labels"][indices]

    indices = rng.permutation(len(u_train_set["images"]))
    u_train_set["images"] = u_train_set["images"][indices]
    u_train_set["labels"] = u_train_set["labels"][indices]
    return l_train_set, u_train_set


def split_l_u_classimb(train_set, n_labels, n_unlabels, tot_class=5, ratio = 0.5):
    # NOTE: this function assume that train_set is shuffled.
    rng = np.random.RandomState(seed=1)
    images = train_set["images"]
    labels = train_set["labels"]
    classes = np.unique(labels)
    n_labels_per_cls = int(n_labels * (ratio/(1.0 + ratio))) // tot_class
    if (tot_class < len(classes)):
        n_labels_shift = (n_labels - (n_labels_per_cls * tot_class)) // (len(classes) - tot_class)

    n_unlabels_per_cls = int(n_unlabels * (ratio/(1.0 + ratio))) // tot_class
    if(tot_class < len(classes)):
        n_unlabels_shift = (n_unlabels - (n_unlabels_per_cls * tot_class)) // (len(classes) - tot_class)
    l_images = []
    l_labels = []
    u_images = []
    u_labels = []
    for c in classes[:tot_class]:
        cls_mask = (labels == c)
        c_images = images[cls_mask]
        c_labels = labels[cls_mask]
        l_images += [c_images[:n_labels_per_cls]]
        l_labels += [c_labels[:n_labels_per_cls]]
        u_images += [c_images[n_labels_per_cls:n_labels_per_cls+n_unlabels_per_cls]]
        u_labels += [c_labels[n_labels_per_cls:n_labels_per_cls+n_unlabels_per_cls]]
    for c in classes[tot_class:]:
        cls_mask = (labels == c)
        c_images = images[cls_mask]
        c_labels = labels[cls_mask]
        l_images += [c_images[:n_labels_shift]]
        l_labels += [c_labels[:n_labels_shift]]
        u_images += [c_images[n_labels_shift: n_labels_shift + n_unlabels_shift]]
        u_labels += [c_labels[n_labels_shift: n_labels_shift + n_unlabels_shift]]

    l_train_set = {"images": np.concatenate(l_images, 0), "labels": np.concatenate(l_labels, 0)}
    u_train_set = {"images": np.concatenate(u_images, 0), "labels": np.concatenate(u_labels, 0)}

    indices = rng.permutation(len(l_train_set["images"]))
    l_train_set["images"] = l_train_set["images"][indices]
    l_train_set["labels"] = l_train_set["labels"][indices]

    indices = rng.permutation(len(u_train_set["images"]))
    u_train_set["images"] = u_train_set["images"][indices]
    u_train_set["labels"] = u_train_set["labels"][indices]
    return l_train_set, u_train_set


def split_test(test_set, tot_class=6):
    rng = np.random.RandomState(seed=1)
    images = test_set["images"]
    labels = test_set['labels']
    classes = np.unique(labels)
    l_images = []
    l_labels = []
    for c in classes[:tot_class]:
        cls_mask = (labels == c)
        c_images = images[cls_mask]
        c_labels = labels[cls_mask]
        l_images += [c_images[:]]
        l_labels += [c_labels[:]]
    test_set = {"images": np.concatenate(l_images, 0), "labels":np.concatenate(l_labels,0)}
    indices = rng.permutation(len(test_set["images"]))
    test_set["images"] = test_set["images"][indices]
    test_set["labels"] = test_set["labels"][indices]
    return test_set

# def get_mnistOOD(root, ood_ratio=0.5):
#     train_data = np.load(os.path.join(root, "mnist_100", "l_train.npy"), allow_pickle=True).item()
#     #train_data['images'] = resize(train_data['images'], (len(train_data['images']), 32, 32))
#     ul_str = "u_train_fashion_ood_" + str(ood_ratio) + ".npy"
#     ul_train_data = np.load(os.path.join(root, "mnist_100", ul_str), allow_pickle=True).item()
#     #ul_train_data['images'] = resize(ul_train_data['images'], (len(ul_train_data['images']), 32, 32))
#     test_data = np.load(os.path.join(root, "mnist_100", "test.npy"), allow_pickle=True).item()
#     #test_data['images'] = resize(test_data['images'], (len(test_data['images']), 32, 32))
#     return train_data, ul_train_data, test_data

def get_cifarClassImb(root, classimb_ratio=0.5):
    rng = np.random.RandomState(seed=1)
    train_data = CIFAR10(root, download=True)
    test_data = CIFAR10(root, False)
    train_data = {"images": train_data.data.astype(np.float32),
                  "labels": np.asarray(train_data.targets).astype(np.int32)}
    test_data = {"images": test_data.data.astype(np.float32),
                 "labels": np.asarray(test_data.targets).astype(np.int32)}

    # permute index of training set
    indices = rng.permutation(len(train_data['images']))
    train_data['images'] = train_data['images'][indices]
    train_data['labels'] = train_data['labels'][indices]
    train_data, ul_train_data = split_l_u_classimb(train_data, n_labels=2400, n_unlabels=20000, tot_class=5, ratio=classimb_ratio)
    return train_data, ul_train_data, test_data


def get_cifarOOD(root, ood_ratio=0.5):
    rng = np.random.RandomState(seed=1)
    train_data = CIFAR10(root, download=True)
    test_data = CIFAR10(root, False)
    train_data = {"images": train_data.data.astype(np.float32),
                  "labels": np.asarray(train_data.targets).astype(np.int32)}
    test_data = {"images": test_data.data.astype(np.float32),
                 "labels": np.asarray(test_data.targets).astype(np.int32)}
    # move class "plane" and "car" to label 8 and 9
    train_data['labels'] -= 2
    test_data['labels'] -= 2
    train_data['labels'][np.where(train_data['labels'] == -2)] = 8
    train_data['labels'][np.where(train_data['labels'] == -1)] = 9
    test_data['labels'][np.where(test_data['labels'] == -2)] = 8
    test_data['labels'][np.where(test_data['labels'] == -1)] = 9
    # permute index of training set
    indices = rng.permutation(len(train_data['images']))
    train_data['images'] = train_data['images'][indices]
    train_data['labels'] = train_data['labels'][indices]
    test_data = split_test(test_data, tot_class=6)
    train_data, ul_train_data = split_l_u(train_data, n_labels=2400, n_unlabels=20000, tot_class=6, ratio=ood_ratio)
    return train_data, ul_train_data, test_data


def get_stl10(root):
    train_data = STL10(root, split="train", download=True)
    ul_train_data = STL10(root, split="unlabeled")
    test_data = STL10(root, split="test")
    train_data = {"images": np.transpose(train_data.data.astype(np.float32), (0, 2, 3, 1)),
                  "labels": train_data.labels}
    ul_train_data = {"images": np.transpose(ul_train_data.data.astype(np.float32), (0, 2, 3, 1)),
                    "labels": ul_train_data.labels}
    test_data = {"images": np.transpose(test_data.data.astype(np.float32), (0, 2, 3, 1)),
                 "labels": test_data.labels}
    return train_data, ul_train_data, test_data


def dataset_split(data, num_data, num_classes, random=False):
    """split dataset into two datasets
    
    Parameters
    -----
    data: dict with keys ["images", "labels"]
        each value is numpy.array
    num_data: int
        number of dataset1
    num_classes: int
        number of classes
    random: bool
        if True, dataset1 is randomly sampled from data.
        if False, dataset1 is uniformly sampled from data,
        which means that the dataset1 contains the same number of samples per class.

    Returns
    -----
    dataset1, dataset2: the same dict as data.
        number of data in dataset1 is num_data.
        number of data in dataset1 is len(data) - num_data.
    """
    dataset1 = {"images": [], "labels": []}
    dataset2 = {"images": [], "labels": []}
    images = data["images"]
    labels = data["labels"]

    # random sampling
    if random:
        dataset1["images"] = images[:num_data]
        dataset1["labels"] = labels[:num_data]
        dataset2["images"] = images[num_data:]
        dataset2["labels"] = labels[num_data:]

    else:
        data_per_class = num_data // num_classes
        for c in range(num_classes):
            c_idx = (labels == c)
            c_imgs = images[c_idx]
            c_lbls = labels[c_idx]
            dataset1["images"].append(c_imgs[:data_per_class])
            dataset1["labels"].append(c_lbls[:data_per_class])
            dataset2["images"].append(c_imgs[data_per_class:])
            dataset2["labels"].append(c_lbls[data_per_class:])
        for k in ("images", "labels"):
            dataset1[k] = np.concatenate(dataset1[k])
            dataset2[k] = np.concatenate(dataset2[k])
    return dataset1, dataset2


def get_zca_normalization_param(images, scale=0.1, eps=1e-10):
    n_data, height, width, channels = images.shape
    images = images.transpose(0, 3, 1, 2)
    images = images.reshape(n_data, channels * height * width)
    image_cov = np.cov(images, rowvar=False)
    U, S, _ = np.linalg.svd(image_cov + scale * np.eye(image_cov.shape[0]))
    zca_decomp = np.dot(U, np.dot(np.diag(1/np.sqrt(S + eps)), U.T))
    mean = images.mean(axis=0)
    return mean, zca_decomp

if __name__ == "__main__":
    #get_mnistOOD(root='/home/krishnateja/PycharmProjects/EfficientSSL/data', ood_ratio=0.5)
    get_cifarOOD(root='/home/krishnateja/PycharmProjects/EfficientSSL/data', ood_ratio=0.5)

File Path: cords/utils/data/datasets/__init__.py
Content:

File Path: cords/utils/data/datasets/__utils/__init__.py
Content:
from .tinyimagenet import TinyImageNet
File Path: cords/utils/data/datasets/__utils/tinyimagenet.py
Content:
import os
import shutil
from torchvision.datasets import ImageFolder
from torchvision.datasets.utils import verify_str_arg
from torchvision.datasets.utils import download_and_extract_archive


def normalize_tin_val_folder_structure(path,
                                       images_folder='images',
                                       annotations_file='val_annotations.txt'):
    # Check if files/annotations are still there to see
    # if we already run reorganize the folder structure.
    images_folder = os.path.join(path, images_folder)
    annotations_file = os.path.join(path, annotations_file)

    # Exists
    if not os.path.exists(images_folder) \
       and not os.path.exists(annotations_file):
        if not os.listdir(path):
            raise RuntimeError('Validation folder is empty.')
        return

    # Parse the annotations
    with open(annotations_file) as f:
        for line in f:
            values = line.split()
            img = values[0]
            label = values[1]
            img_file = os.path.join(images_folder, values[0])
            label_folder = os.path.join(path, label)
            os.makedirs(label_folder, exist_ok=True)
            try:
                shutil.move(img_file, os.path.join(label_folder, img))
            except FileNotFoundError:
                continue

    os.sync()
    assert not os.listdir(images_folder)
    shutil.rmtree(images_folder)
    os.remove(annotations_file)
    os.sync()


class TinyImageNet(ImageFolder):
    """Dataset for TinyImageNet-200"""
    base_folder = 'tiny-imagenet-200'
    zip_md5 = '90528d7ca1a48142e341f4ef8d21d0de'
    splits = ('train', 'val')
    filename = 'tiny-imagenet-200.zip'
    url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'

    def __init__(self, root, split='train', download=False, **kwargs):
        self.data_root = os.path.expanduser(root)
        self.split = verify_str_arg(split, "split", self.splits)

        if download:
            self.download()

        if not self._check_exists():
            raise RuntimeError('Dataset not found.' +
                               ' You can use download=True to download it')
        super().__init__(self.split_folder, **kwargs)

    @property
    def dataset_folder(self):
        return os.path.join(self.data_root, self.base_folder)

    @property
    def split_folder(self):
        return os.path.join(self.dataset_folder, self.split)

    def _check_exists(self):
        return os.path.exists(self.split_folder)

    def extra_repr(self):
        return "Split: {split}".format(**self.__dict__)

    def download(self):
        if self._check_exists():
            return
        download_and_extract_archive(
            self.url, self.data_root, filename=self.filename,
            remove_finished=True, md5=self.zip_md5)
        assert 'val' in self.splits
        normalize_tin_val_folder_structure(
            os.path.join(self.dataset_folder, 'val'))
File Path: cords/utils/models/__init__.py
Content:
from .custom_hyperparam_net import HyperParamNet
from .cifar10net import CifarNet
from .densenet import DenseNet121
from .densenet import DenseNet161
from .densenet import DenseNet169
from .densenet import DenseNet201
from .dla import DLA
from .dla_simple import SimpleDLA
from .dpn import DPN26
from .dpn import DPN92
from .efficientnet import EfficientNetB0
from .googlenet import GoogLeNet
from .googlenet import Inception
from .lenet import LeNet
from .logreg_net import LogisticRegNet, RegressionNet
from .mnist_net import MnistNet
from .mobilenet import MobileNet
from .mobilenetv2 import MobileNetV2
from .pnasnet import PNASNetA
from .pnasnet import PNASNetB
from .preact_resnet import PreActResNet18
from .preact_resnet import PreActResNet34
from .preact_resnet import PreActResNet50
from .preact_resnet import PreActResNet101
from .preact_resnet import PreActResNet152
from .regnet import RegNetX_200MF
from .regnet import RegNetX_400MF
from .regnet import RegNetY_400MF
from .resnet import ResNet18
from .resnet import ResNet34
from .resnet import ResNet50
from .resnet import ResNet101
from .resnet import ResNet152
from .senet import SENet18
from .shufflenet import ShuffleNetG2
from .shufflenet import ShuffleNetG3
from .shufflenetv2 import ShuffleNetV2
from .simpleNN_net import TwoLayerNet
from .simpleNN_net import ThreeLayerNet
from .vgg import VGG
from .resnet164 import ResNet164
from .mobilenetv2tf import MobileNet2
from .wideresnet import WideResNet
from .shakenet import ShakeNet
from .cnn import CNN
from .cnn13 import CNN13
from .lstmclassifier import LSTMClassifier
from .linear_regression import RegressionNet, LogisticNet, DualNet
from .bert_mlp import BERTMLPModel
File Path: cords/utils/models/bert_mlp.py
Content:
from transformers import BertModel, BertConfig
import torch.nn as nn
import torch


class BERTMLPModel(nn.Module):
    def __init__(self, config, checkpoint="bert-base-uncased"):
        super(BERTMLPModel, self).__init__()
        self.config = config
        self.checkpoint = checkpoint
        self.bert = BertModel.from_pretrained(checkpoint)
        classifier_dropout = (
        config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout = nn.Dropout(classifier_dropout)
        ### New layers:
        self.linear1 = nn.Linear(768, config.l1)
        self.linear2 = nn.Linear(config.l1, config.num_classes)

    def get_embedding_dim(self):
        return self.config.l1

    def forward(self, input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        finetune=True, freeze=False, last=False):
        
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        if freeze:
            with torch.no_grad():
                outputs = self.bert(input_ids,
                                    attention_mask=attention_mask,
                                    token_type_ids=token_type_ids,
                                    position_ids=position_ids,
                                    head_mask=head_mask,
                                    inputs_embeds=inputs_embeds,
                                    output_attentions=output_attentions,
                                    output_hidden_states=output_hidden_states,
                                    return_dict=return_dict,)
                # sequence_output has the following shape: (batch_size, sequence_length, 768)
                pooled_output = self.dropout(outputs[1])
                linear1_output = self.linear1(pooled_output) ## extract the 1st token's embeddings              
        elif finetune:
                outputs = self.bert(input_ids,
                                    attention_mask=attention_mask,
                                    token_type_ids=token_type_ids,
                                    position_ids=position_ids,
                                    head_mask=head_mask,
                                    inputs_embeds=inputs_embeds,
                                    output_attentions=output_attentions,
                                    output_hidden_states=output_hidden_states,
                                    return_dict=return_dict,)
                pooled_output = self.dropout(outputs[1])
                # sequence_output has the following shape: (batch_size, sequence_length, 768)
                linear1_output = self.linear1(pooled_output) ## extract the 1st token's embeddings
        else:
            with torch.no_grad():
                outputs = self.bert(input_ids,
                                    attention_mask=attention_mask,
                                    token_type_ids=token_type_ids,
                                    position_ids=position_ids,
                                    head_mask=head_mask,
                                    inputs_embeds=inputs_embeds,
                                    output_attentions=output_attentions,
                                    output_hidden_states=output_hidden_states,
                                    return_dict=return_dict,)
                pooled_output = self.dropout(outputs[1])
            # sequence_output has the following shape: (batch_size, sequence_length, 768)
            linear1_output = self.linear1(pooled_output) ## extract the 1st token's embeddings
        linear2_output = self.linear2(linear1_output)
        if last:
            return linear2_output, linear1_output
        else:
            return linear2_output

if __name__ == "__main__":
    configuration = BertConfig()
    setattr(configuration, 'l1', 512)
    setattr(configuration, 'num_classes', 2)
    model = BERTMLPModel(configuration)
    print(model)
File Path: cords/utils/models/cifar10net.py
Content:
import torch.nn as nn
import torch.nn.functional as F
import torch

class CifarNet(nn.Module):
    def __init__(self):
        super(CifarNet, self).__init__()
        self.embDim = 256
        
        self.conv1 = nn.Conv2d(3,   64,  3)
        self.conv2 = nn.Conv2d(64,  128, 3)
        self.conv3 = nn.Conv2d(128, 256, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 4 * 4, 128)
        self.fc2 = nn.Linear(128, 256)
        self.fc3 = nn.Linear(256, 10)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = self.pool(F.relu(self.conv1(x)))
                out = self.pool(F.relu(self.conv2(out)))
                out = self.pool(F.relu(self.conv3(out)))
                out = out.view(-1, 64 * 4 * 4)
                out = F.relu(self.fc1(out))
                e = F.relu(self.fc2(out))
        else:
            out = self.pool(F.relu(self.conv1(x)))
            out = self.pool(F.relu(self.conv2(out)))
            out = self.pool(F.relu(self.conv3(out)))
            out = out.view(-1, 64 * 4 * 4)
            out = F.relu(self.fc1(out))
            e = F.relu(self.fc2(out))
        out = self.fc3(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim

File Path: cords/utils/models/cnn.py
Content:
import torch
import torch.nn as nn
from .utils import param_init

class CNN(nn.Module):

    def __init__(self, n_out):
        super(CNN, self).__init__()

        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, padding=1),
                                        nn.MaxPool2d(3, stride=2, padding=1),
                                        nn.ReLU(),
                                        nn.Conv2d(16, 32, 3, padding=1),
                                        nn.MaxPool2d(3, stride=2, padding=1),
                                        nn.ReLU()
                                        )
        self.dropout = nn.Dropout(p=0.5)
        self.dense = nn.Linear(32 * 7 * 7, n_out)
        self.embDim = 32*7*7
        param_init(self.modules())  

    def update_batch_stats(self, flag):
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.update_batch_stats = flag

    def get_embedding_dim(self):
        return self.embDim

    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                x = self.conv(x)
                x = x.view(-1, 32 * 7 * 7)
                e = self.dropout(x)
        else:
            x = self.conv(x)
            x = x.view(-1, 32 * 7 * 7)
            e = self.dropout(x)
        x = self.dense(e)
        if last:
            return x, e
        else:
            return x
File Path: cords/utils/models/cnn13.py
Content:
import torch.nn as nn
import torch
from .utils import param_init


class BaseModel(nn.Module):
    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                f = self.feature_extractor(x)
                f = f.mean((2, 3))
        else:
            f = self.feature_extractor(x)
            f = f.mean((2, 3))
        if last:
            return self.classifier(f), f
        else:
            return self.classifier(f)

    def logits_with_feature(self, x):
        f = self.feature_extractor(x)
        c = self.classifier(f.mean((2, 3)))
        return c, f

    def update_batch_stats(self, flag):
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.update_batch_stats = flag


def conv3x3(i_c, o_c, stride=1, bias=False):
    return nn.Conv2d(i_c, o_c, 3, stride, 1, bias=bias)


class BatchNorm2d(nn.BatchNorm2d):
    def __init__(self, channels, momentum=1e-3, eps=1e-3):
        super().__init__(channels)
        self.update_batch_stats = True

    def forward(self, x):
        if self.update_batch_stats or not self.training:
            return super().forward(x)
        else:
            return nn.functional.batch_norm(
                x, None, None, self.weight, self.bias, True, self.momentum, self.eps
            )


def leaky_relu():
    return nn.LeakyReLU(0.1)


class CNN13(BaseModel):
    """
    13-layer CNN

    Parameters
    --------
    num_classes: int
        number of classes
    filters: int
        number of filters
    """
    def __init__(self, num_classes, filters, *args, **kwargs):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            conv3x3(3, filters, bias=True),
            leaky_relu(),
            BatchNorm2d(filters),
            conv3x3(filters, filters, bias=True),
            leaky_relu(),
            BatchNorm2d(filters),
            conv3x3(filters, filters, bias=True),
            leaky_relu(),
            BatchNorm2d(filters),
            nn.MaxPool2d(2, 2),
            conv3x3(filters, 2*filters, bias=True),
            leaky_relu(),
            BatchNorm2d(2*filters),
            conv3x3(2*filters, 2*filters, bias=True),
            leaky_relu(),
            BatchNorm2d(2*filters),
            conv3x3(2*filters, 2*filters, bias=True),
            leaky_relu(),
            BatchNorm2d(2*filters),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(2*filters, 4*filters, 3),
            leaky_relu(),
            BatchNorm2d(4*filters),
            nn.Conv2d(4*filters, 2*filters, 1, bias=False),
            leaky_relu(),
            BatchNorm2d(2*filters),
            nn.Conv2d(2*filters, filters, 1, bias=False),
            leaky_relu(),
            BatchNorm2d(filters)
        )

        self.classifier = nn.Linear(filters, num_classes)
        param_init(self.modules())

        # for m in self.modules():
        #     if isinstance(m, (nn.Conv2d, nn.Linear)):
        #         nn.init.xavier_normal_(m.weight)
        #         if m.bias is not None:
        #             nn.init.constant_(m.bias, 0)


File Path: cords/utils/models/custom_hyperparam_net.py
Content:
import torch.nn as nn
import torch.nn.functional as F
import torch

class HyperParamNet(nn.Module):
    def __init__(self, l1=120, l2=84):
        super(HyperParamNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, l1)
        self.fc2 = nn.Linear(l1, l2)
        self.fc3 = nn.Linear(l2, 10)

    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                x = self.pool(F.relu(self.conv1(x)))
                x = self.pool(F.relu(self.conv2(x)))
                x = x.view(-1, 16 * 5 * 5)
                x = F.relu(self.fc1(x))
                x = F.relu(self.fc2(x))
        else:
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = x.view(-1, 16 * 5 * 5)
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
        out = self.fc3(x)
        if last:
            return out, x
        else:
            return out

File Path: cords/utils/models/densenet.py
Content:
''' DenseNet in PyTorch'

Reference
    Densely Connected Convolutional Networks 
    https://arxiv.org/pdf/1608.06993.pdf
'''


import re
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.model_zoo as model_zoo
from collections import OrderedDict


class _DenseLayer(nn.Sequential):
    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(_DenseLayer, self).__init__()
    

        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),
        self.add_module('relu1', nn.ReLU(inplace=True)),
        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *
                        growth_rate, kernel_size=1, stride=1, bias=False)),
        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),
        self.add_module('relu2', nn.ReLU(inplace=True)),
        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,
                        kernel_size=3, stride=1, padding=1, bias=False)),
        self.drop_rate = drop_rate

    def forward(self, x):
        new_features = super(_DenseLayer, self).forward(x)
        if self.drop_rate > 0:
            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
        return torch.cat([x, new_features], 1)


class _DenseBlock(nn.Sequential):
    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)


class _Transition(nn.Sequential):
    def __init__(self, num_input_features, num_output_features):
        super(_Transition, self).__init__()
        self.add_module('norm', nn.BatchNorm2d(num_input_features))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,
                                          kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))


class DenseNet(nn.Module):
    """
    Parameters
    ----------
    growth_rate: int 
        how many filters to add each layer (`k` in paper)
    block_config: list of 4 ints
        how many layers in each pooling block
    num_init_features: int
        the number of filters to learn in the first convolution layer
    bn_size: int
        multiplicative factor for number of bottle neck layers
          (i.e. bn_size * k features in the bottleneck layer)
    drop_rate: float
        dropout rate after each dense layer
    num_classes: int
        number of classification classes
    """

    def __init__(self, num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),
                 bn_size=4, drop_rate=0, num_classes=10):

        super(DenseNet, self).__init__()

        # First convolution
        self.features = nn.Sequential(OrderedDict([
            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),
            ('norm0', nn.BatchNorm2d(num_init_features)),
            ('relu0', nn.ReLU(inplace=True)),
            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),
        ]))

        # Each denseblock
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,
                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = num_features // 2

        # Embedding dimension
        self.embDim = num_features
        
        # Final batch norm
        self.features.add_module('norm5', nn.BatchNorm2d(num_features))
        
        # Linear layer
        self.classifier = nn.Linear(num_features, num_classes)

        # Official init from torch repo.
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                features = self.features(x)
                out = F.relu(features, inplace=True)
                e = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)
        else:
            features = self.features(x)
            out = F.relu(features, inplace=True)
            e = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)
        out = self.classifier(e)
        if last:
          return out, e
        else:
          return out


    def get_embedding_dim(self):
        return self.embDim


def DenseNet121(**kwargs):
    return DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)


def DenseNet169(**kwargs):
    return DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32), **kwargs)
    

def DenseNet201(**kwargs):
    return DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32), **kwargs)
    

def DenseNet161(**kwargs):
    return DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24), **kwargs)


File Path: cords/utils/models/dla.py
Content:
'''DLA in PyTorch.

Reference:
    Deep Layer Aggregation
    https://arxiv.org/abs/1707.06484
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Root(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1):
        super(Root, self).__init__()
        self.conv = nn.Conv2d(
            in_channels, out_channels, kernel_size,
            stride=1, padding=(kernel_size - 1) // 2, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)


    def forward(self, xs):
        x = torch.cat(xs, 1)
        out = F.relu(self.bn(self.conv(x)))
        return out


class Tree(nn.Module):
    def __init__(self, block, in_channels, out_channels, level=1, stride=1):
        super(Tree, self).__init__()
        self.level = level
        if level == 1:
            self.root = Root(2*out_channels, out_channels)
            self.left_node = block(in_channels, out_channels, stride=stride)
            self.right_node = block(out_channels, out_channels, stride=1)
        else:
            self.root = Root((level+2)*out_channels, out_channels)
            for i in reversed(range(1, level)):
                subtree = Tree(block, in_channels, out_channels,
                               level=i, stride=stride)
                self.__setattr__('level_%d' % i, subtree)
            self.prev_root = block(in_channels, out_channels, stride=stride)
            self.left_node = block(out_channels, out_channels, stride=1)
            self.right_node = block(out_channels, out_channels, stride=1)


    def forward(self, x):
        xs = [self.prev_root(x)] if self.level > 1 else []
        for i in reversed(range(1, self.level)):
            level_i = self.__getattr__('level_%d' % i)
            x = level_i(x)
            xs.append(x)
        x = self.left_node(x)
        xs.append(x)
        x = self.right_node(x)
        xs.append(x)
        out = self.root(xs)
        return out


class DLA(nn.Module):
    def __init__(self, num_classes=10, block=BasicBlock):
        super(DLA, self).__init__()
        self.embDim = 512
        
        self.base = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(True)
        )

        self.layer1 = nn.Sequential(
            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(True)
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(True)
        )

        self.layer3 = Tree(block,  32,  64, level=1, stride=1)
        self.layer4 = Tree(block,  64, 128, level=2, stride=2)
        self.layer5 = Tree(block, 128, 256, level=2, stride=2)
        self.layer6 = Tree(block, 256, 512, level=1, stride=2)
        self.linear = nn.Linear(512, num_classes)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = self.base(x)
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = self.layer4(out)
                out = self.layer5(out)
                out = self.layer6(out)
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = self.base(x)
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = self.layer5(out)
            out = self.layer6(out)
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def test():
    net = DLA()
    print(net)
    x = torch.randn(1, 3, 32, 32)
    y = net(x)
    print(y.size())


if __name__ == '__main__':
    test()

File Path: cords/utils/models/dla_simple.py
Content:
'''Simplified version of DLA in PyTorch.

Note this implementation is not identical to the original paper version.
But it seems works fine.

See dla.py for the original paper version.

Reference:
    Deep Layer Aggregation
    https://arxiv.org/abs/1707.06484
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Root(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1):
        super(Root, self).__init__()
        self.conv = nn.Conv2d(
            in_channels, out_channels, kernel_size,
            stride=1, padding=(kernel_size - 1) // 2, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)


    def forward(self, xs):
        x = torch.cat(xs, 1)
        out = F.relu(self.bn(self.conv(x)))
        return out


class Tree(nn.Module):
    def __init__(self, block, in_channels, out_channels, level=1, stride=1):
        super(Tree, self).__init__()
        self.root = Root(2*out_channels, out_channels)
        if level == 1:
            self.left_tree = block(in_channels, out_channels, stride=stride)
            self.right_tree = block(out_channels, out_channels, stride=1)
        else:
            self.left_tree = Tree(block, in_channels,
                                  out_channels, level=level-1, stride=stride)
            self.right_tree = Tree(block, out_channels,
                                   out_channels, level=level-1, stride=1)


    def forward(self, x):
        out1 = self.left_tree(x)
        out2 = self.right_tree(out1)
        out = self.root([out1, out2])
        return out


class SimpleDLA(nn.Module):
    def __init__(self, num_classes=10, block=BasicBlock):
        super(SimpleDLA, self).__init__()
        self.embDim = 512
        
        self.base = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(True)
        )

        self.layer1 = nn.Sequential(
            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(True)
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(True)
        )

        self.layer3 = Tree(block,  32,  64, level=1, stride=1)
        self.layer4 = Tree(block,  64, 128, level=2, stride=2)
        self.layer5 = Tree(block, 128, 256, level=2, stride=2)
        self.layer6 = Tree(block, 256, 512, level=1, stride=2)
        self.linear = nn.Linear(512, num_classes)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = self.base(x)
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = self.layer4(out)
                out = self.layer5(out)
                out = self.layer6(out)
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = self.base(x)
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = self.layer5(out)
            out = self.layer6(out)
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def test():
    net = SimpleDLA()
    print(net)
    x = torch.randn(1, 3, 32, 32)
    y = net(x)
    print(y.size())


if __name__ == '__main__':
    test()

File Path: cords/utils/models/dpn.py
Content:
'''DPN in PyTorch.

Reference
    Dual Path Networks
    https://arxiv.org/abs/1707.01629
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class Bottleneck(nn.Module):
    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):
        super(Bottleneck, self).__init__()
        self.out_planes = out_planes
        self.dense_depth = dense_depth

        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)
        self.bn2 = nn.BatchNorm2d(in_planes)
        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)

        self.shortcut = nn.Sequential()
        if first_layer:
            self.shortcut = nn.Sequential(
                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_planes+dense_depth)
            )


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        x = self.shortcut(x)
        d = self.out_planes
        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)
        out = F.relu(out)
        return out


class DPN(nn.Module):
    def __init__(self, cfg):
        super(DPN, self).__init__()
        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']
        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']
        self.embDim = out_planes[3]+(num_blocks[3]+1)*dense_depth[3]
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.last_planes = 64
        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)
        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)
        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)
        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)
        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 10)


    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for i,stride in enumerate(strides):
            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))
            self.last_planes = out_planes + (i+2) * dense_depth
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = self.layer4(out)
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out
      
      
    def get_embedding_dim(self):
        return self.embDim


def DPN26():
    cfg = {
        'in_planes': (96,192,384,768),
        'out_planes': (256,512,1024,2048),
        'num_blocks': (2,2,2,2),
        'dense_depth': (16,32,24,128)
    }
    return DPN(cfg)


def DPN92():
    cfg = {
        'in_planes': (96,192,384,768),
        'out_planes': (256,512,1024,2048),
        'num_blocks': (3,4,20,3),
        'dense_depth': (16,32,24,128)
    }
    return DPN(cfg)


def test():
    net = DPN92()
    x = torch.randn(1,3,32,32)
    y = net(x)
    print(y)

# test()

File Path: cords/utils/models/efficientnet.py
Content:
'''EfficientNet in PyTorch.

Reference
    EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
    https://github.com/keras-team/keras-applications/blob/master/keras_applications/efficientnet.py
'''
import torch
import torch.nn as nn
import torch.nn.functional as F


def swish(x):
    return x * x.sigmoid()


def drop_connect(x, drop_ratio):
    keep_ratio = 1.0 - drop_ratio
    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)
    mask.bernoulli_(keep_ratio)
    x.div_(keep_ratio)
    x.mul_(mask)
    return x


class SE(nn.Module):
    '''Squeeze-and-Excitation block with Swish.'''

    def __init__(self, in_channels, se_channels):
        super(SE, self).__init__()
        self.se1 = nn.Conv2d(in_channels, se_channels,
                             kernel_size=1, bias=True)
        self.se2 = nn.Conv2d(se_channels, in_channels,
                             kernel_size=1, bias=True)


    def forward(self, x):
        out = F.adaptive_avg_pool2d(x, (1, 1))
        out = swish(self.se1(out))
        out = self.se2(out).sigmoid()
        out = x * out
        return out


class Block(nn.Module):
    '''expansion + depthwise + pointwise + squeeze-excitation'''

    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 stride,
                 expand_ratio=1,
                 se_ratio=0.,
                 drop_rate=0.):
        super(Block, self).__init__()
        self.stride = stride
        self.drop_rate = drop_rate
        self.expand_ratio = expand_ratio

        # Expansion
        channels = expand_ratio * in_channels
        self.conv1 = nn.Conv2d(in_channels,
                               channels,
                               kernel_size=1,
                               stride=1,
                               padding=0,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(channels)

        # Depthwise conv
        self.conv2 = nn.Conv2d(channels,
                               channels,
                               kernel_size=kernel_size,
                               stride=stride,
                               padding=(1 if kernel_size == 3 else 2),
                               groups=channels,
                               bias=False)
        self.bn2 = nn.BatchNorm2d(channels)

        # SE layers
        se_channels = int(in_channels * se_ratio)
        self.se = SE(channels, se_channels)

        # Output
        self.conv3 = nn.Conv2d(channels,
                               out_channels,
                               kernel_size=1,
                               stride=1,
                               padding=0,
                               bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)

        # Skip connection if in and out shapes are the same (MV-V2 style)
        self.has_skip = (stride == 1) and (in_channels == out_channels)


    def forward(self, x):
        out = x if self.expand_ratio == 1 else swish(self.bn1(self.conv1(x)))
        out = swish(self.bn2(self.conv2(out)))
        out = self.se(out)
        out = self.bn3(self.conv3(out))
        if self.has_skip:
            if self.training and self.drop_rate > 0:
                out = drop_connect(out, self.drop_rate)
            out = out + x
        return out


class EfficientNet(nn.Module):
    def __init__(self, cfg, num_classes=10):
        super(EfficientNet, self).__init__()
        self.cfg = cfg
        self.conv1 = nn.Conv2d(3,
                               32,
                               kernel_size=3,
                               stride=1,
                               padding=1,
                               bias=False)
        self.embDim = cfg['out_channels'][-1]
        self.bn1 = nn.BatchNorm2d(32)
        self.layers = self._make_layers(in_channels=32)
        self.linear = nn.Linear(cfg['out_channels'][-1], num_classes)


    def _make_layers(self, in_channels):
        layers = []
        cfg = [self.cfg[k] for k in ['expansion', 'out_channels', 'num_blocks', 'kernel_size',
                                     'stride']]
        b = 0
        blocks = sum(self.cfg['num_blocks'])
        for expansion, out_channels, num_blocks, kernel_size, stride in zip(*cfg):
            strides = [stride] + [1] * (num_blocks - 1)
            for stride in strides:
                drop_rate = self.cfg['drop_connect_rate'] * b / blocks
                layers.append(
                    Block(in_channels,
                          out_channels,
                          kernel_size,
                          stride,
                          expansion,
                          se_ratio=0.25,
                          drop_rate=drop_rate))
                in_channels = out_channels
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = swish(self.bn1(self.conv1(x)))
                out = self.layers(out)
                out = F.adaptive_avg_pool2d(out, 1)
                e = out.view(out.size(0), -1)
                dropout_rate = self.cfg['dropout_rate']
                if self.training and dropout_rate > 0:
                    e = F.dropout(e, p=dropout_rate)
        else:
            out = swish(self.bn1(self.conv1(x)))
            out = self.layers(out)
            out = F.adaptive_avg_pool2d(out, 1)
            e = out.view(out.size(0), -1)
            dropout_rate = self.cfg['dropout_rate']
            if self.training and dropout_rate > 0:
                e = F.dropout(e, p=dropout_rate)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def EfficientNetB0(num_classes=10):
    cfg = {
        'num_blocks': [1, 2, 2, 3, 3, 4, 1],
        'expansion': [1, 6, 6, 6, 6, 6, 6],
        'out_channels': [16, 24, 40, 80, 112, 192, 320],
        'kernel_size': [3, 3, 5, 3, 5, 5, 3],
        'stride': [1, 2, 2, 2, 1, 2, 1],
        'dropout_rate': 0.2,
        'drop_connect_rate': 0.2,
    }
    return EfficientNet(cfg, num_classes)


def test():
    net = EfficientNetB0()
    x = torch.randn(2, 3, 32, 32)
    y = net(x)
    print(y.shape)


if __name__ == '__main__':
    test()

File Path: cords/utils/models/googlenet.py
Content:
'''GoogLeNet in PyTorch.

Reference:
    GoogLeNet
    https://arxiv.org/abs/1409.4842
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class Inception(nn.Module):
    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):
        super(Inception, self).__init__()
        # 1x1 conv branch
        self.b1 = nn.Sequential(
            nn.Conv2d(in_planes, n1x1, kernel_size=1),
            nn.BatchNorm2d(n1x1),
            nn.ReLU(True),
        )

        # 1x1 conv -> 3x3 conv branch
        self.b2 = nn.Sequential(
            nn.Conv2d(in_planes, n3x3red, kernel_size=1),
            nn.BatchNorm2d(n3x3red),
            nn.ReLU(True),
            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),
            nn.BatchNorm2d(n3x3),
            nn.ReLU(True),
        )

        # 1x1 conv -> 5x5 conv branch
        self.b3 = nn.Sequential(
            nn.Conv2d(in_planes, n5x5red, kernel_size=1),
            nn.BatchNorm2d(n5x5red),
            nn.ReLU(True),
            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),
            nn.BatchNorm2d(n5x5),
            nn.ReLU(True),
            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),
            nn.BatchNorm2d(n5x5),
            nn.ReLU(True),
        )

        # 3x3 pool -> 1x1 conv branch
        self.b4 = nn.Sequential(
            nn.MaxPool2d(3, stride=1, padding=1),
            nn.Conv2d(in_planes, pool_planes, kernel_size=1),
            nn.BatchNorm2d(pool_planes),
            nn.ReLU(True),
        )


    def forward(self, x):
        y1 = self.b1(x)
        y2 = self.b2(x)
        y3 = self.b3(x)
        y4 = self.b4(x)
        return torch.cat([y1,y2,y3,y4], 1)


class GoogLeNet(nn.Module):
    def __init__(self, num_classes=10):
        super(GoogLeNet, self).__init__()
        self.num_classes = num_classes
        self.embDim = 1024
        
        self.pre_layers = nn.Sequential(
            nn.Conv2d(3, 192, kernel_size=3, padding=1),
            nn.BatchNorm2d(192),
            nn.ReLU(True),
        )
        
        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)
        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)
        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)
        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)
        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)
        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)
        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)
        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)
        self.avgpool = nn.AvgPool2d(8, stride=1)
        self.linear = nn.Linear(1024, num_classes)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = self.pre_layers(x)
                out = self.a3(out)
                out = self.b3(out)
                out = self.maxpool(out)
                out = self.a4(out)
                out = self.b4(out)
                out = self.c4(out)
                out = self.d4(out)
                out = self.e4(out)
                out = self.maxpool(out)
                out = self.a5(out)
                out = self.b5(out)
                out = self.avgpool(out)
                e = out.view(out.size(0), -1)
        else:
            out = self.pre_layers(x)
            out = self.a3(out)
            out = self.b3(out)
            out = self.maxpool(out)
            out = self.a4(out)
            out = self.b4(out)
            out = self.c4(out)
            out = self.d4(out)
            out = self.e4(out)
            out = self.maxpool(out)
            out = self.a5(out)
            out = self.b5(out)
            out = self.avgpool(out)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out
     
     
    def get_embedding_dim(self):
        return self.embDim


def test():
    net = GoogLeNet()
    x = torch.randn(1,3,32,32)
    y = net(x)
    print(y.size())

# test()

File Path: cords/utils/models/lenet.py
Content:
'''LeNet in PyTorch.'''


import torch.nn as nn
import torch.nn.functional as F
import torch

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.embDim = 84
        
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1   = nn.Linear(256, 120)
        self.fc2   = nn.Linear(120, 84)
        self.fc3   = nn.Linear(84, 10)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.conv1(x))
                out = F.max_pool2d(out, 2)
                out = F.relu(self.conv2(out))
                out = F.max_pool2d(out, 2)
                out = out.view(out.size(0), -1)
                out = F.relu(self.fc1(out))
                e = F.relu(self.fc2(out))
        else:
            out = F.relu(self.conv1(x))
            out = F.max_pool2d(out, 2)
            out = F.relu(self.conv2(out))
            out = F.max_pool2d(out, 2)
            out = out.view(out.size(0), -1)
            out = F.relu(self.fc1(out))
            e = F.relu(self.fc2(out))
        out = self.fc3(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim

File Path: cords/utils/models/linear_regression.py
Content:
import torch
import torch.nn as nn

class RegressionNet(nn.Module):
    def __init__(self, input_dim):
        super(RegressionNet, self).__init__()
        self.linear = nn.Linear(input_dim, 1)#,bias=False)
        self.feature_dim = input_dim
    
    def forward(self, x, last=False):
        scores = self.linear(x)
        #scores = torch.sigmoid(self.linear(x))
        #return scores.view(-1)
        if last:
            return scores.view(-1), x
        else:
            return scores.view(-1)

    def get_embedding_dim(self):
        return self.feature_dim

class LogisticNet(nn.Module):
    def __init__(self, input_dim):#,num_cls):
        super(LogisticNet, self).__init__()
        self.linear = nn.Linear(input_dim,1)
        #self.feature_dim = input_dim
    
    def forward(self, x, last=False):
        scores = self.linear(x)
        scores = torch.sigmoid(self.linear(x).view(-1))
        return scores

class DualNet(nn.Module):
    def __init__(self, input_dim):
        super(DualNet, self).__init__()
        self.linear = nn.Linear(input_dim, 1,bias=False)
    
    def forward(self, x):
        scores = self.linear(x)
        return scores.view(-1)
File Path: cords/utils/models/logreg_net.py
Content:
import torch.nn as nn
import torch

### Logisitic Regression model
### The softmax will be applied by the torch's CrossEntropyLoss loss function
### Similar to that of a neural network pre-final layer scores.
class LogisticRegNet(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(LogisticRegNet, self).__init__()
        self.linear = nn.Linear(input_dim, num_classes)
        self.feature_dim = input_dim


    def forward(self, x, last=False, freeze=False):
        scores = self.linear(x)
        if last:
            return scores, x
        else:
            return scores


    def get_embedding_dim(self):
        return self.feature_dim


class RegressionNet(nn.Module):
    def __init__(self, input_dim):
        super(RegressionNet, self).__init__()
        self.linear = nn.Linear(input_dim, 1)  # ,bias=False)
        self.feature_dim = input_dim

    def forward(self, x, last=False,freeze=False):

        scores = self.linear(x)

        if last:
            return scores, x
        else:
            return scores#.view(-1)

    def get_embedding_dim(self):
        return self.feature_dim
File Path: cords/utils/models/lstmclassifier.py
Content:
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.nn import functional as F
from cords.utils.data.datasets.SL.builder import loadGloveModel

class LSTMClassifier(nn.Module):
    def __init__(self, num_classes, wordvec_dim, weight_path, num_layers=1, hidden_size=150):
        super(LSTMClassifier, self).__init__()
        self.num_classes = num_classes
        self.hidden_size = hidden_size
        self.embedding_length = wordvec_dim 
        weight_full_path = weight_path+'glove.6B.' + str(wordvec_dim) + 'd.txt'
        wordvec = loadGloveModel(weight_full_path)
        weight = torch.tensor(wordvec.values, dtype=torch.float)  # word embedding for the embedding layer
        
        self.embedding = nn.Embedding(
            weight.shape[0], self.embedding_length)  # Embedding layer
        self.embedding = self.embedding.from_pretrained(
            weight, freeze=False)  # Load pretrianed word embedding, and fine-tuing
        self.lstm = nn.LSTM(self.embedding_length,
                            self.hidden_size, num_layers=num_layers, batch_first=True)  # lstm
        self.fc = nn.Linear(self.hidden_size, self.num_classes)

    def forward(self, input_sentence, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                x = self.embedding(input_sentence)  # (batch_size, batch_dim, embedding_length)
                output, (final_hidden_state, final_cell_state) = self.lstm(x)
        else:
            x = self.embedding(input_sentence)  # (batch_size, batch_dim, embedding_length)
            output, (final_hidden_state, final_cell_state) = self.lstm(x)
        logits = self.fc(final_hidden_state[-1])  # final_hidden_state.size() = (1, batch_size, hidden_size) & logits.size() = (batch_size, num_classes)
        if last:
            return logits, final_hidden_state[-1]
        else:
            return logits

    def get_feature_dim(self):
        return self.hidden_size

    def get_embedding_dim(self):
        return self.hidden_size



File Path: cords/utils/models/mnist_net.py
Content:
import torch
import torch.nn as nn
import torch.nn.functional as F


class MnistNet(nn.Module):
    def __init__(self):
        super(MnistNet, self).__init__()
        self.embDim = 128
        
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = self.conv1(x)
                out = F.relu(out)
                out = self.conv2(out)
                out = F.relu(out)
                out = F.max_pool2d(out, 2)
                out = self.dropout1(out)
                out = torch.flatten(out, 1)
                out = self.fc1(out)
                out = F.relu(out)
                e = self.dropout2(out) 
        else:
            out = self.conv1(x)
            out = F.relu(out)
            out = self.conv2(out)
            out = F.relu(out)
            out = F.max_pool2d(out, 2)
            out = self.dropout1(out)
            out = torch.flatten(out, 1)
            out = self.fc1(out)
            out = F.relu(out)
            e = self.dropout2(out)
        out = self.fc2(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim

File Path: cords/utils/models/mobilenet.py
Content:
'''MobileNet in PyTorch.

Reference
    MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 
    https://arxiv.org/abs/1704.04861
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class Block(nn.Module):
    '''Depthwise conv + Pointwise conv'''
    
    def __init__(self, in_planes, out_planes, stride=1):
        super(Block, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_planes)


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        return out


class MobileNet(nn.Module):
    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1
    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]

    def __init__(self, num_classes=10):
        super(MobileNet, self).__init__()
        self.embDim = 1024
        
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.layers = self._make_layers(in_planes=32)
        self.linear = nn.Linear(1024, num_classes)


    def _make_layers(self, in_planes):
        layers = []
        for x in self.cfg:
            out_planes = x if isinstance(x, int) else x[0]
            stride = 1 if isinstance(x, int) else x[1]
            layers.append(Block(in_planes, out_planes, stride))
            in_planes = out_planes
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layers(out)
                out = F.avg_pool2d(out, 2)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layers(out)
            out = F.avg_pool2d(out, 2)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out

    def get_embedding_dim(self):
        return self.embDim

        
def test():
    net = MobileNet()
    x = torch.randn(1, 3, 32, 32)
    y = net(x)
    print(y.size())

# test()

File Path: cords/utils/models/mobilenetv2.py
Content:
'''MobileNetV2 in PyTorch.

Reference
    Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation
    https://arxiv.org/abs/1801.04381
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class Block(nn.Module):
    '''expand + depthwise + pointwise'''
    
    def __init__(self, in_planes, out_planes, expansion, stride):
        super(Block, self).__init__()
        self.stride = stride

        planes = expansion * in_planes
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn3 = nn.BatchNorm2d(out_planes)

        self.shortcut = nn.Sequential()
        if stride == 1 and in_planes != out_planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),
                nn.BatchNorm2d(out_planes),
            )


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out = out + self.shortcut(x) if self.stride==1 else out
        return out


class MobileNetV2(nn.Module):
    # (expansion, out_planes, num_blocks, stride)
    cfg = [(1,  16, 1, 1),
           (6,  24, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10
           (6,  32, 3, 2),
           (6,  64, 4, 2),
           (6,  96, 3, 1),
           (6, 160, 3, 2),
           (6, 320, 1, 1)]


    def __init__(self, num_classes=10):
        super(MobileNetV2, self).__init__()
        self.embDim = 1280
        
        # NOTE: change conv1 stride 2 -> 1 for CIFAR10
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.layers = self._make_layers(in_planes=32)
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        self.linear = nn.Linear(1280, num_classes)


    def _make_layers(self, in_planes):
        layers = []
        for expansion, out_planes, num_blocks, stride in self.cfg:
            strides = [stride] + [1]*(num_blocks-1)
            for stride in strides:
                layers.append(Block(in_planes, out_planes, expansion, stride))
                in_planes = out_planes
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layers(out)
                out = F.relu(self.bn2(self.conv2(out)))
                # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layers(out)
            out = F.relu(self.bn2(self.conv2(out)))
            # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out

    def get_embedding_dim(self):
        return self.embDim


def test():
    net = MobileNetV2()
    x = torch.randn(2, 3, 32, 32)
    y = net(x)
    print(y.size())

# test()

File Path: cords/utils/models/mobilenetv2tf.py
Content:
import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class BaseBlock(nn.Module):
    alpha = 1

    def __init__(self, input_channel, output_channel, t=6, downsample=False):
        """
            t:  expansion factor, t*input_channel is channel of expansion layer
            alpha:  width multiplier, to get thinner models
            rho:    resolution multiplier, to get reduced representation
        """
        super(BaseBlock, self).__init__()
        self.stride = 2 if downsample else 1
        self.downsample = downsample
        self.shortcut = (not downsample) and (input_channel == output_channel)

        # apply alpha
        input_channel = int(self.alpha * input_channel)
        output_channel = int(self.alpha * output_channel)

        # for main path:
        c = t * input_channel
        # 1x1   point wise conv
        self.conv1 = nn.Conv2d(input_channel, c, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(c)
        # 3x3   depth wise conv
        self.conv2 = nn.Conv2d(c, c, kernel_size=3, stride=self.stride, padding=1, groups=c, bias=False)
        self.bn2 = nn.BatchNorm2d(c)
        # 1x1   point wise conv
        self.conv3 = nn.Conv2d(c, output_channel, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(output_channel)

    def forward(self, inputs):
        # main path
        x = F.relu6(self.bn1(self.conv1(inputs)), inplace=True)
        x = F.relu6(self.bn2(self.conv2(x)), inplace=True)
        x = self.bn3(self.conv3(x))

        # shortcut path
        x = x + inputs if self.shortcut else x

        return x

class MobileNet2(nn.Module):
    def __init__(self, output_size, alpha=1):
        super(MobileNet2, self).__init__()
        self.output_size = output_size

        # first conv layer 
        self.conv0 = nn.Conv2d(3, int(32 * alpha), kernel_size=3, stride=1, padding=1, bias=False)
        self.bn0 = nn.BatchNorm2d(int(32 * alpha))

        # build bottlenecks
        BaseBlock.alpha = alpha
        self.bottlenecks = nn.Sequential(
            BaseBlock(32, 16, t=1, downsample=False),
            BaseBlock(16, 24, downsample=False),
            BaseBlock(24, 24),
            BaseBlock(24, 32, downsample=False),
            BaseBlock(32, 32),
            BaseBlock(32, 32),
            BaseBlock(32, 64, downsample=True),
            BaseBlock(64, 64),
            BaseBlock(64, 64),
            BaseBlock(64, 64),
            BaseBlock(64, 96, downsample=False),
            BaseBlock(96, 96),
            BaseBlock(96, 96),
            BaseBlock(96, 160, downsample=True),
            BaseBlock(160, 160),
            BaseBlock(160, 160),
            BaseBlock(160, 320, downsample=False))

        # last conv layers and fc layer
        self.conv1 = nn.Conv2d(int(320 * alpha), 1280, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(1280)
        self.fc = nn.Linear(1280, output_size)
        self.embDim = 1280
        # weights init
        self.weights_init()

    def weights_init(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))

            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, inputs, last=False, freeze=False):

        if freeze:
            with torch.no_grad():
                # first conv layer
                x = F.relu6(self.bn0(self.conv0(inputs)), inplace=True)
                # assert x.shape[1:] == torch.Size([32, 32, 32])
                # bottlenecks
                x = self.bottlenecks(x)
                # assert x.shape[1:] == torch.Size([320, 8, 8])
                # last conv layer
                x = F.relu6(self.bn1(self.conv1(x)), inplace=True)
                # assert x.shape[1:] == torch.Size([1280,8,8])
                # global pooling and fc (in place of conv 1x1 in paper)
                x = F.adaptive_avg_pool2d(x, 1)
                e = x.view(x.shape[0], -1)
        else:
            # first conv layer
            x = F.relu6(self.bn0(self.conv0(inputs)), inplace=True)
            # assert x.shape[1:] == torch.Size([32, 32, 32])
            # bottlenecks
            x = self.bottlenecks(x)
            # assert x.shape[1:] == torch.Size([320, 8, 8])
            # last conv layer
            x = F.relu6(self.bn1(self.conv1(x)), inplace=True)
            # assert x.shape[1:] == torch.Size([1280,8,8])
            # global pooling and fc (in place of conv 1x1 in paper)
            x = F.adaptive_avg_pool2d(x, 1)
            e = x.view(x.shape[0], -1)
        x = self.fc(e)

        if last:
            return x, e
        else:
            return x

    def get_embedding_dim(self):
        return self.embDim

File Path: cords/utils/models/pnasnet.py
Content:
'''PNASNet in PyTorch.

Reference
    Progressive Neural Architecture Search
    https://arxiv.org/abs/1712.00559
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class SepConv(nn.Module):
    '''Separable Convolution.'''
    def __init__(self, in_planes, out_planes, kernel_size, stride):
        super(SepConv, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, out_planes,
                               kernel_size, stride,
                               padding=(kernel_size-1)//2,
                               bias=False, groups=in_planes)
        self.bn1 = nn.BatchNorm2d(out_planes)


    def forward(self, x):
        return self.bn1(self.conv1(x))


class CellA(nn.Module):
    def __init__(self, in_planes, out_planes, stride=1):
        super(CellA, self).__init__()
        self.stride = stride
        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)
        if stride==2:
            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
            self.bn1 = nn.BatchNorm2d(out_planes)


    def forward(self, x):
        y1 = self.sep_conv1(x)
        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)
        if self.stride==2:
            y2 = self.bn1(self.conv1(y2))
        return F.relu(y1+y2)


class CellB(nn.Module):
    def __init__(self, in_planes, out_planes, stride=1):
        super(CellB, self).__init__()
        self.stride = stride
        # Left branch
        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)
        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3, stride=stride)
        # Right branch
        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5, stride=stride)
        if stride==2:
            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
            self.bn1 = nn.BatchNorm2d(out_planes)
        # Reduce channels
        self.conv2 = nn.Conv2d(2*out_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_planes)


    def forward(self, x):
        # Left branch
        y1 = self.sep_conv1(x)
        y2 = self.sep_conv2(x)
        # Right branch
        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)
        if self.stride==2:
            y3 = self.bn1(self.conv1(y3))
        y4 = self.sep_conv3(x)
        # Concat & reduce channels
        b1 = F.relu(y1+y2)
        b2 = F.relu(y3+y4)
        y = torch.cat([b1,b2], 1)
        return F.relu(self.bn2(self.conv2(y)))


class PNASNet(nn.Module):
    def __init__(self, cell_type, num_cells, num_planes):
        super(PNASNet, self).__init__()
        self.in_planes = num_planes
        self.cell_type = cell_type
        self.embDim = self.in_planes * 4
        
        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(num_planes)

        self.layer1 = self._make_layer(num_planes, num_cells=6)
        self.layer2 = self._downsample(num_planes*2)
        self.layer3 = self._make_layer(num_planes*2, num_cells=6)
        self.layer4 = self._downsample(num_planes*4)
        self.layer5 = self._make_layer(num_planes*4, num_cells=6)

        self.linear = nn.Linear(num_planes*4, 10)


    def _make_layer(self, planes, num_cells):
        layers = []
        for _ in range(num_cells):
            layers.append(self.cell_type(self.in_planes, planes, stride=1))
            self.in_planes = planes
        return nn.Sequential(*layers)


    def _downsample(self, planes):
        layer = self.cell_type(self.in_planes, planes, stride=2)
        self.in_planes = planes
        return layer


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = self.layer4(out)
                out = self.layer5(out)
                out = F.avg_pool2d(out, 8)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = self.layer5(out)
            out = F.avg_pool2d(out, 8)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def PNASNetA():
    return PNASNet(CellA, num_cells=6, num_planes=44)


def PNASNetB():
    return PNASNet(CellB, num_cells=6, num_planes=32)


def test():
    net = PNASNetB()
    x = torch.randn(1,3,32,32)
    y = net(x)
    print(y)

# test()

File Path: cords/utils/models/preact_resnet.py
Content:
'''Pre-activation ResNet in PyTorch.

Reference:
    Identity Mappings in Deep Residual Networks
    https://arxiv.org/abs/1603.05027
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class PreActBlock(nn.Module):
    '''Pre-activation version of the BasicBlock.'''
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(PreActBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)

        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)
            )


    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out += shortcut
        return out


class PreActBottleneck(nn.Module):
    '''Pre-activation version of the original Bottleneck module.'''
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(PreActBottleneck, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)

        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)
            )


    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out = self.conv3(F.relu(self.bn3(out)))
        out += shortcut
        return out


class PreActResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(PreActResNet, self).__init__()
        self.in_planes = 64
        self.embDim = 8 * self.in_planes * block.expansion
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = self.conv1(x)
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = self.layer4(out)
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = self.conv1(x)
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def PreActResNet18():
    return PreActResNet(PreActBlock, [2,2,2,2])


def PreActResNet34():
    return PreActResNet(PreActBlock, [3,4,6,3])


def PreActResNet50():
    return PreActResNet(PreActBottleneck, [3,4,6,3])


def PreActResNet101():
    return PreActResNet(PreActBottleneck, [3,4,23,3])


def PreActResNet152():
    return PreActResNet(PreActBottleneck, [3,8,36,3])


def test():
    net = PreActResNet18()
    y = net((torch.randn(1,3,32,32)))
    print(y.size())

# test()

File Path: cords/utils/models/regnet.py
Content:
'''RegNet in PyTorch.

Reference
    Designing Network Design Spaces
    https://arxiv.org/abs/2003.13678
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class SE(nn.Module):
    '''Squeeze-and-Excitation block.'''

    def __init__(self, in_planes, se_planes):
        super(SE, self).__init__()
        self.se1 = nn.Conv2d(in_planes, se_planes, kernel_size=1, bias=True)
        self.se2 = nn.Conv2d(se_planes, in_planes, kernel_size=1, bias=True)


    def forward(self, x):
        out = F.adaptive_avg_pool2d(x, (1, 1))
        out = F.relu(self.se1(out))
        out = self.se2(out).sigmoid()
        out = x * out
        return out


class Block(nn.Module):
    def __init__(self, w_in, w_out, stride, group_width, bottleneck_ratio, se_ratio):
        super(Block, self).__init__()
        # 1x1
        w_b = int(round(w_out * bottleneck_ratio))
        self.conv1 = nn.Conv2d(w_in, w_b, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(w_b)
        # 3x3
        num_groups = w_b // group_width
        self.conv2 = nn.Conv2d(w_b, w_b, kernel_size=3,
                               stride=stride, padding=1, groups=num_groups, bias=False)
        self.bn2 = nn.BatchNorm2d(w_b)
        # se
        self.with_se = se_ratio > 0
        if self.with_se:
            w_se = int(round(w_in * se_ratio))
            self.se = SE(w_b, w_se)
        # 1x1
        self.conv3 = nn.Conv2d(w_b, w_out, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(w_out)

        self.shortcut = nn.Sequential()
        if stride != 1 or w_in != w_out:
            self.shortcut = nn.Sequential(
                nn.Conv2d(w_in, w_out,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(w_out)
            )


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        if self.with_se:
            out = self.se(out)
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class RegNet(nn.Module):
    def __init__(self, cfg, num_classes=10):
        super(RegNet, self).__init__()
        self.cfg = cfg
        self.in_planes = 64
        self.embDim = self.cfg['widths'][-1]
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(0)
        self.layer2 = self._make_layer(1)
        self.layer3 = self._make_layer(2)
        self.layer4 = self._make_layer(3)
        self.linear = nn.Linear(self.cfg['widths'][-1], num_classes)


    def _make_layer(self, idx):
        depth = self.cfg['depths'][idx]
        width = self.cfg['widths'][idx]
        stride = self.cfg['strides'][idx]
        group_width = self.cfg['group_width']
        bottleneck_ratio = self.cfg['bottleneck_ratio']
        se_ratio = self.cfg['se_ratio']

        layers = []
        for i in range(depth):
            s = stride if i == 0 else 1
            layers.append(Block(self.in_planes, width,
                                s, group_width, bottleneck_ratio, se_ratio))
            self.in_planes = width
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = self.layer4(out)
                out = F.adaptive_avg_pool2d(out, (1, 1))
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = F.adaptive_avg_pool2d(out, (1, 1))
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def RegNetX_200MF():
    cfg = {
        'depths': [1, 1, 4, 7],
        'widths': [24, 56, 152, 368],
        'strides': [1, 1, 2, 2],
        'group_width': 8,
        'bottleneck_ratio': 1,
        'se_ratio': 0,
    }
    return RegNet(cfg)


def RegNetX_400MF():
    cfg = {
        'depths': [1, 2, 7, 12],
        'widths': [32, 64, 160, 384],
        'strides': [1, 1, 2, 2],
        'group_width': 16,
        'bottleneck_ratio': 1,
        'se_ratio': 0,
    }
    return RegNet(cfg)


def RegNetY_400MF():
    cfg = {
        'depths': [1, 2, 7, 12],
        'widths': [32, 64, 160, 384],
        'strides': [1, 1, 2, 2],
        'group_width': 16,
        'bottleneck_ratio': 1,
        'se_ratio': 0.25,
    }
    return RegNet(cfg)


def test():
    net = RegNetX_200MF()
    print(net)
    x = torch.randn(2, 3, 32, 32)
    y = net(x)
    print(y.shape)


if __name__ == '__main__':
    test()

File Path: cords/utils/models/resnet.py
Content:
'''ResNet in PyTorch.
Reference
    Deep Residual Learning for Image Recognition
    https://arxiv.org/abs/1512.03385
'''
from typing import Any, Callable, List, Optional, Type, Union, TypeVar, Dict
import torch
import torch.nn as nn
from torch import Tensor


__all__ = [
    "ResNet",
    "resnet18",
    "resnet34",
    "resnet50",
    "resnet101",
    "resnet152",
    "resnext50_32x4d",
    "resnext101_32x8d",
    "resnext101_64x4d",
    "wide_resnet50_2",
    "wide_resnet101_2",
]


V = TypeVar("V")
def _ovewrite_value_param(param: str, actual: Optional[V], expected: V) -> V:
    if actual is not None:
        if actual != expected:
            raise ValueError(f"The parameter '{param}' expected value {expected} but got {actual} instead.")
    return expected


def _ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: V) -> None:
    if param in kwargs:
        if kwargs[param] != new_value:
            raise ValueError(f"The parameter '{param}' expected value {new_value} but got {kwargs[param]} instead.")
    else:
        kwargs[param] = new_value


def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.Conv2d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        groups=groups,
        bias=False,
        dilation=dilation,
    )


def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class BasicBlock(nn.Module):
    expansion: int = 1

    def __init__(
        self,
        inplanes: int,
        planes: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError("BasicBlock only supports groups=1 and base_width=64")
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    # This variant is also known as ResNet V1.5 and improves accuracy according to
    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.

    expansion: int = 4

    def __init__(
        self,
        inplanes: int,
        planes: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.0)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):
    def __init__(
        self,
        block: Type[Union[BasicBlock, Bottleneck]],
        layers: List[int],
        num_classes: int = 1000,
        zero_init_residual: bool = False,
        groups: int = 1,
        width_per_group: int = 64,
        replace_stride_with_dilation: Optional[List[bool]] = None,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        # _log_api_usage_once(self)
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(
                "replace_stride_with_dilation should be None "
                f"or a 3-element tuple, got {replace_stride_with_dilation}"
            )
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        self.embDim = 512 * block.expansion
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck) and m.bn3.weight is not None:
                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]
                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:
                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]

    def _make_layer(
        self,
        block: Type[Union[BasicBlock, Bottleneck]],
        planes: int,
        blocks: int,
        stride: int = 1,
        dilate: bool = False,
    ) -> nn.Sequential:
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer
            )
        )
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes,
                    planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=norm_layer,
                )
            )

        return nn.Sequential(*layers)

    def _forward_impl(self, x: Tensor, last=False, freeze=False) -> Tensor:
        # See note [TorchScript super()]
        if freeze:
            with torch.no_grad():
                self.eval()
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.maxpool(x)
                x = self.layer1(x)
                x = self.layer2(x)
                x = self.layer3(x)
                x = self.layer4(x)
                x = self.avgpool(x)
                features = torch.flatten(x, 1)
                self.train()
        else:
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.maxpool(x)
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            x = self.avgpool(x)
            features = torch.flatten(x, 1)

        out = self.fc(features)
        if last:
            return out, features
        else:
            return out

    def forward(self, x: Tensor, last=False, freeze=False) -> Tensor:
        return self._forward_impl(x, last, freeze)

    def get_embedding_dim(self):
        return self.embDim

    def get_grads(self) -> torch.Tensor:
        """
        Returns all the gradients concatenated in a single tensor.
        :return: gradients tensor (??)
        """
        grads = []
        for pp in list(self.parameters()):
            if pp.requires_grad: # only using the parameter that require the gradient
                grads.append(pp.grad.view(-1))
        return torch.cat(grads)


def _resnet(
    block: Type[Union[BasicBlock, Bottleneck]],
    layers: List[int],
    **kwargs: Any,
) -> ResNet:
    model = ResNet(block, layers, **kwargs)
    return model



def ResNet18(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.
    Args:
        weights (:class:`~torchvision.models.ResNet18_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNet18_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNet18_Weights
        :members:
    """
    return _resnet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, **kwargs)

def ResNet34(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNet-34 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.
    Args:
        weights (:class:`~torchvision.models.ResNet34_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNet34_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNet34_Weights
        :members:
    """
    return _resnet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, **kwargs)


def ResNet50(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.
    .. note::
       The bottleneck of TorchVision places the stride for downsampling to the second 3x3
       convolution while the original paper places it to the first 1x1 convolution.
       This variant improves the accuracy and is known as `ResNet V1.5
       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.
    Args:
        weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNet50_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNet50_Weights
        :members:
    """
    return _resnet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs)



def ResNet101(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNet-101 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.
    .. note::
       The bottleneck of TorchVision places the stride for downsampling to the second 3x3
       convolution while the original paper places it to the first 1x1 convolution.
       This variant improves the accuracy and is known as `ResNet V1.5
       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.
    Args:
        weights (:class:`~torchvision.models.ResNet101_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNet101_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNet101_Weights
        :members:
    """
    return _resnet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, **kwargs)


def ResNet152(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNet-152 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.
    .. note::
       The bottleneck of TorchVision places the stride for downsampling to the second 3x3
       convolution while the original paper places it to the first 1x1 convolution.
       This variant improves the accuracy and is known as `ResNet V1.5
       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.
    Args:
        weights (:class:`~torchvision.models.ResNet152_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNet152_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNet152_Weights
        :members:
    """
    return _resnet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, **kwargs)


def ResNext50_32x4d(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNeXt-50 32x4d model from
    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.
    Args:
        weights (:class:`~torchvision.models.ResNeXt50_32X4D_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNext50_32X4D_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNeXt50_32X4D_Weights
        :members:
    """
    _ovewrite_named_param(kwargs, "groups", 32)
    _ovewrite_named_param(kwargs, "width_per_group", 4)
    return _resnet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs)


def ResNext101_32x8d(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNeXt-101 32x8d model from
    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.
    Args:
        weights (:class:`~torchvision.models.ResNeXt101_32X8D_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNeXt101_32X8D_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNeXt101_32X8D_Weights
        :members:
    """
    _ovewrite_named_param(kwargs, "groups", 32)
    _ovewrite_named_param(kwargs, "width_per_group", 8)
    return _resnet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, **kwargs)


def ResNext101_64x4d(num_classes: int, **kwargs: Any) -> ResNet:
    """ResNeXt-101 64x4d model from
    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.
    Args:
        weights (:class:`~torchvision.models.ResNeXt101_64X4D_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.ResNeXt101_64X4D_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.ResNeXt101_64X4D_Weights
        :members:
    """
    _ovewrite_named_param(kwargs, "groups", 64)
    _ovewrite_named_param(kwargs, "width_per_group", 4)
    return _resnet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, **kwargs)


def wide_resnet50_2(num_classes: int, **kwargs: Any) -> ResNet:
    """Wide ResNet-50-2 model from
    `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_.
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        weights (:class:`~torchvision.models.Wide_ResNet50_2_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.Wide_ResNet50_2_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.Wide_ResNet50_2_Weights
        :members:
    """
    _ovewrite_named_param(kwargs, "width_per_group", 64 * 2)
    return _resnet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs)


def wide_resnet101_2(num_classes: int, **kwargs: Any) -> ResNet:
    """Wide ResNet-101-2 model from
    `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_.
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-101 has 2048-512-2048
    channels, and in Wide ResNet-101-2 has 2048-1024-2048.
    Args:
        weights (:class:`~torchvision.models.Wide_ResNet101_2_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.Wide_ResNet101_2_Weights` below for
            more details, and possible values. By default, no pre-trained
            weights are used.
        progress (bool, optional): If True, displays a progress bar of the
            download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_
            for more details about this class.
    .. autoclass:: torchvision.models.Wide_ResNet101_2_Weights
        :members:
    """
    _ovewrite_named_param(kwargs, "width_per_group", 64 * 2)
    return _resnet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, **kwargs)
File Path: cords/utils/models/resnet164.py
Content:
'''ResNet for cifar in pytorch

Reference:
    Deep residual learning for image recognition
        https://arxiv.org/abs/1512.03385
    Identity mappings in deep residual networks
        https://arxiv.org/abs/1603.05027
'''

import torch
import torch.nn as nn
import math


def conv3x3(in_planes, out_planes, stride=1):
    " 3x3 convolution with padding "
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


class BasicBlock(nn.Module):
    expansion=1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion=4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes*4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class PreActBasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(PreActBasicBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = conv3x3(planes, planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.bn1(x)
        out = self.relu(out)

        if self.downsample is not None:
            residual = self.downsample(out)

        out = self.conv1(out)

        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv2(out)

        out += residual

        return out


class PreActBottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(PreActBottleneck, self).__init__()
        self.bn1 = nn.BatchNorm2d(inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, bias=False)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.bn1(x)
        out = self.relu(out)

        if self.downsample is not None:
            residual = self.downsample(out)

        out = self.conv1(out)

        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv2(out)

        out = self.bn3(out)
        out = self.relu(out)
        out = self.conv3(out)

        out += residual

        return out


class ResNet_Cifar(nn.Module):

    def __init__(self, block, layers, num_classes=10):
        super(ResNet_Cifar, self).__init__()
        self.inplanes = 16
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(block, 16, layers[0])
        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)
        self.avgpool = nn.AvgPool2d(8, stride=1)
        self.fc = nn.Linear(64 * block.expansion, num_classes)
        self.embDim = 64 * block.expansion

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion)
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)

                x = self.layer1(x)
                x = self.layer2(x)
                x = self.layer3(x)

                x = self.avgpool(x)
                x = x.view(x.size(0), -1)
        else:
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)

            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)

            x = self.avgpool(x)
            x = x.view(x.size(0), -1)
        out = self.fc(x)
        if last:
            return out, x
        else:
            return out

    def get_embedding_dim(self):
        return self.embDim

class PreAct_ResNet_Cifar(nn.Module):

    def __init__(self, block, layers, num_classes=10):
        super(PreAct_ResNet_Cifar, self).__init__()
        self.inplanes = 16
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.layer1 = self._make_layer(block, 16, layers[0])
        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)
        self.bn = nn.BatchNorm2d(64*block.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.avgpool = nn.AvgPool2d(8, stride=1)
        self.fc = nn.Linear(64*block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes*block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes*block.expansion, kernel_size=1, stride=stride, bias=False)
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes*block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        x = self.bn(x)
        x = self.relu(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x



def resnet20_cifar(**kwargs):
    model = ResNet_Cifar(BasicBlock, [3, 3, 3], **kwargs)
    return model


def resnet32_cifar(**kwargs):
    model = ResNet_Cifar(BasicBlock, [5, 5, 5], **kwargs)
    return model


def resnet44_cifar(**kwargs):
    model = ResNet_Cifar(BasicBlock, [7, 7, 7], **kwargs)
    return model


def resnet56_cifar(**kwargs):
    model = ResNet_Cifar(BasicBlock, [9, 9, 9], **kwargs)
    return model


def resnet110_cifar(**kwargs):
    model = ResNet_Cifar(BasicBlock, [18, 18, 18], **kwargs)
    return model


def resnet1202_cifar(**kwargs):
    model = ResNet_Cifar(BasicBlock, [200, 200, 200], **kwargs)
    return model


def ResNet164(num_class):
    model = ResNet_Cifar(Bottleneck, [18, 18, 18], num_classes=num_class)
    return model


def resnet1001_cifar(**kwargs):
    model = ResNet_Cifar(Bottleneck, [111, 111, 111], **kwargs)
    return model


def preact_resnet110_cifar(**kwargs):
    model = PreAct_ResNet_Cifar(PreActBasicBlock, [18, 18, 18], **kwargs)
    return model


def preact_resnet164_cifar(**kwargs):
    model = PreAct_ResNet_Cifar(PreActBottleneck, [18, 18, 18], **kwargs)
    return model


def preact_resnet1001_cifar(**kwargs):
    model = PreAct_ResNet_Cifar(PreActBottleneck, [111, 111, 111], **kwargs)
    return model

File Path: cords/utils/models/resnext.py
Content:
'''ResNeXt in PyTorch.

Reference
    Aggregated Residual Transformations for Deep Neural Networks
    https://arxiv.org/abs/1611.05431
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class Block(nn.Module):
    '''Grouped convolution block.'''
    expansion = 2

    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):
        super(Block, self).__init__()
        group_width = cardinality * bottleneck_width
        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(group_width)
        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)
        self.bn2 = nn.BatchNorm2d(group_width)
        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*group_width:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*group_width)
            )


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNeXt(nn.Module):
    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):
        super(ResNeXt, self).__init__()
        self.cardinality = cardinality
        self.bottleneck_width = bottleneck_width
        self.in_planes = 64
        self.embDim = cardinality*bottleneck_width*8
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(num_blocks[0], 1)
        self.layer2 = self._make_layer(num_blocks[1], 2)
        self.layer3 = self._make_layer(num_blocks[2], 2)
        # self.layer4 = self._make_layer(num_blocks[3], 2)
        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)


    def _make_layer(self, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))
            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width
        # Increase bottleneck_width by 2 after each stage.
        self.bottleneck_width *= 2
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                # out = self.layer4(out)
                out = F.avg_pool2d(out, 8)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            # out = self.layer4(out)
            out = F.avg_pool2d(out, 8)
            e = out.view(out.size(0), -1) 
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def ResNeXt29_2x64d(num_classes=10):
    return ResNeXt([3,3,3], 2, 64, num_classes)


def ResNeXt29_4x64d(num_classes=10):
    return ResNeXt([3,3,3], 4, 64, num_classes)


def ResNeXt29_8x64d(num_classes=10):
    return ResNeXt([3,3,3], 8, 64, num_classes)


def ResNeXt29_32x4d(num_classes=10):
    return ResNeXt([3,3,3], 32, 4, num_classes)


def test_resnext():
    net = ResNeXt29_2x64d()
    x = torch.randn(1,3,32,32)
    y = net(x)
    print(y.size())

# test_resnext()

File Path: cords/utils/models/senet.py
Content:
'''SENet in PyTorch.

SENet is the winner of ImageNet-2017. The paper is not released yet.
'''
import torch
import torch.nn as nn
import torch.nn.functional as F


class BasicBlock(nn.Module):
    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes)
            )

        # SE layers
        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear
        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        # Squeeze
        w = F.avg_pool2d(out, out.size(2))
        w = F.relu(self.fc1(w))
        w = F.sigmoid(self.fc2(w))
        # Excitation
        out = out * w  # New broadcasting feature from v0.2!

        out += self.shortcut(x)
        out = F.relu(out)
        return out


class PreActBlock(nn.Module):
    def __init__(self, in_planes, planes, stride=1):
        super(PreActBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)

        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)
            )

        # SE layers
        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)
        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)


    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))

        # Squeeze
        w = F.avg_pool2d(out, out.size(2))
        w = F.relu(self.fc1(w))
        w = F.sigmoid(self.fc2(w))
        # Excitation
        out = out * w

        out += shortcut
        return out


class SENet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(SENet, self).__init__()
        self.in_planes = 64
        self.embDim = 512
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512, num_classes)


    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = self.layer4(out)
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)     
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim


def SENet18():
    return SENet(PreActBlock, [2,2,2,2])


def test():
    net = SENet18()
    y = net(torch.randn(1,3,32,32))
    print(y.size())

# test()

File Path: cords/utils/models/shakenet.py
Content:
import itertools
import torch
import torch.nn as nn
import torch.nn.functional as F
from .utils import param_init


class BaseModel(nn.Module):
    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                f = self.feature_extractor(x)
                f = f.mean((2, 3))
        else:
            f = self.feature_extractor(x)
            f = f.mean((2, 3))
        if last:
            return self.classifier(f), f
        else:
            return self.classifier(f)

    def logits_with_feature(self, x):
        f = self.feature_extractor(x)
        c = self.classifier(f.mean((2, 3)))
        return c, f

    def update_batch_stats(self, flag):
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.update_batch_stats = flag


def conv3x3(i_c, o_c, stride=1, bias=False):
    return nn.Conv2d(i_c, o_c, 3, stride, 1, bias=bias)


class BatchNorm2d(nn.BatchNorm2d):
    def __init__(self, channels, momentum=1e-3, eps=1e-3):
        super().__init__(channels)
        self.update_batch_stats = True

    def forward(self, x):
        if self.update_batch_stats or not self.training:
            return super().forward(x)
        else:
            return nn.functional.batch_norm(
                x, None, None, self.weight, self.bias, True, self.momentum, self.eps
            )


def leaky_relu():
    return nn.LeakyReLU(0.1)


class _ShakeShake(nn.Module):
    def __init__(self, branch1, branch2):
        super().__init__()
        self.branch1 = branch1
        self.branch2 = branch2

    def forward(self, x):
        a = self.branch1(x)
        b = self.branch2(x)
        if not self.training:
            return 0.5 * (a + b)
        mu = a.new([a.shape[0]] + [1] * (len(a.shape) - 1)).uniform_()
        mixf = a + mu * (b - a)
        mixb = a + mu[::1] * (b - a)
        return (mixf - mixb).detach() + mixb


class _SkipBranch(nn.Module):
    def __init__(self, branch1, branch2, bn):
        super().__init__()
        self.branch1 = branch1
        self.branch2 = branch2
        self.bn = bn

    def forward(self, x):
        a = self.branch1(x[..., ::2, ::2])
        b = self.branch2(x[..., 1::2, 1::2])
        x = torch.cat([a, b], 1)
        return self.bn(x)


def _branch(filters, channels, stride=1):
    return nn.Sequential(
        nn.ReLU(),
        conv3x3(channels, filters, stride),
        BatchNorm2d(filters),
        nn.ReLU(),
        conv3x3(filters, filters),
        BatchNorm2d(filters)
    )


class _Residual(nn.Module):
    def __init__(self, channels, filters, stride=1):
        super().__init__()
        self.branch = _ShakeShake(
            _branch(channels, filters, stride),
            _branch(channels, filters, stride)
        )

        if stride == 2:
            branch1 = nn.Sequential(nn.ReLU(), nn.Conv2d(channels//2, filters >> 1, 1, bias=False))
            branch2 = nn.Sequential(nn.ReLU(), nn.Conv2d(channels//2, filters >> 1, 1, bias=False))
            bn = BatchNorm2d(filters)
            self.skip = _SkipBranch(branch1, branch2, bn)
        elif channels != filters:
            self.skip = nn.Sequential(
                nn.Conv2d(channels, filters, 1, bias=False),
                BatchNorm2d(filters)
            )

    def forward(self, x):
        return self.branch(x) + self.skip(x)


class ShakeNet(BaseModel):
    """
    Shake-Shake model

    Parameters
    --------
    num_classes: int
        number of classes
    filters: int
        number of filters
    scales: int
        number of scales
    repeat: int
        number of residual blocks per scale
    dropout: float
        dropout ratio (None indicates dropout is unused)
    """
    def __init__(self, num_classes, filters, scales, repeat, dropout=None, *args, **kwargs):
        super().__init__()

        feature_extractor = [conv3x3(3, 16)]
        channels = 16

        for scale, i in itertools.product(range(scales), range(repeat)):
            if i == 0:
                feature_extractor.append(_Residual(channels, filters << scale, stride = 2 if scale else 1))
            else:
                feature_extractor.append(_Residual(channels, filters << scale))

            channels = filters << scale

        self.feature_extractor = nn.Sequential(*feature_extractor)

        classifier = []
        if dropout is not None:
            classifier.append(nn.Dropout(dropout))
        classifier.append(nn.Linear(channels, num_classes))

        param_init(self.modules())

File Path: cords/utils/models/shufflenet.py
Content:
'''ShuffleNet in PyTorch.

Reference
    ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
    https://arxiv.org/abs/1707.01083
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class ShuffleBlock(nn.Module):
    def __init__(self, groups):
        super(ShuffleBlock, self).__init__()
        self.groups = groups


    def forward(self, x):
        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''
        N,C,H,W = x.size()
        g = self.groups
        return x.view(N,g,C//g,H,W).permute(0,2,1,3,4).reshape(N,C,H,W)


class Bottleneck(nn.Module):
    def __init__(self, in_planes, out_planes, stride, groups):
        super(Bottleneck, self).__init__()
        self.stride = stride
        mid_planes = int(out_planes/4)
        g = 1 if in_planes==24 else groups
        
        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_planes)
        self.shuffle1 = ShuffleBlock(groups=g)
        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_planes)
        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)
        self.bn3 = nn.BatchNorm2d(out_planes)

        self.shortcut = nn.Sequential()
        if stride == 2:
            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))


    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.shuffle1(out)
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        res = self.shortcut(x)
        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)
        return out


class ShuffleNet(nn.Module):
    def __init__(self, cfg):
        super(ShuffleNet, self).__init__()
        out_planes = cfg['out_planes']
        num_blocks = cfg['num_blocks']
        groups = cfg['groups']
        self.embDim = out_planes[2]
        
        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(24)
        self.in_planes = 24
        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)
        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)
        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)
        self.linear = nn.Linear(out_planes[2], 10)


    def _make_layer(self, out_planes, num_blocks, groups):
        layers = []
        for i in range(num_blocks):
            stride = 2 if i == 0 else 1
            cat_planes = self.in_planes if i == 0 else 0
            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))
            self.in_planes = out_planes
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)            
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim
        

def ShuffleNetG2():
    cfg = {
        'out_planes': [200,400,800],
        'num_blocks': [4,8,4],
        'groups': 2
    }
    return ShuffleNet(cfg)


def ShuffleNetG3():
    cfg = {
        'out_planes': [240,480,960],
        'num_blocks': [4,8,4],
        'groups': 3
    }
    return ShuffleNet(cfg)


def test():
    net = ShuffleNetG2()
    x = torch.randn(1,3,32,32)
    y = net(x)
    print(y)

# test()

File Path: cords/utils/models/shufflenetv2.py
Content:
'''ShuffleNetV2 in PyTorch.

Reference
    ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
    https://arxiv.org/abs/1807.11164
'''


import torch
import torch.nn as nn
import torch.nn.functional as F


class ShuffleBlock(nn.Module):
    def __init__(self, groups=2):
        super(ShuffleBlock, self).__init__()
        self.groups = groups


    def forward(self, x):
        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''
        N, C, H, W = x.size()
        g = self.groups
        return x.view(N, g, C//g, H, W).permute(0, 2, 1, 3, 4).reshape(N, C, H, W)


class SplitBlock(nn.Module):
    def __init__(self, ratio):
        super(SplitBlock, self).__init__()
        self.ratio = ratio


    def forward(self, x):
        c = int(x.size(1) * self.ratio)
        return x[:, :c, :, :], x[:, c:, :, :]


class BasicBlock(nn.Module):
    def __init__(self, in_channels, split_ratio=0.5):
        super(BasicBlock, self).__init__()
        self.split = SplitBlock(split_ratio)
        in_channels = int(in_channels * split_ratio)
        self.conv1 = nn.Conv2d(in_channels, in_channels,
                               kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.conv2 = nn.Conv2d(in_channels, in_channels,
                               kernel_size=3, stride=1, padding=1, groups=in_channels, bias=False)
        self.bn2 = nn.BatchNorm2d(in_channels)
        self.conv3 = nn.Conv2d(in_channels, in_channels,
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(in_channels)
        self.shuffle = ShuffleBlock()


    def forward(self, x):
        x1, x2 = self.split(x)
        out = F.relu(self.bn1(self.conv1(x2)))
        out = self.bn2(self.conv2(out))
        out = F.relu(self.bn3(self.conv3(out)))
        out = torch.cat([x1, out], 1)
        out = self.shuffle(out)
        return out


class DownBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DownBlock, self).__init__()
        mid_channels = out_channels // 2
        # left
        self.conv1 = nn.Conv2d(in_channels, in_channels,
                               kernel_size=3, stride=2, padding=1, groups=in_channels, bias=False)
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.conv2 = nn.Conv2d(in_channels, mid_channels,
                               kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_channels)
        # right
        self.conv3 = nn.Conv2d(in_channels, mid_channels,
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(mid_channels)
        self.conv4 = nn.Conv2d(mid_channels, mid_channels,
                               kernel_size=3, stride=2, padding=1, groups=mid_channels, bias=False)
        self.bn4 = nn.BatchNorm2d(mid_channels)
        self.conv5 = nn.Conv2d(mid_channels, mid_channels,
                               kernel_size=1, bias=False)
        self.bn5 = nn.BatchNorm2d(mid_channels)

        self.shuffle = ShuffleBlock()


    def forward(self, x):
        # left
        out1 = self.bn1(self.conv1(x))
        out1 = F.relu(self.bn2(self.conv2(out1)))
        # right
        out2 = F.relu(self.bn3(self.conv3(x)))
        out2 = self.bn4(self.conv4(out2))
        out2 = F.relu(self.bn5(self.conv5(out2)))
        # concat
        out = torch.cat([out1, out2], 1)
        out = self.shuffle(out)
        return out


class ShuffleNetV2(nn.Module):
    def __init__(self, net_size):
        super(ShuffleNetV2, self).__init__()
        out_channels = configs[net_size]['out_channels']
        num_blocks = configs[net_size]['num_blocks']
        self.embDim = out_channels[3]
        
        self.conv1 = nn.Conv2d(3, 24, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(24)
        self.in_channels = 24
        self.layer1 = self._make_layer(out_channels[0], num_blocks[0])
        self.layer2 = self._make_layer(out_channels[1], num_blocks[1])
        self.layer3 = self._make_layer(out_channels[2], num_blocks[2])
        self.conv2 = nn.Conv2d(out_channels[2], out_channels[3],
                               kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels[3])
        self.linear = nn.Linear(out_channels[3], 10)


    def _make_layer(self, out_channels, num_blocks):
        layers = [DownBlock(self.in_channels, out_channels)]
        for i in range(num_blocks):
            layers.append(BasicBlock(out_channels))
            self.in_channels = out_channels
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = F.relu(self.bn1(self.conv1(x)))
                # out = F.max_pool2d(out, 3, stride=2, padding=1)
                out = self.layer1(out)
                out = self.layer2(out)
                out = self.layer3(out)
                out = F.relu(self.bn2(self.conv2(out)))
                out = F.avg_pool2d(out, 4)
                e = out.view(out.size(0), -1)
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            # out = F.max_pool2d(out, 3, stride=2, padding=1)
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = F.relu(self.bn2(self.conv2(out)))
            out = F.avg_pool2d(out, 4)
            e = out.view(out.size(0), -1)
        out = self.linear(e)
        if last:
            return out, e
        else:
            return out


    def get_embedding_dim(self):
        return self.embDim
        

configs = {
    0.5: {
        'out_channels': (48, 96, 192, 1024),
        'num_blocks': (3, 7, 3)
    },

    1: {
        'out_channels': (116, 232, 464, 1024),
        'num_blocks': (3, 7, 3)
    },
    1.5: {
        'out_channels': (176, 352, 704, 1024),
        'num_blocks': (3, 7, 3)
    },
    2: {
        'out_channels': (224, 488, 976, 2048),
        'num_blocks': (3, 7, 3)
    }
}


def test():
    net = ShuffleNetV2(net_size=0.5)
    x = torch.randn(3, 3, 32, 32)
    y = net(x)
    print(y.shape)


# test()

File Path: cords/utils/models/simpleNN_net.py
Content:
import torch.nn as nn
import torch.nn.functional as F
import torch

class TwoLayerNet(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_units):
        super(TwoLayerNet, self).__init__()
        self.linear1 = nn.Linear(input_dim, hidden_units)
        self.linear2 = nn.Linear(hidden_units, num_classes)
        self.feature_dim = hidden_units
    

    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                l1scores = F.relu(self.linear1(x))
        else:
            l1scores = F.relu(self.linear1(x))
        scores = self.linear2(l1scores)
        if last:
            return scores, l1scores
        else:
            return scores


    def get_feature_dim(self):
        return self.feature_dim


    def get_embedding_dim(self):
        return self.feature_dim


class ThreeLayerNet(nn.Module):
    def __init__(self, input_dim, num_classes, h1, h2):
        super(ThreeLayerNet, self).__init__()
        self.linear1 = nn.Linear(input_dim, h1)
        self.linear2 = nn.Linear(h1, h2)
        self.linear3 = nn.Linear(h2, num_classes)
        self.feature_dim = h2

    
    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                l1scores = F.relu(self.linear1(x))
                l2scores = F.relu(self.linear2(l1scores))
        else:
            l1scores = F.relu(self.linear1(x))
            l2scores = F.relu(self.linear2(l1scores))
        scores = self.linear3(l2scores)
        if last:
            return scores, l2scores
        else:
            return scores


    def get_feature_dim(self):
        return self.feature_dim


    def get_embedding_dim(self):
        return self.feature_dim
File Path: cords/utils/models/utils.py
Content:
import math
import torch.nn as nn
import torch.nn.functional as F
import torch


def apply_weight_decay(modules, decay_rate):
    """apply weight decay to weight parameters in nn.Conv2d and nn.Linear"""
    for m in modules:
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            m.weight.data -= decay_rate * m.weight.data


def param_init(modules):
    for m in modules:
        if isinstance(m, nn.Conv2d):
            f, _, k, _ = m.weight.shape
            nn.init.normal_(m.weight, 0, 1./math.sqrt(0.5 * k * k * f))
        elif isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight)
            nn.init.constant_(m.bias, 0)


"""
For exponential moving average
"""
def __ema(p1, p2, factor):
    return factor * p1 + (1 - factor) * p2


def __param_update(ema_model, raw_model, factor):
    """ema for trainable parameters"""
    for ema_p, raw_p in zip(ema_model.parameters(), raw_model.parameters()):
        ema_p.data = __ema(ema_p.data, raw_p.data, factor)


def __buffer_update(ema_model, raw_model, factor):
    """ema for buffer parameters (e.g., running_mean and running_var in nn.BatchNorm2d)"""
    for ema_p, raw_p in zip(ema_model.buffers(), raw_model.buffers()):
        ema_p.data = __ema(ema_p.data, raw_p.data, factor)
    # """copy buffer parameters (e.g., running_mean and running_var in nn.BatchNorm2d)"""
    # for ema_p, raw_p in zip(ema_model.buffers(), raw_model.buffers()):
    #     ema_p.copy_(raw_p)


def ema_update(ema_model, raw_model, ema_factor, weight_decay_factor=None, global_step=None):
    if global_step is not None:
        ema_factor = min(1 - 1 / (global_step+1), ema_factor)
    __param_update(ema_model, raw_model, ema_factor)
    __buffer_update(ema_model, raw_model, ema_factor)
    if weight_decay_factor is not None:
        apply_weight_decay(ema_model.modules(), weight_decay_factor)


File Path: cords/utils/models/vgg.py
Content:
'''VGG11/13/16/19 in Pytorch.'''


import torch
import torch.nn as nn


cfg = {
    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}


class VGG(nn.Module):
    def __init__(self, vgg_name):
        super(VGG, self).__init__()
        self.features = self._make_layers(cfg[vgg_name])
        self.embDim = 512
        self.classifier = nn.Linear(512, 10)


    def _make_layers(self, cfg):
        layers = []
        in_channels = 3
        for x in cfg:
            if x == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),
                           nn.BatchNorm2d(x),
                           nn.ReLU(inplace=True)]
                in_channels = x
        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]
        return nn.Sequential(*layers)


    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                out = self.features(x)
                e = out.view(out.size(0), -1)
        else:
            out = self.features(x)
            e = out.view(out.size(0), -1)
        out = self.classifier(e)
        if last:
            return out, e
        else:
            return out
        

    def get_embedding_dim(self):
        return self.embDim


def test():
    net = VGG('VGG11')
    x = torch.randn(2,3,32,32)
    y = net(x)
    print(y.size())

# test()

File Path: cords/utils/models/wideresnet.py
Content:
import torch
import torch.nn as nn
import torch.nn.functional as F
from .utils import param_init

class BaseModel(nn.Module):
    def forward(self, x, last=False, freeze=False):
        if freeze:
            with torch.no_grad():
                f = self.feature_extractor(x)
                f = f.mean((2, 3))
        else:
            f = self.feature_extractor(x)
            f = f.mean((2, 3))
        if last:
            return self.classifier(f), f
        else:
            return self.classifier(f)

    def logits_with_feature(self, x):
        f = self.feature_extractor(x)
        c = self.classifier(f.mean((2, 3)))
        return c, f

    def update_batch_stats(self, flag):
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.update_batch_stats = flag


def conv3x3(i_c, o_c, stride=1, bias=False):
    return nn.Conv2d(i_c, o_c, 3, stride, 1, bias=bias)


class BatchNorm2d(nn.BatchNorm2d):
    def __init__(self, channels, momentum=1e-3, eps=1e-3):
        super().__init__(channels)
        self.update_batch_stats = True

    def forward(self, x):
        if self.update_batch_stats or not self.training:
            return super().forward(x)
        else:
            return nn.functional.batch_norm(
                x, None, None, self.weight, self.bias, True, self.momentum, self.eps
            )


def leaky_relu():
    return nn.LeakyReLU(0.1)


class _Residual(nn.Module):
    def __init__(self, input_channels, output_channels, stride=1, activate_before_residual=False):
        super().__init__()
        layer = []
        if activate_before_residual:
            self.pre_act = nn.Sequential(
                BatchNorm2d(input_channels),
                leaky_relu()
            )
        else:
            self.pre_act = nn.Identity()
            layer.append(BatchNorm2d(input_channels))
            layer.append(leaky_relu())
        layer.append(conv3x3(input_channels, output_channels, stride))
        layer.append(BatchNorm2d(output_channels))
        layer.append(leaky_relu())
        layer.append(conv3x3(output_channels, output_channels))

        if stride >= 2 or input_channels != output_channels:
            self.identity = nn.Conv2d(input_channels, output_channels, 1, stride, bias=False)
        else:
            self.identity = nn.Identity()

        self.layer = nn.Sequential(*layer)

    def forward(self, x):
        x = self.pre_act(x)
        return self.identity(x) + self.layer(x)


class WideResNet(BaseModel):
    """
    ResNet

    Parameters
    --------
    num_classes: int
        number of classes
    filters: int
        number of filters
    scales: int
        number of scales
    repeat: int
        number of residual blocks per scale
    dropout: float
        dropout ratio (None indicates dropout is unused)
    """
    def __init__(self, num_classes, filters, scales, repeat, dropout=None, *args, **kwargs):
        super().__init__()
        feature_extractor = [conv3x3(3, 16)]
        channels = 16
        for scale in range(scales):
            feature_extractor.append(
                _Residual(channels, filters<<scale, 2 if scale else 1, activate_before_residual = (scale == 0))
            )
            channels = filters << scale
            for _ in range(repeat - 1):
                feature_extractor.append(
                    _Residual(channels, channels)
                )

        feature_extractor.append(BatchNorm2d(channels))
        feature_extractor.append(leaky_relu())
        self.feature_extractor = nn.Sequential(*feature_extractor)
        classifier = []
        if dropout is not None:
            classifier.append(nn.Dropout(dropout))
        classifier.append(nn.Linear(channels, num_classes))
        self.embDim = channels
        self.classifier = nn.Sequential(*classifier)
        param_init(self.modules())

    def get_embedding_dim(self):
        return self.embDim

File Path: cords/utils/utils.py
Content:
import os
import glob
import pandas as pd
import numpy as np
from asyncore import read
import os
import os.path as osp
import glob
from re import L
from turtle import mode
import pandas as pd
import numpy as np
from .config_utils import check_dir_exist


"""
############################## Global Arguments ##############################
"""
COLUMN_NAMES_DICT = {
                'epoch': 'Epoch',
                'trn_loss': 'Training Loss',
                'trn_acc': 'Training Accuracy',
                'val_loss': 'Validation Loss',
                'val_acc': 'Validation Accuracy',
                'tst_loss': 'Test Loss',
                'tst_acc': 'Test Accuracy',
                'time': 'Timing'}

"""
############################## Functions ##############################
"""

def init_results_dict(print_args):
    results_dict = {}
    for print_arg in print_args:
        results_dict[print_arg] = [] 
    return results_dict


def sllogtodf(log_file, print_args, tmp_df_dict):
    with open(log_file, "r") as fp:
        read_lines = fp.readlines()
        if "Total time taken by " in read_lines[-1]:
            results_dict = init_results_dict(print_args)
            for i in range(1, len(read_lines)-11):
                for print_arg in print_args:
                    if 'takes' not in read_lines[i]:
                        if COLUMN_NAMES_DICT[print_arg] in read_lines[i]:
                            results_dict[print_arg].append(float(read_lines[i].split(COLUMN_NAMES_DICT[print_arg])[1].split(': ')[1].split(" ")[0]))
            for print_arg in print_args:
                if print_arg == 'time':
                    tmp_df_dict[COLUMN_NAMES_DICT[print_arg]] = generate_cumulative_timing(np.array(results_dict[print_arg]))
                else:
                    tmp_df_dict[COLUMN_NAMES_DICT[print_arg]] = results_dict[print_arg]
            tmp_df = pd.DataFrame(tmp_df_dict)
            return tmp_df
        else:
            return None


def generate_cumulative_timing(mod_timing):
        tmp = 0
        mod_cum_timing = np.zeros(len(mod_timing))
        for i in range(len(mod_timing)):
            tmp += mod_timing[i]
            mod_cum_timing[i] = tmp
        return (mod_cum_timing / 3600).tolist()
        

def sllogstodfs(results_dir, print_args=["epoch", "trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"]):
    """
    Convert Supervised Learning Logs to Dictionary of DataFrames
    """
    dirname = osp.abspath(osp.expanduser(results_dir))
    check_dir_exist(dirname)

    """
    ############################## Main Code ##############################
    """
    sub_dir = glob.glob(osp.join(dirname, 'SL'))
    column_names = ['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every', 'Run']
    for print_arg in print_args:
        assert print_arg in COLUMN_NAMES_DICT, "Please add column name corresponding to the print argument in COLUMN_NAMES_DICT."
        column_names.append(COLUMN_NAMES_DICT[print_arg])
    """
    Current Folder Structure:
    all_logs_dir = os.path.join(results_dir, 
                                self.cfg.setting,
                                self.cfg.dataset.name,
                                subset_selection_name,
                                self.cfg.model.architecture,
                                str(self.cfg.dss_args.fraction),
                                str(self.cfg.dss_args.select_every),
                                str(self.cfg.train_args.run))
    """   
    incomplete_runs = []
    dfs_dict = {}
    full_dfs_dict = {}
    nonadaptive_dfs_dict = {'Random':{},
            'GlobalOrder_fl':{},
            'GlobalOrder_gc':{},
            'GlobalOrder_logdet':{},
            'GlobalOrder_supfl':{}}

    #DFs for Non-Adaptive Strategies
    for setting in sub_dir: #SL, SSL...
        dset_dir = glob.glob(osp.join(setting, '*')) 
        setting_value = osp.basename(setting)
        #dsets = [osp.basename(dset) for dset in dset_dir]
        for dset in dset_dir: #CIFAR10, SST2,....
            strategy_dir = glob.glob(osp.join(dset,  '*')) 
            dset_value = osp.basename(dset)
            for strategy in strategy_dir: #Random, Glister, GradMatch
                model_dir = glob.glob(osp.join(strategy,  '*')) 
                strategy_value = osp.basename(strategy)
                if strategy_value in ['Full', 'Random', 'GlobalOrder_fl', 'GlobalOrder_gc', 'GlobalOrder_logdet', 'GlobalOrder_supfl']:
                    for model in model_dir: #ResNet, BERTMLP,....
                        #Full doesn't depend on fraction
                        model_value = osp.basename(model)
                        if strategy_value == 'Full':
                            run_dir = glob.glob(osp.join(model,  '1', '1', '*'))
                            df_name = "_".join([setting_value, dset_value, model_value])  
                            if df_name not in full_dfs_dict.keys():
                                full_dfs_dict[df_name] = pd.DataFrame(columns=column_names)
                            for run in run_dir: #0, 1, 2, 3, ...
                                run_value = osp.basename(run)
                                log_dir = glob.glob(osp.join(run, '*.log'))
                                for log_file in log_dir: #.log files
                                    tmp_df_dict = {'Setting': setting_value, 'Dataset': dset_value, 'Strategy': strategy_value, 
                                                    'Model': model_value, 'Fraction': '1', 'Select every': '1', 'Run': run_value}
                                            
                                    tmp_df = sllogtodf(log_file, print_args, tmp_df_dict)
                                    if tmp_df is not None:        
                                        full_dfs_dict[df_name] = pd.concat([full_dfs_dict[df_name], tmp_df])  
                                    else:
                                        incomplete_runs.append('_'.join([setting_value, dset_value, strategy_value, model_value, '1', '1', run_value]))
                        else:
                            fraction_dir = glob.glob(osp.join(model,  '*')) 
                            for fraction in fraction_dir: #0.1, 0.2, 0.3,....
                                run_dir = glob.glob(osp.join(fraction, '1', '*')) 
                                fraction_value = osp.basename(fraction)
                                df_name = "_".join([setting_value, dset_value, model_value, fraction_value])
                                if df_name not in nonadaptive_dfs_dict[strategy_value].keys():
                                    nonadaptive_dfs_dict[strategy_value][df_name] = pd.DataFrame(columns=column_names)
                                for run in run_dir: #0, 1, 2, 3, ...
                                    run_value = osp.basename(run)
                                    log_dir = glob.glob(osp.join(run, '*.log'))
                                    for log_file in log_dir: #.log files
                                        tmp_df_dict = {'Setting': setting_value, 'Dataset': dset_value, 'Strategy': strategy_value, 
                                                    'Model': model_value, 'Fraction': fraction_value, 'Select every': '1', 'Run': run_value}
                                            
                                        tmp_df = sllogtodf(log_file, print_args, tmp_df_dict)
                                        if tmp_df is not None:        
                                            nonadaptive_dfs_dict[strategy_value][df_name] = pd.concat([nonadaptive_dfs_dict[strategy_value][df_name], tmp_df])  
                                        else:
                                            incomplete_runs.append('_'.join([setting_value, dset_value, strategy_value, model_value, fraction_value, '1', run_value]))
    
    #DFs for Adaptive Strategies
    for setting in sub_dir: #SL, SSL...
        dset_dir = glob.glob(osp.join(setting, '*')) 
        setting_value = osp.basename(setting)
        #dsets = [osp.basename(dset) for dset in dset_dir]
        for dset in dset_dir: #CIFAR10, SST2,....
            strategy_dir = glob.glob(osp.join(dset,  '*')) 
            dset_value = osp.basename(dset)
            for strategy in strategy_dir: #Random, Glister, GradMatch
                model_dir = glob.glob(osp.join(strategy,  '*')) 
                strategy_value = osp.basename(strategy)
                if strategy_value not in ['Full', 'Random', 'GlobalOrder_fl', 'GlobalOrder_gc', 'GlobalOrder_logdet', 'GlobalOrder_supfl']:
                    for model in model_dir: #ResNet, BERTMLP,....
                        fraction_dir = glob.glob(osp.join(model,  '*')) 
                        model_value = osp.basename(model)
                        for fraction in fraction_dir: #0.1, 0.2, 0.3,....
                            sel_dir = glob.glob(osp.join(fraction, '*')) 
                            fraction_value = osp.basename(fraction)
                            for sel in sel_dir: #1, 5, 10, 20, ...
                                run_dir = glob.glob(osp.join(sel, '*'))
                                sel_value = osp.basename(sel)
                                df_name = "_".join([setting_value, dset_value, model_value, fraction_value, sel_value])
                                
                                if df_name not in dfs_dict.keys():
                                    dfs_dict[df_name] = pd.DataFrame(columns=column_names)
                                    full_df_name = "_".join([setting_value, dset_value, model_value])  
                                    if full_df_name in full_dfs_dict.keys():
                                        dfs_dict[df_name] = pd.concat([dfs_dict[df_name], full_dfs_dict[full_df_name]])
                                    for nonadap_strategy in nonadaptive_dfs_dict.keys():
                                        nonadap_df_name = "_".join([setting_value, dset_value, model_value, fraction_value])  
                                        if nonadap_df_name in nonadaptive_dfs_dict[nonadap_strategy].keys():
                                            dfs_dict[df_name] = pd.concat([dfs_dict[df_name], nonadaptive_dfs_dict[nonadap_strategy][nonadap_df_name]])

                                for run in run_dir: #0, 1, 2, 3, ...
                                    run_value = osp.basename(run)
                                    log_dir = glob.glob(osp.join(run, '*.log'))
                                    for log_file in log_dir: #.log files
                                        tmp_df_dict = {'Setting': setting_value, 'Dataset': dset_value, 'Strategy': strategy_value, 
                                                    'Model': model_value, 'Fraction': fraction_value, 'Select every': sel_value, 'Run': run_value}
                                            
                                        tmp_df = sllogtodf(log_file, print_args, tmp_df_dict)
                                        if tmp_df is not None:        
                                            dfs_dict[df_name] = pd.concat([dfs_dict[df_name], tmp_df])
                                        else:
                                            incomplete_runs.append('_'.join([setting_value, dset_value, strategy_value, model_value, fraction_value, sel_value, run_value]))
                                    
    aggregated_columns = [COLUMN_NAMES_DICT[print_arg] for print_arg in print_args]
    aggregated_columns.remove("Epoch")
    aggregated_dfs_dict = {}
    for df_name in dfs_dict.keys():
        aggregated_df = dfs_dict[df_name].groupby(['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every', 'Epoch'])
        mean = aggregated_df.mean()[aggregated_columns]
        std = aggregated_df.std()[aggregated_columns]
        std = std.fillna(0)
        merged_df = pd.merge(mean, std, on=['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every', 'Epoch'], 
        suffixes=('_mean', '_std'), how="inner")
        merged_columns = list(merged_df.columns)
        merged_columns.append("Epoch")
        merged_df = merged_df.reset_index()
        merged_df = merged_df.groupby(['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every']).agg({merged_column : lambda x: list(x) for merged_column in merged_columns})
        merged_df = merged_df.reset_index()
        aggregated_dfs_dict[df_name] = merged_df
    
    return dfs_dict, aggregated_dfs_dict


def sllogstoxl(results_dir, print_args=["epoch", "trn_loss", "trn_acc", "val_loss", "val_acc", "tst_loss", "tst_acc", "time"], out_file='sl_output.xlsx'):
    """
    Convert Supervised Learning Logs to Excel File
    """
    dirname = osp.abspath(osp.expanduser(results_dir))
    check_dir_exist(dirname)

    out_file_path = osp.abspath(osp.expanduser(out_file))
    mean_file = osp.basename(out_file).split(".")[0] +  "_aggregated." + osp.basename(out_file).split(".")[1]
    mean_file_path = osp.abspath(osp.expanduser(mean_file))
        
    """
    ############################## Main Code ##############################
    """
    with pd.ExcelWriter(out_file_path, engine='xlsxwriter', mode='w') as writer: 
        sub_dir = glob.glob(osp.join(dirname, 'SL'))
        column_names = ['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every', 'Run']
        for print_arg in print_args:
            assert print_arg in COLUMN_NAMES_DICT, "Please add column name corresponding to the print argument in COLUMN_NAMES_DICT."
            column_names.append(COLUMN_NAMES_DICT[print_arg])
        """
        Current Folder Structure:
        all_logs_dir = os.path.join(results_dir, 
                                    self.cfg.setting,
                                    self.cfg.dataset.name,
                                    subset_selection_name,
                                    self.cfg.model.architecture,
                                    str(self.cfg.dss_args.fraction),
                                    str(self.cfg.dss_args.select_every),
                                    str(self.cfg.train_args.run))
        """   
        incomplete_runs = []
        dfs_dict = {}
        full_dfs_dict = {}
        nonadaptive_dfs_dict = {'Random':{},
                'GlobalOrder_fl':{},
                'GlobalOrder_gc':{},
                'GlobalOrder_logdet':{},
                'GlobalOrder_supfl':{}}

        #DFs for Non-Adaptive Strategies
        for setting in sub_dir: #SL, SSL...
            dset_dir = glob.glob(osp.join(setting, '*')) 
            setting_value = osp.basename(setting)
            #dsets = [osp.basename(dset) for dset in dset_dir]
            for dset in dset_dir: #CIFAR10, SST2,....
                strategy_dir = glob.glob(osp.join(dset,  '*')) 
                dset_value = osp.basename(dset)
                if dset_value == 'rotten_tomatoes':
                    dset_value = 'rt'
                for strategy in strategy_dir: #Random, Glister, GradMatch
                    model_dir = glob.glob(osp.join(strategy,  '*')) 
                    strategy_value = osp.basename(strategy)
                    if strategy_value in ['Full', 'Random', 'GlobalOrder_fl', 'GlobalOrder_gc', 'GlobalOrder_logdet', 'GlobalOrder_supfl']:
                        for model in model_dir: #ResNet, BERTMLP,....
                            #Full doesn't depend on fraction
                            model_value = osp.basename(model)
                            if strategy_value == 'Full':
                                run_dir = glob.glob(osp.join(model,  '1', '1', '*'))
                                df_name = "_".join([setting_value, dset_value, model_value])  
                                if df_name not in full_dfs_dict.keys():
                                    full_dfs_dict[df_name] = pd.DataFrame(columns=column_names)
                                for run in run_dir: #0, 1, 2, 3, ...
                                    run_value = osp.basename(run)
                                    log_dir = glob.glob(osp.join(run, '*.log'))
                                    for log_file in log_dir: #.log files
                                        tmp_df_dict = {'Setting': setting_value, 'Dataset': dset_value, 'Strategy': strategy_value, 
                                                     'Model': model_value, 'Fraction': '1', 'Select every': '1', 'Run': run_value}
                                              
                                        tmp_df = sllogtodf(log_file, print_args, tmp_df_dict)
                                        if tmp_df is not None:        
                                            full_dfs_dict[df_name] = pd.concat([full_dfs_dict[df_name], tmp_df])  
                                        else:
                                            incomplete_runs.append('_'.join([setting_value, dset_value, strategy_value, model_value, '1', '1', run_value]))
                            else:
                                fraction_dir = glob.glob(osp.join(model,  '*')) 
                                for fraction in fraction_dir: #0.1, 0.2, 0.3,....
                                    run_dir = glob.glob(osp.join(fraction, '1', '*')) 
                                    fraction_value = osp.basename(fraction)
                                    df_name = "_".join([setting_value, dset_value, model_value, fraction_value])
                                    if df_name not in nonadaptive_dfs_dict[strategy_value].keys():
                                        nonadaptive_dfs_dict[strategy_value][df_name] = pd.DataFrame(columns=column_names)
                                    for run in run_dir: #0, 1, 2, 3, ...
                                        run_value = osp.basename(run)
                                        log_dir = glob.glob(osp.join(run, '*.log'))
                                        for log_file in log_dir: #.log files
                                            tmp_df_dict = {'Setting': setting_value, 'Dataset': dset_value, 'Strategy': strategy_value, 
                                                     'Model': model_value, 'Fraction': fraction_value, 'Select every': '1', 'Run': run_value}
                                              
                                            tmp_df = sllogtodf(log_file, print_args, tmp_df_dict)
                                            if tmp_df is not None:        
                                                nonadaptive_dfs_dict[strategy_value][df_name] = pd.concat([nonadaptive_dfs_dict[strategy_value][df_name], tmp_df])  
                                            else:
                                                incomplete_runs.append('_'.join([setting_value, dset_value, strategy_value, model_value, fraction_value, '1', run_value]))
        
        #DFs for Adaptive Strategies
        for setting in sub_dir: #SL, SSL...
            dset_dir = glob.glob(osp.join(setting, '*')) 
            setting_value = osp.basename(setting)
            #dsets = [osp.basename(dset) for dset in dset_dir]
            for dset in dset_dir: #CIFAR10, SST2,....
                strategy_dir = glob.glob(osp.join(dset,  '*')) 
                dset_value = osp.basename(dset)
                if dset_value == 'rotten_tomatoes':
                    dset_value = 'rt'
                for strategy in strategy_dir: #Random, Glister, GradMatch
                    model_dir = glob.glob(osp.join(strategy,  '*')) 
                    strategy_value = osp.basename(strategy)
                    if strategy_value not in ['Full', 'Random', 'GlobalOrder_fl', 'GlobalOrder_gc', 'GlobalOrder_logdet', 'GlobalOrder_supfl']:
                        for model in model_dir: #ResNet, BERTMLP,....
                            fraction_dir = glob.glob(osp.join(model,  '*')) 
                            model_value = osp.basename(model)
                            for fraction in fraction_dir: #0.1, 0.2, 0.3,....
                                sel_dir = glob.glob(osp.join(fraction, '*')) 
                                fraction_value = osp.basename(fraction)
                                for sel in sel_dir: #1, 5, 10, 20, ...
                                    run_dir = glob.glob(osp.join(sel, '*'))
                                    sel_value = osp.basename(sel)
                                    df_name = "_".join([setting_value, dset_value, model_value, fraction_value, sel_value])
                                    
                                    if df_name not in dfs_dict.keys():
                                        dfs_dict[df_name] = pd.DataFrame(columns=column_names)
                                        full_df_name = "_".join([setting_value, dset_value, model_value])  
                                        if full_df_name in full_dfs_dict.keys():
                                            dfs_dict[df_name] = pd.concat([dfs_dict[df_name], full_dfs_dict[full_df_name]])
                                        for nonadap_strategy in nonadaptive_dfs_dict.keys():
                                            nonadap_df_name = "_".join([setting_value, dset_value, model_value, fraction_value])  
                                            if nonadap_df_name in nonadaptive_dfs_dict[nonadap_strategy].keys():
                                                dfs_dict[df_name] = pd.concat([dfs_dict[df_name], nonadaptive_dfs_dict[nonadap_strategy][nonadap_df_name]])

                                    for run in run_dir: #0, 1, 2, 3, ...
                                        run_value = osp.basename(run)
                                        log_dir = glob.glob(osp.join(run, '*.log'))
                                        for log_file in log_dir: #.log files
                                            tmp_df_dict = {'Setting': setting_value, 'Dataset': dset_value, 'Strategy': strategy_value, 
                                                     'Model': model_value, 'Fraction': fraction_value, 'Select every': sel_value, 'Run': run_value}
                                              
                                            tmp_df = sllogtodf(log_file, print_args, tmp_df_dict)
                                            if tmp_df is not None:        
                                                dfs_dict[df_name] = pd.concat([dfs_dict[df_name], tmp_df])
                                            else:
                                                incomplete_runs.append('_'.join([setting_value, dset_value, strategy_value, model_value, fraction_value, sel_value, run_value]))

        for df_name in dfs_dict.keys():
            dfs_dict[df_name].to_excel(writer, sheet_name=df_name)
        #writer.save()

    with pd.ExcelWriter(mean_file_path, engine='xlsxwriter', mode='w') as writer: 
        aggregated_columns = [COLUMN_NAMES_DICT[print_arg] for print_arg in print_args]
        aggregated_columns.remove("Epoch")
        aggregated_dfs_dict = {}
        for df_name in dfs_dict.keys():
            aggregated_df = dfs_dict[df_name].groupby(['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every', 'Epoch'])
            mean = aggregated_df.mean()[aggregated_columns]
            std = aggregated_df.std()[aggregated_columns]
            std = std.fillna(0)
            merged_df = pd.merge(mean, std, on=['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every', 'Epoch'], 
            suffixes=('_mean', '_std'), how="inner")
            merged_columns = list(merged_df.columns)
            merged_columns.append("Epoch")
            merged_df = merged_df.reset_index()
            merged_df = merged_df.groupby(['Setting', 'Dataset', 'Strategy', 'Model', 'Fraction', 'Select every']).agg({merged_column : lambda x: list(x) for merged_column in merged_columns})
            merged_df = merged_df.reset_index()
            aggregated_dfs_dict[df_name] = merged_df
        
        for df_name in aggregated_dfs_dict.keys():
            aggregated_dfs_dict[df_name].to_excel(writer, sheet_name=df_name)
        #writer.save()
File Path: docs/source/conf.py
Content:
# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
# import os
# import sys
# sys.path.insert(0, os.path.abspath('.'))


# -- Project information -----------------------------------------------------

project = 'cords'
copyright = '2021, Krishnateja Killamsetty, Dheeraj Bhat, Rishabh Iyer'
author = 'Krishnateja Killamsetty, Dheeraj Bhat, Rishabh Iyer'

# The full version, including alpha/beta/rc tags
release = 'v1.0'
master_doc = 'index'

import sys
import os
import subprocess

#subprocess.call('pip install numpydoc', shell=True)
#subprocess.call('pip install torch', shell=True)
#subprocess.call('pip install torchvision', shell=True)
#subprocess.call('pip install scikit-learn', shell=True)
#subprocess.call('pip install torchvision', shell=True)
#subprocess.call('pip install apricot-select', shell=True)
#subprocess.call('pip install numba', shell=True)
#subprocess.call('pip install numpy', shell=True)
#subprocess.call('pip install scipy', shell=True)
#subprocess.call('pip install tqdm', shell=True)
#subprocess.call('pip install sphinxcontrib-napoleon', shell=True)
#subprocess.call('pip install sphinxcontrib-bibtex', shell=True)
#subprocess.call('pip install sphinx-rtd-theme', shell=True)

sys.path.insert(0, os.path.abspath('../..'))

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.autosummary',
    'sphinx.ext.doctest',
    'sphinx.ext.todo',
    'sphinx.ext.coverage',
    'sphinx.ext.mathjax',
    'sphinx.ext.viewcode',
    #'numpydoc'
    'sphinxcontrib.napoleon',
    'sphinxcontrib.bibtex'
]

bibtex_bibfiles = ['refs.bib']
autosummary_generate = True
numpydoc_show_class_members = False
class_members_toctree = False

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
# source_suffix = ['.rst', '.md']
source_suffix = '.rst'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.


# The short X.Y version.
# If you really want the short one use the next line.
# version = '.'.join(pomegranate.__version__.split('.')[0:2])
# Use this version if you want the full version string in the document
#version = apricot.__version__
# The full version, including alpha/beta/rc tags.
#release = apricot.__version__


# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']


# -- Options for HTML output -------------------------------------------------

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'sphinx_rtd_theme'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']
html_logo = "cords_logo.png"
html_theme_options = {
    'logo_only': True,
    'display_version': True,
}

File Path: examples/HPO/__init__.py
Content:


File Path: examples/HPO/image_classification/__init__.py
Content:


File Path: examples/HPO/image_classification/python_code/__init__.py
Content:


File Path: examples/HPO/image_classification/python_code/vision_paramtuning.py
Content:
import argparse
from cords.utils.config_utils import load_config_data
from ray.tune.suggest.hyperopt import HyperOptSearch
from ray.tune.suggest.bayesopt import BayesOptSearch
from ray.tune.suggest.skopt import SkOptSearch
from ray.tune.suggest.dragonfly import DragonflySearch
from ray.tune.suggest.ax import AxSearch
from ray.tune.suggest.bohb import TuneBOHB
from ray.tune.suggest.nevergrad import NevergradSearch
from ray.tune.suggest.optuna import OptunaSearch
from ray.tune.suggest.zoopt import ZOOptSearch
from ray.tune.suggest.sigopt import SigOptSearch
from ray.tune.suggest.hebo import HEBOSearch
from ray.tune.schedulers import AsyncHyperBandScheduler
from ray.tune.schedulers import HyperBandScheduler
from ray.tune.schedulers.hb_bohb import HyperBandForBOHB
from ray.tune.suggest import BasicVariantGenerator
from ray import tune
import ray
import sys
from examples.HPO.image_classification.python_code.vision_train_sl import TrainClassifier

# @ray.remote(num_gpus=1)
class HyperParamTuning:
    def __init__(self, config_file_data, train_config_data):
        # self.cfg = load_config_data(config_file)
        self.cfg = config_file_data
        self.train_class = TrainClassifier(train_config_data)
        # self.train_class = TrainClassifier(self.cfg['subset_config'])
        self.train_class.cfg.train_args.print_every = 1
        self.search_algo = self.get_search_algo(self.cfg.search_algo, self.cfg.space, self.cfg.metric, self.cfg.mode)
        self.scheduler = self.get_scheduler(self.cfg.scheduler, self.cfg.metric, self.cfg.mode)
        # save subset method, to be used in log dir name
        self.subset_method = self.train_class.cfg.dss_args.type

    def param_tune(self, config):
        #update parameters in config dict
        new_config = self.update_parameters(self.train_class.cfg, config)
        self.train_class.cfg = new_config
        # turn on reporting to ray every time
        self.train_class.cfg.report_tune = True
        self.train_class.train()

    def start_eval(self):
        if self.search_algo is None:
            analysis = tune.run(
                self.param_tune,
                num_samples=self.cfg.num_evals,
                config=self.cfg.space,
                search_alg=self.search_algo,
                scheduler=self.scheduler,
                resources_per_trial=self.cfg.resources,
                local_dir=self.cfg.log_dir+self.subset_method+'/',
                log_to_file=True,
                name=self.cfg.name,
                resume=self.cfg.resume)
        else:
            analysis = tune.run(
                self.param_tune,
                num_samples=self.cfg.num_evals,
                search_alg=self.search_algo,
                scheduler=self.scheduler,
                resources_per_trial=self.cfg.resources,
                local_dir=self.cfg.log_dir+self.subset_method+'/',
                log_to_file=True,
                name=self.cfg.name,
                resume=self.cfg.resume)
        best_config = analysis.get_best_config(metric=self.cfg.metric, mode=self.cfg.mode)
        print("Best Config: ", best_config)

        if self.cfg.final_train:
            self.final_train(best_config)

    def get_search_algo(self, method, space, metric, mode):
        
        # HyperOptSearch 
        if method == "hyperopt" or method == "TPE":
            search = HyperOptSearch(space, metric = metric, mode = mode)
        # BayesOptSearch
        elif method == "bayesopt" or method == "BO":
            search = BayesOptSearch(space, metric = metric, mode = mode)
        # SkoptSearch
        elif method == "skopt" or method == "SKBO":
            search = SkOptSearch(space, metric = metric, mode = mode)
        # DragonflySearch
        elif method == "dragonfly" or method == "SBO":
            search = DragonflySearch(space, metric = metric, mode = mode)
        # AxSearch
        elif method == "ax" or method == "BBO":
            search = AxSearch(space, metric = metric, mode = mode)
        # TuneBOHB
        elif method == "tunebohb" or method == "BOHB":
            search = TuneBOHB(space, metric = metric, mode = mode)
        # NevergradSearch
        elif method == "nevergrad" or method == "GFO":
            search = NevergradSearch(space, metric = metric, mode = mode)
        # OptunaSearch
        elif method == "optuna" or method == "OSA":
            search = OptunaSearch(space, metric = metric, mode = mode)
        # ZOOptSearch
        elif method == "zoopt" or method == "ZOO":
            search = ZOOptSearch(space, metric = metric, mode = mode)
        # SigOptSearch
        elif method == "sigopt":
            search = SigOptSearch(space, metric = metric, mode = mode)
        # HEBOSearch
        elif method == "hebo" or method == "HEBO":
            search = HEBOSearch(space, metric = metric, mode = mode)
        else:
            return None

        return search

    def get_scheduler(self, method, metric, mode):

        if method == "ASHA" or method == "asha":
            scheduler = AsyncHyperBandScheduler(metric = metric, mode = mode, 
                                                max_t = self.train_class.cfg.train_args.num_epochs)
        elif method == "hyperband" or method == "HB":
            scheduler = HyperBandScheduler(metric = metric, mode = mode, 
                        max_t = self.train_class.cfg.train_args.num_epochs)
        elif method == "BOHB":
            scheduler = HyperBandForBOHB(metric = metric, mode = mode)
        else:
            return None
        
        return scheduler
    
    def final_train(self, best_params):
        # change strategy to Full (i.e use whole dataset)
        # update (optimized) parameters
        new_config = self.update_parameters(self.train_class.cfg, best_params)
        new_config.dss_args.type = 'Full'
        # new_config.dss_args.type = 'GradMatchPB'
        # new_config.dss_args.fraction = 0.3
        # new_config.dss_args.select_every = 20
        # new_config.dss_args.lam = 0
        # new_config.dss_args.selection_type = 'PerBatch'
        # new_config.dss_args.v1 = True
        # new_config.dss_args.valid = False
        # new_config.dss_args.eps = 1e-100
        # new_config.dss_args.linear_layer = True
        # new_config.dss_args.kappa = 0
        self.train_class.cfg = new_config
        self.train_class.train()
    
    def update_parameters(self, config, new_config):
        # a generic function to update parameters
        if 'learning_rate' in new_config:
            config.optimizer.lr = new_config['learning_rate']
        if 'learning_rate1' in new_config:
            config.optimizer.lr1 = new_config['learning_rate1']
        if 'learning_rate2' in new_config:
            config.optimizer.lr2 = new_config['learning_rate2']
        if 'learning_rate3' in new_config:
            config.optimizer.lr3 = new_config['learning_rate3']
        if 'optimizer' in new_config:
            config.optimizer.type = new_config['optimizer']
        if 'nesterov' in new_config:
            config.optimizer.nesterov = new_config['nesterov']
        if 'scheduler' in new_config:
            config.scheduler.type = new_config['scheduler']
        # if 'trn_batch_size' in new_config:
        #     config.dataloader.batch_size = new_config['trn_batch_size']
        if 'gamma' in new_config:
            config.scheduler.gamma = new_config['gamma']
        # if 'epochs' in new_config:
        #     config.train_args.num_epochs = new_config['epochs']
        # if 'trn_batch_size' in new_config:
        #     config.dataloader.batch_size = new_config['trn_batch_size']
        # if 'hidden_size' in new_config:
        #     config.model.hidden_size = new_config['hidden_size']
        # if 'num_layers' in new_config:
        #     config.model.num_layers = new_config['num_layers']
        return config
        



if __name__ == "__main__":
    argparser = argparse.ArgumentParser()
    argparser.add_argument("--config_file", default="configs/config_hyper_param_tuning.py")    
    args = argparser.parse_args()

    hyperparam_tuning = HyperParamTuning(load_config_data(args.config_file))
    hyperparam_tuning.start_eval()

File Path: examples/HPO/image_classification/python_code/vision_run_hpo.py
Content:
from examples.HPO.image_classification.python_code.vision_train_sl import TrainClassifier
import argparse
from examples.HPO.image_classification.python_code.vision_paramtuning import HyperParamTuning
from cords.utils.config_utils import load_config_data


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--fraction', type=float, default=0.1, help='fraction in subset selection')
    parser.add_argument('--select_every', type=int, default=20, help='perform subset selection every _ epochs')
    parser.add_argument('--change', type=int, default=1, help='change params mentioned for train class?')
    parser.add_argument('--config_file', type=str, default='/home/kk/cords/configs/SL/config_gradmatchpb-warm_cifar100.py')
    parser.add_argument('--config_hp', type=str, default='/home/kk/cords/configs/SL/config_hyper_param_tuning_cifar100.py')
    parser.add_argument('--scheduler', type=str, default='asha')
    parser.add_argument('--search_algo', type=str, default='TPE')
    parser.add_argument('--num_classes', type=int, default=10)
    parser.add_argument('--kappa', type=float, default=0.35)
    parser.add_argument('--dataset', type=str, default='svhn')
    parser.add_argument('--num_evals', type=int, default=27)
    parser.add_argument('--is_hp', type=int, default=1, help='do we perform hyper parameter tuning?')
    parser.add_argument('--final_train', type=int, default=1, help='need final training hyper parameter tuning?')
    args = parser.parse_args()

    if bool(args.is_hp):
        config_hp_data = load_config_data(args.config_hp)
        config_hp_data.final_train = bool(args.final_train)
        config_hp_data.subset_config = args.config_file
        config_hp_data.scheduler = args.scheduler
        config_hp_data.search_algo = args.search_algo
        config_hp_data.num_evals = args.num_evals
        train_config_data = load_config_data(args.config_file)
        if bool(args.change):
            train_config_data.dss_args.fraction = args.fraction
            train_config_data.dss_args.select_every = args.select_every
            train_config_data.report_tune = True
            train_config_data.model.numclasses = args.num_classes
            train_config_data.dataset.name = args.dataset
            train_config_data.dss_args.kappa = args.kappa
            #train_config_data.train_args.device = 'cuda'
        hyperparamtuning = HyperParamTuning(config_hp_data, train_config_data)
        hyperparamtuning.start_eval()
    else:
        config_file_data = load_config_data(args.config_file)
        if bool(args.change):
            config_file_data.dss_args.fraction = args.fraction
            config_file_data.dss_args.select_every = args.select_every
            config_file_data.model.numclasses = args.num_classes
            config_file_data.dataset.name = args.dataset
            config_file_data.dss_args.kappa = args.kappa
            #config_file_data.train_args.device = 'cuda'
        classifier = TrainClassifier(config_file_data)
        classifier.train()

File Path: examples/HPO/image_classification/python_code/vision_train_sl.py
Content:
import logging
import os
import os.path as osp
import sys
import time
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from ray import tune
from torch.utils.data import Subset
from cords.utils.config_utils import load_config_data
from cords.utils.data.data_utils import WeightedSubset
from cords.utils.data.data_utils import collate
from cords.utils.data.dataloader.SL.adaptive import GLISTERDataLoader, OLRandomDataLoader, \
    CRAIGDataLoader, GradMatchDataLoader, RandomDataLoader
from cords.utils.data.dataloader.SL.nonadaptive import FacLocDataLoader
from cords.utils.data.datasets.SL import gen_dataset
from cords.utils.models import *
from cords.utils.data.data_utils.collate import *
import pickle

class TrainClassifier:
    def __init__(self, config_file_data):
        self.cfg = config_file_data
        results_dir = osp.abspath(osp.expanduser(self.cfg.train_args.results_dir))
        
        if self.cfg.dss_args.type != "Full":
            all_logs_dir = os.path.join(results_dir, self.cfg.setting,
                                        self.cfg.dss_args.type,
                                        self.cfg.dataset.name,
                                        str(self.cfg.dss_args.fraction),
                                        str(self.cfg.dss_args.select_every))
        else:
            all_logs_dir = os.path.join(results_dir, self.cfg.setting,
                                        self.cfg.dss_args.type,
                                        self.cfg.dataset.name)

        os.makedirs(all_logs_dir, exist_ok=True)
        # setup logger
        plain_formatter = logging.Formatter("[%(asctime)s] %(name)s %(levelname)s: %(message)s",
                                            datefmt="%m/%d %H:%M:%S")
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        s_handler = logging.StreamHandler(stream=sys.stdout)
        s_handler.setFormatter(plain_formatter)
        s_handler.setLevel(logging.INFO)
        self.logger.addHandler(s_handler)
        f_handler = logging.FileHandler(os.path.join(all_logs_dir, self.cfg.dataset.name + "_" +
                                                     self.cfg.dss_args.type + ".log"))
        f_handler.setFormatter(plain_formatter)
        f_handler.setLevel(logging.DEBUG)
        self.logger.addHandler(f_handler)
        self.logger.propagate = False

    """
    ############################## Loss Evaluation ##############################
    """

    def model_eval_loss(self, data_loader, model, criterion):
        total_loss = 0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(data_loader):
                inputs, targets = inputs.to(self.cfg.train_args.device), \
                                  targets.to(self.cfg.train_args.device, non_blocking=True)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                total_loss += loss.item()
        return total_loss

    """
    ############################## Model Creation ##############################
    """

    def create_model(self):
        if self.cfg.model.architecture == 'RegressionNet':
            model = RegressionNet(self.cfg.model.input_dim)
        elif self.cfg.model.architecture == 'ResNet18':
            model = ResNet18(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MnistNet':
            model = MnistNet()
        elif self.cfg.model.architecture == 'ResNet164':
            model = ResNet164(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNet':
            model = MobileNet(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNetV2':
            model = MobileNetV2(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNet2':
            model = MobileNet2(output_size=self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'HyperParamNet':
            model = HyperParamNet(self.cfg.model.l1, self.cfg.model.l2)
        elif self.cfg.model.architecture == 'ThreeLayerNet':
            model = ThreeLayerNet(self.cfg.model.input_dim, self.cfg.model.numclasses, 
	    self.cfg.model.h1, self.cfg.model.h2)
        elif self.cfg.model.architecture == 'LSTM':
            model = LSTMClassifier(self.cfg.model.numclasses, self.cfg.model.wordvec_dim, \
                 self.cfg.model.weight_path, self.cfg.model.num_layers, self.cfg.model.hidden_size)
        model = model.to(self.cfg.train_args.device)
        return model

    """
    ############################## Loss Type, Optimizer and Learning Rate Scheduler ##############################
    """

    def loss_function(self):
        if self.cfg.loss.type == "CrossEntropyLoss":
            criterion = nn.CrossEntropyLoss()
            criterion_nored = nn.CrossEntropyLoss(reduction='none')
        elif self.cfg.loss.type == "MeanSquaredLoss":
            criterion = nn.MSELoss()
            criterion_nored = nn.MSELoss(reduction='none')
        return criterion, criterion_nored

    def optimizer_with_scheduler(self, model):
        if self.cfg.optimizer.type == 'sgd':
            optimizer = optim.SGD( [
                                    {"params": model.linear.parameters(), "lr": self.cfg.optimizer.lr1},
                                    {"params": model.layer4.parameters(), "lr": self.cfg.optimizer.lr2},
                                    {"params": model.layer3.parameters(), "lr": self.cfg.optimizer.lr2},
                                    {"params": model.layer2.parameters(), "lr": self.cfg.optimizer.lr2},
                                    {"params": model.layer1.parameters(), "lr": self.cfg.optimizer.lr2},
                                    {"params": model.conv1.parameters(), "lr": self.cfg.optimizer.lr3},
                                    ],
                                    lr=self.cfg.optimizer.lr,
                                  momentum=self.cfg.optimizer.momentum,
                                  weight_decay=self.cfg.optimizer.weight_decay,
                                  nesterov=self.cfg.optimizer.nesterov)
        elif self.cfg.optimizer.type == "adam":
            optimizer = optim.Adam(model.parameters(), lr=self.cfg.optimizer.lr)
        elif self.cfg.optimizer.type == "rmsprop":
            optimizer = optim.RMSprop(model.parameters(), lr=self.cfg.optimizer.lr)

        if self.cfg.scheduler.type == 'cosine_annealing':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                                                   T_max=self.cfg.scheduler.T_max)
        elif self.cfg.scheduler.type == 'linear_decay':
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 
                                                        step_size=self.cfg.scheduler.stepsize, 
                                                        gamma=self.cfg.scheduler.gamma)
        else:
            scheduler = None
        return optimizer, scheduler

    @staticmethod
    def generate_cumulative_timing(mod_timing):
        tmp = 0
        mod_cum_timing = np.zeros(len(mod_timing))
        for i in range(len(mod_timing)):
            tmp += mod_timing[i]
            mod_cum_timing[i] = tmp
        return mod_cum_timing

    @staticmethod
    def save_ckpt(state, ckpt_path):
        torch.save(state, ckpt_path)

    @staticmethod
    def load_ckpt(ckpt_path, model, optimizer):
        checkpoint = torch.load(ckpt_path)
        start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        loss = checkpoint['loss']
        metrics = checkpoint['metrics']
        return start_epoch, model, optimizer, loss, metrics

    def count_pkl(self, path):
        if not osp.exists(path):
            return -1
        return_val = 0
        file = open(path, 'rb')
        while(True):
            try:
                _ = pickle.load(file)
                return_val += 1
            except EOFError:
                break
        file.close()
        return return_val

    def train(self):
        """
        ############################## General Training Loop with Data Selection Strategies ##############################
        """
        # Loading the Dataset
        logger = self.logger
        logger.info(self.cfg)
        if self.cfg.dataset.feature == 'classimb':
            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                               self.cfg.dataset.name,
                                                               self.cfg.dataset.feature,
                                                               classimb_ratio=self.cfg.dataset.classimb_ratio, dataset=self.cfg.dataset)
        else:
            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                               self.cfg.dataset.name,
                                                               self.cfg.dataset.feature, dataset=self.cfg.dataset)

        trn_batch_size = self.cfg.dataloader.batch_size
        val_batch_size = self.cfg.dataloader.batch_size
        tst_batch_size = self.cfg.dataloader.batch_size

        if self.cfg.dataset.name == "sst2_facloc" and self.count_pkl(self.cfg.dataset.ss_path) == 1 and self.cfg.dss_args.type == 'FacLoc':
            self.cfg.dss_args.type = 'Full'
            file_ss = open(self.cfg.dataset.ss_path, 'rb')
            ss_indices = pickle.load(file_ss)
            file_ss.close()
            trainset = torch.utils.data.Subset(trainset, ss_indices)

        if 'collate_fn' not in self.cfg.dataloader.keys():
            collate_fn = None
        else:
            collate_fn = self.cfg.dataloader.collate_fn

        # Creating the Data Loaders
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size,
                                                  shuffle=False, pin_memory=True, collate_fn = collate_fn)

        valloader = torch.utils.data.DataLoader(validset, batch_size=val_batch_size,
                                                shuffle=False, pin_memory=True, collate_fn = collate_fn)

        testloader = torch.utils.data.DataLoader(testset, batch_size=tst_batch_size,
                                                 shuffle=False, pin_memory=True, collate_fn = collate_fn)

        substrn_losses = list()  # np.zeros(cfg['train_args']['num_epochs'])
        trn_losses = list()
        val_losses = list()  # np.zeros(cfg['train_args']['num_epochs'])
        tst_losses = list()
        subtrn_losses = list()
        timing = list()
        trn_acc = list()
        val_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])
        tst_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])
        subtrn_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])

        # Checkpoint file
        checkpoint_dir = osp.abspath(osp.expanduser(self.cfg.ckpt.dir))
        ckpt_dir = os.path.join(checkpoint_dir, self.cfg.setting,
                                self.cfg.dss_args.type,
                                self.cfg.dataset.name,
                                str(self.cfg.dss_args.fraction),
                                str(self.cfg.dss_args.select_every))
        checkpoint_path = os.path.join(ckpt_dir, 'model.pt')
        os.makedirs(ckpt_dir, exist_ok=True)

        # Model Creation
        model = self.create_model()
        # model1 = self.create_model()

        # Loss Functions
        criterion, criterion_nored = self.loss_function()

        # Getting the optimizer and scheduler
        optimizer, scheduler = self.optimizer_with_scheduler(model)

        """
        ############################## Custom Dataloader Creation ##############################
        """

        if 'collate_fn' not in self.cfg.dss_args:
                self.cfg.dss_args.collate_fn = None

        if self.cfg.dss_args.type in ['GradMatch', 'GradMatchPB', 'GradMatch-Warm', 'GradMatchPB-Warm']:
            """
            ############################## GradMatch Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = GradMatchDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                             batch_size=self.cfg.dataloader.batch_size,
                                             shuffle=self.cfg.dataloader.shuffle,
                                             pin_memory=self.cfg.dataloader.pin_memory,
                                             collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['GLISTER', 'GLISTER-Warm', 'GLISTERPB', 'GLISTERPB-Warm']:
            """
            ############################## GLISTER Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device
            dataloader = GLISTERDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                           batch_size=self.cfg.dataloader.batch_size,
                                           shuffle=self.cfg.dataloader.shuffle,
                                           pin_memory=self.cfg.dataloader.pin_memory)

        elif self.cfg.dss_args.type in ['CRAIG', 'CRAIG-Warm', 'CRAIGPB', 'CRAIGPB-Warm']:
            """
            ############################## CRAIG Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = CRAIGDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                         batch_size=self.cfg.dataloader.batch_size,
                                         shuffle=self.cfg.dataloader.shuffle,
                                         pin_memory=self.cfg.dataloader.pin_memory)

        elif self.cfg.dss_args.type in ['Random', 'Random-Warm']:
            """
            ############################## Random Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = RandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory, 
                                          collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type == ['OLRandom', 'OLRandom-Warm']:
            """
            ############################## OLRandom Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = OLRandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type == 'FacLoc':
            """
            ############################## Facility Location Dataloader Additional Arguments ##############################
            """
            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.model = model
            self.cfg.dss_args.data_type = self.cfg.dataset.type
            
            dataloader = FacLocDataLoader(trainloader, valloader, self.cfg.dss_args, logger, 
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory, 
                                          collate_fn = self.cfg.dss_args.collate_fn)
            if self.cfg.dataset.name == "sst2_facloc" and self.count_pkl(self.cfg.dataset.ss_path) < 1:

                ss_indices = dataloader.subset_indices
                file_ss = open(self.cfg.dataset.ss_path, 'wb')
                try:
                    pickle.dump(ss_indices, file_ss)
                except EOFError:
                    pass
                file_ss.close()

        elif self.cfg.dss_args.type == 'Full':
            """
            ############################## Full Dataloader Additional Arguments ##############################
            """
            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))

            dataloader = torch.utils.data.DataLoader(wt_trainset,
                                                     batch_size=self.cfg.dataloader.batch_size,
                                                     shuffle=self.cfg.dataloader.shuffle,
                                                     pin_memory=self.cfg.dataloader.pin_memory,
                                                     collate_fn=self.cfg.dss_args.collate_fn)

        """
        ################################################# Checkpoint Loading #################################################
        """

        if self.cfg.ckpt.is_load:
            start_epoch, model, optimizer, ckpt_loss, load_metrics = self.load_ckpt(checkpoint_path, model, optimizer)
            logger.info("Loading saved checkpoint model at epoch: {0:d}".format(start_epoch))
            for arg in load_metrics.keys():
                if arg == "val_loss":
                    val_losses = load_metrics['val_loss']
                if arg == "val_acc":
                    val_acc = load_metrics['val_acc']
                if arg == "tst_loss":
                    tst_losses = load_metrics['tst_loss']
                if arg == "tst_acc":
                    tst_acc = load_metrics['tst_acc']
                if arg == "trn_loss":
                    trn_losses = load_metrics['trn_loss']
                if arg == "trn_acc":
                    trn_acc = load_metrics['trn_acc']
                if arg == "subtrn_loss":
                    subtrn_losses = load_metrics['subtrn_loss']
                if arg == "subtrn_acc":
                    subtrn_acc = load_metrics['subtrn_acc']
                if arg == "time":
                    timing = load_metrics['time']
        else:
            start_epoch = 0

        """
        ################################################# Training Loop #################################################
        """

        for epoch in range(start_epoch, self.cfg.train_args.num_epochs):
            subtrn_loss = 0
            subtrn_correct = 0
            subtrn_total = 0
            model.train()
            start_time = time.time()
            cum_weights = 0
            for _, (inputs, targets, weights) in enumerate(dataloader):
                inputs = inputs.to(self.cfg.train_args.device)
                targets = targets.to(self.cfg.train_args.device, non_blocking=True)
                weights = weights.to(self.cfg.train_args.device)
                optimizer.zero_grad()
                outputs = model(inputs)
                losses = criterion_nored(outputs, targets)
                if self.cfg.is_reg:
                    loss = torch.dot(losses.view(-1), weights / (weights.sum()))
                else:
                    loss = torch.dot(losses, weights / (weights.sum()))
                loss.backward()
                subtrn_loss += (loss.item() * weights.sum())
                cum_weights += weights.sum()
                optimizer.step()
                if not self.cfg.is_reg:
                    _, predicted = outputs.max(1)
                    subtrn_total += targets.size(0)
                    subtrn_correct += predicted.eq(targets).sum().item()
            epoch_time = time.time() - start_time
            if cum_weights != 0:
                subtrn_loss = subtrn_loss/cum_weights
            if not scheduler == None:
                scheduler.step()
            timing.append(epoch_time)
            print_args = self.cfg.train_args.print_args

            """
            ################################################# Evaluation Loop #################################################
            """

            if ((epoch + 1) % self.cfg.train_args.print_every == 0) or (epoch == self.cfg.train_args.num_epochs - 1):
                trn_loss = 0
                trn_correct = 0
                trn_total = 0
                val_loss = 0
                val_correct = 0
                val_total = 0
                tst_correct = 0
                tst_total = 0
                tst_loss = 0
                model.eval()

                if ("trn_loss" in print_args) or ("trn_acc" in print_args):
                    samples =0
                    with torch.no_grad():
                        for _, (inputs, targets) in enumerate(trainloader):
                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            trn_loss += (loss.item() * trainloader.batch_size)
                            samples += targets.shape[0]
                            if "trn_acc" in print_args:
                                _, predicted = outputs.max(1)
                                trn_total += targets.size(0)
                                trn_correct += predicted.eq(targets).sum().item()
                        trn_loss = trn_loss/samples
                        trn_losses.append(trn_loss)

                    if "trn_acc" in print_args:
                        trn_acc.append(trn_correct / trn_total)

                if ("val_loss" in print_args) or ("val_acc" in print_args):
                    samples =0
                    with torch.no_grad():
                        for _, (inputs, targets) in enumerate(valloader):
                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            val_loss += (loss.item() * valloader.batch_size)
                            samples += targets.shape[0]
                            if "val_acc" in print_args:
                                _, predicted = outputs.max(1)
                                val_total += targets.size(0)
                                val_correct += predicted.eq(targets).sum().item()
                        val_loss = val_loss/samples
                        val_losses.append(val_loss)

                    if "val_acc" in print_args:
                        val_acc.append(val_correct / val_total)

                if ("tst_loss" in print_args) or ("tst_acc" in print_args):
                    samples =0
                    with torch.no_grad():
                        for _, (inputs, targets) in enumerate(testloader):
                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            tst_loss += (loss.item() * testloader.batch_size)
                            samples += targets.shape[0]
                            if "tst_acc" in print_args:
                                _, predicted = outputs.max(1)
                                tst_total += targets.size(0)
                                tst_correct += predicted.eq(targets).sum().item()
                        tst_loss = tst_loss/samples
                        tst_losses.append(tst_loss)

                    if "tst_acc" in print_args:
                        tst_acc.append(tst_correct / tst_total)

                if "subtrn_acc" in print_args:
                    subtrn_acc.append(subtrn_correct / subtrn_total)

                if "subtrn_losses" in print_args:
                    subtrn_losses.append(subtrn_loss)

                print_str = "Epoch: " + str(epoch + 1)

                """
                ################################################# Results Printing #################################################
                """

                for arg in print_args:

                    if arg == "val_loss":
                        print_str += " , " + "Validation Loss: " + str(val_losses[-1])

                    if arg == "val_acc":
                        print_str += " , " + "Validation Accuracy: " + str(val_acc[-1])

                    if arg == "tst_loss":
                        print_str += " , " + "Test Loss: " + str(tst_losses[-1])

                    if arg == "tst_acc":
                        print_str += " , " + "Test Accuracy: " + str(tst_acc[-1])

                    if arg == "trn_loss":
                        print_str += " , " + "Training Loss: " + str(trn_losses[-1])

                    if arg == "trn_acc":
                        print_str += " , " + "Training Accuracy: " + str(trn_acc[-1])

                    if arg == "subtrn_loss":
                        print_str += " , " + "Subset Loss: " + str(subtrn_losses[-1])

                    if arg == "subtrn_acc":
                        print_str += " , " + "Subset Accuracy: " + str(subtrn_acc[-1])

                    if arg == "time":
                        print_str += " , " + "Timing: " + str(timing[-1])

                # report metric to ray for hyperparameter optimization
                if 'report_tune' in self.cfg and self.cfg.report_tune  and len(dataloader):
                    tune.report(mean_accuracy=val_acc[-1])

                logger.info(print_str)

            """
            ################################################# Checkpoint Saving #################################################
            """

            if ((epoch + 1) % self.cfg.ckpt.save_every == 0) and self.cfg.ckpt.is_save:

                metric_dict = {}

                for arg in print_args:
                    if arg == "val_loss":
                        metric_dict['val_loss'] = val_losses
                    if arg == "val_acc":
                        metric_dict['val_acc'] = val_acc
                    if arg == "tst_loss":
                        metric_dict['tst_loss'] = tst_losses
                    if arg == "tst_acc":
                        metric_dict['tst_acc'] = tst_acc
                    if arg == "trn_loss":
                        metric_dict['trn_loss'] = trn_losses
                    if arg == "trn_acc":
                        metric_dict['trn_acc'] = trn_acc
                    if arg == "subtrn_loss":
                        metric_dict['subtrn_loss'] = subtrn_losses
                    if arg == "subtrn_acc":
                        metric_dict['subtrn_acc'] = subtrn_acc
                    if arg == "time":
                        metric_dict['time'] = timing

                ckpt_state = {
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'loss': self.loss_function(),
                    'metrics': metric_dict
                }

                # save checkpoint
                self.save_ckpt(ckpt_state, checkpoint_path)
                logger.info("Model checkpoint saved at epoch: {0:d}".format(epoch + 1))

        """
        ################################################# Results Summary #################################################
        """

        logger.info(self.cfg.dss_args.type + " Selection Run---------------------------------")
        logger.info("Final SubsetTrn: {0:f}".format(subtrn_loss))
        if "val_loss" in print_args:
            if "val_acc" in print_args:
                logger.info("Validation Loss: %.2f , Validation Accuracy: %.2f", val_loss, val_acc[-1])
            else:
                logger.info("Validation Loss: %.2f", val_loss)

        if "tst_loss" in print_args:
            if "tst_acc" in print_args:
                logger.info("Test Loss: %.2f, Test Accuracy: %.2f", tst_loss, tst_acc[-1])
            else:
                logger.info("Test Data Loss: %f", tst_loss)
        logger.info('---------------------------------------------------------------------')
        logger.info(self.cfg.dss_args.type)
        logger.info('---------------------------------------------------------------------')

        """
        ################################################# Final Results Logging #################################################
        """

        if "val_acc" in print_args:
            val_str = "Validation Accuracy, "
            for val in val_acc:
                val_str = val_str + " , " + str(val)
            logger.info(val_str)

        if "tst_acc" in print_args:
            tst_str = "Test Accuracy, "
            for tst in tst_acc:
                tst_str = tst_str + " , " + str(tst)
            logger.info(tst_str)

        if "time" in print_args:
            time_str = "Time, "
            for t in timing:
                time_str = time_str + " , " + str(t)
            logger.info(timing)

        omp_timing = np.array(timing)
        omp_cum_timing = list(self.generate_cumulative_timing(omp_timing))
        logger.info("Total time taken by %s = %.4f ", self.cfg.dss_args.type, omp_cum_timing[-1])

File Path: examples/HPO/text_classification/python_files/text_paramtuning.py
Content:
import argparse
from cords.utils.config_utils import load_config_data
from ray.tune.suggest.hyperopt import HyperOptSearch
from ray.tune.suggest.bayesopt import BayesOptSearch
from ray.tune.suggest.skopt import SkOptSearch
from ray.tune.suggest.dragonfly import DragonflySearch
from ray.tune.suggest.ax import AxSearch
from ray.tune.suggest.bohb import TuneBOHB
from ray.tune.suggest.nevergrad import NevergradSearch
from ray.tune.suggest.optuna import OptunaSearch
from ray.tune.suggest.zoopt import ZOOptSearch
from ray.tune.suggest.sigopt import SigOptSearch
from ray.tune.suggest.hebo import HEBOSearch
from ray.tune.schedulers import AsyncHyperBandScheduler
from ray.tune.schedulers import HyperBandScheduler
from ray.tune.schedulers.hb_bohb import HyperBandForBOHB
from ray.tune.suggest import BasicVariantGenerator
from ray import tune
import ray
import sys
from examples.HPO.text_classification.python_files.text_train_sl import TrainClassifier

# @ray.remote(num_gpus=1)
class HyperParamTuning:
    def __init__(self, config_file_data, train_config_data):
        # self.cfg = load_config_data(config_file)
        self.cfg = config_file_data
        self.train_class = TrainClassifier(train_config_data)
        # self.train_class = TrainClassifier(self.cfg['subset_config'])
        self.train_class.cfg.train_args.print_every = 1
        self.search_algo = self.get_search_algo(self.cfg.search_algo, self.cfg.space, self.cfg.metric, self.cfg.mode)
        self.scheduler = self.get_scheduler(self.cfg.scheduler, self.cfg.metric, self.cfg.mode)
        # save subset method, to be used in log dir name
        self.subset_method = self.train_class.cfg.dss_args.type

    def param_tune(self, config):
        #update parameters in config dict
        new_config = self.update_parameters(self.train_class.cfg, config)
        self.train_class.cfg = new_config
        # turn on reporting to ray every time
        self.train_class.cfg.report_tune = True
        self.train_class.train()

    def start_eval(self):
        if self.search_algo is None:
            analysis = tune.run(
                self.param_tune,
                num_samples=self.cfg.num_evals,
                config=self.cfg.space,
                search_alg=self.search_algo,
                scheduler=self.scheduler,
                resources_per_trial=self.cfg.resources,
                local_dir=self.cfg.log_dir+self.subset_method+'/',
                log_to_file=True,
                name=self.cfg.name,
                resume=self.cfg.resume)
        else:
            analysis = tune.run(
                self.param_tune,
                num_samples=self.cfg.num_evals,
                search_alg=self.search_algo,
                scheduler=self.scheduler,
                resources_per_trial=self.cfg.resources,
                local_dir=self.cfg.log_dir+self.subset_method+'/',
                log_to_file=True,
                name=self.cfg.name,
                resume=self.cfg.resume)
        best_config = analysis.get_best_config(metric=self.cfg.metric, mode=self.cfg.mode)
        print("Best Config: ", best_config)

        if self.cfg.final_train:
            self.final_train(best_config)

    def get_search_algo(self, method, space, metric, mode):
        
        # HyperOptSearch 
        if method == "hyperopt" or method == "TPE":
            search = HyperOptSearch(space, metric = metric, mode = mode)
        # BayesOptSearch
        elif method == "bayesopt" or method == "BO":
            search = BayesOptSearch(space, metric = metric, mode = mode)
        # SkoptSearch
        elif method == "skopt" or method == "SKBO":
            search = SkOptSearch(space, metric = metric, mode = mode)
        # DragonflySearch
        elif method == "dragonfly" or method == "SBO":
            search = DragonflySearch(space, metric = metric, mode = mode)
        # AxSearch
        elif method == "ax" or method == "BBO":
            search = AxSearch(space, metric = metric, mode = mode)
        # TuneBOHB
        elif method == "tunebohb" or method == "BOHB":
            search = TuneBOHB(space, metric = metric, mode = mode)
        # NevergradSearch
        elif method == "nevergrad" or method == "GFO":
            search = NevergradSearch(space, metric = metric, mode = mode)
        # OptunaSearch
        elif method == "optuna" or method == "OSA":
            search = OptunaSearch(space, metric = metric, mode = mode)
        # ZOOptSearch
        elif method == "zoopt" or method == "ZOO":
            search = ZOOptSearch(space, metric = metric, mode = mode)
        # SigOptSearch
        elif method == "sigopt":
            search = SigOptSearch(space, metric = metric, mode = mode)
        # HEBOSearch
        elif method == "hebo" or method == "HEBO":
            search = HEBOSearch(space, metric = metric, mode = mode)
        else:
            return None

        return search

    def get_scheduler(self, method, metric, mode):

        if method == "ASHA":
            scheduler = AsyncHyperBandScheduler(metric = metric, mode = mode, 
                                                max_t = self.train_class.cfg.train_args.num_epochs)
        elif method == "hyperband" or method == "HB":
            scheduler = HyperBandScheduler(metric = metric, mode = mode, 
                        max_t = self.train_class.cfg.train_args.num_epochs)
        elif method == "BOHB":
            scheduler = HyperBandForBOHB(metric = metric, mode = mode)
        else:
            return None
        
        return scheduler
    
    def final_train(self, best_params):
        # change strategy to Full (i.e use whole dataset)
        # update (optimized) parameters
        new_config = self.update_parameters(self.train_class.cfg, best_params)

        if self.cfg.final_train_type == 'full':
            new_config.dss_args.type = 'Full'
        elif self.cfg.final_train_type == 'gmpb':
            new_config.dss_args.type = 'GradMatchPB'
            new_config.dss_args.fraction = 0.3
            new_config.dss_args.select_every = 5
            new_config.dss_args.lam = 0
            new_config.dss_args.selection_type = 'PerBatch'
            new_config.dss_args.v1 = True
            new_config.dss_args.valid = False
            new_config.dss_args.eps = 1e-100
            new_config.dss_args.linear_layer = True
            new_config.dss_args.kappa = 0
        else:
            print('Unknow final_train_type in Hyperparameter tuning class. Exiting...')
            exit(1)

        self.train_class.cfg = new_config
        self.train_class.train()
    
    def update_parameters(self, config, new_config):
        # a generic function to update parameters
        if 'learning_rate' in new_config:
            config.optimizer.lr = new_config['learning_rate']
        if 'learning_rate1' in new_config:
            config.optimizer.lr1 = new_config['learning_rate1']
        if 'learning_rate2' in new_config:
            config.optimizer.lr2 = new_config['learning_rate2']
        if 'learning_rate3' in new_config:
            config.optimizer.lr3 = new_config['learning_rate3']
        if 'optimizer' in new_config:
            config.optimizer.type = new_config['optimizer']
        if 'nesterov' in new_config:
            config.optimizer.nesterov = new_config['nesterov']
        if 'scheduler' in new_config:
            config.scheduler.type = new_config['scheduler']
        # if 'trn_batch_size' in new_config:
        #     config.dataloader.batch_size = new_config['trn_batch_size']
        if 'gamma' in new_config:
            config.scheduler.gamma = new_config['gamma']
        if 'epochs' in new_config:
            config.train_args.num_epochs = new_config['epochs']
        if 'trn_batch_size' in new_config:
            config.dataloader.batch_size = new_config['trn_batch_size']
        if 'hidden_size' in new_config:
            config.model.hidden_size = new_config['hidden_size']
        if 'num_layers' in new_config:
            config.model.num_layers = new_config['num_layers']
        return config
        



if __name__ == "__main__":
    argparser = argparse.ArgumentParser()
    argparser.add_argument("--config_file", default="configs/config_hyper_param_tuning.py")    
    args = argparser.parse_args()

    hyperparam_tuning = HyperParamTuning(load_config_data(args.config_file))
    hyperparam_tuning.start_eval()

File Path: examples/HPO/text_classification/python_files/text_run_hpo.py
Content:
from examples.HPO.text_classification.python_files.text_train_sl import TrainClassifier
import argparse
from examples.HPO.text_classification.python_files.text_paramtuning import HyperParamTuning
from cords.utils.config_utils import load_config_data
import torch


if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--lr', type=float, default=0.001, help='learning rate')
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--hidden_size', type=int, default=150)
    parser.add_argument('--num_layers', type=int, default=2)

    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--num_classes', type=int, default=2)
    parser.add_argument('--print_every', type=int, default=1)
    parser.add_argument('--wordvec_dim', type=int, default=300, help='Dimension of GloVe vectors')

    parser.add_argument('--fraction', type=float, default=0.1, help='fraction in subset selection')
    parser.add_argument('--ft_type', type=str, default='full', help='final_train type after hp tuning. full/gmpb')
    parser.add_argument('--select_every', type=int, default=5, help='perform subset selection every _ epochs')
    parser.add_argument('--change', type=int, default=1, help='change params mentioned for train class?')

    parser.add_argument('--config_file', type=str, default='/home/ayush/Documents/abhishek/cords/configs/SL/config_full_glove_trec6.py')
    parser.add_argument('--config_hp', type=str, default='/home/ayush/Documents/abhishek/cords/configs/SL/config_hp.py')
    parser.add_argument('--is_hp', type=int, default=1, help='do we perform hyper parameter tuning?')
    parser.add_argument('--final_train', type=int, default=1, help='need final training hyper parameter tuning?')

    parser.add_argument('--ss_gen', type=int, default=0, help='gen facloc ss')

    args = parser.parse_args()
    weight_path = '/home/ayush/Documents/abhishek/data/glove.6B/'

    if bool(args.is_hp):
        config_hp_data = load_config_data(args.config_hp)
        config_hp_data.final_train = bool(args.final_train)
        config_hp_data.final_train_type = args.ft_type
        config_hp_data.subset_config = args.config_file
        
        train_config_data = load_config_data(args.config_file)
        if bool(args.change):
            # train_config_data.optimizer.lr = args.lr
            # train_config_data.dataloader.batchsize = args.batch_size
            # train_config_data.model.hidden_size = args.hidden_size
            # train_config_data.model.num_layers = args.num_layers

            # train_config_data.train_args.num_epochs = args.epochs
            # train_config_data.train_args.print_every = args.print_every
            # train_config_data.dataset.wordvec_dim = args.wordvec_dim
            # train_config_data.dataset.weight_path = weight_path
            # train_config_data.model.wordvec_dim = args.wordvec_dim
            # train_config_data.model.weight_path = weight_path

            train_config_data.dss_args.fraction = args.fraction
            train_config_data.dss_args.select_every = args.select_every
            train_config_data.train_args.device = 'cuda'

        if train_config_data.dss_args.type == 'AdapFacLoc' or 'FacLoc':
            if bool(args.ss_gen):
                train_class_ = TrainClassifier(train_config_data)
                train_class_.train(end_before_training=True)
                del train_class_
                torch.cuda.empty_cache()
            else:
                hyperparamtuning = HyperParamTuning(config_hp_data, train_config_data)
                hyperparamtuning.start_eval()
        else: 
            hyperparamtuning = HyperParamTuning(config_hp_data, train_config_data)
            hyperparamtuning.start_eval()
    else:
        config_file_data = load_config_data(args.config_file)
        if bool(args.change):
            # config_file_data.optimizer.lr = args.lr
            # config_file_data.dataloader.batchsize = args.batch_size
            # config_file_data.model.hidden_size = args.hidden_size
            # config_file_data.model.num_layers = args.num_layers

            config_file_data.train_args.num_epochs = args.epochs
            # config_file_data.train_args.print_every = args.print_every
            # config_file_data.dataset.wordvec_dim = args.wordvec_dim
            # config_file_data.dataset.weight_path = weight_path
            # config_file_data.model.wordvec_dim = args.wordvec_dim
            # config_file_data.model.weight_path = weight_path

            # config_file_data.dss_args.fraction = args.fraction
            # config_file_data.dss_args.select_every = args.select_every
            # config_file_data.train_args.device = 'cuda'

        classifier = TrainClassifier(config_file_data)
        classifier.train()

File Path: examples/HPO/text_classification/python_files/text_train_sl.py
Content:
import logging
import os
import os.path as osp
import sys
import time
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from ray import tune
from torch.utils.data import Subset
from cords.utils.config_utils import load_config_data
from cords.utils.data.data_utils import WeightedSubset
from cords.utils.data.dataloader.SL.adaptive import GLISTERDataLoader, OLRandomDataLoader, \
    CRAIGDataLoader, GradMatchDataLoader, RandomDataLoader, AdapWeightsDataLoader
from cords.utils.data.dataloader.SL.nonadaptive import FacLocDataLoader
from cords.utils.data.datasets.SL import gen_dataset
from cords.utils.models import *
from cords.utils.data.data_utils.collate import *
import pickle

class TrainClassifier:
    def __init__(self, config_file_data):
        # self.config_file = config_file
        # self.cfg = load_config_data(self.config_file)
        self.cfg = config_file_data
        results_dir = osp.abspath(osp.expanduser(self.cfg.train_args.results_dir))
        all_logs_dir = os.path.join(results_dir, self.cfg.setting,
                                    self.cfg.dss_args.type,
                                    self.cfg.dataset.name,
                                    str(self.cfg.dss_args.fraction),
                                    str(self.cfg.dss_args.select_every))

        os.makedirs(all_logs_dir, exist_ok=True)
        # setup logger
        plain_formatter = logging.Formatter("[%(asctime)s] %(name)s %(levelname)s: %(message)s",
                                            datefmt="%m/%d %H:%M:%S")
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        s_handler = logging.StreamHandler(stream=sys.stdout)
        s_handler.setFormatter(plain_formatter)
        s_handler.setLevel(logging.INFO)
        self.logger.addHandler(s_handler)
        f_handler = logging.FileHandler(os.path.join(all_logs_dir, self.cfg.dataset.name + "_" +
                                                     self.cfg.dss_args.type + ".log"))
        f_handler.setFormatter(plain_formatter)
        f_handler.setLevel(logging.DEBUG)
        self.logger.addHandler(f_handler)
        self.logger.propagate = False

    """
    ############################## Loss Evaluation ##############################
    """

    def model_eval_loss(self, data_loader, model, criterion):
        total_loss = 0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(data_loader):
                inputs, targets = inputs.to(self.cfg.train_args.device), \
                                  targets.to(self.cfg.train_args.device, non_blocking=True)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                total_loss += loss.item()
        return total_loss

    """
    ############################## Model Creation ##############################
    """

    def create_model(self):
        if self.cfg.model.architecture == 'ResNet18':
            model = ResNet18(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MnistNet':
            model = MnistNet()
        elif self.cfg.model.architecture == 'ResNet164':
            model = ResNet164(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNet':
            model = MobileNet(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNetV2':
            model = MobileNetV2(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNet2':
            model = MobileNet2(output_size=self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'HyperParamNet':
            model = HyperParamNet(self.cfg.model.l1, self.cfg.model.l2)
        elif self.cfg.model.architecture == 'ThreeLayerNet':
            model = ThreeLayerNet(self.cfg.model.input_dim, self.cfg.model.num_classes, self.cfg.model.h1, self.cfg.model.h2)
        elif self.cfg.model.architecture == 'LSTM':
            model = LSTMClassifier(self.cfg.model.numclasses, self.cfg.model.wordvec_dim, \
                 self.cfg.model.weight_path, self.cfg.model.num_layers, self.cfg.model.hidden_size)
        model = model.to(self.cfg.train_args.device)
        return model

    """
    ############################## Loss Type, Optimizer and Learning Rate Scheduler ##############################
    """

    def loss_function(self):
        if self.cfg.loss.type == "CrossEntropyLoss":
            criterion = nn.CrossEntropyLoss()
            criterion_nored = nn.CrossEntropyLoss(reduction='none')
        elif self.cfg.loss.type == "MeanSquaredLoss":
            criterion = nn.MSELoss()
            criterion_nored = nn.MSELoss(reduction='none')
        return criterion, criterion_nored

    def optimizer_with_scheduler(self, model):
        if self.cfg.optimizer.type == 'sgd':
            
            optimizer = optim.SGD(model.parameters(), lr=self.cfg.optimizer.lr,
                                  momentum=self.cfg.optimizer.momentum,
                                  weight_decay=self.cfg.optimizer.weight_decay,
                                  nesterov=self.cfg.optimizer.nesterov)
        elif self.cfg.optimizer.type == "adam":
            optimizer = optim.Adam(model.parameters(), lr=self.cfg.optimizer.lr)
        elif self.cfg.optimizer.type == "rmsprop":
            optimizer = optim.RMSprop(model.parameters(), lr=self.cfg.optimizer.lr)

        if self.cfg.scheduler.type == 'cosine_annealing':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                                                   T_max=self.cfg.scheduler.T_max)
        else:
            scheduler = None
        return optimizer, scheduler

    @staticmethod
    def generate_cumulative_timing(mod_timing):
        tmp = 0
        mod_cum_timing = np.zeros(len(mod_timing))
        for i in range(len(mod_timing)):
            tmp += mod_timing[i]
            mod_cum_timing[i] = tmp
        return mod_cum_timing

    @staticmethod
    def save_ckpt(state, ckpt_path):
        torch.save(state, ckpt_path)

    @staticmethod
    def load_ckpt(ckpt_path, model, optimizer):
        checkpoint = torch.load(ckpt_path)
        start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        loss = checkpoint['loss']
        metrics = checkpoint['metrics']
        return start_epoch, model, optimizer, loss, metrics

    def count_pkl(self, path):
        if not osp.exists(path):
            return -1
        return_val = 0
        file = open(path, 'rb')
        while(True):
            try:
                _ = pickle.load(file)
                return_val += 1
            except EOFError:
                break
        file.close()
        return return_val

    def train(self, end_before_training = False):
        """
        ############################## General Training Loop with Data Selection Strategies ##############################
        """
        # Loading the Dataset
        logger = self.logger
        logger.info(self.cfg)
        if self.cfg.dataset.feature == 'classimb':
            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                               self.cfg.dataset.name,
                                                               self.cfg.dataset.feature,
                                                               classimb_ratio=self.cfg.dataset.classimb_ratio, dataset=self.cfg.dataset)
        else:
            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                               self.cfg.dataset.name,
                                                               self.cfg.dataset.feature, dataset=self.cfg.dataset)

        trn_batch_size = self.cfg.dataloader.batch_size
        val_batch_size = self.cfg.dataloader.batch_size
        tst_batch_size = self.cfg.dataloader.batch_size

        if 'ss_path' in self.cfg.dataset and self.cfg.dataset.use_ss_if_exists and self.count_pkl(self.cfg.dataset.ss_path) == 1:
        #for performing facloc only once during HP tuning
        #Better do facloc once(using end_before_training) then start anything other(HP tuning) using the
        #found indices to avoid multiple parallel facloc computations
            file_ss = open(self.cfg.dataset.ss_path, 'rb')
            ss_indices = pickle.load(file_ss)
            file_ss.close()
            trainset = torch.utils.data.Subset(trainset, ss_indices)

        # Creating the Data Loaders
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size,
                                                  shuffle=False, pin_memory=True, 
                                                  collate_fn = self.cfg.dataloader.collate_fn)

        valloader = torch.utils.data.DataLoader(validset, batch_size=val_batch_size,
                                                shuffle=False, pin_memory=True, 
                                                collate_fn = self.cfg.dataloader.collate_fn)

        testloader = torch.utils.data.DataLoader(testset, batch_size=tst_batch_size,
                                                 shuffle=False, pin_memory=True, 
                                                 collate_fn = self.cfg.dataloader.collate_fn)

        substrn_losses = list()  # np.zeros(configdata['train_args']['num_epochs'])
        trn_losses = list()
        val_losses = list()  # np.zeros(configdata['train_args']['num_epochs'])
        tst_losses = list()
        subtrn_losses = list()
        timing = list()
        trn_acc = list()
        val_acc = list()  # np.zeros(configdata['train_args']['num_epochs'])
        tst_acc = list()  # np.zeros(configdata['train_args']['num_epochs'])
        subtrn_acc = list()  # np.zeros(configdata['train_args']['num_epochs'])

        # Checkpoint file
        checkpoint_dir = osp.abspath(osp.expanduser(self.cfg.ckpt.dir))
        ckpt_dir = os.path.join(checkpoint_dir, self.cfg.setting,
                                self.cfg.dss_args.type,
                                self.cfg.dataset.name,
                                str(self.cfg.dss_args.fraction),
                                str(self.cfg.dss_args.select_every))
        checkpoint_path = os.path.join(ckpt_dir, 'model.pt')
        os.makedirs(ckpt_dir, exist_ok=True)

        # Model Creation
        model = self.create_model()
        # model1 = self.create_model()

        # Loss Functions
        criterion, criterion_nored = self.loss_function()

        # Getting the optimizer and scheduler
        optimizer, scheduler = self.optimizer_with_scheduler(model)

        """
        ############################## Custom Dataloader Creation ##############################
        """

        if not 'collate_fn' in self.cfg.dss_args:
                self.cfg.dss_args.collate_fn = None

        if self.cfg.dss_args.type in ['GradMatch', 'GradMatchPB', 'GradMatch-Warm', 'GradMatchPB-Warm']:
            """
            ############################## GradMatch Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = GradMatchDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                             batch_size=self.cfg.dataloader.batch_size,
                                             shuffle=self.cfg.dataloader.shuffle,
                                             pin_memory=self.cfg.dataloader.pin_memory,
                                             collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['GLISTER', 'GLISTER-Warm', 'GLISTERPB', 'GLISTERPB-Warm']:
            """
            ############################## GLISTER Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device
            dataloader = GLISTERDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                           batch_size=self.cfg.dataloader.batch_size,
                                           shuffle=self.cfg.dataloader.shuffle,
                                           pin_memory=self.cfg.dataloader.pin_memory)

        elif self.cfg.dss_args.type in ['CRAIG', 'CRAIG-Warm', 'CRAIGPB', 'CRAIGPB-Warm']:
            """
            ############################## CRAIG Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = CRAIGDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                         batch_size=self.cfg.dataloader.batch_size,
                                         shuffle=self.cfg.dataloader.shuffle,
                                         pin_memory=self.cfg.dataloader.pin_memory)

        elif self.cfg.dss_args.type in ['Random', 'Random-Warm']:
            """
            ############################## Random Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = RandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory, 
                                          collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type == ['OLRandom', 'OLRandom-Warm']:
            """
            ############################## OLRandom Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = OLRandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type == 'FacLoc':
            """
            ############################## Facility Location Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.model = model
            self.cfg.dss_args.data_type = self.cfg.dataset.type
            
            dataloader = FacLocDataLoader(trainloader, valloader, self.cfg.dss_args, logger, 
                                        batch_size=self.cfg.dataloader.batch_size,
                                        shuffle=self.cfg.dataloader.shuffle,
                                        pin_memory=self.cfg.dataloader.pin_memory, 
                                        collate_fn = self.cfg.dss_args.collate_fn)

            if 'ss_path' in self.cfg.dataset and self.count_pkl(self.cfg.dataset.ss_path) < 1:
                #save subset indices if a ss_path is provided. Useful in HP tuning to avoid multiple facloc computations.
                #to avoid multiple parallel facloc computations, do facloc once(using end_before_training) then start HP tuning
                ss_indices = dataloader.subset_indices
                file_ss = open(self.cfg.dataset.ss_path, 'wb')
                try:
                    pickle.dump(ss_indices, file_ss)
                except EOFError:
                    pass
                file_ss.close()
        elif self.cfg.dss_args.type == 'AdapFacLoc':
            """
            ############################## Adaptive Facility Location Dataloader Additional Arguments ##############################
            """
            num_contents = self.count_pkl(self.cfg.dataset.ss_path)
            if num_contents < 1:
                self.cfg.dss_args.device = self.cfg.train_args.device
                self.cfg.dss_args.model = model
                self.cfg.dss_args.data_type = self.cfg.dataset.type
                
                facloc_time = time.time()
                dataloader = FacLocDataLoader(trainloader, valloader, self.cfg.dss_args, logger, 
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory, 
                                            collate_fn = self.cfg.dss_args.collate_fn)
                ss_indices = list(dataloader.subset_indices)
                facloc_time = time.time() - facloc_time
                print("Type of ss_indices:", type(ss_indices))
                file_ss = open(self.cfg.dataset.ss_path, 'wb')
                try:
                    pickle.dump(ss_indices, file_ss)
                except EOFError:
                    pass
                file_ss.close()
                print("AdapFacLoc takes facloc time of:", facloc_time)
            elif num_contents == 1:
                print("We are in adapfacloc atleast once!")
                file_ss = open(self.cfg.dataset.ss_path, 'rb')
                ss_indices = pickle.load(file_ss)
                file_ss.close()

                self.cfg.dss_args.model = model
                self.cfg.dss_args.loss = criterion_nored
                self.cfg.dss_args.eta = self.cfg.optimizer.lr
                self.cfg.dss_args.num_classes = self.cfg.model.numclasses
                self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
                self.cfg.dss_args.device = self.cfg.train_args.device
                
                dataloader = AdapWeightsDataLoader(trainloader, valloader, self.cfg.dss_args, logger, ss_indices, 
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory, 
                                            collate_fn = self.cfg.dss_args.collate_fn)
        elif self.cfg.dss_args.type == 'Full':
            """
            ############################## Full Dataloader Additional Arguments ##############################
            """
            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))

            dataloader = torch.utils.data.DataLoader(wt_trainset,
                                                     batch_size=self.cfg.dataloader.batch_size,
                                                     shuffle=self.cfg.dataloader.shuffle,
                                                     pin_memory=self.cfg.dataloader.pin_memory,
                                                     collate_fn=self.cfg.dss_args.collate_fn)

        """
        ################################################# Checkpoint Loading #################################################
        """

        if self.cfg.ckpt.is_load:
            start_epoch, model, optimizer, ckpt_loss, load_metrics = self.load_ckpt(checkpoint_path, model, optimizer)
            logger.info("Loading saved checkpoint model at epoch: {0:d}".format(start_epoch))
            for arg in load_metrics.keys():
                if arg == "val_loss":
                    val_losses = load_metrics['val_loss']
                if arg == "val_acc":
                    val_acc = load_metrics['val_acc']
                if arg == "tst_loss":
                    tst_losses = load_metrics['tst_loss']
                if arg == "tst_acc":
                    tst_acc = load_metrics['tst_acc']
                if arg == "trn_loss":
                    trn_losses = load_metrics['trn_loss']
                if arg == "trn_acc":
                    trn_acc = load_metrics['trn_acc']
                if arg == "subtrn_loss":
                    subtrn_losses = load_metrics['subtrn_loss']
                if arg == "subtrn_acc":
                    subtrn_acc = load_metrics['subtrn_acc']
                if arg == "time":
                    timing = load_metrics['time']
        else:
            start_epoch = 0

        """
        ################################################# Training Loop #################################################
        """

        if end_before_training:
            torch.cuda.empty_cache()
            return

        for epoch in range(start_epoch, self.cfg.train_args.num_epochs):
            subtrn_loss = 0
            subtrn_correct = 0
            subtrn_total = 0
            model.train()
            start_time = time.time()
            for _, (inputs, targets, weights) in enumerate(dataloader):
                inputs = inputs.to(self.cfg.train_args.device)
                targets = targets.to(self.cfg.train_args.device, non_blocking=True)
                weights = weights.to(self.cfg.train_args.device)
                optimizer.zero_grad()
                outputs = model(inputs)
                losses = criterion_nored(outputs, targets)
                loss = torch.dot(losses, weights / (weights.sum()))
                # loss = torch.dot(losses, weights / len(inputs))
                loss.backward()
                subtrn_loss += loss.item()
                optimizer.step()
                if not self.cfg.is_reg:
                    _, predicted = outputs.max(1)
                    subtrn_total += targets.size(0)
                    subtrn_correct += predicted.eq(targets).sum().item()
            epoch_time = time.time() - start_time
            if not scheduler == None:
                scheduler.step()
            timing.append(epoch_time)
            print_args = self.cfg.train_args.print_args

            """
            ################################################# Evaluation Loop #################################################
            """

            if ((epoch + 1) % self.cfg.train_args.print_every == 0) or (epoch == self.cfg.train_args.num_epochs - 1):
                trn_loss = 0
                trn_correct = 0
                trn_total = 0
                val_loss = 0
                val_correct = 0
                val_total = 0
                tst_correct = 0
                tst_total = 0
                tst_loss = 0
                model.eval()

                if ("trn_loss" in print_args) or ("trn_acc" in print_args):
                    with torch.no_grad():
                        for _, (inputs, targets) in enumerate(trainloader):
                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            trn_loss += loss.item()
                            if "trn_acc" in print_args:
                                _, predicted = outputs.max(1)
                                trn_total += targets.size(0)
                                trn_correct += predicted.eq(targets).sum().item()
                        trn_losses.append(trn_loss)

                    if "trn_acc" in print_args:
                        trn_acc.append(trn_correct / trn_total)

                if ("val_loss" in print_args) or ("val_acc" in print_args):
                    with torch.no_grad():
                        for _, (inputs, targets) in enumerate(valloader):
                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            val_loss += loss.item()
                            if "val_acc" in print_args:
                                _, predicted = outputs.max(1)
                                val_total += targets.size(0)
                                val_correct += predicted.eq(targets).sum().item()
                        val_losses.append(val_loss)

                    if "val_acc" in print_args:
                        val_acc.append(val_correct / val_total)

                if ("tst_loss" in print_args) or ("tst_acc" in print_args):
                    with torch.no_grad():
                        for _, (inputs, targets) in enumerate(testloader):
                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            tst_loss += loss.item()
                            if "tst_acc" in print_args:
                                _, predicted = outputs.max(1)
                                tst_total += targets.size(0)
                                tst_correct += predicted.eq(targets).sum().item()
                        tst_losses.append(tst_loss)

                    if "tst_acc" in print_args:
                        tst_acc.append(tst_correct / tst_total)

                if "subtrn_acc" in print_args:
                    subtrn_acc.append(subtrn_correct / subtrn_total)

                if "subtrn_losses" in print_args:
                    subtrn_losses.append(subtrn_loss)

                print_str = "Epoch: " + str(epoch + 1)

                """
                ################################################# Results Printing #################################################
                """

                for arg in print_args:

                    if arg == "val_loss":
                        print_str += " , " + "Validation Loss: " + str(val_losses[-1])

                    if arg == "val_acc":
                        print_str += " , " + "Validation Accuracy: " + str(val_acc[-1])

                    if arg == "tst_loss":
                        print_str += " , " + "Test Loss: " + str(tst_losses[-1])

                    if arg == "tst_acc":
                        print_str += " , " + "Test Accuracy: " + str(tst_acc[-1])

                    if arg == "trn_loss":
                        print_str += " , " + "Training Loss: " + str(trn_losses[-1])

                    if arg == "trn_acc":
                        print_str += " , " + "Training Accuracy: " + str(trn_acc[-1])

                    if arg == "subtrn_loss":
                        print_str += " , " + "Subset Loss: " + str(subtrn_losses[-1])

                    if arg == "subtrn_acc":
                        print_str += " , " + "Subset Accuracy: " + str(subtrn_acc[-1])

                    if arg == "time":
                        print_str += " , " + "Timing: " + str(timing[-1])

                # report metric to ray for hyperparameter optimization
                if 'report_tune' in self.cfg and self.cfg.report_tune and len(dataloader):
                    tune.report(mean_accuracy=val_acc[-1])

                logger.info(print_str)

            """
            ################################################# Checkpoint Saving #################################################
            """

            if ((epoch + 1) % self.cfg.ckpt.save_every == 0) and self.cfg.ckpt.is_save:

                metric_dict = {}

                for arg in print_args:
                    if arg == "val_loss":
                        metric_dict['val_loss'] = val_losses
                    if arg == "val_acc":
                        metric_dict['val_acc'] = val_acc
                    if arg == "tst_loss":
                        metric_dict['tst_loss'] = tst_losses
                    if arg == "tst_acc":
                        metric_dict['tst_acc'] = tst_acc
                    if arg == "trn_loss":
                        metric_dict['trn_loss'] = trn_losses
                    if arg == "trn_acc":
                        metric_dict['trn_acc'] = trn_acc
                    if arg == "subtrn_loss":
                        metric_dict['subtrn_loss'] = subtrn_losses
                    if arg == "subtrn_acc":
                        metric_dict['subtrn_acc'] = subtrn_acc
                    if arg == "time":
                        metric_dict['time'] = timing

                ckpt_state = {
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'loss': self.loss_function(),
                    'metrics': metric_dict
                }

                # save checkpoint
                self.save_ckpt(ckpt_state, checkpoint_path)
                logger.info("Model checkpoint saved at epoch: {0:d}".format(epoch + 1))

        """
        ################################################# Results Summary #################################################
        """

        logger.info(self.cfg.dss_args.type + " Selection Run---------------------------------")
        logger.info("Final SubsetTrn: {0:f}".format(subtrn_loss))
        if "val_loss" in print_args:
            if "val_acc" in print_args:
                logger.info("Validation Loss: %.2f , Validation Accuracy: %.2f", val_loss, val_acc[-1])
            else:
                logger.info("Validation Loss: %.2f", val_loss)

        if "tst_loss" in print_args:
            if "tst_acc" in print_args:
                logger.info("Test Loss: %.2f, Test Accuracy: %.2f", tst_loss, tst_acc[-1])
            else:
                logger.info("Test Data Loss: %f", tst_loss)
        logger.info('---------------------------------------------------------------------')
        logger.info(self.cfg.dss_args.type)
        logger.info('---------------------------------------------------------------------')

        """
        ################################################# Final Results Logging #################################################
        """

        if "val_acc" in print_args:
            val_str = "Validation Accuracy, "
            for val in val_acc:
                val_str = val_str + " , " + str(val)
            logger.info(val_str)

        if "tst_acc" in print_args:
            tst_str = "Test Accuracy, "
            for tst in tst_acc:
                tst_str = tst_str + " , " + str(tst)
            logger.info(tst_str)

        if "time" in print_args:
            time_str = "Time, "
            for t in timing:
                time_str = time_str + " , " + str(t)
            logger.info(timing)

        omp_timing = np.array(timing)
        omp_cum_timing = list(self.generate_cumulative_timing(omp_timing))
        logger.info("Total time taken by %s = %.4f ", self.cfg.dss_args.type, omp_cum_timing[-1])

File Path: examples/SL/__init__.py
Content:


File Path: examples/SL/image_classification/__init__.py
Content:


File Path: examples/SL/image_classification/python_files/run_cifar10_glister.py
Content:
"""
This run file shows how to use the default SL train loop provided in CORDS and 
run it with default arguments using the configuration files provided in CORDS.
"""


from train_sl import TrainClassifier
from cords.utils.config_utils import load_config_data

#CORDS comes with some predefined configuration files that mentiones the format of 
config_file = "configs/SL/config_glister-warm_cifar10.py"
#config_file = "configs/SL/config_glister_boston.py"
#config_file = "configs/SL/config_full_boston.py"

config_data = load_config_data(config_file)
classifier = TrainClassifier(config_data)

classifier.cfg.dss_args.fraction = 0.1
classifier.cfg.dss_args.select_every = 5
classifier.cfg.train_args.device = 'cuda'
classifier.cfg.train_args.print_every = 1
classifier.cfg.is_reg = True


#classifier.configdata.dss_args.type = "Random"
#classifier.configdata.dss_args.type = "Full"
#classifier.configdata.dataset.name = "LawSchool" #"abalone"
classifier.train()

File Path: examples/SL/text_classification/python_files/__init__.py
Content:


File Path: examples/SL/text_classification/python_files/cords_sst2_lstm_glove.py
Content:
from cords.utils.data.datasets.SL.builder import SSTDataset, loadGloveModel
import torch
import torch.nn as nn
import torch.optim
from torch.utils.data import Dataset, DataLoader
from cords.utils.models import LSTMClassifier
import argparse
import time
from cords.utils.data.dataloader.SL.adaptive import GLISTERDataLoader, OLRandomDataLoader, \
    CRAIGDataLoader, GradMatchDataLoader, RandomDataLoader

import logging
from dotmap import DotMap
import torchtext

'''
source: https://github.com/VictoriousRaptor/sst-clf-torch
'''

def collate_fn_weighted(data):
    """Pad data in a batch.
    Parameters
    ----------
    data : list((tensor, int), )
        data and label in a batch
    Returns
    -------
    tuple(tensor, tensor)
    """
    # data: [(tensor, label), ...]
    max_len = max([i[0].shape[0] for i in data])
    labels = torch.tensor([i[1] for i in data], dtype=torch.long)
    weights = torch.tensor([i[2] for i in data], dtype=torch.float)
    padded = torch.zeros((len(data), max_len), dtype=torch.long)
    # randomizing might be better
    for i, _ in enumerate(padded):
        padded[i][:data[i][0].shape[0]] = data[i][0]
    return padded, labels, weights

def collate_fn(data):
    """Pad data in a batch.
    Parameters
    ----------
    data : list((tensor, int), )
        data and label in a batch
    Returns
    -------
    tuple(tensor, tensor)
    """
    max_len = max([i[0].shape[0] for i in data])
    labels = torch.tensor([i[1] for i in data], dtype=torch.long)
    padded = torch.zeros((len(data), max_len), dtype=torch.long)
    # randomizing might be better
    for i, _ in enumerate(padded):
        padded[i][:data[i][0].shape[0]] = data[i][0]
    return padded, labels


def evaluation(data_iter, model, args):
    # Evaluating the given model
    model.eval()
    with torch.no_grad():
        corrects = 0
        avg_loss = 0
        for data, label in data_iter:
            sentences = data.to(args.device, non_blocking=True)
            labels = label.to(args.device, non_blocking=True)
            logit = model(sentences)
            corrects += (torch.max(logit, 1)[1].view(labels.size()).data == labels.data).sum().item()
        size = len(data_iter.dataset)
        model.train()
        return 100.0 * corrects / size


def main():
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    parser = argparse.ArgumentParser()
    parser.add_argument('--lr', type=float, default=0.001, help='learning rate')
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--epoch', type=int, default=10)
    parser.add_argument('--label_num', type=int, default=2, help='Target label numbers')
    parser.add_argument('--log_interval', type=int, default=50)
    parser.add_argument('--wordvec_dim', type=int, default=300, help='Dimension of GloVe vectors')
    parser.add_argument('--model_name', type=str, default='lstm', help='Which model to use')
    parser.add_argument('--ss', type=int, default=0, help='which ss to use. 0 = no ss, 1 = gradmatch, 2 = gradmatchpb')
    parser.add_argument('--fraction', type=float, default=0.1, help='fraction in subset selection')
    parser.add_argument('--select_every', type=int, default=3, help='perform subset selection every _ epochs')
    parser.add_argument('--dataset_path', type=str, default=r'/home/ayush/Documents/abhishek/data/SST/', help='PATH to dataset')

    args = parser.parse_args()
    args.device = device
    args.ss = int(args.ss)
    start = time.time()
    wordvec = loadGloveModel(r'/home/ayush/Documents/abhishek/glove.6B/glove.6B.' + str(args.wordvec_dim) + 'd.txt')
    args.weight = torch.tensor(wordvec.values, dtype=torch.float)  # word embedding for the embedding layer

    # Datasets
    training_set = SSTDataset(args.dataset_path, 'train', args.label_num, args.wordvec_dim, wordvec)
    testing_set = SSTDataset(args.dataset_path, 'test', args.label_num, args.wordvec_dim, wordvec)
    validation_set = SSTDataset(args.dataset_path, 'dev', args.label_num, args.wordvec_dim, wordvec)

    training_iter = DataLoader(dataset=training_set,
                               batch_size=args.batch_size,
                               num_workers=0, shuffle=True, collate_fn=collate_fn, pin_memory=True)
    testing_iter = DataLoader(dataset=testing_set,
                              batch_size=args.batch_size,
                              num_workers=0, collate_fn=collate_fn, pin_memory=True)
    validation_iter = DataLoader(dataset=validation_set,
                                 batch_size=args.batch_size,
                                 num_workers=0, collate_fn=collate_fn, pin_memory=True)
            
    # training_iter = torchtext.data.BucketIterator(training_set, args.batch_size, device = args.device, 
    #                             sort_key=lambda x: x[0].shape[0],
    #                             repeat=True, 
    #                             sort=False, 
    #                             shuffle=True,
    #                             sort_within_batch=True)
    # testing_iter = torchtext.data.BucketIterator(testing_set, args.batch_size, device = args.device, 
    #                             sort_key=lambda x: x[0].shape[0],
    #                             repeat=True, 
    #                             sort=False, 
    #                             shuffle=True,
    #                             sort_within_batch=True)
    # validation_iter = torchtext.data.BucketIterator(validation_set, args.batch_size, device = args.device, 
    #                             sort_key=lambda x: x[0].shape[0],
    #                             repeat=True, 
    #                             sort=False, 
    #                             shuffle=True,
    #                             sort_within_batch=True)


    print('Time for loading glove',args.wordvec_dim,'and creating torch dataloaders:', time.time() - start)

    model_name = args.model_name.lower()
    print("Model:", model_name)

    # Select model
    if model_name == 'lstm':
        model = LSTMClassifier(args).to(device)

    del wordvec  # Save some memory

    criterion = nn.CrossEntropyLoss()
    criterion_nored = nn.CrossEntropyLoss(reduction='none')
    # optimizer = optim.SGD(model.parameters(), lr=args.lr)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    if args.ss == 1:
        dss_args=dict(type="GradMatch",
                            fraction=args.fraction,
                            select_every=args.select_every,
                            lam=0.5,
                            selection_type='PerClassPerGradient',
                            v1=True,
                            valid=False,
                            kappa=0,
                            eps=1e-100,
                            linear_layer=True,
                            model=model,
                            loss=criterion_nored,
                            eta = args.lr,
                            num_classes = args.label_num,
                            device = args.device
                            )
    elif args.ss == 2:
        dss_args=dict(type="GradMatchPB",
                            fraction=args.fraction,
                            select_every=args.select_every,
                            lam=0,
                            selection_type='PerBatch',
                            v1=True,
                            valid=False,
                            eps=1e-100,
                            linear_layer=True,
                            kappa=0,
                            model=model,
                            loss=criterion_nored,
                            eta = args.lr,
                            num_classes = args.label_num,
                            device = args.device
                            )
    if args.ss > 0:
        logger = logging.getLogger(__name__)
        dss_args = DotMap(dss_args)
        dataloader = GradMatchDataLoader(training_iter, validation_iter, dss_args, logger,
                                                batch_size=args.batch_size,
                                                shuffle=True,
                                                pin_memory=True,
                                                collate_fn=collate_fn_weighted)

    step = 0
    loss_sum = 0
    test_acc = []
    best_acc = 0
    best_epoch = 0

    timing = []
    # timing_process = []
    if args.ss > 0:
        for epoch in range(1, args.epoch+1):
            subtrn_loss = 0
            subtrn_correct = 0.0
            subtrn_total = 0.0
            model.train()
            start_time = time.time()
            # start_time_process = time.process_time()
            for _, (inputs, targets, weights) in enumerate(dataloader):
                inputs = inputs.to(args.device)
                targets = targets.to(args.device, non_blocking=True)
                weights = weights.to(args.device)
                optimizer.zero_grad()
                outputs = model(inputs)
                losses = criterion_nored(outputs, targets)
                loss = torch.dot(losses, weights / (weights.sum()))
                loss.backward()
                subtrn_loss += loss.item()

                loss_sum += subtrn_loss
                if step % args.log_interval == 0:
                    print("epoch", epoch, end='  ')
                    print("avg loss: %.5f" % (loss_sum / args.log_interval))
                    loss_sum = 0
                    step = 0
                step += 1

                optimizer.step()
                _, predicted = outputs.max(1)
                subtrn_total += targets.size(0)
                subtrn_correct += predicted.eq(targets).sum().item()
            
            epoch_time = time.time() - start_time
            timing.append(epoch_time)
            # timing_process.append(time.process_time()-start_time_process)
            
            acc = evaluation(testing_iter, model, args)
            if acc > best_acc:
                best_acc = acc
                best_epoch = epoch
            test_acc.append(acc)
            print('test acc {:.4f}'.format(acc))
            print('train acc {:.4f}, {:.4f}'.format(evaluation(training_iter, model, args), subtrn_correct/subtrn_total))
    elif args.ss == 0:
        for epoch in range(1, args.epoch + 1):
            model.train()
            start_time = time.time()
            # start_time_process = time.process_time()
            for data, label in training_iter:
                sentences = data.to(device, non_blocking=True)  # Asynchronous loading
                labels = label.to(device, non_blocking=True)

                optimizer.zero_grad()
                logits = model(sentences)
                loss = criterion(logits, labels)
                loss_sum += loss.data

                if step % args.log_interval == 0:
                    print("epoch", epoch, end='  ')
                    print("avg loss: %.5f" % (loss_sum / args.log_interval))
                    loss_sum = 0
                    step = 0
                step += 1

                loss.backward()
                optimizer.step()

            epoch_time = time.time() - start_time
            timing.append(epoch_time)
            # timing_process.append(time.process_time()-start_time_process)

            acc = evaluation(testing_iter, model, args)
            if acc > best_acc:
                best_acc = acc
                best_epoch = epoch
            test_acc.append(acc)
            print('test acc {:.4f}'.format(acc))
            print('train acc {:.4f}'.format(evaluation(training_iter, model, args)))

    print('')
    print('best: epoch {}, test acc {:.4f}'.format(best_epoch, best_acc))
    timing = [round(i, 5) for i in timing]
    print('total wall clock time(s):', sum(timing), 'avg wall clock time per epoch(s):',sum(timing)/len(timing))
    print('wall clock time for eah epoch:', timing)
    # print('total process time:', sum(timing_process), 'avg process time:',sum(timing_process)/len(timing_process))
    # print('process time for eah epoch:', timing_process)

    print("Parameters:")
    delattr(args, 'weight')
    for attr, value in sorted(args.__dict__.items()):
        print("\t{}={}".format(attr.upper(), value))
    if args.ss > 0:
        print("dss_args:")
        delattr(dss_args, 'model')
        delattr(dss_args, 'loss')
        for attr, value in sorted(dss_args.__dict__.items()):
            print("\t{}={}".format(attr.upper(), value))


if __name__ == "__main__":
    main()

File Path: examples/SSL/__init__.py
Content:


File Path: examples/SSL/image_classification/__init__.py
Content:


File Path: examples/SSL/image_classification/python_files/__init__.py
Content:


File Path: examples/SSL/image_classification/python_files/run_ssl.py
Content:
from train_ssl import TrainClassifier
from cords.utils.config_utils import load_config_data

def main():
    config_file = "configs/SSL/config_gradmatch_vat_cifar10.py"
    config_data = load_config_data(config_file)
    classifier = TrainClassifier(config_data)
    classifier.cfg.dss_args.fraction = 0.1
    classifier.cfg.dss_args.select_every = 1
    classifier.cfg.train_args.device = 'cuda'
    classifier.cfg.train_args.print_every = 1
    classifier.train()


if __name__ == "__main__":
    main()    
File Path: examples/__init__.py
Content:


File Path: gradio_hpo.py
Content:
import gradio as gr
from train_sl import TrainClassifier
from cords.utils.config_utils import load_config_data
from cords.utils.data.datasets.SL import gen_dataset
import os, sys
from dotmap import DotMap
import pandas as pd
from matplotlib import pyplot as plt
from fastapi import FastAPI

# CUSTOM_PATH = "/gradio_hpo"
# app = FastAPI()

def return_search_space(dset):
    if dset == 'TREC6':
        return {
                'num_layers': [1],
                'hidden_size': [64, 128],
                'learning_rate': [0.001, 0.01, 0.1]
                }

def return_full_results():
    search_space = {
                'num_layers': [1, 2],
                'hidden_size': [64, 128, 256],
                'learning_rate': [0.001, 0.01, 0.1]
                }
    
    search_configs = []
    key1, key2, key3 = list(search_space.keys())[0], list(search_space.keys())[1], list(search_space.keys())[2]
    for i in range(len(search_space[key1])):
        for j in range(len(search_space[key2])):
            for k in range(len(search_space[key3])):
                search_configs.append({key1: search_space[key1][i], key2: search_space[key2][j], key3: search_space[key3][k]})

    full_results = [[0.866, 287.729421377182], 
                            [0.874, 287.24425220489502], [0.276, 287.74825239181519], [0.868, 287.75174808502197], [0.858, 287.92841958999634], [0.374, 287.94037413597107], [0.856, 287.2798364162445], [0.874, 287.7269721031189], [0.276, 287.57475852966309], [0.878, 287.33644080162048], [0.864, 287.51423025131226], [0.278, 287.54987382888794], [0.86, 287.80005693435669], [0.888, 287.19879388809204], [0.286, 287.0971245765686], [0.886, 287.03492903709412], [0.87, 287.60258483886719], [0.412, 287.89436173439026]]

    full_results_dict = {}
    for i in range(len(full_results)):
        full_results_dict[tuple(sorted(search_configs[i].items()))] = full_results[i]
    return full_results_dict


def update_model_text(choice):
    if choice == "MNIST":
      return gr.update(choices=['LeNet'], value="LeNet"), return_search_space(choice)
    elif choice == "TREC6":
        return gr.update(choices=['LSTM'], value="LSTM"), return_search_space(choice)
    elif choice == 'CIFAR10':
        return gr.update(choices=['ResNet18'], value="ResNet18"), return_search_space(choice)


def update_out_text(choice):
    # print(choice)
    return gr.update(label='Best Test Accuracy obtained by '+ str(choice)), gr.update(label="Time taken by " + str(choice) + " in seconds")


def hpo(dset, ml_model, strategy, budget):
    metric = 'cossim'
    kw  =  0.01
    search_space = return_search_space(dset)
    dset = dset.lower()

    if dset in ['mnist', 'cifar10']:
        feat_model = 'dino_cls'
    elif dset in ['trec6']:
        feat_model = 'all-distilroberta-v1'
    
    temperature  =  1
    strategy = strategy.lower()
    per_class = True
    if ml_model == 'LeNet':
        ml_model = 'MnistNet'
    else:
        ml_model = ml_model
    run_cnt = 0
    if strategy in ['gradmatchpb', 'craigpb', 'glister']:
        if dset in ['trec6']:
            select_every = 3
        else:
            select_every = 3
    else:
        select_every = 1

    submod_function = 'disp_min_pc'
    data_dir = '../data'
    stochastic_subsets_file = os.path.join(os.path.abspath(data_dir), dset + '_' + feat_model + '_' + metric + '_' + 'gc_pc' + '_' + str(kw) + '_' + str(budget/100) + '_stochastic_subsets.pkl')
    gc_stochastic_subsets_file = os.path.join(os.path.abspath(data_dir), dset + '_' + feat_model + '_' + metric + '_' + 'gc_pc' + '_' + str(kw) + '_' + str(budget/100) + '_stochastic_subsets.pkl')
    global_order_file = os.path.join(os.path.abspath(data_dir), dset + '_' + feat_model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl')
    
    #Pre-trained HPO full results
    full_results_dict = return_full_results()
    
    #Naive code for now! Will have an improved version soon
    search_configs = []
    key1, key2, key3 = list(search_space.keys())[0], list(search_space.keys())[1], list(search_space.keys())[2]
    for i in range(len(search_space[key1])):
        for j in range(len(search_space[key2])):
            for k in range(len(search_space[key3])):
                search_configs.append({key1: search_space[key1][i], key2: search_space[key2][j], key3: search_space[key3][k]})

    trainset, validset, testset, num_cls = gen_dataset('../data/TREC6',
                                                       'hf_trec6',
                                                        'dss', 
                                                        dataset=DotMap(dict(name="hf_trec6",
                                                                    datadir="../data/TREC6/",
                                                                    feature="dss",
                                                                    type="text",
                                                                    wordvec_dim=300,
                                                                    weight_path='../data/glove.6B/',)))
    results_dict = {}
    for strat in [strategy]:
        if dset in ['trec6']:
            config_file = "configs/SL/config_" + strat + "_glove_" + dset + ".py"
        else:
            config_file = "configs/SL/config_" + strat + "_" + dset + ".py"
        config_data = load_config_data(config_file)
        config_data.train_args.device = 'cuda'
        config_data.train_args.run = run_cnt
        config_data.train_args.wandb = False
        if dset in ['trec6']:
            config_data.train_args.num_epochs = 20
            config_data.train_args.print_every = 5
            config_data.dataloader.batch_size = 16
        else:
            config_data.train_args.num_epochs = 10
            config_data.train_args.print_every = 1
            config_data.scheduler.T_max = 10
            config_data.scheduler.type = "cosine_annealing"
            config_data.optimizer.type = 'sgd'
            config_data.optimizer.lr = 5e-2
            config_data.dataloader.batch_size = 128
        config_data.train_args.print_args=["tst_loss", "tst_acc", "time"]
        config_data.dss_args.fraction = (budget/100)    
        config_data.dss_args.global_order_file = global_order_file
        config_data.dss_args.gc_stochastic_subsets_file = gc_stochastic_subsets_file
        config_data.dss_args.stochastic_subsets_file = stochastic_subsets_file
        config_data.dss_args.gc_ratio = 0.1
        config_data.dss_args.kw = kw
        config_data.dss_args.per_class = per_class
        config_data.dss_args.temperature = temperature
        config_data.dss_args.submod_function = submod_function
        config_data.model.architecture = ml_model
        config_data.dss_args.select_every = select_every
        classifier = TrainClassifier(config_data)
        results_dict[strat] = []
        for search_config in search_configs:
            if dset in ['trec6']:
                if "num_layers" in search_config:
                    classifier.cfg.model.num_layes = search_config["num_layers"]

                if "hidden_size" in search_config:
                    classifier.cfg.model.hidden_size = search_config["hidden_size"]

                if "learning_rate" in search_config:
                    classifier.cfg.optimizer.lr = search_config["learning_rate"]    
            trn_acc, val_acc, tst_acc, best_acc, omp_cum_timing = classifier.train(trainset=trainset, validset=validset, testset=testset, num_cls=num_cls)
            results_dict[strat].append([best_acc[-1], omp_cum_timing[-1]])
    
    best_strat_acc = -1
    best_full_acc = -1
    best_strat_idx = -1
    strat_tuning_time = 0
    full_tuning_time = 0

    for i in range(len(search_configs)):
        strat_tuning_time += results_dict[strategy][i][1]
        full_tuning_time += full_results_dict[tuple(sorted(search_configs[i].items()))][1]

        if results_dict[strategy][i][0] > best_strat_acc:
            best_strat_acc = results_dict[strategy][i][0]
            best_strat_config = search_configs[i]
        
        if full_results_dict[tuple(sorted(search_configs[i].items()))][0] > best_full_acc:
            best_full_acc = full_results_dict[tuple(sorted(search_configs[i].items()))][0]
            best_full_config = search_configs[i]

    return best_strat_config, full_results_dict[tuple(sorted(best_strat_config.items()))][0], strat_tuning_time , best_full_config, best_full_acc, full_tuning_time


with gr.Blocks(title = "Hyper-parameter Optimization") as demo:
    with gr.Row():
        with gr.Column():
            dset = gr.Dropdown(choices=['TREC6'], label='Dataset Name')
            model = gr.Radio(["LSTM"], label="Model Architecture")
            strategy = gr.Dropdown(choices=['Random', 'AdaptiveRandom', 'MILO', 'WRE', 'SGE', 'MILOFixed', 'GradMatchPB', 'CraigPB', 'GLISTER'], label='Subset Selection Strategy')
            budget = gr.Slider(minimum=1, maximum=100, label='Budget (in %)')
            search_space = gr.JSON(label='Hyper-parameter Search Space')
            submit = gr.Button(value="Perform Grid Search")
        with gr.Column():
            # df1 = gr.DataFrame(label='Hyper-parameter Optimization Results')
            with gr.Row():
                strat_config = gr.JSON(label='Best Configuration obtained using selected strategy')
                strat_acc = gr.Number(label="Best Validation Accuracy using selected strategy")
                strat_timing = gr.Number(label="Time taken for tuning using selected strategy in seconds")
            with gr.Row():
                full_config = gr.JSON(label='Best Configuration obtained using Full')
                full_acc = gr.Number(label="Best Validation Accuracy using Full")
                full_timing = gr.Number(label="Time taken for tuning using Full")
    dset.change(fn=update_model_text, inputs=dset, outputs=[model, search_space])

    # strategy.change(fn=update_out_text, inputs=strategy, outputs=[strat_acc, strat_timing])
    
    submit.click(fn=hpo, inputs=[dset, model, strategy, budget], outputs=[strat_config, strat_acc, strat_timing, full_config, full_acc, full_timing])
demo.launch()
# app = gr.mount_gradio_app(app, demo, path=CUSTOM_PATH)
File Path: gradio_sl.py
Content:
import gradio as gr
from train_sl import TrainClassifier
from cords.utils.config_utils import load_config_data
import os, sys
import pandas as pd
from matplotlib import pyplot as plt

def train_model(dset, ml_model, strategy, budget):
    metric = 'cossim'
    kw  =  0.01
    dset = dset.lower()
    if dset in ['mnist', 'cifar10']:
        feat_model = 'dino_cls'
    elif dset in ['trec6']:
        feat_model = 'all-distilroberta-v1'
    temperature  =  1
    strategy = strategy.lower()
    per_class = True
    if ml_model == 'LeNet':
        ml_model = 'MnistNet'
    else:
        ml_model = ml_model
    run_cnt = 0
    if strategy in ['gradmatchpb', 'craigpb', 'glister']:
        if dset in ['trec6']:
            select_every = 3
        else:
            select_every = 10
    else:
        select_every = 1
    submod_function = 'disp_min_pc'
    data_dir = '../data'
    stochastic_subsets_file = os.path.join(os.path.abspath(data_dir), dset + '_' + feat_model + '_' + metric + '_' + 'gc_pc' + '_' + str(kw) + '_' + str(budget/100) + '_stochastic_subsets.pkl')
    gc_stochastic_subsets_file = os.path.join(os.path.abspath(data_dir), dset + '_' + feat_model + '_' + metric + '_' + 'gc_pc' + '_' + str(kw) + '_' + str(budget/100) + '_stochastic_subsets.pkl')
    global_order_file = os.path.join(os.path.abspath(data_dir), dset + '_' + feat_model + '_' + metric + '_' + submod_function + '_' + str(kw) + '_global_order.pkl')
    results_dict = {}
    # Pre-trained Full Runs   
    if dset == 'mnist':
        results_dict['full'] = [[], [], [0.115 , 0.9789 , 0.9827 , 0.9861 , 0.9832 , 0.9849 , 0.9861 , 0.988 , 0.9874 , 0.9901 , 0.9868 , 0.9864 , 0.9882 , 0.9866 , 0.988 , 0.9892 , 0.9891 , 0.9899 , 0.9878 , 0.9898 , 0.9905 , 0.991 , 0.9887 , 0.9901 , 0.9888 , 0.9906 , 0.9908 , 0.9892 , 0.9905 , 0.9916 , 0.9906 , 0.9907 , 0.9901 , 0.9917 , 0.9925 , 0.9921 , 0.9916 , 0.9922 , 0.9919 , 0.9915 , 0.992 , 0.992 , 0.9918 , 0.992 , 0.9915 , 0.9919 , 0.9918 , 0.9917 , 0.9918 , 0.9918 , 0.9918], 
        [0.115 , 0.9789 , 0.9827 , 0.9861 , 0.9861 , 0.9861 , 0.9861 , 0.988 , 0.988 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9901 , 0.9905 , 0.991 , 0.991 , 0.991 , 0.991 , 0.991 , 0.991 , 0.991 , 0.991 , 0.9916 , 0.9916 , 0.9916 , 0.9916 , 0.9917 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925 , 0.9925], 
        [0 , 14.389130115509033 , 28.760189294815063 , 43.04677176475525 , 57.34686803817749 , 71.64872980117798 , 86.07794213294983 , 100.39223647117615 , 114.78056263923645 , 129.3134298324585 , 143.80610918998718 , 158.21936583518982 , 172.56326031684875 , 187.00763177871704 , 201.51328587532043 , 216.04423141479492 , 230.46502709388733 , 244.85350632667542 , 259.26120710372925 , 273.64386916160583 , 288.0403118133545 , 302.349862575531 , 316.72026205062866 , 331.15454030036926 , 345.6315724849701 , 360.1029076576233 , 374.5343918800354 , 388.9018943309784 , 403.3672478199005 , 417.79295468330383 , 432.27216124534607 , 446.7308900356293 , 461.1081693172455 , 475.48193740844727 , 489.92033982276917 , 504.29099321365356 , 518.7385609149933 , 533.1931157112122 , 548.4689939022064 , 563.1488707065582 , 577.4998891353607 , 591.9615330696106 , 606.4804599285126 , 620.812980890274 , 635.2540225982666 , 649.7750537395477 , 664.1136946678162 , 678.4558100700378 , 692.8708505630493 , 707.2919359207153 , 721.8091855049133]]
    
    if dset == 'trec6':
        results_dict['full']= [[], [], [0.276 , 0.272 , 0.386 , 0.406 , 0.572 , 0.812 , 0.816 , 0.844 , 0.852 , 0.864 , 0.858 , 0.86 , 0.856 , 0.852 , 0.872 , 0.862 , 0.852 , 0.864 , 0.85 , 0.874 , 0.862], [0.276 , 0.276 , 0.386 , 0.406 , 0.572 , 0.812 , 0.816 , 0.844 , 0.852 , 0.864 , 0.864 , 0.864 , 0.864 , 0.864 , 0.872 , 0.872 , 0.872 , 0.872 , 0.872 , 0.874 , 0.874],
        [0 , 14.302120447158813 , 28.568480014801025 , 42.83177137374878 , 57.09772491455078 , 71.36303496360779 , 85.70327568054199 , 100.13131880760193 , 114.56033658981323 , 128.99154114723206 , 143.42615032196045 , 157.8575897216797 , 172.2942578792572 , 186.73123860359192 , 201.16823267936707 , 215.60326504707336 , 230.04524636268616 , 244.4774525165558 , 258.89773440361023 , 273.30184841156006 , 287.69908905029297]]
    
    if dset == 'cifar10':
        results_dict['full'] = [[], [], [0.1091 , 0.4922 , 0.6473 , 0.6424 , 0.7345 , 0.773 , 0.8007 , 0.8084 , 0.8106 , 0.8107 , 0.8413 , 0.8004 , 0.8509 , 0.8669 , 0.835 , 0.8398 , 0.8738 , 0.8732 , 0.8359 , 0.8331 , 0.8751 , 0.8556 , 0.8802 , 0.8899 , 0.8775 , 0.8866 , 0.8777 , 0.8952 , 0.8935 , 0.8957 , 0.8999 , 0.9051 , 0.899 , 0.9134 , 0.9107 , 0.9135 , 0.918 , 0.9232 , 0.9273 , 0.9287 , 0.9303 , 0.9353 , 0.9372 , 0.9362 , 0.9366 , 0.9381 , 0.9394 , 0.9387 , 0.9387 , 0.9384 , 0.9383],
         [0.1091 , 0.4922 , 0.6473 , 0.6473 , 0.7345 , 0.773 , 0.8007 , 0.8084 , 0.8106 , 0.8107 , 0.8413 , 0.8413 , 0.8509 , 0.8669 , 0.8669 , 0.8669 , 0.8738 , 0.8738 , 0.8738 , 0.8738 , 0.8751 , 0.8751 , 0.8802 , 0.8899 , 0.8899 , 0.8899 , 0.8899 , 0.8952 , 0.8952 , 0.8957 , 0.8999 , 0.9051 , 0.9051 , 0.9134 , 0.9134 , 0.9135 , 0.918 , 0.9232 , 0.9273 , 0.9287 , 0.9303 , 0.9353 , 0.9372 , 0.9372 , 0.9372 , 0.9381 , 0.9394 , 0.9394 , 0.9394 , 0.9394 , 0.9394],
         [0 , 51.98912858963013 , 103.96543478965759 , 156.05607771873474 , 208.0158998966217 , 259.933513879776 , 311.9376890659332 , 363.79946732521057 , 415.7738411426544 , 467.86292028427124 , 519.9261643886566 , 571.9207320213318 , 624.090588092804 , 676.4059774875641 , 728.5902750492096 , 780.7833154201508 , 833.0365333557129 , 885.2812588214874 , 937.6802985668182 , 989.459990978241 , 1041.473831653595 , 1093.491066455841 , 1145.7025349140167 , 1197.7461080551147 , 1249.7591750621796 , 1301.9592096805573 , 1353.8092248439789 , 1405.6296582221985 , 1457.5615322589874 , 1509.4092235565186 , 1561.2287991046906 , 1613.277204990387 , 1665.5947706699371 , 1717.441056728363 , 1769.4924371242523 , 1821.5556795597076 , 1873.6177792549133 , 1925.6751248836517 , 1977.7274763584137 , 2029.6933171749115 , 2081.691498041153 , 2133.5037961006165 , 2185.614940404892 , 2237.853115081787 , 2290.0417246818542 , 2342.3756618499756 , 2394.863676548004 , 2446.7810294628143 , 2498.856172800064 , 2551.005379676819 , 2603.0458319187164]]

    results_df = pd.DataFrame({'Strategy': ['Full']*len(results_dict['full'][-1]), 'Time': results_dict['full'][-1], 'Accuracy': results_dict['full'][3]})
    for strat in [strategy]: #, 'full']:
        if dset in ['trec6']:
            config_file = "configs/SL/config_" + strat + "_glove_" + dset + ".py"
        else:
            config_file = "configs/SL/config_" + strat + "_" + dset + ".py"
        config_data = load_config_data(config_file)
        config_data.train_args.device = 'cuda'
        config_data.train_args.run = run_cnt
        config_data.train_args.wandb = False
        config_data.train_args.print_args=["tst_loss", "tst_acc", "time"]
        if dset in ['trec6']:
            config_data.train_args.num_epochs = 20
            # if strat == 'full':
            #     config_data.train_args.num_epochs = 5
            config_data.train_args.print_every = 5
            config_data.dataloader.batch_size = 16
        elif dset in ['mnist']:
            config_data.train_args.num_epochs = 50
            config_data.train_args.print_every = 10
            config_data.scheduler.T_max = 50
            # if strat == 'full':
            #     config_data.train_args.num_epochs = 5
            #     config_data.train_args.print_every = 1
            #     config_data.scheduler.T_max = 5
            config_data.scheduler.type = "cosine_annealing"
            
            config_data.optimizer.type = 'sgd'
            config_data.optimizer.lr = 5e-2
            config_data.dataloader.batch_size = 128
        elif dset in ['cifar10']:
            config_data.train_args.num_epochs = 10
            config_data.train_args.print_every = 10
            config_data.scheduler.T_max = 10
            # if strat == 'full':
            #     config_data.train_args.num_epochs = 5
            #     config_data.train_args.print_every = 1
            #     config_data.scheduler.T_max = 5
            config_data.scheduler.type = "cosine_annealing"
            
            config_data.optimizer.type = 'sgd'
            config_data.optimizer.lr = 5e-2
            config_data.dataloader.batch_size = 128
        config_data.dss_args.fraction = (budget/100)
            
        config_data.dss_args.global_order_file = global_order_file
        config_data.dss_args.gc_stochastic_subsets_file = gc_stochastic_subsets_file
        config_data.dss_args.stochastic_subsets_file = stochastic_subsets_file
        config_data.dss_args.gc_ratio = 0.1
        config_data.dss_args.kw = kw
        config_data.dss_args.per_class = per_class
        config_data.dss_args.temperature = temperature
        config_data.dss_args.submod_function = submod_function
        config_data.model.architecture = ml_model
        config_data.dss_args.select_every = select_every
        classifier = TrainClassifier(config_data)
        trn_acc, val_acc, tst_acc, best_acc, omp_cum_timing = classifier.train()
        results_dict[strat] = [trn_acc, val_acc, tst_acc, best_acc, omp_cum_timing]
        results_df = results_df.append(pd.DataFrame({'Strategy': [strat.upper()]*len(results_dict[strat][-1]), 'Time': results_dict[strat][-1], 'Accuracy': results_dict[strat][3]}))
    
    fig = gr.LinePlot.update(
            results_df,
            x="Time",
            y="Accuracy",
            color="Strategy",
            title="Test Accuracy Convergence Curves",
            stroke_dash="Strategy",
            x_title='Time taken (in Secs)',
            y_title='Test Accuracy',
            # x_lim=[0, 50],
            tooltip=['Strategy', 'Accuracy'],
            stroke_dash_legend_title="Strategy",
            height=300,
            width=500
        )

    return fig, results_dict[strategy][3][-1], results_dict[strategy][-1][-1], results_dict['full'][3][-1], results_dict['full'][-1][-1]


def update_model_text(choice):
    if choice == "MNIST":
      return gr.update(choices=['LeNet'], value="LeNet")
    elif choice == "TREC6":
        return gr.update(choices=['LSTM'], value="LSTM")
    elif choice == 'CIFAR10':
        return gr.update(choices=['ResNet18'], value="ResNet18")


def update_out_text(choice):
    # print(choice)
    return gr.update(label='Best Test Accuracy obtained by '+ str(choice)), gr.update(label="Time taken by " + str(choice) + " in seconds")


with gr.Blocks(title = "Classifier Training") as demo:
    with gr.Row():
        with gr.Column():
            dset = gr.Dropdown(choices=['MNIST', 'TREC6', 'CIFAR10'], label='Dataset Name')
            model = gr.Radio(["LeNet", "LSTM", "ResNet18"], label="Model Architecture")
            strategy = gr.Dropdown(choices=['Random', 'AdaptiveRandom', 'MILO', 'WRE', 'SGE', 'MILOFixed', 'GradMatchPB', 'CraigPB', 'GLISTER'], label='Subset Selection Strategy')
            budget = gr.Slider(minimum=0, maximum=100, label='Budget (in %)')
            submit = gr.Button(value="Train Model")
        with gr.Column():
            #plot = gr.Plot(label='Convergence Curves')
            plot = gr.LinePlot(show_label=False).style(container=False)
            with gr.Row():
                strat_acc = gr.Number(label='Test Accuracy obtained by selected strategy')
                strat_timing = gr.Number(label="Time taken by selected strategy in seconds")
            with gr.Row():
               full_acc = gr.Number(label='Test Accuracy obtained by full')
               full_timing = gr.Number(label="Time taken by full in seconds")
    dset.change(fn=update_model_text, inputs=dset, outputs=model)

    # strategy.change(fn=update_out_text, inputs=strategy, outputs=[strat_acc, strat_timing])
    
    submit.click(fn=train_model, inputs=[dset, model, strategy, budget], outputs=[plot, strat_acc, strat_timing, full_acc, full_timing])
demo.launch()
File Path: setup.py
Content:
from setuptools import setup
import setuptools
import os

long_description = "CORDS is COReset and Data Selection library for making machine learning time, energy, cost, and compute efficient. CORDS is built on top of PyTorch. Today, deep learning systems are extremely compute-intensive, with significant turnaround times, energy inefficiencies, higher costs, and resource requirements [7, 8]. CORDS is an effort to make deep learning more energy, cost, resource, and time-efficient while not sacrificing accuracy."

setup(
    name='cords',
    version='v0.0.4',
    author='Krishnateja Killamsetty, Dheeraj Bhat, Rishabh Iyer',
    author_email='krishnatejakillamsetty@gmail.com',
    #packages=['cords', 'cords/selectionstrategies', 'cords/utils'],
    url='https://github.com/decile-team/cords',
    license='LICENSE.txt',
    packages=setuptools.find_packages(),
    description='cords is a package for data subset selection for efficient and robust machine learning.',
    long_description=long_description,
    long_description_content_type='text/markdown',
    install_requires=[
        "apricot-select>=0.6.0",
        "numba>=0.43.0",
        "scipy>=1.5.0",
        "scikit-learn",
        "torch>=1.8.0",
        "torchvision",
        "tqdm>=4.24.0",
        "sphinxcontrib-napoleon",
        "sphinxcontrib-bibtex",
        "sphinx-rtd-theme",
        "matplotlib",
        "numpy>=1.19.0",
        "torchvision>=0.10.1",
        "pillow>=8.4.0",
        "pandas>=1.1.0",
        "torchtext~=0.10.1",
        "scikit-image>=0.17.0",
        "pyyaml~=5.4.1",
        "dotmap~=1.3.24",
        "setuptools>=58.0.4",
        "ray[tune]",
        "ray[default]",
        "datasets"
            ],
)

File Path: tests/test_glister.py
Content:
# Sanity checks for glister
def test_grads_val():
    print("grads_val")
    assert True


def test_grads_train():
    print("grads_train")
    assert True


def test_taylor_approx():
    print("taylor_approx")
    assert True


def test_select():
    print("select")
    assert True

File Path: tests/test_integration.py
Content:
# test if the whole workflow is executable



def test_craig():
    print("craig")
    assert True


def test_data_selection():
    print("data_selection")
    assert True


def test_fixed_weight():
    print("fixed_weight")
    assert True


def test_glister():
    print("glister")
    assert True


def test_omp_gradmatch():
    print("omp_gradmatch")
    assert True


def test_random():
    print("random")
    assert True


def test_submodular_selection():
    print("submodular_selection")
    assert True

File Path: train_hpo.py
Content:
import argparse
from cords.utils.config_utils import load_config_data
from ray.tune.suggest.hyperopt import HyperOptSearch
from ray.tune.suggest.bayesopt import BayesOptSearch
from ray.tune.suggest.skopt import SkOptSearch
from ray.tune.suggest.dragonfly import DragonflySearch
from ray.tune.suggest.ax import AxSearch
from ray.tune.suggest.bohb import TuneBOHB
from ray.tune.suggest.nevergrad import NevergradSearch
from ray.tune.suggest.optuna import OptunaSearch
from ray.tune.suggest.zoopt import ZOOptSearch
from ray.tune.suggest.sigopt import SigOptSearch
from ray.tune.suggest.hebo import HEBOSearch
from ray.tune.schedulers import AsyncHyperBandScheduler
from ray.tune.schedulers import HyperBandScheduler
from ray.tune.schedulers.hb_bohb import HyperBandForBOHB
from ray import tune
from train_sl import TrainClassifier


class HyperParamTuning:
    def __init__(self, config_file_data, train_config_data):
        self.cfg = config_file_data
        self.train_class = TrainClassifier(train_config_data)
        self.train_class.cfg.train_args.print_every = 1
        self.search_algo = self.get_search_algo(self.cfg.search_algo, self.cfg.space, self.cfg.metric, self.cfg.mode)
        self.scheduler = self.get_scheduler(self.cfg.scheduler, self.cfg.metric, self.cfg.mode)
        # save subset method, to be used in log dir name
        self.subset_method = self.train_class.cfg.dss_args.type

    def param_tune(self, config):
        #update parameters in config dict
        new_config = self.update_parameters(self.train_class.cfg, config)
        self.train_class.cfg = new_config
        # turn on reporting to ray every time
        self.train_class.cfg.report_tune = True
        self.train_class.train()

    def start_eval(self):
        if self.search_algo is None:
            analysis = tune.run(
                self.param_tune,
                num_samples=self.cfg.num_evals,
                config=self.cfg.space,
                search_alg=self.search_algo,
                scheduler=self.scheduler,
                resources_per_trial=self.cfg.resources,
                local_dir=self.cfg.log_dir+self.subset_method+'/',
                log_to_file=True,
                name=self.cfg.name,
                resume=self.cfg.resume)
        else:
            analysis = tune.run(
                self.param_tune,
                num_samples=self.cfg.num_evals,
                search_alg=self.search_algo,
                scheduler=self.scheduler,
                resources_per_trial=self.cfg.resources,
                local_dir=self.cfg.log_dir+self.subset_method+'/',
                log_to_file=True,
                name=self.cfg.name,
                resume=self.cfg.resume)
        best_config = analysis.get_best_config(metric=self.cfg.metric, mode=self.cfg.mode)
        print("Best Config: ", best_config)

        if self.cfg['final_train']:
            self.final_train(best_config)

    def get_search_algo(self, method, space, metric, mode):
        
        # HyperOptSearch 
        if method == "hyperopt" or method == "TPE":
            search = HyperOptSearch(space, metric = metric, mode = mode)
        # BayesOptSearch
        elif method == "bayesopt" or method == "BO":
            search = BayesOptSearch(space, metric = metric, mode = mode)
        # SkoptSearch
        elif method == "skopt" or method == "SKBO":
            search = SkOptSearch(space, metric = metric, mode = mode)
        # DragonflySearch
        elif method == "dragonfly" or method == "SBO":
            search = DragonflySearch(space, metric = metric, mode = mode)
        # AxSearch
        elif method == "ax" or method == "BBO":
            search = AxSearch(space, metric = metric, mode = mode)
        # TuneBOHB
        elif method == "tunebohb" or method == "BOHB":
            search = TuneBOHB(space, metric = metric, mode = mode)
        # NevergradSearch
        elif method == "nevergrad" or method == "GFO":
            search = NevergradSearch(space, metric = metric, mode = mode)
        # OptunaSearch
        elif method == "optuna" or method == "OSA":
            search = OptunaSearch(space, metric = metric, mode = mode)
        # ZOOptSearch
        elif method == "zoopt" or method == "ZOO":
            search = ZOOptSearch(space, metric = metric, mode = mode)
        # SigOptSearch
        elif method == "sigopt":
            search = SigOptSearch(space, metric = metric, mode = mode)
        # HEBOSearch
        elif method == "hebo" or method == "HEBO":
            search = HEBOSearch(space, metric = metric, mode = mode)
        else:
            search = None

        return search

    def get_scheduler(self, method, metric, mode):

        if method == "ASHA" or method == "asha":
            scheduler = AsyncHyperBandScheduler(metric = metric, mode = mode, 
                                                max_t = self.train_class.cfg.train_args.num_epochs)
        elif method == "hyperband" or method == "HB":
            scheduler = HyperBandScheduler(metric = metric, mode = mode, 
                        max_t = self.train_class.cfg.train_args.num_epochs)
        elif method == "BOHB":
            scheduler = HyperBandForBOHB(metric = metric, mode = mode)
        else:
            scheduler = None
        return scheduler
    
    def final_train(self, best_params):
        # change strategy to Full (i.e use whole dataset)
        # update (optimized) parameters
        new_config = self.update_parameters(self.train_class.cfg, best_params)

        if self.cfg.final_train_type in ['Full', 'full']:
            new_config.dss_args.type = 'Full'
        elif self.cfg.final_train_type in ['GradMatchPB', 'gmpb']:
            new_config.dss_args.type = 'GradMatchPB'
            new_config.dss_args.fraction = 0.3
            new_config.dss_args.select_every = 5
            new_config.dss_args.lam = 0
            new_config.dss_args.selection_type = 'PerBatch'
            new_config.dss_args.v1 = True
            new_config.dss_args.valid = False
            new_config.dss_args.eps = 1e-100
            new_config.dss_args.linear_layer = True
            new_config.dss_args.kappa = 0.5
        else:
            print('Unknow final_train_type in Hyperparameter tuning class. Exiting...')
            exit(1)

        self.train_class.cfg = new_config
        self.train_class.train()
    
    def update_parameters(self, config, new_config):
        # a generic function to update parameters
        if 'learning_rate' in new_config:
            config.optimizer.lr = new_config['learning_rate']
        if 'learning_rate1' in new_config:
            config.optimizer.lr1 = new_config['learning_rate1']
        if 'learning_rate2' in new_config:
            config.optimizer.lr2 = new_config['learning_rate2']
        if 'learning_rate3' in new_config:
            config.optimizer.lr3 = new_config['learning_rate3']
        if 'optimizer' in new_config:
            config.optimizer.type = new_config['optimizer']
        if 'nesterov' in new_config:
            config.optimizer.nesterov = new_config['nesterov']
        if 'scheduler' in new_config:
            config.scheduler.type = new_config['scheduler']
        if 'gamma' in new_config:
            config.scheduler.gamma = new_config['gamma']
        if 'epochs' in new_config:
            config.train_args.num_epochs = new_config['epochs']
        if 'trn_batch_size' in new_config:
            config.dataloader.batch_size = new_config['trn_batch_size']
        if 'hidden_size' in new_config:
            config.model.hidden_size = new_config['hidden_size']
        if 'num_layers' in new_config:
            config.model.num_layers = new_config['num_layers']
        return config


if __name__ == "__main__":
    argparser = argparse.ArgumentParser()
    argparser.add_argument("--config_file", default="configs/config_hyper_param_tuning.py")    
    args = argparser.parse_args()

    hyperparam_tuning = HyperParamTuning(load_config_data(args.config_file))
    hyperparam_tuning.start_eval()
File Path: train_sl.py
Content:
import logging
import wandb
import os
import os.path as osp
import sys
import time
import torch
import math
import numpy as np
import torch.nn as nn
import torch.optim as optim
from ray import tune
from cords.selectionstrategies.helpers.ssl_lib.param_scheduler import scheduler as step_scheduler
from cords.utils.data.data_utils import WeightedSubset
from cords.utils.data.dataloader.SL.adaptive import GLISTERDataLoader, AdaptiveRandomDataLoader, StochasticGreedyDataLoader,\
    CRAIGDataLoader, GradMatchDataLoader, RandomDataLoader, WeightedRandomDataLoader, MILODataLoader, SELCONDataLoader
from cords.utils.data.dataloader.SL.nonadaptive import FacLocDataLoader, MILOFixedDataLoader
from cords.utils.data.datasets.SL import gen_dataset
from cords.utils.models import *
from cords.utils.data.data_utils.collate import *
import pickle
from datetime import datetime

class TrainClassifier:
    def __init__(self, config_file_data):
        self.cfg = config_file_data
        results_dir = osp.abspath(osp.expanduser(self.cfg.train_args.results_dir))
        
        if self.cfg.dss_args.type in ['StochasticGreedyExploration', 'WeightedRandomExploration', 'SGE', 'WRE']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + "_" + str(self.cfg.dss_args.kw)
        elif self.cfg.dss_args.type in ['MILO']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + "_" + str(self.cfg.dss_args.gc_ratio) + "_" + str(self.cfg.dss_args.kw)
        else:
            subset_selection_name = self.cfg.dss_args.type
            
        all_logs_dir = os.path.join(results_dir, 
                                    self.cfg.setting,
                                    self.cfg.dataset.name,
                                    subset_selection_name,
                                    self.cfg.model.architecture,
                                    str(self.cfg.dss_args.fraction),
                                    str(self.cfg.dss_args.select_every),
                                    str(self.cfg.train_args.run))

        os.makedirs(all_logs_dir, exist_ok=True)
        # setup logger
        plain_formatter = logging.Formatter("[%(asctime)s] %(name)s %(levelname)s: %(message)s",
                                            datefmt="%m/%d %H:%M:%S")
        now = datetime.now()
        current_time = now.strftime("%y/%m/%d %H:%M:%S")
        self.logger = logging.getLogger(__name__+"  " + current_time)
        self.logger.setLevel(logging.INFO)
        s_handler = logging.StreamHandler(stream=sys.stdout)
        s_handler.setFormatter(plain_formatter)
        s_handler.setLevel(logging.INFO)
        self.logger.addHandler(s_handler)
        f_handler = logging.FileHandler(os.path.join(all_logs_dir, self.cfg.dataset.name + "_" +
                                                     self.cfg.dss_args.type + ".log"), mode='w')
        f_handler.setFormatter(plain_formatter)
        f_handler.setLevel(logging.DEBUG)
        self.logger.addHandler(f_handler)
        self.logger.propagate = False

    
    """
    ############################## Loss Evaluation ##############################
    """

    def model_eval_loss(self, data_loader, model, criterion):
        total_loss = 0
        with torch.no_grad():
            for _, (inputs, targets) in enumerate(data_loader):
                inputs, targets = inputs.to(self.cfg.train_args.device), \
                                  targets.to(self.cfg.train_args.device, non_blocking=True)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                total_loss += loss.item()
        return total_loss

    """
    ############################## Model Creation ##############################
    """

    def create_model(self):
        if self.cfg.model.architecture == 'RegressionNet':
            model = RegressionNet(self.cfg.model.input_dim)
        elif self.cfg.model.architecture == 'ResNet18':
            model = ResNet18(self.cfg.model.numclasses)
            if self.cfg.dataset.name in ['cifar10', 'cifar100', 'tinyimagenet']:
                model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                model.maxpool = nn.Identity()
        elif self.cfg.model.architecture == 'ResNet101':
            model = ResNet101(self.cfg.model.numclasses)
            if self.cfg.dataset.name in ['cifar10', 'cifar100', 'tinyimagenet']:
                model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                model.maxpool = nn.Identity()
        elif self.cfg.model.architecture == 'MnistNet':
            model = MnistNet()
        elif self.cfg.model.architecture == 'ResNet164':
            model = ResNet164(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNet':
            model = MobileNet(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNetV2':
            model = MobileNetV2(self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'MobileNet2':
            model = MobileNet2(output_size=self.cfg.model.numclasses)
        elif self.cfg.model.architecture == 'HyperParamNet':
            model = HyperParamNet(self.cfg.model.l1, self.cfg.model.l2)
        elif self.cfg.model.architecture == 'ThreeLayerNet':
            model = ThreeLayerNet(self.cfg.model.input_dim, self.cfg.model.num_classes, self.cfg.model.h1, self.cfg.model.h2)
        elif self.cfg.model.architecture == 'LSTM':
            model = LSTMClassifier(self.cfg.model.numclasses, self.cfg.model.wordvec_dim, \
                 self.cfg.model.weight_path, self.cfg.model.num_layers, self.cfg.model.hidden_size)
        else:
            raise(NotImplementedError)
        model = model.to(self.cfg.train_args.device)
        return model

    """
    ############################## Loss Type, Optimizer and Learning Rate Scheduler ##############################
    """

    def loss_function(self):
        if self.cfg.loss.type == "CrossEntropyLoss":
            criterion = nn.CrossEntropyLoss()
            criterion_nored = nn.CrossEntropyLoss(reduction='none')
        elif self.cfg.loss.type == "MeanSquaredLoss":
            criterion = nn.MSELoss()
            criterion_nored = nn.MSELoss(reduction='none')
        return criterion, criterion_nored

    def optimizer_with_scheduler(self, model):
        if self.cfg.optimizer.type == 'sgd':
            if ('ResNet' in self.cfg.model.architecture) and ('lr1' in self.cfg.optimizer.keys()) and ('lr2' in self.cfg.optimizer.keys()) and ('lr3' in self.cfg.optimizer.keys()):
                optimizer = optim.SGD( [{"params": model.linear.parameters(), "lr": self.cfg.optimizer.lr1},
                                        {"params": model.layer4.parameters(), "lr": self.cfg.optimizer.lr2},
                                        {"params": model.layer3.parameters(), "lr": self.cfg.optimizer.lr2},
                                        {"params": model.layer2.parameters(), "lr": self.cfg.optimizer.lr2},
                                        {"params": model.layer1.parameters(), "lr": self.cfg.optimizer.lr2},
                                        {"params": model.conv1.parameters(), "lr": self.cfg.optimizer.lr3}],
                                    lr=self.cfg.optimizer.lr,
                                    momentum=self.cfg.optimizer.momentum,
                                    weight_decay=self.cfg.optimizer.weight_decay,
                                    nesterov=self.cfg.optimizer.nesterov)
            else:
                optimizer = optim.SGD(model.parameters(),
                                    lr=self.cfg.optimizer.lr,
                                    momentum=self.cfg.optimizer.momentum,
                                    weight_decay=self.cfg.optimizer.weight_decay,
                                    nesterov=self.cfg.optimizer.nesterov)
        elif self.cfg.optimizer.type == "adam":
            optimizer = optim.Adam(model.parameters(), lr=self.cfg.optimizer.lr, weight_decay=self.cfg.optimizer.weight_decay)
        elif self.cfg.optimizer.type == "rmsprop":
            optimizer = optim.RMSprop(model.parameters(), lr=self.cfg.optimizer.lr)

        if self.cfg.scheduler.type == 'cosine_annealing':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                                                   T_max=self.cfg.scheduler.T_max)
        elif self.cfg.scheduler.type == 'cosine_annealing_WS':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,
                                                                   T_0=self.cfg.scheduler.T_0,
                                                                   T_mult=self.cfg.scheduler.T_mult)
        elif self.cfg.scheduler.type == 'linear_decay':
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 
                                                        step_size=self.cfg.scheduler.stepsize, 
                                                        gamma=self.cfg.scheduler.gamma)
        elif self.cfg.scheduler.type == 'multistep':    
            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.cfg.scheduler.milestones,
                                                             gamma=self.cfg.scheduler.gamma)
        elif self.cfg.scheduler.type == 'cosine_annealing_step':
            scheduler = step_scheduler.CosineAnnealingLR(optimizer, max_iteration=self.cfg.scheduler.max_steps)
        else:
            scheduler = None
        return optimizer, scheduler

    @staticmethod
    def generate_cumulative_timing(mod_timing):
        tmp = 0
        mod_cum_timing = np.zeros(len(mod_timing))
        for i in range(len(mod_timing)):
            tmp += mod_timing[i]
            mod_cum_timing[i] = tmp
        return mod_cum_timing

    @staticmethod
    def save_ckpt(state, ckpt_path):
        torch.save(state, ckpt_path)

    @staticmethod
    def load_ckpt(ckpt_path, model, optimizer):
        checkpoint = torch.load(ckpt_path)
        start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        loss = checkpoint['loss']
        metrics = checkpoint['metrics']
        return start_epoch, model, optimizer, loss, metrics

    def count_pkl(self, path):
        if not osp.exists(path):
            return -1
        return_val = 0
        file = open(path, 'rb')
        while(True):
            try:
                _ = pickle.load(file)
                return_val += 1
            except EOFError:
                break
        file.close()
        return return_val

    def train(self, **kwargs):
        """
        ############################## General Training Loop with Data Selection Strategies ##############################
        """
        # Loading the Dataset
        logger = self.logger
        if ('trainset' in kwargs) and ('validset' in kwargs) and ('testset' in kwargs) and ('num_cls' in kwargs):
            trainset, validset, testset, num_cls = kwargs['trainset'], kwargs['validset'], kwargs['testset'], kwargs['num_cls']
        else:
            #logger.info(self.cfg)
            if self.cfg.dataset.feature == 'classimb':
                trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                                self.cfg.dataset.name,
                                                                self.cfg.dataset.feature,
                                                                classimb_ratio=self.cfg.dataset.classimb_ratio, dataset=self.cfg.dataset)
            else:
                trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                                self.cfg.dataset.name,
                                                                self.cfg.dataset.feature, dataset=self.cfg.dataset)

        trn_batch_size = self.cfg.dataloader.batch_size
        val_batch_size = self.cfg.dataloader.batch_size
        tst_batch_size = self.cfg.dataloader.batch_size

        batch_sampler = lambda _, __ : None
        drop_last = False
        if self.cfg.dss_args.type in ['SELCON']:
            drop_last = True
            assert(self.cfg.dataset.name in ['LawSchool_selcon', 'Community_Crime'])
            if self.cfg.dss_arg.batch_sampler == 'sequential':
                batch_sampler = lambda dataset, bs : torch.utils.data.BatchSampler(
                    torch.utils.data.SequentialSampler(dataset), batch_size=bs, drop_last=True
                )   # sequential
            elif self.cfg.dss_arg.batch_sampler == 'random':
                batch_sampler = lambda dataset, bs : torch.utils.data.BatchSampler(
                    torch.utils.data.RandomSampler(dataset), batch_size=bs, drop_last=True
                )   # random


        if self.cfg.dataset.name == "sst2_facloc" and self.count_pkl(self.cfg.dataset.ss_path) == 1 and self.cfg.dss_args.type == 'FacLoc':
            self.cfg.dss_args.type = 'Full'
            file_ss = open(self.cfg.dataset.ss_path, 'rb')
            ss_indices = pickle.load(file_ss)
            file_ss.close()
            trainset = torch.utils.data.Subset(trainset, ss_indices)

        if 'collate_fn' not in self.cfg.dataloader.keys():
            collate_fn = None
        else:
            collate_fn = self.cfg.dataloader.collate_fn

        # Creating the Data Loaders
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size, sampler=batch_sampler(trainset, trn_batch_size),
                                                  shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)

        valloader = torch.utils.data.DataLoader(validset, batch_size=val_batch_size, sampler=batch_sampler(validset, val_batch_size),
                                                shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)

        testloader = torch.utils.data.DataLoader(testset, batch_size=tst_batch_size, sampler=batch_sampler(testset, tst_batch_size),
                                                 shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)
	
        train_eval_loader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size * 20, sampler=batch_sampler(trainset, trn_batch_size),
                                                  shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)

        val_eval_loader = torch.utils.data.DataLoader(validset, batch_size=val_batch_size * 20, sampler=batch_sampler(validset, val_batch_size),
                                                shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)

        test_eval_loader = torch.utils.data.DataLoader(testset, batch_size=tst_batch_size * 20, sampler=batch_sampler(testset, tst_batch_size),
                                                 shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)
						 
        substrn_losses = list()  # np.zeros(cfg['train_args']['num_epochs'])
        trn_losses = list()
        val_losses = list()  # np.zeros(cfg['train_args']['num_epochs'])
        tst_losses = list()
        subtrn_losses = list()
        timing = []
        trn_acc = list()
        val_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])
        tst_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])
        best_acc = list()
        curr_best_acc = 0
        subtrn_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])

        # Checkpoint file
        checkpoint_dir = osp.abspath(osp.expanduser(self.cfg.ckpt.dir))
        
        if self.cfg.dss_args.type in ['StochasticGreedyExploration', 'WeightedRandomExploration', 'SGE', 'WRE']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + "_" + str(self.cfg.dss_args.kw)
        elif self.cfg.dss_args.type in ['MILO']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + "_" + str(self.cfg.dss_args.gc_ratio) + "_" + str(self.cfg.dss_args.kw)
        else:
            subset_selection_name = self.cfg.dss_args.type
        
        ckpt_dir = os.path.join(checkpoint_dir, 
                                self.cfg.setting,
                                self.cfg.dataset.name,
                                subset_selection_name,
                                self.cfg.model.architecture,
                                str(self.cfg.dss_args.fraction),
                                str(self.cfg.dss_args.select_every),
                                str(self.cfg.train_args.run))
                                
        checkpoint_path = os.path.join(ckpt_dir, 'model.pt')
        os.makedirs(ckpt_dir, exist_ok=True)

        # Model Creation
        model = self.create_model()
        if self.cfg.train_args.wandb:
            wandb.watch(model)

        # model1 = self.create_model()

        #Initial Checkpoint Directory
        init_ckpt_dir = os.path.abspath(os.path.expanduser("checkpoints"))
        os.makedirs(init_ckpt_dir, exist_ok=True)
        
        model_name = ""
        for key in self.cfg.model.keys():
            if r"/" not in str(self.cfg.model[key]):
                model_name += (str(self.cfg.model[key]) + "_")

        if model_name[-1] == "_":
            model_name = model_name[:-1]
            
        if not os.path.exists(os.path.join(init_ckpt_dir, model_name + ".pt")):
            ckpt_state = {'state_dict': model.state_dict()}
            # save checkpoint
            self.save_ckpt(ckpt_state, os.path.join(init_ckpt_dir, model_name + ".pt"))
        else:
            checkpoint = torch.load(os.path.join(init_ckpt_dir, model_name + ".pt"))
            model.load_state_dict(checkpoint['state_dict'])

        # Loss Functions
        criterion, criterion_nored = self.loss_function()

        
        if self.cfg.scheduler.type == "cosine_annealing_step":
            if self.cfg.dss_args.type == "Full":
                self.cfg.scheduler.max_steps = math.ceil(len(list(dataloader.batch_sampler)) * self.cfg.train_args.num_epochs)
            else:
                self.cfg.scheduler.max_steps = math.ceil(len(list(dataloader.subset_loader.batch_sampler)) * self.cfg.train_args.num_epochs)
                 # * self.cfg.dss_args.fraction)

        # Getting the optimizer and scheduler
        optimizer, scheduler = self.optimizer_with_scheduler(model)

        """
        ############################## Custom Dataloader Creation ##############################
        """

        if 'collate_fn' not in self.cfg.dss_args:
                self.cfg.dss_args.collate_fn = None

        if self.cfg.dss_args.type in ['GradMatch', 'GradMatchPB', 'GradMatch-Warm', 'GradMatchPB-Warm']:
            """
            ############################## GradMatch Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = GradMatchDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                             batch_size=self.cfg.dataloader.batch_size,
                                             shuffle=self.cfg.dataloader.shuffle,
                                             pin_memory=self.cfg.dataloader.pin_memory,
                                             collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['GLISTER', 'GLISTER-Warm', 'GLISTERPB', 'GLISTERPB-Warm']:
            """
            ############################## GLISTER Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device
            dataloader = GLISTERDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                           batch_size=self.cfg.dataloader.batch_size,
                                           shuffle=self.cfg.dataloader.shuffle,
                                           pin_memory=self.cfg.dataloader.pin_memory,
                                           collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['CRAIG', 'CRAIG-Warm', 'CRAIGPB', 'CRAIGPB-Warm']:
            """
            ############################## CRAIG Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = CRAIGDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                         batch_size=self.cfg.dataloader.batch_size,
                                         shuffle=self.cfg.dataloader.shuffle,
                                         pin_memory=self.cfg.dataloader.pin_memory,
                                         collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['Random', 'Random-Warm']:
            """
            ############################## Random Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = RandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory, 
                                          collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['AdaptiveRandom', 'AdaptiveRandom-Warm']:
            """
            ############################## AdaptiveRandom Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = AdaptiveRandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['MILOFixed', 'MILOFixed-Warm']:
            """
            ############################## MILOFixed Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = MILOFixedDataLoader(trainloader, self.cfg.dss_args, logger,
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory, 
                                          collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['WeightedRandomExploration', 'WeightedRandomExploration-Warm', 'WRE', 'WRE-Warm']:
            """
            ############################## WeightedRandomDataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = WeightedRandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['StochasticGreedyExploration', 'StochasticGreedyExploration-Warm', 'SGE', 'SGE-Warm']:
            """
            ############################## StochasticGreedyDataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = StochasticGreedyDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['MILO']:
            """
            ############################## MILODataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = MILODataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            collate_fn = self.cfg.dss_args.collate_fn)
        
        elif self.cfg.dss_args.type == 'FacLoc':
            """
            ############################## Facility Location Dataloader Additional Arguments ##############################
            """
            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.model = model
            self.cfg.dss_args.data_type = self.cfg.dataset.type
            
            dataloader = FacLocDataLoader(trainloader, valloader, self.cfg.dss_args, logger, 
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory, 
                                          collate_fn = self.cfg.dss_args.collate_fn)
        elif self.cfg.dss_args.type == 'Full':
            """
            ############################## Full Dataloader Additional Arguments ##############################
            """
            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))

            dataloader = torch.utils.data.DataLoader(wt_trainset,
                                                     batch_size=self.cfg.dataloader.batch_size,
                                                     shuffle=self.cfg.dataloader.shuffle,
                                                     pin_memory=self.cfg.dataloader.pin_memory,
                                                     collate_fn=self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['SELCON']:
            """
            ############################## SELCON Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.lr = self.cfg.optimizer.lr
            self.cfg.dss_args.loss = criterion_nored # doubt: or criterion
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.optimizer = optimizer
            self.cfg.dss_args.criterion = criterion
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.batch_size = self.cfg.dataloader.batch_size
            
            # todo: not done yet
            self.cfg.dss_args.delta = torch.tensor(self.cfg.dss_args.delta)
            # self.cfg.dss_args.linear_layer = self.cfg.dss_args.linear_layer # already there, check glister init
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            
            dataloader = SELCONDataLoader(trainset, validset, trainloader, valloader, self.cfg.dss_args, logger,
                                           batch_size=self.cfg.dataloader.batch_size,
                                           shuffle=self.cfg.dataloader.shuffle,
                                           pin_memory=self.cfg.dataloader.pin_memory)

        else:
            raise NotImplementedError

        if self.cfg.dss_args.type in ['SELCON']:        
            is_selcon = True
        else:
            is_selcon = False


        """
        ################################################# Checkpoint Loading #################################################
        """

        if self.cfg.ckpt.is_load:
            start_epoch, model, optimizer, ckpt_loss, load_metrics = self.load_ckpt(checkpoint_path, model, optimizer)
            logger.info("Loading saved checkpoint model at epoch: {0:d}".format(start_epoch))
            for arg in load_metrics.keys():
                if arg == "val_loss":
                    val_losses = load_metrics['val_loss']
                if arg == "val_acc":
                    val_acc = load_metrics['val_acc']
                if arg == "tst_loss":
                    tst_losses = load_metrics['tst_loss']
                if arg == "tst_acc":
                    tst_acc = load_metrics['tst_acc']
                    best_acc = load_metrics['best_acc']
                if arg == "trn_loss":
                    trn_losses = load_metrics['trn_loss']
                if arg == "trn_acc":
                    trn_acc = load_metrics['trn_acc']
                if arg == "subtrn_loss":
                    subtrn_losses = load_metrics['subtrn_loss']
                if arg == "subtrn_acc":
                    subtrn_acc = load_metrics['subtrn_acc']
                if arg == "time":
                    timing = load_metrics['time']
        else:
            start_epoch = 0

        """
        ################################################# Training Loop #################################################
        """
        train_time = 0
        for epoch in range(start_epoch, self.cfg.train_args.num_epochs+1):
            """
            ################################################# Evaluation Loop #################################################
            """
            print_args = self.cfg.train_args.print_args
            if (epoch % self.cfg.train_args.print_every == 0) or (epoch == self.cfg.train_args.num_epochs) or (epoch == 0):
                trn_loss = 0
                trn_correct = 0
                trn_total = 0
                val_loss = 0
                val_correct = 0
                val_total = 0
                tst_correct = 0
                tst_total = 0
                tst_loss = 0
                model.eval()
                logger_dict = {}
                if ("trn_loss" in print_args) or ("trn_acc" in print_args):
                    samples=0
		            
                    with torch.no_grad():
                        for _, data in enumerate(train_eval_loader):
                            if is_selcon:
                                inputs, targets, _ = data
                            else:
                                inputs, targets = data

                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            trn_loss += (loss.item() * train_eval_loader.batch_size)
                            samples += targets.shape[0]
                            if "trn_acc" in print_args:
                                if is_selcon: predicted = outputs
                                else: _, predicted = outputs.max(1)
                                trn_total += targets.size(0)
                                trn_correct += predicted.eq(targets).sum().item()
                        trn_loss = trn_loss/samples
                        trn_losses.append(trn_loss)
                        logger_dict['trn_loss'] = trn_loss
                    if "trn_acc" in print_args:
                        trn_acc.append(trn_correct / trn_total)
                        logger_dict['trn_acc'] = trn_correct / trn_total

                if ("val_loss" in print_args) or ("val_acc" in print_args):
                    samples =0
                    with torch.no_grad():
                        for _, data in enumerate(val_eval_loader):
                            if is_selcon:
                                inputs, targets, _ = data
                            else:
                                inputs, targets = data

                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            val_loss += (loss.item() * val_eval_loader.batch_size)
                            samples += targets.shape[0]
                            if "val_acc" in print_args:
                                if is_selcon: predicted = outputs
                                else: _, predicted = outputs.max(1)
                                val_total += targets.size(0)
                                val_correct += predicted.eq(targets).sum().item()
                        val_loss = val_loss/samples
                        val_losses.append(val_loss)
                        logger_dict['val_loss'] = val_loss

                    if "val_acc" in print_args:
                        val_acc.append(val_correct / val_total)
                        logger_dict['val_acc'] = val_correct / val_total

                if ("tst_loss" in print_args) or ("tst_acc" in print_args):
                    samples =0
                    with torch.no_grad():
                        for _, data in enumerate(test_eval_loader):
                            if is_selcon:
                                inputs, targets, _ = data
                            else:
                                inputs, targets = data

                            inputs, targets = inputs.to(self.cfg.train_args.device), \
                                              targets.to(self.cfg.train_args.device, non_blocking=True)
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)
                            tst_loss += (loss.item() * test_eval_loader.batch_size)
                            samples += targets.shape[0]
                            if "tst_acc" in print_args:
                                if is_selcon: predicted = outputs
                                else: _, predicted = outputs.max(1)
                                tst_total += targets.size(0)
                                tst_correct += predicted.eq(targets).sum().item()
                        tst_loss = tst_loss/samples
                        tst_losses.append(tst_loss)
                        logger_dict['tst_loss'] = tst_loss

                    if (tst_correct/tst_total) > curr_best_acc:
                        curr_best_acc = (tst_correct/tst_total)

                    if "tst_acc" in print_args:
                        tst_acc.append(tst_correct / tst_total)
                        best_acc.append(curr_best_acc)
                        logger_dict['tst_acc'] = tst_correct / tst_total
                        logger_dict['best_acc'] = curr_best_acc

                if "subtrn_acc" in print_args:
                    if epoch == 0:
                        subtrn_acc.append(0)
                        logger_dict['subtrn_acc'] = 0
                    else:    
                        subtrn_acc.append(subtrn_correct / subtrn_total)
                        logger_dict['subtrn_acc'] = subtrn_correct / subtrn_total

                if "subtrn_losses" in print_args:
                    if epoch == 0:
                        subtrn_losses.append(0)
                        logger_dict['subtrn_loss'] = 0
                    else: 
                        subtrn_losses.append(subtrn_loss)
                        logger_dict['subtrn_loss'] = subtrn_loss

                print_str = "Epoch: " + str(epoch)
                logger_dict['Epoch'] = epoch
                logger_dict['Time'] = train_time
                timing.append(train_time)
                
                if self.cfg.train_args.wandb:
                    wandb.log(logger_dict)

                """
                ################################################# Results Printing #################################################
                """

                for arg in print_args:
                    if arg == "val_loss":
                        print_str += " , " + "Validation Loss: " + str(val_losses[-1])

                    if arg == "val_acc":
                        print_str += " , " + "Validation Accuracy: " + str(val_acc[-1])

                    if arg == "tst_loss":
                        print_str += " , " + "Test Loss: " + str(tst_losses[-1])

                    if arg == "tst_acc":
                        print_str += " , " + "Test Accuracy: " + str(tst_acc[-1])
                        print_str += " , " + "Best Accuracy: " + str(best_acc[-1])

                    if arg == "trn_loss":
                        print_str += " , " + "Training Loss: " + str(trn_losses[-1])

                    if arg == "trn_acc":
                        print_str += " , " + "Training Accuracy: " + str(trn_acc[-1])

                    if arg == "subtrn_loss":
                        print_str += " , " + "Subset Loss: " + str(subtrn_losses[-1])

                    if arg == "subtrn_acc":
                        print_str += " , " + "Subset Accuracy: " + str(subtrn_acc[-1])

                    if arg == "time":
                        print_str += " , " + "Timing: " + str(timing[-1])

                # report metric to ray for hyperparameter optimization
                if 'report_tune' in self.cfg and self.cfg.report_tune and len(dataloader) and epoch > 0:
                    tune.report(mean_accuracy=np.array(val_acc).max())

                logger.info(print_str)

            subtrn_loss = 0
            subtrn_correct = 0
            subtrn_total = 0
            model.train()
            start_time = time.time()

            for _, (inputs, targets, weights) in enumerate(dataloader):
                inputs = inputs.to(self.cfg.train_args.device)
                targets = targets.to(self.cfg.train_args.device, non_blocking=True)
                weights = weights.to(self.cfg.train_args.device)
                optimizer.zero_grad()
                outputs = model(inputs)
                losses = criterion_nored(outputs, targets)
                loss = torch.dot(losses, weights / (weights.sum()))
                loss.backward()
                subtrn_loss += loss.item()
                optimizer.step()
                if self.cfg.scheduler.type == "cosine_annealing_step":
                    scheduler.step()
                if not self.cfg.is_reg:
                    _, predicted = outputs.max(1)
                    subtrn_total += targets.size(0)
                    subtrn_correct += predicted.eq(targets).sum().item()
            epoch_time = time.time() - start_time
            if (scheduler is not None) and (self.cfg.scheduler.type != "cosine_annealing_step"):
                scheduler.step()
            # timing.append(epoch_time)
            train_time += epoch_time
            

            """
            ################################################# Checkpoint Saving #################################################
            """

            if ((epoch + 1) % self.cfg.ckpt.save_every == 0) and self.cfg.ckpt.is_save:

                metric_dict = {}

                for arg in print_args:
                    if arg == "val_loss":
                        metric_dict['val_loss'] = val_losses
                    if arg == "val_acc":
                        metric_dict['val_acc'] = val_acc
                    if arg == "tst_loss":
                        metric_dict['tst_loss'] = tst_losses
                    if arg == "tst_acc":
                        metric_dict['tst_acc'] = tst_acc
                        metric_dict['best_acc'] = best_acc
                    if arg == "trn_loss":
                        metric_dict['trn_loss'] = trn_losses
                    if arg == "trn_acc":
                        metric_dict['trn_acc'] = trn_acc
                    if arg == "subtrn_loss":
                        metric_dict['subtrn_loss'] = subtrn_losses
                    if arg == "subtrn_acc":
                        metric_dict['subtrn_acc'] = subtrn_acc
                    if arg == "time":
                        metric_dict['time'] = timing

                ckpt_state = {
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'loss': self.loss_function(),
                    'metrics': metric_dict
                }

                # save checkpoint
                self.save_ckpt(ckpt_state, checkpoint_path)
                logger.info("Model checkpoint saved at epoch: {0:d}".format(epoch + 1))

        """
        ################################################# Results Summary #################################################
        """
        original_idxs = set([x for x in range(len(trainset))])
        encountered_idxs = []
        if self.cfg.dss_args.type != 'Full':
            for key in dataloader.selected_idxs.keys():
                encountered_idxs.extend(dataloader.selected_idxs[key])
            encountered_idxs = set(encountered_idxs)
            rem_idxs = original_idxs.difference(encountered_idxs)
            encountered_percentage = len(encountered_idxs)/len(original_idxs)

            logger.info("Selected Indices: ") 
            logger.info(dataloader.selected_idxs)
            logger.info("Percentages of data samples encountered during training: %.2f", encountered_percentage)
            logger.info("Not Selected Indices: ")
            logger.info(rem_idxs)

            if self.cfg.train_args.wandb:
                wandb.log({
                           "Data Samples Encountered(in %)": encountered_percentage
                           })
                           
        logger.info(self.cfg.dss_args.type + " Selection Run---------------------------------")
        logger.info("Final SubsetTrn: {0:f}".format(subtrn_loss))
        if "val_loss" in print_args:
            if "val_acc" in print_args:
                logger.info("Validation Loss: %.2f , Validation Accuracy: %.2f", val_loss, val_acc[-1])
            else:
                logger.info("Validation Loss: %.2f", val_loss)

        if "tst_loss" in print_args:
            if "tst_acc" in print_args:
                logger.info("Test Loss: %.2f, Test Accuracy: %.2f, Best Accuracy: %.2f", tst_loss, tst_acc[-1], best_acc[-1])
            else:
                logger.info("Test Data Loss: %f", tst_loss)
        logger.info('---------------------------------------------------------------------')
        logger.info(self.cfg.dss_args.type)
        logger.info('---------------------------------------------------------------------')

        """
        ################################################# Final Results Logging #################################################
        """

        if "val_acc" in print_args:
            val_str = "Validation Accuracy: "
            for val in val_acc:
                if val_str == "Validation Accuracy: ":
                    val_str = val_str + str(val)
                else:
                    val_str = val_str + " , " + str(val)
            logger.info(val_str)

        if "tst_acc" in print_args:
            tst_str = "Test Accuracy: "
            for tst in tst_acc:
                if tst_str == "Test Accuracy: ":
                    tst_str = tst_str + str(tst)
                else:
                    tst_str = tst_str + " , " + str(tst)
            logger.info(tst_str)

            tst_str = "Best Accuracy: "
            for tst in best_acc:
                if tst_str == "Best Accuracy: ":
                    tst_str = tst_str + str(tst)
                else:
                    tst_str = tst_str + " , " + str(tst)
            logger.info(tst_str)

        if "time" in print_args:
            time_str = "Time: "
            for t in timing:
                if time_str == "Time: ":
                    time_str = time_str + str(t)
                else:
                    time_str = time_str + " , " + str(t)
            logger.info(time_str)

        omp_timing = np.array(timing)
        # omp_cum_timing = list(self.generate_cumulative_timing(omp_timing))
        logger.info("Total time taken by %s = %.4f ", self.cfg.dss_args.type, omp_timing[-1])
        return trn_acc, val_acc, tst_acc, best_acc, omp_timing

File Path: train_ssl.py
Content:
import logging
import numpy, random, time, json, copy
import numpy as np
import os.path as osp
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from cords.utils.data.data_utils import WeightedSubset
from cords.utils.models import WideResNet, ShakeNet, CNN13, CNN
from cords.utils.data.datasets.SSL import utils as dataset_utils
from cords.selectionstrategies.helpers.ssl_lib.algs.builder import gen_ssl_alg
from cords.selectionstrategies.helpers.ssl_lib.algs import utils as alg_utils
from cords.utils.models import utils as model_utils
from cords.selectionstrategies.helpers.ssl_lib.consistency.builder import gen_consistency
from cords.utils.data.datasets.SSL import gen_dataset
from cords.selectionstrategies.helpers.ssl_lib.param_scheduler import scheduler
from cords.selectionstrategies.helpers.ssl_lib.misc.meter import Meter
from cords.utils.data.dataloader.SSL.adaptive import *
from cords.utils.config_utils import load_config_data
import time
import os
import sys


class TrainClassifier:
    def __init__(self, config_file_data):
        self.cfg = config_file_data
        results_dir = osp.abspath(osp.expanduser(self.cfg.train_args.results_dir))
        all_logs_dir = os.path.join(results_dir, self.cfg.setting,
                                    self.cfg.dss_args.type,
                                    self.cfg.dataset.name,
                                    str(self.cfg.dss_args.fraction),
                                    str(self.cfg.dss_args.select_every))

        os.makedirs(all_logs_dir, exist_ok=True)
        # setup logger
        plain_formatter = logging.Formatter("[%(asctime)s] %(name)s %(levelname)s: %(message)s",
                                            datefmt="%m/%d %H:%M:%S")
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        s_handler = logging.StreamHandler(stream=sys.stdout)
        s_handler.setFormatter(plain_formatter)
        s_handler.setLevel(logging.INFO)
        self.logger.addHandler(s_handler)
        f_handler = logging.FileHandler(os.path.join(all_logs_dir, self.cfg.dataset.name + ".log"))
        f_handler.setFormatter(plain_formatter)
        f_handler.setLevel(logging.DEBUG)
        self.logger.addHandler(f_handler)
        self.logger.propagate = False
        self.logger.info(self.cfg)

    """
    ############################## Model Creation ##############################
    """

    def gen_model(self, name, num_classes, img_size):
        scale = int(np.ceil(np.log2(img_size)))
        if name == "wrn":
            return WideResNet(num_classes, 32, scale, 4)
        elif name == "shake":
            return ShakeNet(num_classes, 32, scale, 4)
        elif name == "cnn13":
            return CNN13(num_classes, 32)
        elif name == 'cnn':
            return CNN(num_classes)
        else:
            raise NotImplementedError

    """
    ############################## Model Evaluation ##############################
    """

    @staticmethod
    def evaluation(raw_model, eval_model, loader, device):
        raw_model.eval()
        eval_model.eval()
        sum_raw_acc = sum_acc = sum_loss = 0
        with torch.no_grad():
            for (data, labels) in loader:
                data, labels = data.to(device), labels.to(device)
                preds = eval_model(data)
                raw_preds = raw_model(data)
                loss = F.cross_entropy(preds, labels)
                sum_loss += loss.item()
                acc = (preds.max(1)[1] == labels).float().mean()
                raw_acc = (raw_preds.max(1)[1] == labels).float().mean()
                sum_acc += acc.item()
                sum_raw_acc += raw_acc.item()
        mean_raw_acc = sum_raw_acc / len(loader)
        mean_acc = sum_acc / len(loader)
        mean_loss = sum_loss / len(loader)
        raw_model.train()
        eval_model.train()
        return mean_raw_acc, mean_acc, mean_loss

    """
    ############################## Model Parameters Update ##############################
    """

    def param_update(self,
                     cur_iteration,
                     model,
                     teacher_model,
                     optimizer,
                     ssl_alg,
                     consistency,
                     labeled_data,
                     ul_weak_data,
                     ul_strong_data,
                     labels,
                     average_model,
                     weights=None,
                     ood=False
                     ):
        # if ood:
        #     model.update_batch_stats(False)
        start_time = time.time()
        all_data = torch.cat([labeled_data, ul_weak_data, ul_strong_data], 0)
        forward_func = model.forward
        stu_logits = forward_func(all_data)
        labeled_preds = stu_logits[:labeled_data.shape[0]]

        stu_unlabeled_weak_logits, stu_unlabeled_strong_logits = torch.chunk(stu_logits[labels.shape[0]:], 2, dim=0)

        if self.cfg.optimizer.tsa:
            none_reduced_loss = F.cross_entropy(labeled_preds, labels, reduction="none")
            L_supervised = alg_utils.anneal_loss(
                labeled_preds, labels, none_reduced_loss, cur_iteration + 1,
                self.cfg.train_args.iteration, labeled_preds.shape[1], self.cfg.optimizer.tsa_schedule)
        else:
            L_supervised = F.cross_entropy(labeled_preds, labels)

        if self.cfg.ssl_args.coef > 0:
            # get target values
            if teacher_model is not None:  # get target values from teacher model
                t_forward_func = teacher_model.forward
                tea_logits = t_forward_func(all_data)
                tea_unlabeled_weak_logits, _ = torch.chunk(tea_logits[labels.shape[0]:], 2, dim=0)
            else:
                t_forward_func = forward_func
                tea_unlabeled_weak_logits = stu_unlabeled_weak_logits

            # calc consistency loss
            model.update_batch_stats(False)
            y, targets, mask = ssl_alg(
                stu_preds=stu_unlabeled_strong_logits,
                tea_logits=tea_unlabeled_weak_logits.detach(),
                w_data=ul_strong_data,
                subset=False,
                stu_forward=forward_func,
                tea_forward=t_forward_func
            )
            model.update_batch_stats(True)
            # if not ood:
            #     model.update_batch_stats(True)
            if weights is None:
                L_consistency = consistency(y, targets, mask, weak_prediction=tea_unlabeled_weak_logits.softmax(1))
            else:
                L_consistency = consistency(y, targets, mask * weights,
                                            weak_prediction=tea_unlabeled_weak_logits.softmax(1))
        else:
            L_consistency = torch.zeros_like(L_supervised)
            mask = None

        # calc total loss
        coef = scheduler.exp_warmup(self.cfg.ssl_args.coef, int(self.cfg.scheduler.warmup_iter), cur_iteration + 1)
        loss = L_supervised + coef * L_consistency
        if self.cfg.ssl_args.em > 0:
            loss -= self.cfg.ssl_args.em * \
                    (stu_unlabeled_weak_logits.softmax(1) * F.log_softmax(stu_unlabeled_weak_logits, 1)).sum(1).mean()

        # update parameters
        cur_lr = optimizer.param_groups[0]["lr"]
        optimizer.zero_grad()
        loss.backward()
        if self.cfg.optimizer.weight_decay > 0:
            decay_coeff = self.cfg.optimizer.weight_decay * cur_lr
            model_utils.apply_weight_decay(model.modules(), decay_coeff)
        optimizer.step()

        # update teacher parameters by exponential moving average
        if self.cfg.ssl_args.ema_teacher:
            model_utils.ema_update(
                teacher_model, model, self.cfg.ssl_args.ema_teacher_factor,
                self.cfg.optimizer.weight_decay * cur_lr if self.cfg.ssl_args.ema_apply_wd else None,
                cur_iteration if self.cfg.ssl_args.ema_teacher_warmup else None)
        # update evaluation model's parameters by exponential moving average
        if self.cfg.ssl_eval_args.weight_average:
            model_utils.ema_update(
                average_model, model, self.cfg.ssl_eval_args.wa_ema_factor,
                self.cfg.optimizer.weight_decay * cur_lr if self.cfg.ssl_eval_args.wa_apply_wd else None)

        # calculate accuracy for labeled data
        acc = (labeled_preds.max(1)[1] == labels).float().mean()

        return {
            "acc": acc,
            "loss": loss.item(),
            "sup loss": L_supervised.item(),
            "ssl loss": L_consistency.item(),
            "mask": mask.float().mean().item() if mask is not None else 1,
            "coef": coef,
            "sec/iter": (time.time() - start_time)
        }

    """
    ############################## Calculate selected ID points percentage  ##############################
    """

    def get_ul_ood_ratio(self, ul_dataset):
        actual_lbls = ul_dataset.dataset.dataset['labels'][ul_dataset.indices]
        bincnt = numpy.bincount(actual_lbls, minlength=10)
        self.logger.info("Ratio of ID points selected: {0:f}".format((bincnt[:6].sum() / bincnt.sum()).item()))

    """
    ############################## Calculate selected ID points percentage  ##############################
    """

    def get_ul_classimb_ratio(self, ul_dataset):
        actual_lbls = ul_dataset.dataset.dataset['labels'][ul_dataset.indices]
        bincnt = numpy.bincount(actual_lbls, minlength=10)
        self.logger.info("Ratio of points selected from under-represented classes: {0:f}".format(
            (bincnt[:5].sum() / bincnt.sum()).item()))

    """
    ############################## Main File ##############################
    """

    def train(self):
        logger = self.logger
        # set seed
        torch.manual_seed(self.cfg.train_args.seed)
        numpy.random.seed(self.cfg.train_args.seed)
        random.seed(self.cfg.train_args.seed)
        device = self.cfg.train_args.device
        # build data loader
        logger.info("load dataset")
        lt_data, ult_data, test_data, num_classes, img_size = gen_dataset(self.cfg.dataset.root, self.cfg.dataset.name,
                                                                          False, self.cfg, logger)
        # set consistency type
        consistency = gen_consistency(self.cfg.ssl_args.consis, self.cfg)
        consistency_nored = gen_consistency(self.cfg.ssl_args.consis + '_red', self.cfg)
        # set ssl algorithm
        ssl_alg = gen_ssl_alg(self.cfg.ssl_args.alg, self.cfg)
        # build student model
        model = self.gen_model(self.cfg.model.architecture, num_classes, img_size).to(device)
        # build teacher model
        if self.cfg.ssl_args.ema_teacher:
            teacher_model = self.gen_model(self.cfg.model.architecture, num_classes, img_size).to(device)
            teacher_model.load_state_dict(model.state_dict())
        else:
            teacher_model = None
        # for evaluation
        if self.cfg.ssl_eval_args.weight_average:
            average_model = self.gen_model(self.cfg.model.architecture, num_classes, img_size).to(device)
            average_model.load_state_dict(model.state_dict())
        else:
            average_model = None

        """
        Subset selection arguments
        """
        if self.cfg.dss_args.type == 'Full':
            max_iteration = self.cfg.train_args.iteration
        else:
            if self.cfg.train_args.max_iter != -1:
                max_iteration = self.cfg.train_args.max_iter
            else:
                max_iteration = int(self.cfg.train_args.iteration * self.cfg.dss_args.fraction)

        # Create Data Loaders
        ult_seq_loader = DataLoader(ult_data, batch_size=self.cfg.dataloader.ul_batch_size,
                                    shuffle=False, pin_memory=True)

        lt_seq_loader = DataLoader(lt_data, batch_size=self.cfg.dataloader.l_batch_size,
                                   shuffle=False, pin_memory=True)

        test_loader = DataLoader(
            test_data,
            1,
            shuffle=False,
            drop_last=False,
            num_workers=self.cfg.dataloader.num_workers
        )

        """
        ############################## Custom Dataloader Creation ##############################
        """

        if self.cfg.dss_args.type in ['GradMatch', 'GradMatchPB', 'GradMatch-Warm', 'GradMatchPB-Warm']:
            """
            ############################## GradMatch Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.tea_model = teacher_model
            self.cfg.dss_args.ssl_alg = ssl_alg
            self.cfg.dss_args.loss = consistency_nored
            self.cfg.dss_args.num_classes = num_classes
            self.cfg.dss_args.num_iters = self.cfg.train_args.iteration
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.device = self.cfg.train_args.device

            ult_loader = GradMatchDataLoader(ult_seq_loader, lt_seq_loader, self.cfg.dss_args, logger=logger,
                                             batch_size=self.cfg.dataloader.ul_batch_size,
                                             pin_memory=self.cfg.dataloader.pin_memory,
                                             num_workers=self.cfg.dataloader.num_workers)


        elif self.cfg.dss_args.type in ['RETRIEVE', 'RETRIEVE-Warm', 'RETRIEVEPB', 'RETRIEVEPB-Warm']:
            """
            ############################## RETRIEVE Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.tea_model = teacher_model
            self.cfg.dss_args.ssl_alg = ssl_alg
            self.cfg.dss_args.loss = consistency_nored
            self.cfg.dss_args.num_classes = num_classes
            self.cfg.dss_args.num_iters = max_iteration
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.device = self.cfg.train_args.device

            ult_loader = RETRIEVEDataLoader(ult_seq_loader, lt_seq_loader, self.cfg.dss_args, logger=logger,
                                            batch_size=self.cfg.dataloader.ul_batch_size,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            num_workers=self.cfg.dataloader.num_workers)


        elif self.cfg.dss_args.type in ['CRAIG', 'CRAIG-Warm', 'CRAIGPB', 'CRAIGPB-Warm']:
            """
            ############################## CRAIG Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.tea_model = teacher_model
            self.cfg.dss_args.ssl_alg = ssl_alg
            self.cfg.dss_args.loss = consistency_nored
            self.cfg.dss_args.num_classes = num_classes
            self.cfg.dss_args.num_iters = max_iteration
            self.cfg.dss_args.device = self.cfg.train_args.device
            ult_loader = CRAIGDataLoader(ult_seq_loader, lt_seq_loader, self.cfg.dss_args, logger=logger,
                                         batch_size=self.cfg.dataloader.ul_batch_size,
                                         pin_memory=self.cfg.dataloader.pin_memory,
                                         num_workers=self.cfg.dataloader.num_workers)


        elif self.cfg.dss_args.type in ['Random', 'Random-Warm']:
            """
            ############################## Random Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_classes = num_classes
            self.cfg.dss_args.num_iters = max_iteration
            self.cfg.dss_args.device = self.cfg.train_args.device
            ult_loader = RandomDataLoader(ult_seq_loader, self.cfg.dss_args, logger=logger,
                                          batch_size=self.cfg.dataloader.ul_batch_size,
                                          pin_memory=self.cfg.dataloader.pin_memory,
                                          num_workers=self.cfg.dataloader.num_workers)


        elif self.cfg.dss_args.type == ['OLRandom', 'OLRandom-Warm']:
            """
            ############################## OLRandom Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_classes = num_classes
            self.cfg.dss_args.num_iters = max_iteration
            self.cfg.dss_args.device = self.cfg.train_args.device
            ult_loader = OLRandomDataLoader(ult_seq_loader, self.cfg.dss_args, logger=logger,
                                            batch_size=self.cfg.dataloader.ul_batch_size,
                                            pin_memory=self.cfg.dataloader.pin_memory,
                                            num_workers=self.cfg.dataloader.num_workers)

        elif self.cfg.dss_args.type == 'Full':
            """
            ############################## Full Dataloader Additional Arguments ##############################
            """
            wt_trainset = WeightedSubset(ult_data, list(range(len(ult_data))), [1] * len(ult_data))

            ult_loader = torch.utils.data.DataLoader(wt_trainset,
                                                     batch_size=self.cfg.dataloader.ul_batch_size,
                                                     pin_memory=self.cfg.dataloader.pin_memory,
                                                     num_workers=self.cfg.dataloader.num_workers)

        model.train()
        logger.info(model)

        if self.cfg.optimizer.type == "sgd":
            optimizer = optim.SGD(
                model.parameters(), self.cfg.optimizer.lr, self.cfg.optimizer.momentum, 
                weight_decay=self.cfg.optimizer.weight_decay, nesterov=self.cfg.optimizer.nesterov)
        elif self.cfg.optimizer.type == "adam":
            optimizer = optim.Adam(
                model.parameters(), self.cfg.optimizer.lr, (self.cfg.optimizer.momentum, 0.999), 
                weight_decay=self.cfg.optimizer.weight_decay)
        else:
            raise NotImplementedError

        # set lr scheduler
        if self.cfg.scheduler.lr_decay == "cos":
            if self.cfg.dss_args.type == 'Full':
                lr_scheduler = scheduler.CosineAnnealingLR(optimizer, max_iteration)
            else:
                lr_scheduler = scheduler.CosineAnnealingLR(optimizer,
                                                           self.cfg.train_args.iteration * self.cfg.dss_args.fraction)
        elif self.cfg.scheduler.lr_decay == "step":
            # TODO: fixed milestones
            lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [400000, ], self.cfg.scheduler.lr_decay_rate)
        else:
            raise NotImplementedError

        # init meter
        metric_meter = Meter()
        test_acc_list = []
        best_acc_list = []
        curr_best_acc = 0
        raw_acc_list = []
        logger.info("training")

        if self.cfg.dataset.feature == 'ood':
            self.get_ul_ood_ratio(ult_loader.dataset)
        elif self.cfg.dataset.feature == 'classimb':
            self.get_ul_classimb_ratio(ult_loader.dataset)

        iter_count = 1
        subset_selection_time = 0
        training_time = 0

        while iter_count <= max_iteration:
            lt_loader = DataLoader(
                lt_data,
                self.cfg.dataloader.l_batch_size,
                sampler=dataset_utils.InfiniteSampler(len(lt_data), len(list(
                    ult_loader.batch_sampler)) * self.cfg.dataloader.l_batch_size),
                num_workers=self.cfg.dataloader.num_workers
            )

            logger.debug("Data loader iteration count is: {0:d}".format(len(list(ult_loader.batch_sampler))))
            for batch_idx, (l_data, ul_data) in enumerate(zip(lt_loader, ult_loader)):
                batch_start_time = time.time()
                if iter_count > max_iteration:
                    break
                l_aug, labels = l_data
                ul_w_aug, ul_s_aug, _, weights = ul_data
                if self.cfg.dataset.feature in ['ood', 'classimb']:
                    ood = True
                else:
                    ood = False
                params = self.param_update(
                    iter_count, model, teacher_model, optimizer, ssl_alg,
                    consistency, l_aug.to(device), ul_w_aug.to(device),
                    ul_s_aug.to(device), labels.to(device),
                    average_model, weights=weights.to(device), ood=ood)
                training_time += (time.time() - batch_start_time)
                # moving average for reporting losses and accuracy
                metric_meter.add(params, ignores=["coef"])
                # display losses every cfg.disp iterations
                if ((iter_count + 1) % self.cfg.train_args.disp) == 0:
                    state = metric_meter.state(
                        header=f'[{iter_count + 1}/{max_iteration}]',
                        footer=f'ssl coef {params["coef"]:.4g} | lr {optimizer.param_groups[0]["lr"]:.4g}'
                    )
                    logger.info(state)
                lr_scheduler.step()
                if ((iter_count + 1) % self.cfg.ckpt.checkpoint) == 0 or (iter_count + 1) == max_iteration:
                    with torch.no_grad():
                        if self.cfg.ssl_eval_args.weight_average:
                            eval_model = average_model
                        else:
                            eval_model = model
                        logger.info("test")
                        mean_raw_acc, mean_test_acc, mean_test_loss = self.evaluation(model, eval_model, test_loader,
                                                                                      device)
                        logger.info("test loss %f | test acc. %f | raw acc. %f", mean_test_loss, mean_test_acc,
                                    mean_raw_acc)
                        test_acc_list.append(mean_test_acc)
                        if mean_test_acc > curr_best_acc:
                            curr_best_acc = mean_test_acc
                        best_acc_list.append(curr_best_acc)
                        raw_acc_list.append(mean_raw_acc)
                    torch.save(model.state_dict(), os.path.join(self.cfg.train_args.results_dir, "model_checkpoint.pth"))
                    torch.save(optimizer.state_dict(),
                               os.path.join(self.cfg.train_args.results_dir, "optimizer_checkpoint.pth"))
                iter_count += 1

        numpy.save(os.path.join(self.cfg.train_args.results_dir, "evaluation_results"), test_acc_list)
        numpy.save(os.path.join(self.cfg.train_args.results_dir, "raw_results"), raw_acc_list)
        logger.info("Total Time taken: %f", training_time + subset_selection_time)
        logger.info("Subset Selection Time: %f", subset_selection_time)
        accuracies = {}
        for i in [1, 10, 20, 50]:
            logger.info("mean test acc. over last %d checkpoints: %f", i, numpy.median(test_acc_list[-i:]))
            logger.info("mean best acc. over last %d checkpoints: %f", i, numpy.median(best_acc_list[-i:]))
            logger.info("mean test acc. for raw model over last %d checkpoints: %f", i, numpy.median(raw_acc_list[-i:]))
            accuracies[f"last{i}"] = numpy.median(test_acc_list[-i:])
            accuracies[f"best{i}"] = numpy.median(best_acc_list[-i:])

        with open(os.path.join(self.cfg.train_args.results_dir, "results.json"), "w") as f:
            json.dump(accuracies, f, sort_keys=True)

if __name__ == "__main__":
    torch.multiprocessing.freeze_support()
File Path: transformers_train_sl.py
Content:
import logging
import os
import os.path as osp
import sys
import time
import torch
import pickle
import numpy as np
import torch.nn as nn
import torch.optim as optim
from ray import tune
from cords.utils.data.data_utils import WeightedSubset
from cords.utils.data.dataloader.SL.adaptive import GLISTERDataLoader, AdaptiveRandomDataLoader, StochasticGreedyDataLoader,\
    CRAIGDataLoader, GradMatchDataLoader, RandomDataLoader, AdapWeightsDataLoader, WeightedRandomDataLoader, MILODataLoader
from cords.utils.data.dataloader.SL.nonadaptive import FacLocDataLoader, MILOFixedDataLoader
from cords.utils.data.datasets.SL import gen_dataset
from cords.utils.models import *
from cords.utils.data.data_utils.collate import *
from datetime import datetime
from transformers import AutoTokenizer, get_scheduler, BertConfig, AdamW
import wandb
import evaluate


LABEL_MAPPINGS = {'glue_sst2':'label', 
                  'hf_trec6':'coarse_label', 
                  'imdb':'label',
                  'rotten_tomatoes': 'label',
                  'tweet_eval': 'label'}

SENTENCE_MAPPINGS = {'glue_sst2': 'sentence', 
                    'hf_trec6':'text',  
                    'imdb':'text',
                    'rotten_tomatoes': 'text',
                    'tweet_eval': 'text'}

def tokenize_function(tokenizer, example, text_column):
    return tokenizer(example[text_column], padding = 'max_length', truncation=True)


def compute_metrics(eval_preds):
    metric = evaluate.load("accuracy")
    logits, labels = eval_preds
    predictions = torch.argmax(logits, dim=-1)
    return metric.compute(predictions=predictions, references=labels)

class TrainClassifier:
    def __init__(self, config_file_data):
        # self.config_file = config_file
        # self.cfg = load_config_data(self.config_file)
        self.cfg = config_file_data
        results_dir = osp.abspath(osp.expanduser(self.cfg.train_args.results_dir))

        if self.cfg.dss_args.type in ['StochasticGreedyExploration', 'WeightedRandomExploration', 'SGE', 'WRE']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + "_" + str(self.cfg.dss_args.kw)
        elif self.cfg.dss_args.type in ['MILO']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + self.cfg.dss_args.gc_ratio + "_" + str(self.cfg.dss_args.kw)
        else:
            subset_selection_name = self.cfg.dss_args.type
            
        all_logs_dir = os.path.join(results_dir, 
                                    self.cfg.setting,
                                    self.cfg.dataset.name,
                                    subset_selection_name,
                                    self.cfg.model.architecture,
                                    str(self.cfg.dss_args.fraction),
                                    str(self.cfg.dss_args.select_every),
                                    str(self.cfg.train_args.run))

        os.makedirs(all_logs_dir, exist_ok=True)
        # setup logger
        plain_formatter = logging.Formatter("[%(asctime)s] %(name)s %(levelname)s: %(message)s",
                                            datefmt="%m/%d %H:%M:%S")
        now = datetime.now()
        current_time = now.strftime("%y/%m/%d %H:%M:%S")
        self.logger = logging.getLogger(__name__+"  " + current_time)
        self.logger.setLevel(logging.INFO)
        s_handler = logging.StreamHandler(stream=sys.stdout)
        s_handler.setFormatter(plain_formatter)
        s_handler.setLevel(logging.INFO)
        self.logger.addHandler(s_handler)
        f_handler = logging.FileHandler(os.path.join(all_logs_dir, self.cfg.dataset.name + "_" +
                                                     self.cfg.dss_args.type + ".log"), mode='w')
        f_handler.setFormatter(plain_formatter)
        f_handler.setLevel(logging.DEBUG)
        self.logger.addHandler(f_handler)
        self.logger.propagate = False


    """
    ############################## Loss Evaluation ##############################
    """

    def model_eval_loss(self, data_loader, model, criterion):
        total_loss = 0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(data_loader):
                inputs, targets = inputs.to(self.cfg.train_args.device), \
                                  targets.to(self.cfg.train_args.device, non_blocking=True)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                total_loss += loss.item()
        return total_loss

    """
    ############################## Model Creation ##############################
    """
    def create_model(self):
        if self.cfg.model.architecture == 'BERTMLP':
            model = BERTMLPModel(self.cfg.model.bert_config, self.cfg.model.checkpoint)
        model = model.to(self.cfg.train_args.device)
        return model

    """
    ############################## Loss Type, Optimizer and Learning Rate Scheduler ##############################
    """
    def loss_function(self):
        if self.cfg.loss.type == "CrossEntropyLoss":
            criterion = nn.CrossEntropyLoss()
            criterion_nored = nn.CrossEntropyLoss(reduction='none')
        elif self.cfg.loss.type == "MeanSquaredLoss":
            criterion = nn.MSELoss()
            criterion_nored = nn.MSELoss(reduction='none')
        return criterion, criterion_nored

    def optimizer_with_scheduler(self, model, num_training_steps):
        if self.cfg.optimizer.type == 'sgd':
            
            optimizer = optim.SGD(model.parameters(), lr=self.cfg.optimizer.lr,
                                  momentum=self.cfg.optimizer.momentum,
                                  weight_decay=self.cfg.optimizer.weight_decay,
                                  nesterov=self.cfg.optimizer.nesterov)
        elif self.cfg.optimizer.type == "adam":
            optimizer = optim.Adam(model.parameters(), lr=self.cfg.optimizer.lr)
        elif self.cfg.optimizer.type == "rmsprop":
            optimizer = optim.RMSprop(model.parameters(), lr=self.cfg.optimizer.lr)
        elif self.cfg.optimizer.type == "adamw":
            optimizer = AdamW(model.parameters(), lr=self.cfg.optimizer.lr)
        
        if self.cfg.scheduler.type == 'cosine_annealing':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                                                   T_max=self.cfg.scheduler.T_max)
        elif self.cfg.scheduler.type == 'linear_decay':
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 
                                                        step_size=self.cfg.scheduler.stepsize, 
                                                        gamma=self.cfg.scheduler.gamma)
        # if self.cfg.scheduler.type == 'linear':
        #     scheduler = get_scheduler("linear",
        #                             optimizer=optimizer,
        #                             num_warmup_steps=self.cfg.scheduler.warmup_steps,
        #                             num_training_steps=num_training_steps
        #                             )
        else:
            scheduler = None
        return optimizer, scheduler

    @staticmethod
    def generate_cumulative_timing(mod_timing):
        tmp = 0
        mod_cum_timing = np.zeros(len(mod_timing))
        for i in range(len(mod_timing)):
            tmp += mod_timing[i]
            mod_cum_timing[i] = tmp
        return mod_cum_timing

    @staticmethod
    def save_ckpt(state, ckpt_path):
        torch.save(state, ckpt_path)

    @staticmethod
    def load_ckpt(ckpt_path, model, optimizer):
        checkpoint = torch.load(ckpt_path)
        start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        loss = checkpoint['loss']
        metrics = checkpoint['metrics']
        return start_epoch, model, optimizer, loss, metrics

    def count_pkl(self, path):
        if not osp.exists(path):
            return -1
        return_val = 0
        file = open(path, 'rb')
        while(True):
            try:
                _ = pickle.load(file)
                return_val += 1
            except EOFError:
                break
        file.close()
        return return_val

    def train(self, end_before_training = False):
        """
        ############################## General Training Loop with Data Selection Strategies ##############################
        """
        # Loading the Dataset
        logger = self.logger
        #logger.info(self.cfg)

        tokenizer = AutoTokenizer.from_pretrained(self.cfg.model.checkpoint)

        if self.cfg.dataset.feature == 'classimb':
            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                               self.cfg.dataset.name + '_transformer',
                                                               self.cfg.dataset.feature,
                                                               classimb_ratio=self.cfg.dataset.classimb_ratio, 
                                                               dataset=self.cfg.dataset,
                                                               tokenizer=tokenizer)
        else:
            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,
                                                               self.cfg.dataset.name + '_transformer',
                                                               self.cfg.dataset.feature, 
                                                               dataset=self.cfg.dataset,
                                                               tokenizer=tokenizer)

        trn_batch_size = self.cfg.dataloader.batch_size
        val_batch_size = self.cfg.dataloader.batch_size
        tst_batch_size = self.cfg.dataloader.batch_size

        
        
        assert (self.cfg.dataset.name in list(SENTENCE_MAPPINGS.keys())) and (self.cfg.dataset.name in list(LABEL_MAPPINGS.keys())), \
    "Please add the SENTENCE and LABEL column names to the SENTENCE_MAPPING and LABEL_MAPPINGS dictionaries in transformers_train_sl.py file."
        
        # tokenizer_mapping = lambda example: tokenize_function(tokenizer, example, SENTENCE_MAPPINGS[self.cfg.dataset.name])
        # trainset = trainset.map(tokenizer_mapping, batched=True) 
        # trainset = trainset.remove_columns([SENTENCE_MAPPINGS[self.cfg.dataset.name], "idx"])
        # trainset = trainset.rename_column(LABEL_MAPPINGS[self.cfg.dataset.name], "labels")
        # trainset.set_format("torch")
        # trainset = trainset.shuffle(seed=42)

        # validset = validset.map(tokenizer_mapping, batched=True)
        # validset = validset.remove_columns([SENTENCE_MAPPINGS[self.cfg.dataset.name], "idx"])
        # validset = validset.rename_column(LABEL_MAPPINGS[self.cfg.dataset.name], "labels")
        # validset.set_format("torch")
        # validset = validset.shuffle(seed=42)
        
        # testset = testset.map(tokenizer_mapping, batched=True)
        # testset = testset.remove_columns([SENTENCE_MAPPINGS[self.cfg.dataset.name], "idx"])
        # testset = testset.rename_column(LABEL_MAPPINGS[self.cfg.dataset.name], "labels")
        # testset.set_format("torch")
        # testset = testset.shuffle(seed=42)

        # Creating the Data Loaders
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size,
                                                  shuffle=False, pin_memory=True)

        valloader = torch.utils.data.DataLoader(validset, batch_size=val_batch_size,
                                                shuffle=False, pin_memory=True)

        testloader = torch.utils.data.DataLoader(testset, batch_size=tst_batch_size,
                                                 shuffle=False, pin_memory=True, collate_fn = self.cfg.dataloader.collate_fn)

        train_eval_loader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size * 20,
                                                  shuffle=False, pin_memory=True)

        val_eval_loader = torch.utils.data.DataLoader(validset, batch_size=val_batch_size * 20,
                                                shuffle=False, pin_memory=True)

        test_eval_loader = torch.utils.data.DataLoader(testset, batch_size=tst_batch_size * 20,
                                                 shuffle=False, pin_memory=True)

        substrn_losses = list()  # np.zeros(configdata['train_args']['num_epochs'])
        trn_losses = list()
        val_losses = list()  # np.zeros(configdata['train_args']['num_epochs'])
        tst_losses = list()
        subtrn_losses = list()
        timing = [0]
        trn_acc = list()
        val_acc = list()  # np.zeros(configdata['train_args']['num_epochs'])
        tst_acc = list()  # np.zeros(configdata['train_args']['num_epochs'])
        best_acc = list()
        curr_best_acc = 0
        subtrn_acc = list()  # np.zeros(configdata['train_args']['num_epochs'])

        # Checkpoint file
        checkpoint_dir = osp.abspath(osp.expanduser(self.cfg.ckpt.dir))
        
        if self.cfg.dss_args.type in ['StochasticGreedyExploration', 'WeightedRandomExploration', 'SGE', 'WRE']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + "_" + str(self.cfg.dss_args.kw)
        elif self.cfg.dss_args.type in ['MILO']:
            subset_selection_name = self.cfg.dss_args.type + "_" + self.cfg.dss_args.submod_function + self.cfg.dss_args.gc_ratio + "_" + str(self.cfg.dss_args.kw)
        else:
            subset_selection_name = self.cfg.dss_args.type

        ckpt_dir = os.path.join(checkpoint_dir, 
                                self.cfg.setting,
                                self.cfg.dataset.name,
                                subset_selection_name,
                                self.cfg.model.architecture,
                                str(self.cfg.dss_args.fraction),
                                str(self.cfg.dss_args.select_every),
                                str(self.cfg.train_args.run))
                                
        checkpoint_path = os.path.join(ckpt_dir, 'model.pt')
        os.makedirs(ckpt_dir, exist_ok=True)

        # Model Creation
        model = self.create_model()
        if self.cfg.train_args.wandb:
            wandb.watch(model)
        # model1 = self.create_model()

        #Initial Checkpoint Directory
        init_ckpt_dir = os.path.abspath(os.path.expanduser("checkpoints"))
        os.makedirs(init_ckpt_dir, exist_ok=True)
        
        model_name = ""
        for key in self.cfg.model.keys():
            if (r"/" not in str(self.cfg.model[key])) and (key not in ['bert_config']):
                model_name += (str(self.cfg.model[key]) + "_")

        if model_name[-1] == "_":
            model_name = model_name[:-1]
            
        if not os.path.exists(os.path.join(init_ckpt_dir, model_name + ".pt")):
            ckpt_state = {'state_dict': model.state_dict()}
            # save checkpoint
            self.save_ckpt(ckpt_state, os.path.join(init_ckpt_dir, model_name + ".pt"))
        else:
            checkpoint = torch.load(os.path.join(init_ckpt_dir, model_name + ".pt"))
            model.load_state_dict(checkpoint['state_dict'])

        # Loss Functions
        criterion, criterion_nored = self.loss_function()

        num_training_steps = self.cfg.train_args.num_epochs * len(trainloader)
        # Getting the optimizer and scheduler
        optimizer, scheduler = self.optimizer_with_scheduler(model, num_training_steps)

        """
        ############################## Custom Dataloader Creation ##############################
        """

        if not 'collate_fn' in self.cfg.dss_args:
                self.cfg.dss_args.collate_fn = None

        if self.cfg.dss_args.type in ['GradMatch', 'GradMatchPB', 'GradMatch-Warm', 'GradMatchPB-Warm']:
            """
            ############################## GradMatch Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = GradMatchDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                             batch_size=self.cfg.dataloader.batch_size,
                                             shuffle=self.cfg.dataloader.shuffle,
                                             pin_memory=self.cfg.dataloader.pin_memory)
                                             #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['GLISTER', 'GLISTER-Warm', 'GLISTERPB', 'GLISTERPB-Warm']:
            """
            ############################## GLISTER Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.eta = self.cfg.optimizer.lr
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device
            
            dataloader = GLISTERDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                           batch_size=self.cfg.dataloader.batch_size,
                                           shuffle=self.cfg.dataloader.shuffle,
                                           pin_memory=self.cfg.dataloader.pin_memory,)
                                           #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['CRAIG', 'CRAIG-Warm', 'CRAIGPB', 'CRAIGPB-Warm']:
            """
            ############################## CRAIG Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.model = model
            self.cfg.dss_args.loss = criterion_nored
            self.cfg.dss_args.num_classes = self.cfg.model.numclasses
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
            self.cfg.dss_args.device = self.cfg.train_args.device

            dataloader = CRAIGDataLoader(trainloader, valloader, self.cfg.dss_args, logger,
                                         batch_size=self.cfg.dataloader.batch_size,
                                         shuffle=self.cfg.dataloader.shuffle,
                                         pin_memory=self.cfg.dataloader.pin_memory,)
                                         #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['Random', 'Random-Warm']:
            """
            ############################## Random Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = RandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory, )
                                          #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['AdaptiveRandom', 'AdaptiveRandom-Warm']:
            """
            ############################## AdaptiveRandom Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = AdaptiveRandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,)
                                            #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['MILOFixed', 'MILOFixed-Warm']:
            """
            ############################## MILOFixed Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = MILOFixedDataLoader(trainloader, self.cfg.dss_args, logger,
                                          batch_size=self.cfg.dataloader.batch_size,
                                          shuffle=self.cfg.dataloader.shuffle,
                                          pin_memory=self.cfg.dataloader.pin_memory,) 
                                          #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['WeightedRandomExploration', 'WeightedRandomExploration-Warm', 'WRE', 'WRE-Warm']:
            """
            ############################## WeightedRandom Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = WeightedRandomDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,)
                                            #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type in ['StochasticGreedyExploration', 'StochasticGreedyExploration-Warm', 'SGE', 'SGE-Warm']:
            """
            ############################## StochasticGreedy Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = StochasticGreedyDataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,)
                                            #collate_fn = self.cfg.dss_args.collate_fn
					    

        elif self.cfg.dss_args.type in ['MILO']:
            """
            ############################## MILO Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs

            dataloader = MILODataLoader(trainloader, self.cfg.dss_args, logger,
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,)
					    #collate_fn = self.cfg.dss_args.collate_fn)

        elif self.cfg.dss_args.type == 'FacLoc':
            """
            ############################## Facility Location Dataloader Additional Arguments ##############################
            """
            self.cfg.dss_args.device = self.cfg.train_args.device
            self.cfg.dss_args.model = model
            self.cfg.dss_args.data_type = self.cfg.dataset.type
            
            dataloader = FacLocDataLoader(trainloader, valloader, self.cfg.dss_args, logger, 
                                        batch_size=self.cfg.dataloader.batch_size,
                                        shuffle=self.cfg.dataloader.shuffle,
                                        pin_memory=self.cfg.dataloader.pin_memory,) 
                                        #collate_fn = self.cfg.dss_args.collate_fn)

            if 'ss_path' in self.cfg.dataset and self.count_pkl(self.cfg.dataset.ss_path) < 1:
                #save subset indices if a ss_path is provided. Useful in HP tuning to avoid multiple facloc computations.
                #to avoid multiple parallel facloc computations, do facloc once(using end_before_training) then start HP tuning
                ss_indices = dataloader.subset_indices
                file_ss = open(self.cfg.dataset.ss_path, 'wb')
                try:
                    pickle.dump(ss_indices, file_ss)
                except EOFError:
                    pass
                file_ss.close()

        elif self.cfg.dss_args.type == 'AdapFacLoc':
            """
            ############################## Adaptive Facility Location Dataloader Additional Arguments ##############################
            """
            num_contents = self.count_pkl(self.cfg.dataset.ss_path)
            if num_contents < 1:
                self.cfg.dss_args.device = self.cfg.train_args.device
                self.cfg.dss_args.model = model
                self.cfg.dss_args.data_type = self.cfg.dataset.type
                
                facloc_time = time.time()
                dataloader = FacLocDataLoader(trainloader, valloader, self.cfg.dss_args, logger, 
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,) 
                                            #collate_fn = self.cfg.dss_args.collate_fn)
                ss_indices = list(dataloader.subset_indices)
                facloc_time = time.time() - facloc_time
                print("Type of ss_indices:", type(ss_indices))
                file_ss = open(self.cfg.dataset.ss_path, 'wb')
                try:
                    pickle.dump(ss_indices, file_ss)
                except EOFError:
                    pass
                file_ss.close()
                print("AdapFacLoc takes facloc time of:", facloc_time)
            elif num_contents == 1:
                print("We are in adapfacloc atleast once!")
                file_ss = open(self.cfg.dataset.ss_path, 'rb')
                ss_indices = pickle.load(file_ss)
                file_ss.close()

                self.cfg.dss_args.model = model
                self.cfg.dss_args.loss = criterion_nored
                self.cfg.dss_args.eta = self.cfg.optimizer.lr
                self.cfg.dss_args.num_classes = self.cfg.model.numclasses
                self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs
                self.cfg.dss_args.device = self.cfg.train_args.device
                
                dataloader = AdapWeightsDataLoader(trainloader, valloader, self.cfg.dss_args, logger, ss_indices, 
                                            batch_size=self.cfg.dataloader.batch_size,
                                            shuffle=self.cfg.dataloader.shuffle,
                                            pin_memory=self.cfg.dataloader.pin_memory,) 
                                            #collate_fn = self.cfg.dss_args.collate_fn)
        elif self.cfg.dss_args.type == 'Full':
            """
            ############################## Full Dataloader Additional Arguments ##############################
            """
            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))

            dataloader = torch.utils.data.DataLoader(wt_trainset,
                                                     batch_size=self.cfg.dataloader.batch_size,
                                                     shuffle=self.cfg.dataloader.shuffle,
                                                     pin_memory=self.cfg.dataloader.pin_memory,)
                                                     #collate_fn=self.cfg.dss_args.collate_fn)

        """
        ################################################# Checkpoint Loading #################################################
        """

        if self.cfg.ckpt.is_load:
            start_epoch, model, optimizer, ckpt_loss, metric_dict = self.load_ckpt(checkpoint_path, model, optimizer)
            logger.info("Loading saved checkpoint model at epoch: {0:d}".format(start_epoch))
            for arg in metric_dict.keys():
                if arg == "val_loss":
                    val_losses = metric_dict['val_loss']
                if arg == "val_acc":
                    val_acc = metric_dict['val_acc']
                if arg == "tst_loss":
                    tst_losses = metric_dict['tst_loss']
                if arg == "tst_acc":
                    tst_acc = metric_dict['tst_acc']
                    best_acc = metric_dict['best_acc']
                if arg == "trn_loss":
                    trn_losses = metric_dict['trn_loss']
                if arg == "trn_acc":
                    trn_acc = metric_dict['trn_acc']
                if arg == "subtrn_loss":
                    subtrn_losses = metric_dict['subtrn_loss']
                if arg == "subtrn_acc":
                    subtrn_acc = metric_dict['subtrn_acc']
                if arg == "time":
                    timing = metric_dict['time']
        else:
            start_epoch = 0

        """
        ################################################# Training Loop #################################################
        """

        if end_before_training:
            torch.cuda.empty_cache()
            return
        
        train_time = 0
        for epoch in range(start_epoch, self.cfg.train_args.num_epochs+1):
            """
            ################################################# Evaluation Loop #################################################
            """
            print_args = self.cfg.train_args.print_args
            if ((epoch + 1) % self.cfg.train_args.print_every == 0) or (epoch == self.cfg.train_args.num_epochs - 1):
                trn_loss = 0
                val_loss = 0
                tst_loss = 0
                model.eval()
                logger_dict = {}
                if ("trn_loss" in print_args) or ("trn_acc" in print_args):
                    if "trn_acc" in print_args:
                        metric = evaluate.load('accuracy', experiment_id = self.cfg.train_args.wandb_name)

                    with torch.no_grad():
                        for batch in train_eval_loader:
                            batch = {k: v.to(self.cfg.train_args.device) for k, v in batch.items()}
                            outputs = model(**batch)
                            loss = criterion(outputs, batch['labels'].view(-1))
                            trn_loss += loss.item()
                            if "trn_acc" in print_args:
                                predictions = torch.argmax(outputs, dim=-1)
                                metric.add_batch(predictions=predictions, references=batch["labels"])
                        trn_losses.append(trn_loss)
                        logger_dict['trn_loss'] = trn_loss
                    if "trn_acc" in print_args:
                        trn_acc.append(metric.compute()['accuracy'])
                        logger_dict['trn_acc'] = trn_acc[-1]

                if ("val_loss" in print_args) or ("val_acc" in print_args):
                    if "val_acc" in print_args:
                        metric = evaluate.load('accuracy', experiment_id = self.cfg.train_args.wandb_name)

                    with torch.no_grad():
                        for batch in val_eval_loader:
                            batch = {k: v.to(self.cfg.train_args.device) for k, v in batch.items()}
                            outputs = model(**batch)
                            loss = criterion(outputs, batch['labels'].view(-1))
                            val_loss += loss.item()
                            if "val_acc" in print_args:
                                predictions = torch.argmax(outputs, dim=-1)
                                metric.add_batch(predictions=predictions, references=batch["labels"])
                        val_losses.append(val_loss)
                        logger_dict['val_loss'] = val_loss

                    if "val_acc" in print_args:
                        val_acc.append(metric.compute()['accuracy'])
                        logger_dict['val_acc'] = val_acc[-1]

                if ("tst_loss" in print_args) or ("tst_acc" in print_args):
                    if "tst_acc" in print_args:
                        metric = evaluate.load('accuracy', experiment_id = self.cfg.train_args.wandb_name)

                    with torch.no_grad():
                        for batch in test_eval_loader:
                            batch = {k: v.to(self.cfg.train_args.device) for k, v in batch.items()}
                            outputs = model(**batch)
                            loss = criterion(outputs, batch['labels'].view(-1))
                            tst_loss += loss.item()
                            if "tst_acc" in print_args:
                                predictions = torch.argmax(outputs, dim=-1)
                                metric.add_batch(predictions=predictions, references=batch["labels"])
                        tst_losses.append(tst_loss)
                        logger_dict['tst_loss'] = tst_loss
                    if "tst_acc" in print_args:
                        tst_acc.append(metric.compute()['accuracy'])
                        if tst_acc[-1] > curr_best_acc:
                            curr_best_acc = tst_acc[-1]
                        logger_dict['tst_acc'] = tst_acc[-1]
                        logger_dict['best_acc'] = curr_best_acc

                if "subtrn_acc" in print_args:
                    if epoch == 0:
                        subtrn_acc.append(0)
                        logger_dict['subtrn_acc'] = 0
                    else:    
                        subtrn_acc.append(curr_subtrn_acc)
                        logger_dict['subtrn_acc'] = curr_subtrn_acc

                if "subtrn_losses" in print_args:
                    if epoch == 0:
                        subtrn_losses.append(0) 
                        logger_dict['subtrn_loss'] = 0   
                    else:
                        subtrn_losses.append(subtrn_loss)
                        logger_dict['subtrn_loss'] = subtrn_loss

                print_str = "Epoch: " + str(epoch)
                logger_dict['Epoch'] = epoch
                logger_dict['Time'] = train_time
                
                if self.cfg.train_args.wandb:
                    wandb.log(logger_dict)

                """
                ################################################# Results Printing #################################################
                """

                for arg in print_args:

                    if arg == "val_loss":
                        print_str += " , " + "Validation Loss: " + str(val_losses[-1])

                    if arg == "val_acc":
                        print_str += " , " + "Validation Accuracy: " + str(val_acc[-1])

                    if arg == "tst_loss":
                        print_str += " , " + "Test Loss: " + str(tst_losses[-1])

                    if arg == "tst_acc":
                        print_str += " , " + "Test Accuracy: " + str(tst_acc[-1])
                        print_str += " , " + "Test Accuracy: " + str(best_acc[-1])

                    if arg == "trn_loss":
                        print_str += " , " + "Training Loss: " + str(trn_losses[-1])

                    if arg == "trn_acc":
                        print_str += " , " + "Training Accuracy: " + str(trn_acc[-1])

                    if arg == "subtrn_loss":
                        print_str += " , " + "Subset Loss: " + str(subtrn_losses[-1])

                    if arg == "subtrn_acc":
                        print_str += " , " + "Subset Accuracy: " + str(subtrn_acc[-1])

                    if arg == "time":
                        print_str += " , " + "Timing: " + str(timing[-1])

                # report metric to ray for hyperparameter optimization
                if 'report_tune' in self.cfg and self.cfg.report_tune and len(dataloader) and epoch > 0:
                    tune.report(mean_accuracy=val_acc[-1])

                logger.info(print_str)

            subtrn_loss = 0
            model.train()
            start_time = time.time()
            metric= evaluate.load("accuracy", experiment_id = self.cfg.train_args.wandb_name)
            for batch in dataloader:
                batch = {k: v.to(self.cfg.train_args.device) for k, v in batch.items()}
                weights = batch['weights']
                batch.pop('weights')
                outputs = model(**batch, finetune=True)     
                losses = criterion_nored(outputs, batch['labels'].view(-1))
                loss = torch.dot(losses, weights.view(-1) / (weights.sum()))
                # loss = torch.dot(losses, weights / len(inputs))
                loss.backward()
                subtrn_loss += loss.item()
                optimizer.step()
                optimizer.zero_grad()
                if not self.cfg.is_reg:
                    predictions = torch.argmax(outputs, dim=-1)
                    metric.add_batch(predictions=predictions, references=batch["labels"])
            epoch_time = time.time() - start_time
            if not scheduler == None:
                scheduler.step()
            timing.append(epoch_time)
            train_time += epoch_time
            
            curr_subtrn_acc = metric.compute()['accuracy']
            #if "subtrn_acc" in print_args:
            #    subtrn_acc.append(metric.compute()['accuracy'])

            
            """
            ################################################# Checkpoint Saving #################################################
            """

            if ((epoch + 1) % self.cfg.ckpt.save_every == 0) and self.cfg.ckpt.is_save:

                metric_dict = {}

                for arg in print_args:
                    if arg == "val_loss":
                        metric_dict['val_loss'] = val_losses
                    if arg == "val_acc":
                        metric_dict['val_acc'] = val_acc
                    if arg == "tst_loss":
                        metric_dict['tst_loss'] = tst_losses
                    if arg == "tst_acc":
                        metric_dict['tst_acc'] = tst_acc
                        metric_dict['best_acc'] = best_acc
                    if arg == "trn_loss":
                        metric_dict['trn_loss'] = trn_losses
                    if arg == "trn_acc":
                        metric_dict['trn_acc'] = trn_acc
                    if arg == "subtrn_loss":
                        metric_dict['subtrn_loss'] = subtrn_losses
                    if arg == "subtrn_acc":
                        metric_dict['subtrn_acc'] = subtrn_acc
                    if arg == "time":
                        metric_dict['time'] = timing

                ckpt_state = {
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'loss': self.loss_function(),
                    'metrics': metric_dict
                }

                # save checkpoint
                self.save_ckpt(ckpt_state, checkpoint_path)
                logger.info("Model checkpoint saved at epoch: {0:d}".format(epoch + 1))

        """
        ################################################# Results Summary #################################################
        """
        original_idxs = set([x for x in range(len(trainset))])
        encountered_idxs = []
        if self.cfg.dss_args.type != 'Full':
            for key in dataloader.selected_idxs.keys():
                encountered_idxs.extend(dataloader.selected_idxs[key])
            encountered_idxs = set(encountered_idxs)
            rem_idxs = original_idxs.difference(encountered_idxs)
            encountered_percentage = len(encountered_idxs)/len(original_idxs)

            logger.info("Selected Indices: ") 
            logger.info(dataloader.selected_idxs)
            logger.info("Percentages of data samples encountered during training: %.2f", encountered_percentage)
            logger.info("Not Selected Indices: ")
            logger.info(rem_idxs)
            if self.cfg.train_args.wandb:
                wandb.log({
                           "Data Samples Encountered(in %)": encountered_percentage
                           })

        logger.info(self.cfg.dss_args.type + " Selection Run---------------------------------")
        logger.info("Final SubsetTrn: {0:f}".format(subtrn_loss))
        if "val_loss" in print_args:
            if "val_acc" in print_args:
                logger.info("Validation Loss: %.2f , Validation Accuracy: %.2f", val_loss, val_acc[-1])
            else:
                logger.info("Validation Loss: %.2f", val_loss)

        if "tst_loss" in print_args:
            if "tst_acc" in print_args:
                logger.info("Test Loss: %.2f, Test Accuracy: %.2f, Best Accuracy: %.2f", tst_loss, tst_acc[-1], best_acc[-1])
            else:
                logger.info("Test Data Loss: %f", tst_loss)
        logger.info('---------------------------------------------------------------------')
        logger.info(self.cfg.dss_args.type)
        logger.info('---------------------------------------------------------------------')

        """
        ################################################# Final Results Logging #################################################
        """

        if "val_acc" in print_args:
            val_str = "Validation Accuracy: "
            for val in val_acc:
                if val_str == "Validation Accuracy: ":
                    val_str = val_str + str(val)
                else:
                    val_str = val_str + " , " + str(val)
            logger.info(val_str)

        if "tst_acc" in print_args:
            tst_str = "Test Accuracy: "
            for tst in tst_acc:
                if tst_str == "Test Accuracy: ":
                    tst_str = tst_str + str(tst)
                else:
                    tst_str = tst_str + " , " + str(tst)
            logger.info(tst_str)

            tst_str = "Best Accuracy: "
            for tst in best_acc:
                if tst_str == "Best Accuracy: ":
                    tst_str = tst_str + str(tst)
                else:
                    tst_str = tst_str + " , " + str(tst)
            logger.info(tst_str)

        if "time" in print_args:
            time_str = "Time: "
            for t in timing:
                if time_str == "Time: ":
                    time_str = time_str + str(t)
                else:
                    time_str = time_str + " , " + str(t)
            logger.info(time_str)

        omp_timing = np.array(timing)
        omp_cum_timing = list(self.generate_cumulative_timing(omp_timing))
        logger.info("Total time taken by %s = %.4f ", self.cfg.dss_args.type, omp_cum_timing[-1])
        return trn_acc, val_acc, tst_acc, best_acc

Output:
{
    "experimental_code": "from ray import tune\n\nconfig = dict(setting= \"hyperparamtuning\",\n\n# parameter for subset selection\n# all settings for subset selection will be fetched from here\nsubset_config = \"configs/SL/config_gradmatchpb_glove_sst2.py\",\n\n# parameters for hyper-parameter tuning\n# search space for hyper-parameter tuning\nspace = dict(learning_rate=tune.uniform(0.001, 0.1), \n        # optimizer= tune.choice(['sgd', 'adam']),\n        hidden_size = tune.choice([64, 128, 256]),\n        trn_batch_size= tune.choice([16, 32, 64]),\n        # num_layers = tune.choice([1, 2])\n        ),\n\n# tuning algorithm \nsearch_algo = \"TPE\",\n\n# number of hyper-parameter set to try\nnum_evals = 27,\n\n# metric to be optimized, for 'mean_loss' metric mode should be 'min'\nmetric = \"mean_accuracy\",\nmode = \"max\",\n\n# scheduler to be used (i.e ASHAScheduler)\n# scheduler terminates trials that perform poorly\n# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html\n# scheduler = 'hyperband',\nscheduler = 'ASHA',\n\n# where to store logs\nlog_dir = \"RayLogs/\",\n\n# resume hyper-parameter tuning from previous log\n# specify 'name' (i.e main_2021-03-09_18-33-56) below\nresume = False,\n\n# only required if you want to resume from previous checkpoint\n# it can also be specified if you don't want to resume\nname = None,\n\n# specify resources to be used per trial\n# i.e {'gpu':1, 'cpu':2}\n# resources = {'gpu':1, 'cpu':2},\nresources = {'gpu':0.5, 'cpu':1},\n\n# if True, trains model on Full dataset with the best parameter selected.\nfinal_train = True,\n\nfinal_train_type = 'full' # full, gmpb\n\n)",
    "experimental_info": "Overall Hyper-parameter Optimization (HPO) Configuration:\n- Setting: Hyperparameter tuning.\n- Subset selection configuration: Uses 'configs/SL/config_gradmatchpb_glove_sst2.py'.\n- Search space: \n  - `learning_rate`: Uniformly sampled between 0.001 and 0.1.\n  - `hidden_size`: Choices among 64, 128, 256.\n  - `trn_batch_size`: Choices among 16, 32, 64.\n- Tuning algorithm: TPE (Tree-structured Parzen Estimator).\n- Number of evaluations: 27.\n- Metric to be optimized: `mean_accuracy`.\n- Optimization mode: `max` (maximize).\n- Scheduler: ASHA (Asynchronous Successive Halving Algorithm).\n- Logging directory: 'RayLogs/'.\n- Resume from previous log: False.\n- Resources per trial: {'gpu': 0.5, 'cpu': 1}.\n- Final training: True (trains the model on the full dataset with the best parameters found).\n- Final training type: 'full'."
}
