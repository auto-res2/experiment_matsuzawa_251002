
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The core methodology is the 'new linear scaling rule,' which simplifies HPO by reducing the search for optimal learning rate (η) and number of steps (T) to a single scalar variable, r = η × T. The method estimates r for small privacy budgets (ε) using random search to find two empirical points (ε1, r(ε1)) and (ε2, r(ε2)). It then fits a linear approximation to these points to estimate the optimal r for any desired target εf. The theoretical intuition is motivated by analyzing the excess empirical risk of private Gradient Descent (GD) and bounding the 'noisy radius,' the distance between noisy and non-noisy iterates, which is shown to be proportional to η × T. The method incorporates specific design choices to maximize the signal-to-noise ratio and accelerate training: full batch gradient computation (DP-GD), unit clipping (C=1), zero initialization of linear classifiers, SGD with momentum (ρ=0.9), and Privacy Loss Variable (PLV) accounting, which offers tighter privacy guarantees than RDP.

# Repository Content
File Path: examples/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: examples/classification/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: examples/classification/data/make_k_shot_without_dev.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""The datasets in the k-shot folder contain dev.tsv; we make the test set the dev set in the new k-shot.

python -m classification.data.make_k_shot_without_dev
"""
import os

from ml_swissknife import utils

join = os.path.join

base_dir = '/nlp/scr/lxuechen/data/lm-bff/data/k-shot'
new_dir = '/nlp/scr/lxuechen/data/lm-bff/data/k-shot-no-dev'

task_names = ("SST-2", "QNLI", "MNLI", "QQP")
for task_name in task_names:
    folder = join(base_dir, task_name)
    new_folder = join(new_dir, task_name)

    for name in utils.listdir(folder):
        subfolder = join(folder, name)
        new_subfolder = join(new_folder, name)
        os.makedirs(new_subfolder, exist_ok=True)

        train = join(subfolder, 'train.tsv')
        new_train = join(new_subfolder, 'train.tsv')
        os.system(f'cp {train} {new_train}')

        if task_name == "MNLI":
            test = join(subfolder, 'test_matched.tsv')
            new_dev = join(new_subfolder, 'dev_matched.tsv')
            os.system(f'cp {test} {new_dev}')

            test = join(subfolder, 'test_mismatched.tsv')
            new_dev = join(new_subfolder, 'dev_mismatched.tsv')
            os.system(f'cp {test} {new_dev}')
        else:
            test = join(subfolder, 'test.tsv')
            new_dev = join(new_subfolder, 'dev.tsv')
            os.system(f'cp {test} {new_dev}')

File Path: examples/classification/data/make_valid_data.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Make the separate validation data, so that we don't tune on dev set.

python -m classification.data.make_valid_data
"""
import os

import fire
import numpy as np
import tqdm


def write_lines(path, lines, mode="w"):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, mode) as f:
        f.writelines(lines)
        print(len(lines))


def main():
    valid_percentage = 0.1
    original_dir = "/nlp/scr/lxuechen/data/lm-bff/data/original"
    new_dir = "/nlp/scr/lxuechen/data/lm-bff/data/glue-with-validation"

    task_folders = ("GLUE-SST-2", "QNLI", "QQP")
    for task_folder in task_folders:
        # Create train and valid splits.
        full_train_path = os.path.join(original_dir, task_folder, 'train.tsv')
        with open(full_train_path, 'r') as f:
            full_train = f.readlines()

        header = full_train[0]
        full_train = full_train[1:]  # Remove header.

        indices = np.random.permutation(len(full_train))
        new_valid_size = int(len(indices) * valid_percentage)
        new_train_size = len(indices) - new_valid_size
        new_train_indices = indices[:new_train_size]
        new_valid_indices = indices[new_train_size:]
        assert len(new_train_indices) == new_train_size
        assert len(new_valid_indices) == new_valid_size

        new_train = [header] + [full_train[i] for i in new_train_indices]
        new_valid = [header] + [full_train[i] for i in new_valid_indices]

        new_train_path = os.path.join(new_dir, task_folder, 'train.tsv')
        new_valid_path = os.path.join(new_dir, task_folder, 'dev.tsv')

        write_lines(new_train_path, new_train)
        write_lines(new_valid_path, new_valid)
        del new_train, new_valid, new_train_path, new_valid_path
        del new_train_size, new_train_indices
        del new_valid_size, new_valid_indices

        # Make test!
        test_path = os.path.join(original_dir, task_folder, 'dev.tsv')
        new_test_path = os.path.join(new_dir, task_folder, 'test.tsv')
        os.system(f'cp {test_path} {new_test_path}')
        del test_path, new_test_path

    # Make valid set for MNLI; different, since matched/mismatched!
    task_folder = "MNLI"
    matched_genres = ['slate', 'government', 'telephone', 'travel', 'fiction']
    mismatched_genres = ['letters', 'verbatim', 'facetoface', 'oup', 'nineeleven']
    full_train_path = os.path.join(original_dir, task_folder, 'train.tsv')
    with open(full_train_path, 'r') as f:
        full_train = f.readlines()
        full_train_csv = [line.split('\t') for line in full_train]

        # Check the lengths are correct.
        l = len(full_train_csv[0])
        for line in full_train_csv:
            assert l == len(line)

    # Remove header.
    header = full_train[0]
    header_csv = full_train_csv[0]

    full_train = full_train[1:]
    full_train_csv = full_train_csv[1:]

    # Get index of genre.
    genre_index = header_csv.index('genre')

    # Shuffle both!
    indices = np.random.permutation(len(full_train))
    full_train = [full_train[i] for i in indices]
    full_train_csv = [full_train_csv[i] for i in indices]

    # Split validation.
    new_valid_size = int(len(indices) * valid_percentage)
    new_matched_valid_size = new_mismatched_valid_size = new_valid_size // 2

    # Fetch the indices.
    new_train_indices = []
    new_matched_valid_indices = []
    new_mismatched_valid_indices = []
    matched_count = mismatched_count = 0
    for i, row in enumerate(full_train_csv):
        genre = row[genre_index]
        if genre in matched_genres and matched_count < new_matched_valid_size:
            new_matched_valid_indices.append(i)
            matched_count += 1
        elif genre in mismatched_genres and mismatched_count < new_mismatched_valid_size:
            new_mismatched_valid_indices.append(i)
            mismatched_count += 1
        else:
            new_train_indices.append(i)

    new_matched_valid_indices = set(new_matched_valid_indices)
    new_mismatched_valid_indices = set(new_mismatched_valid_indices)

    new_train = [header]
    new_matched_valid = [header]
    new_mismatched_valid = [header]
    for i, line in tqdm.tqdm(enumerate(full_train)):
        if i in new_matched_valid_indices:
            new_matched_valid.append(line)
        elif i in new_mismatched_valid_indices:
            new_mismatched_valid.append(line)
        else:
            new_train.append(line)

    new_train_path = os.path.join(new_dir, task_folder, 'train.tsv')
    new_matched_valid_path = os.path.join(new_dir, task_folder, 'dev_matched.tsv')
    new_mismatched_valid_path = os.path.join(new_dir, task_folder, 'dev_mismatched.tsv')

    write_lines(new_train_path, new_train)
    write_lines(new_matched_valid_path, new_matched_valid)
    write_lines(new_mismatched_valid_path, new_mismatched_valid)

    matched_test_path = os.path.join(original_dir, task_folder, 'dev_matched.tsv')
    new_matched_test_path = os.path.join(new_dir, task_folder, 'test_matched.tsv')
    os.system(f'cp {matched_test_path} {new_matched_test_path}')

    mismatched_test_path = os.path.join(original_dir, task_folder, 'dev_mismatched.tsv')
    new_mismatched_test_path = os.path.join(new_dir, task_folder, 'test_mismatched.tsv')
    os.system(f'cp {mismatched_test_path} {new_mismatched_test_path}')


if __name__ == "__main__":
    fire.Fire(main)

File Path: examples/classification/run_classification.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Finetuning the library models for sequence classification on GLUE."""

import collections
import copy
import dataclasses
from dataclasses import dataclass, field
from datetime import datetime
import gc
import json
import logging
import os
from typing import Callable, Dict, Optional

from filelock import FileLock
import numpy as np
from ml_swissknife import utils
import torch
from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction
from transformers import GlueDataTrainingArguments as DataTrainingArguments
from transformers import GlueDataset
from transformers import HfArgumentParser, set_seed

from private_transformers import PrivacyEngine
from .src.common import true_tags
from .src.compiled_args import PrivacyArguments, TrainingArguments, AuxiliaryArguments
from .src.dataset import FewShotDataset
from .src.models import (
    BertForPromptFinetuning, RobertaForPromptFinetuning, AlbertForPromptFinetuning, DistilBertForPromptFinetuning,
    resize_token_type_embeddings
)
from .src.processors import num_labels_mapping, output_modes_mapping, compute_metrics_mapping, bound_mapping
from .src.trainer import Trainer

logger = logging.getLogger(__name__)

os.environ["WANDB_DISABLED"] = "true"


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """
    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )
    # Few-shot type
    #   - finetune: standard fine-tuning
    #   - prompt: prompt-based fine-tuning
    #   - prompt-demo: prompt-based fine-tuning with demonstrations
    few_shot_type: str = field(
        default='prompt-demo',
        metadata={"help": "Few-shot learning model type. Choice: finetune, prompt, prompt-demo"}
    )

    # Only for BERT-type model
    random_segment: bool = field(
        default=False,
        metadata={"help": "Whether to reinitialize the token type embeddings (only for BERT)."}
    )

    static_embedding: str = field(
        default="no"
    )
    static_lm_head: str = field(
        default="no"
    )
    attention_only: str = field(
        default="no"
    )

    randomly_initialize: str = field(
        default="no",
        metadata={"help": "Randomly initialize the model; useful only for ablation studies."}
    )

    def __post_init__(self):
        self.static_embedding = self.static_embedding.lower() in true_tags  # noqa
        self.static_lm_head = self.static_lm_head.lower() in true_tags  # noqa
        self.attention_only = self.attention_only.lower() in true_tags  # noqa
        self.randomly_initialize = self.randomly_initialize.lower() in true_tags  # noqa


@dataclass
class DynamicDataTrainingArguments(DataTrainingArguments):
    """
    Arguments for dynamic training.
    """
    num_k: Optional[int] = field(
        default=16,
        metadata={"help": "Number of training instances per class"}
    )

    num_sample: Optional[int] = field(
        default=16,
        metadata={"help": "Number of samples (for inference) in fine-tuning with demonstrations"}
    )

    num_demo: Optional[int] = field(
        default=1,
        metadata={"help": "Number of demonstrations from each class"}
    )

    auto_demo: bool = field(
        default=True,
        metadata={"help": "Automatically generate template for using demonstrations"}
    )

    # For prompting
    template: str = field(
        default=None,
        metadata={"help": "Template"}
    )

    mapping: str = field(
        default=None,
        metadata={"help": "Label word mapping"}
    )

    template_path: str = field(
        default=None,
        metadata={
            "help": "Path to a txt file that stores all the templates, one per line. Do not set this when prompt_path "
                    "is used"}
    )

    mapping_path: str = field(
        default=None,
        metadata={
            "help": "Path to a txt file that stores all the label word mappings, one per line. Do not set this when "
                    "prompt_path is used"}
    )

    prompt_path: str = field(
        default=None,
        metadata={"help": "Path to a txt file that stores all the prompts (templates and mappings), one per line"}
    )

    template_id: int = field(
        default=None,
        metadata={"help": "Template id if using template_path"}
    )

    mapping_id: int = field(
        default=None,
        metadata={"help": "Mapping id if using template_path"}
    )

    prompt_id: int = field(
        default=None,
        metadata={"help": "Prompt id if using prompt_path"}
    )

    top_n_template: int = field(
        default=None,
        metadata={"help": "Use top-n template in the template path"}
    )

    # For logging
    tag: str = field(
        default='',
        metadata={"help": "Set the tag and find the result easier in the log."}
    )

    # For filtering when using demonstrations
    demo_filter: bool = field(
        default=False,
        metadata={"help": "Only use similar instances in demonstrations"}
    )

    demo_filter_rate: float = field(
        default=0.5,
        metadata={"help": "Only use top-x\% similar instances in demonstrations"}
    )

    demo_filter_model: str = field(
        default=None,
        metadata={
            "help": "Model name for demonstration filter embeddings. Will load embeddings based on the model name."}
    )

    debug_mode: bool = field(
        default=False,
        metadata={"help": "Debug mode"}
    )

    # For max length
    double_demo: bool = field(
        default=False,
        metadata={"help": "Use double length for using demonstrations"}
    )

    first_sent_limit: int = field(
        default=None,
        metadata={"help": "Limit the length of the first sentence (i.e., sent_0)"}
    )

    other_sent_limit: int = field(
        default=None,
        metadata={"help": "Limit the length of sentences other than the first sentence"}
    )

    use_full_length: bool = field(
        default=None,
        metadata={"help": "Use the full length (512)"}
    )

    # GPT-3's in-context learning
    gpt3_in_context_head: bool = field(
        default=False,
        metadata={"help": "GPT-3's in-context learning (context at the beginning)"}
    )

    gpt3_in_context_tail: bool = field(
        default=False,
        metadata={"help": "GPT-3's in-context learning (context at the end)"}
    )

    gpt3_in_context_num: int = field(
        default=32,
        metadata={"help": "Number of context examples"}
    )

    truncate_head: bool = field(
        default=False,
        metadata={"help": "When exceeding the maximum length, truncate the head instead of the tail."}
    )

    # Do not set up the following fields. They are set up automatically.
    prompt: bool = field(
        default=False,
        metadata={"help": "Whether to use prompt-based fine-tuning"}
    )
    template_list: tuple = field(
        default=None,
        metadata={"help": "(DO NOT List of templates (only initialized after the program starts."}
    )

    inference_time_demo: bool = field(
        default=False,
        metadata={"help": "Do not use demonstrations during inference time; "
                          "the original paper attaches to each test example a few training examples as demo -- "
                          "apparently this breaks privacy. We turn this off by default here."}
    )


@dataclass
class DynamicTrainingArguments(TrainingArguments):
    # For ensemble
    array_id: int = field(
        default=-1,
        metadata={"help": "Array ID (contains seed and hyper-paramter search) to idenfity the model"}
    )

    model_id: int = field(
        default=-1,
        metadata={"help": "Model ID (contains template information) to identify the model"}
    )

    save_logit: bool = field(
        default=False,
        metadata={"help": "Save test file logit with name $TASK-$MODEL_ID-$ARRAY_ID.npy"}
    )

    save_logit_dir: str = field(
        default=None,
        metadata={"help": "Where to save the prediction result"}
    )

    # Regularization
    fix_layers: int = field(
        default=0,
        metadata={"help": "Fix bottom-n layers when optimizing"}
    )

    # Training
    save_at_last: bool = field(
        default=False,
        metadata={"help": "Instead of saving the best (dev performance) checkpoint, save the last checkpoint"}
    )

    # Turn off train/test
    no_train: bool = field(
        default=False,
        metadata={"help": "No training"}
    )
    no_predict: bool = field(
        default=False,
        metadata={"help": "No test"}
    )

    evaluate_after_training: bool = field(
        default=True, metadata={"help": "Always run evaluation after training ends."}
    )

    def __post_init__(self):
        super(DynamicTrainingArguments, self).__post_init__()


def main():
    parser = HfArgumentParser(
        (ModelArguments, DynamicDataTrainingArguments, DynamicTrainingArguments, PrivacyArguments, AuxiliaryArguments)
    )
    model_args, data_args, training_args, privacy_args, auxiliary_args = parser.parse_args_into_dataclasses()

    if not os.path.exists(training_args.output_dir):
        print(f"output_dir doesn't exists, mkdir now: {training_args.output_dir}")
        os.makedirs(training_args.output_dir)

    if 'prompt' in model_args.few_shot_type:
        data_args.prompt = True

    if training_args.no_train:
        training_args.do_train = False
    if training_args.no_predict:
        training_args.do_predict = False

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,
    )

    # TODO: Hacky mapping creation. Refactor this in the future.
    #  Currently gets replace if mapping_id and mapping_path is set.
    if data_args.task_name == "sst-2":
        data_args.mapping = "{'0':'terrible','1':'great'}"
    elif data_args.task_name == "mnli":
        data_args.mapping = "{'contradiction': 'no', 'entailment': 'yes', 'neutral': 'maybe'}"
    elif data_args.task_name == "qnli":
        data_args.mapping = "{'not_entailment': 'no', 'entailment': 'yes'}"
    elif data_args.task_name == "qqp":
        data_args.mapping = "{'1': 'yes', '0': 'no'}"  # 1 -- equivalent, 0 -- not equivalent.
    else:
        raise ValueError(f"Unknown task: {data_args.task_name}")

    # Load prompt/template/mapping file
    if data_args.prompt:
        if data_args.prompt_path is not None:
            assert data_args.prompt_id is not None
            prompt_list = []
            with open(data_args.prompt_path) as f:
                for line in f:
                    line = line.strip()
                    template, mapping = line.split('\t')
                    prompt_list.append((template, mapping))

            data_args.template, data_args.mapping = prompt_list[data_args.prompt_id]
            logger.info(
                "Specify load the %d-th prompt: %s | %s" % (data_args.prompt_id, data_args.template, data_args.mapping))
        else:
            if data_args.template_path is not None:
                with open(data_args.template_path) as f:
                    data_args.template_list = []
                    for line in f:
                        line = line.strip()
                        if len(line) > 0:
                            data_args.template_list.append(line)

                # Load top-n templates
                if data_args.top_n_template is not None:
                    data_args.template_list = data_args.template_list[:data_args.top_n_template]
                logger.info("Load top-%d templates from %s" % (len(data_args.template_list), data_args.template_path))

                # ... or load i-th template
                if data_args.template_id is not None:
                    data_args.template = data_args.template_list[data_args.template_id]
                    data_args.template_list = None
                    logger.info("Specify load the %d-th template: %s" % (data_args.template_id, data_args.template))

            if data_args.mapping_path is not None:
                assert data_args.mapping_id is not None  # Only can use one label word mapping
                with open(data_args.mapping_path) as f:
                    mapping_list = []
                    for line in f:
                        line = line.strip()
                        mapping_list.append(line)

                data_args.mapping = mapping_list[data_args.mapping_id]
                logger.info("Specify using the %d-th mapping: %s" % (data_args.mapping_id, data_args.mapping))

    # Check save path
    if (
        os.path.exists(training_args.output_dir)
        and os.listdir(training_args.output_dir)
        and training_args.do_train
        and not training_args.overwrite_output_dir
    ):
        raise ValueError(f"Output directory ({training_args.output_dir}) already exists.")

    logger.warning(
        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
        training_args.local_rank,
        training_args.device,
        training_args.n_gpu,
        bool(training_args.local_rank != -1),
        training_args.fp16,
    )
    logger.info("Training/evaluation parameters %s", training_args)

    # Set seed
    set_seed(training_args.seed)

    try:
        num_labels = num_labels_mapping[data_args.task_name]
        output_mode = output_modes_mapping[data_args.task_name]
        logger.info(
            "Task name: {}, number of labels: {}, output mode: {}".format(data_args.task_name, num_labels, output_mode))
    except KeyError:
        raise ValueError("Task not found: %s" % (data_args.task_name))

    # Automatically generate template for using demonstrations
    if data_args.auto_demo and model_args.few_shot_type == 'prompt-demo':
        # GPT-3's in-context learning
        if data_args.gpt3_in_context_head or data_args.gpt3_in_context_tail:
            logger.info("Automatically convert the template to GPT-3's in-context learning.")
            assert data_args.template_list is None

            old_template = data_args.template
            new_template = old_template + ''
            old_template = old_template.replace('*cls*', '')
            # Single sentence or sentence pair?
            sent_num = 1
            if "_1" in old_template:
                sent_num = 2
            for instance_id in range(data_args.gpt3_in_context_num):
                sub_template = old_template + ''
                # Replace sent_id
                for sent_id in range(sent_num):
                    sub_template = sub_template.replace("_{}*".format(sent_id),
                                                        "_{}*".format(sent_num + sent_num * instance_id + sent_id))
                # Replace mask
                sub_template = sub_template.replace("*mask*", "*labelx_{}*".format(instance_id))
                if data_args.gpt3_in_context_tail:
                    new_template = new_template + sub_template  # Put context at the end
                else:
                    new_template = sub_template + new_template  # Put context at the beginning
            logger.info("| {} => {}".format(data_args.template, new_template))
            data_args.template = new_template
        else:
            logger.info("Automatically convert the template to using demonstrations.")
            if data_args.template_list is not None:
                for i in range(len(data_args.template_list)):
                    old_template = data_args.template_list[i]
                    new_template = old_template + ''
                    old_template = old_template.replace('*cls*', '')
                    # Single sentence or sentence pair?
                    sent_num = 1
                    if "_1" in old_template:
                        sent_num = 2
                    for label_id in range(num_labels):
                        sub_template = old_template + ''
                        # Replace sent id
                        for sent_id in range(sent_num):
                            sub_template = sub_template.replace("_{}*".format(sent_id),
                                                                "_{}*".format(sent_num + sent_num * label_id + sent_id))
                        # Replace mask
                        sub_template = sub_template.replace("*mask*", "*label_{}*".format(label_id))
                        new_template = new_template + sub_template
                    logger.info("| {} => {}".format(data_args.template_list[i], new_template))
                    data_args.template_list[i] = new_template
            else:
                old_template = data_args.template
                new_template = old_template + ''
                old_template = old_template.replace('*cls*', '')
                # Single sentence or sentence pair?
                sent_num = 1
                if "_1" in old_template:
                    sent_num = 2
                for label_id in range(num_labels):
                    sub_template = old_template + ''
                    # Replace sent id
                    for sent_id in range(sent_num):
                        sub_template = sub_template.replace("_{}".format(sent_id),
                                                            "_{}".format(sent_num + sent_num * label_id + sent_id))
                    # Replace mask
                    sub_template = sub_template.replace("*mask*", "*label_{}*".format(label_id))
                    new_template = new_template + sub_template
                logger.info("| {} => {}".format(data_args.template, new_template))
                data_args.template = new_template

    # Create config
    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=num_labels,
        finetuning_task=data_args.task_name,
        cache_dir=model_args.cache_dir,
    )

    if 'prompt' in model_args.few_shot_type:
        if config.model_type == 'roberta':
            model_fn = RobertaForPromptFinetuning
        elif config.model_type == 'bert':
            model_fn = BertForPromptFinetuning
        elif config.model_type == 'albert':
            model_fn = AlbertForPromptFinetuning
        elif config.model_type == 'distilbert':
            model_fn = DistilBertForPromptFinetuning
        else:
            raise NotImplementedError
    elif model_args.few_shot_type == 'finetune':
        model_fn = AutoModelForSequenceClassification
    else:
        raise NotImplementedError
    special_tokens = []

    # Create tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        additional_special_tokens=special_tokens,
        cache_dir=model_args.cache_dir, use_fast=False
    )
    print(f' | tokenizer: {tokenizer}, size: {len(tokenizer)} \n\n\n')

    # Get our special datasets.
    if model_args.few_shot_type == "finetune":
        assert data_args.num_sample == 1
        train_dataset = GlueDataset(data_args, tokenizer, mode="train")
        eval_dataset = (
            GlueDataset(data_args, tokenizer, mode="dev")
            if training_args.do_eval else None
        )
        test_dataset = (
            GlueDataset(data_args, tokenizer, mode="test")
            if training_args.do_predict or training_args.evaluate_test_split
            else None
        )

        if eval_dataset is not None:
            eval_dataset.num_sample = 1
        if test_dataset is not None:
            test_dataset.num_sample = 1
    else:
        use_demo = "demo" in model_args.few_shot_type
        train_dataset = FewShotDataset(data_args, tokenizer=tokenizer, mode="train", use_demo=use_demo)
        eval_dataset = (
            FewShotDataset(data_args, tokenizer=tokenizer, mode="dev", use_demo=use_demo)
            if training_args.do_eval else None
        )
        test_dataset = (
            FewShotDataset(data_args, tokenizer=tokenizer, mode="test", use_demo=use_demo)
            if training_args.do_predict or training_args.evaluate_test_split else None
        )
    print(f" *** dataset sizes: ")
    for _tag, _ds in zip(("train", "valid", "test"), (train_dataset, eval_dataset, test_dataset)):
        if _ds is not None:
            print(f'{_tag}: {len(_ds)}')
    print(f" ***")

    set_seed(training_args.seed)

    model = model_fn.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
    )
    print(" | model type: ")
    print(type(model))

    if model_args.attention_only:
        model.requires_grad_(False)
        for name, param in model.named_parameters():
            if 'query' in name or 'value' in name or 'classifier' in name or 'lm_head' in name:
                param.requires_grad_(True)
        if model_args.static_lm_head and hasattr(model, 'lm_head'):
            model.lm_head.requires_grad_(False)
    else:
        model.requires_grad_(True)
        if model_args.static_embedding:
            model.get_input_embeddings().requires_grad_(False)

    if model_args.randomly_initialize:
        # Only reinit the params which require gradients.
        model_old = copy.deepcopy(model)  # Copy pretrained model.
        model.init_weights()

        params = tuple(model.parameters())
        params_old = tuple(model_old.parameters())
        for param, param_old in utils.zip_(params, params_old):
            if not param.requires_grad:
                param.data.copy_(param_old.data)

        del model_old
        gc.collect()
        torch.cuda.empty_cache()
    print(f"attention_only: {model_args.attention_only} | randomly_initialize: {model_args.randomly_initialize}")

    named_params = [(name, param) for name, param in model.named_parameters() if param.requires_grad]
    print('Params to update: ')
    print(json.dumps([name for name, param in named_params], indent=4))
    num_differentiable_params = utils.count_parameters(model, only_differentiable=True)
    print(f'Number of differentiable params: {num_differentiable_params / 1e6:.3f} million')

    # For BERT, increase the size of the segment (token type) embeddings
    if config.model_type == 'bert':
        model.resize_token_embeddings(len(tokenizer))
        resize_token_type_embeddings(model, new_num_types=10, random_segment=model_args.random_segment)

    # Pass dataset and argument information to the model
    if data_args.prompt:
        model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()
        print(f" | Classification label_word_list: {model.label_word_list}")
        print(f"   converted words: {tokenizer.convert_ids_to_tokens(model.label_word_list)}")
    if output_modes_mapping[data_args.task_name] == 'regression':
        # lower / upper bounds
        model.lb, model.ub = bound_mapping[data_args.task_name]
        print(f" | Regression lb: {model.lb}, ub: {model.ub}")
    model.model_args = model_args
    model.data_args = data_args
    model.tokenizer = tokenizer

    # Build metric
    def build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:
        def compute_metrics_fn(p: EvalPrediction):
            # Note: the eval dataloader is sequential, so the examples are in order.
            # We average the logits over each sample for using demonstrations.
            predictions = p.predictions
            num_logits = predictions.shape[-1]
            logits = predictions.reshape([eval_dataset.num_sample, -1, num_logits])
            logits = logits.mean(axis=0)

            if num_logits == 1:
                preds = np.squeeze(logits)
            else:
                preds = np.argmax(logits, axis=1)

            # Just for sanity, assert label ids are the same.
            label_ids = p.label_ids.reshape([eval_dataset.num_sample, -1])
            label_ids_avg = label_ids.mean(axis=0)
            label_ids_avg = label_ids_avg.astype(p.label_ids.dtype)
            assert (label_ids_avg - label_ids[0]).mean() < 1e-2
            label_ids = label_ids[0]

            return compute_metrics_mapping[task_name](task_name, preds, label_ids)

        return compute_metrics_fn

    # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        model_args=model_args,
        privacy_args=privacy_args,
        auxiliary_args=auxiliary_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=build_compute_metrics_fn(data_args.task_name)
    )
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)],
         'weight_decay': training_args.weight_decay},
        {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
    ]
    optimizer = trainer.optimizer = torch.optim.AdamW(
        optimizer_grouped_parameters,
        lr=training_args.learning_rate,
        betas=(training_args.adam_beta1, training_args.adam_beta2),
        eps=training_args.adam_epsilon,
    )
    if training_args.lr_decay:  # Default linear decay.
        training_setup = trainer.get_training_setup()
        t_total = training_setup["t_total"]
        # `trainer.optimizer` is not None here, so no optimizer is created.
        trainer.create_optimizer_and_scheduler(num_training_steps=t_total)
    else:
        trainer.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(trainer.optimizer, lambda _: 1.)

    if privacy_args.non_private:
        privacy_args.noise_multiplier = 0.
        privacy_args.per_example_max_grad_norm = None
    else:
        total_train_batch_size = training_args.gradient_accumulation_steps * training_args.per_device_train_batch_size
        privacy_engine = PrivacyEngine(
            module=model,
            batch_size=total_train_batch_size,
            sample_size=len(train_dataset),
            epochs=training_args.num_train_epochs,
            max_grad_norm=privacy_args.per_example_max_grad_norm,
            noise_multiplier=privacy_args.noise_multiplier,
            target_epsilon=privacy_args.target_epsilon,
            target_delta=privacy_args.target_delta,
            accounting_mode=privacy_args.accounting_mode,
            clipping_mode=privacy_args.clipping_mode,
            skip_checks=True,
        )
        # Originally, it could have been null.
        privacy_args.noise_multiplier = privacy_engine.noise_multiplier
        privacy_args.target_delta = privacy_engine.target_delta

        print('privacy_args: ')
        print(json.dumps(privacy_args.__dict__, indent=4))
        privacy_engine.attach(optimizer)

    # Training
    if training_args.do_train:
        # Write argparse.
        utils.jdump(
            {**training_args.__dict__, **model_args.__dict__, **data_args.__dict__, **privacy_args.__dict__},
            os.path.join(training_args.output_dir, 'argparse.json'),
            default=lambda x: str(x),
        )
        print(data_args.mapping)
        print(data_args.template)

        # Don't reload.
        trainer.train(model_path=None)
        # Use the early stop, so do not save the model in the end (unless specify save_at_last)
        if training_args.save_at_last:
            trainer.save_model(training_args.output_dir)

        if trainer.is_world_process_zero():
            tokenizer.save_pretrained(training_args.output_dir)
            torch.save(model_args, os.path.join(training_args.output_dir, "model_args.bin"))
            torch.save(data_args, os.path.join(training_args.output_dir, "data_args.bin"))

    if training_args.do_eval or training_args.do_predict:
        # Reload the best checkpoint (for eval or predict).
        logger.info("*** Loading best checkpoint ***")
        model = model_fn.from_pretrained(training_args.output_dir)
        model = model.to(training_args.device)
        trainer.model = model
        if data_args.prompt:
            model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()
        if output_modes_mapping[data_args.task_name] == 'regression':
            # lower / upper bounds
            model.lb, model.ub = bound_mapping[data_args.task_name]
        model.model_args = model_args
        model.data_args = data_args
        model.tokenizer = tokenizer

    # Evaluation
    final_result = {'time': str(datetime.today())}

    eval_results = {}
    if training_args.do_eval:
        logger.info("*** Validate ***")

        eval_datasets = []
        eval_task_names = []
        eval_splits = []
        for split, dataset in zip(('dev', 'test'), (eval_dataset, test_dataset)):
            if split == "test" and not training_args.evaluate_test_split:
                continue

            eval_datasets.append(dataset)
            eval_task_names.append(data_args.task_name)
            eval_splits.append(split)

            # --- lxuechen: This block depends on `split`.
            if data_args.task_name == "mnli":
                mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
                eval_task_names.append(mnli_mm_data_args.task_name)
                eval_splits.append(split)
                if model_args.few_shot_type == "finetune":
                    mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode=split)
                    mnli_mm_dataset.num_sample = 1
                    eval_datasets.append(mnli_mm_dataset)
                else:
                    eval_datasets.append(
                        FewShotDataset(
                            mnli_mm_data_args, tokenizer=tokenizer, mode=split,
                            use_demo=('demo' in model_args.few_shot_type),
                        )
                    )
            # ---

        results_json = collections.defaultdict(dict)
        for eval_dataset, eval_task_name, eval_split in zip(eval_datasets, eval_task_names, eval_splits):
            trainer.compute_metrics = build_compute_metrics_fn(eval_dataset.args.task_name)
            output = trainer.evaluate(eval_dataset=eval_dataset)
            eval_result = output.metrics

            # --- lxuechen: My evaluation procedure.
            if eval_result is not None:
                if not privacy_args.non_private:
                    privacy_spent = privacy_engine.get_privacy_spent(accounting_mode="all", lenient=True)
                    to_record_dict = {**eval_result, **privacy_spent}
                else:
                    to_record_dict = eval_result

                if training_args.evaluate_test_split:
                    results_json[eval_split][eval_task_name] = to_record_dict
                else:
                    results_json[eval_task_name] = to_record_dict
            # ---

        output_path = os.path.join(training_args.output_dir, "final_results.json")
        utils.jdump(results_json, output_path)

    test_results = {}
    if training_args.do_predict:
        logging.info("*** Test ***")

        test_datasets = [test_dataset]
        if data_args.task_name == "mnli":
            mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
            if model_args.few_shot_type == "finetune":
                mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode="test")
                mnli_mm_dataset.num_sample = 1
                test_datasets.append(mnli_mm_dataset)
            else:
                test_datasets.append(
                    FewShotDataset(
                        mnli_mm_data_args,
                        tokenizer=tokenizer, mode="test", use_demo=('demo' in model_args.few_shot_type)
                    )
                )

        for test_dataset in test_datasets:
            trainer.compute_metrics = build_compute_metrics_fn(test_dataset.args.task_name)
            output = trainer.evaluate(eval_dataset=test_dataset)
            test_result = output.metrics

            output_test_file = os.path.join(
                training_args.output_dir, f"test_results_{test_dataset.args.task_name}.txt"
            )
            if trainer.is_world_process_zero():
                with open(output_test_file, "w") as writer:
                    logger.info("***** Test results {} *****".format(test_dataset.args.task_name))
                    for key, value in test_result.items():
                        logger.info("  %s = %s", key, value)
                        writer.write("%s = %s\n" % (key, value))
                        final_result[test_dataset.args.task_name + '_test_' + key] = value

                if training_args.save_logit:
                    predictions = output.predictions
                    num_logits = predictions.shape[-1]
                    logits = predictions.reshape([test_dataset.num_sample, -1, num_logits]).mean(axis=0)
                    np.save(os.path.join(training_args.save_logit_dir,
                                         "{}-{}-{}.npy".format(test_dataset.task_name, training_args.model_id,
                                                               training_args.array_id)), logits)

            test_results.update(test_result)

    with FileLock('log.lock'):
        with open('log', 'a') as f:
            final_result.update(vars(model_args))
            final_result.update(vars(training_args))
            final_result.update(vars(data_args))
            if 'evaluation_strategy' in final_result:
                final_result.pop('evaluation_strategy')
            f.write(str(final_result) + '\n')

    return eval_results


if __name__ == "__main__":
    main()

File Path: examples/classification/run_wrapper.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Wrapper launcher script."""

import os

import fire

from .src import common


def _get_command(
    task_name,
    output_dir,
    model_name_or_path,
    data_dir,
    learning_rate,
    clipping_mode: str,
    non_private,
    target_epsilon,
    few_shot_type,
    seed,
    attention_only,
    static_lm_head,
    static_embedding,
    randomly_initialize,
    per_device_train_batch_size,
    batch_size,
    num_train_epochs,
    eval_steps,
    eval_spectrum,
    max_spectrum_batches,
    max_lanczos_iter,
    store_grads,
    orthogonal_projection_path,
    orthogonal_projection_rank,
):
    task_name_to_factor = {
        "sst-2": 1, "qnli": 2, "qqp": 6, "mnli": 6,
    }
    factor = task_name_to_factor[task_name]

    if batch_size is None:
        base_batch_size = 1000
        # This batch size selection roughly ensures the sampling rates on different
        # datasets are in the same ballpark.
        batch_size = int(base_batch_size * factor)
    gradient_accumulation_steps = batch_size // per_device_train_batch_size

    if num_train_epochs is None:
        base_num_train_epochs = 3
        num_train_epochs = int(base_num_train_epochs * factor)

    if learning_rate is None:
        if non_private.lower() in ('yes', 'y', 'true', 't'):
            learning_rate = 5e-5
        else:
            learning_rate = 5e-4

    data_dir = f"{data_dir}/{common.task_name2suffix_name[task_name]}"
    template = {
        "sst-2": "*cls**sent_0*_It_was*mask*.*sep+*",
        "mnli": "*cls**sent-_0*?*mask*,*+sentl_1**sep+*",
        "qnli": "*cls**sent-_0*?*mask*,*+sentl_1**sep+*",
        "qqp": "*cls**sent-_0**mask*,*+sentl_1**sep+*",
    }[task_name]

    # Epochs chosen roughly to match e2e number of updates. We didn't hyperparameter tune on classification tasks :)
    cmd = f'''
python -m classification.run_classification \
  --task_name {task_name} \
  --data_dir {data_dir} \
  --output_dir {output_dir} \
  --overwrite_output_dir \
  --model_name_or_path {model_name_or_path} \
  --few_shot_type {few_shot_type} \
  --num_k 1 \
  --num_sample 1 --seed {seed} \
  --template {template} \
  --non_private {non_private} \
  --num_train_epochs {num_train_epochs} \
  --target_epsilon {target_epsilon} \
  --per_device_train_batch_size {per_device_train_batch_size} \
  --gradient_accumulation_steps {gradient_accumulation_steps} \
  --per_device_eval_batch_size 8 \
  --per_example_max_grad_norm 0.1 --clipping_mode {clipping_mode} \
  --learning_rate {learning_rate} \
  --lr_decay yes \
  --adam_epsilon 1e-08 \
  --weight_decay 0 \
  --max_seq_len 256 \
  --evaluation_strategy steps --eval_steps {eval_steps} --evaluate_before_training True \
  --do_train --do_eval \
  --first_sent_limit 200 --other_sent_limit 200 --truncate_head yes \
  --attention_only {attention_only} --static_lm_head {static_lm_head} --static_embedding {static_embedding} \
  --randomly_initialize {randomly_initialize} \
  --eval_spectrum {eval_spectrum} --max_spectrum_batches {max_spectrum_batches} --max_lanczos_iter {max_lanczos_iter} \
  --store_grads {store_grads}'''
    if orthogonal_projection_path is not None:
        cmd += f' --orthogonal_projection_path {orthogonal_projection_path}'
        cmd += f' --orthogonal_projection_rank {orthogonal_projection_rank}'
    return cmd


def main(
    output_dir,
    task_name,
    few_shot_type="prompt",
    seed=42,
    model_name_or_path="roberta-base",
    data_dir="classification/data/original",
    learning_rate=None,
    clipping_mode="ghost",
    non_private="no",
    target_epsilon=8,
    attention_only="no",
    static_lm_head="no",
    static_embedding="no",
    per_device_train_batch_size=20,
    eval_steps=10,
    eval_spectrum="no",
    max_spectrum_batches=2,
    max_lanczos_iter=2,
    randomly_initialize="no",
    batch_size=None,
    num_train_epochs=None,
    store_grads="no",
    orthogonal_projection_path=None,
    orthogonal_projection_rank=100,
):
    command = _get_command(
        output_dir=output_dir,
        task_name=task_name,
        model_name_or_path=model_name_or_path,
        data_dir=data_dir,
        learning_rate=learning_rate,
        clipping_mode=clipping_mode,
        non_private=non_private,
        target_epsilon=target_epsilon,
        few_shot_type=few_shot_type,
        seed=seed,
        attention_only=attention_only,
        static_lm_head=static_lm_head,
        static_embedding=static_embedding,
        per_device_train_batch_size=per_device_train_batch_size,
        eval_steps=eval_steps,
        eval_spectrum=eval_spectrum,
        max_spectrum_batches=max_spectrum_batches,
        max_lanczos_iter=max_lanczos_iter,
        randomly_initialize=randomly_initialize,
        batch_size=batch_size,
        num_train_epochs=num_train_epochs,
        store_grads=store_grads,
        orthogonal_projection_path=orthogonal_projection_path,
        orthogonal_projection_rank=orthogonal_projection_rank,
    )
    print('Running command:')
    print(command)
    os.system(command)


if __name__ == "__main__":
    fire.Fire(main)

File Path: examples/classification/spectral_analysis/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: examples/classification/spectral_analysis/density.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Code for converting Lanczos outputs to densities."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math

import numpy as np


def eigv_to_density(eig_vals, all_weights=None, grids=None,
                    grid_len=10000, sigma_squared=None, grid_expand=1e-2):
    """Compute the smoothed spectral density from a set of eigenvalues.

    Convolves the given eigenvalues with a Gaussian kernel, weighting the values
    by all_weights (or uniform weighting if all_weights is None). Example output
    can be seen in Figure 1 of https://arxiv.org/pdf/1901.10159.pdf. Visualizing
    the estimated density can be done by calling plt.plot(grids, density). There
    is likely not a best value of sigma_squared that works for all use cases,
    so it is recommended to try multiple values in the range [1e-5,1e-1].

    Args:
      eig_vals: Array of shape [num_draws, order]
      all_weights: Array of shape [num_draws, order], if None then weights will be
        taken to be uniform.
      grids: Array of shape [grid_len], the smoothed spectrum will be plotted
        in the interval [grids[0], grids[-1]]. If None then grids will be
        computed based on max and min eigenvalues and grid length.
      grid_len: Integer specifying number of grid cells to use, only used if
        grids is None
      sigma_squared: Scalar. Controls the smoothing of the spectrum estimate.
        If None, an appropriate value is inferred.
      grid_expand: Controls the window of values that grids spans.
        grids[0] = smallest eigenvalue - grid_expand.
        grids[-1] = largest_eigenvalue + grid_expand.

    Returns:
      density: Array of shape [grid_len], the estimated density, averaged over
        all draws.
      grids: Array of shape [grid_len]. The values the density is estimated on.
    """
    if all_weights is None:
        all_weights = np.ones(eig_vals.shape) * 1.0 / float(eig_vals.shape[1])
    num_draws = eig_vals.shape[0]

    lambda_max = np.nanmean(np.max(eig_vals, axis=1), axis=0) + grid_expand
    lambda_min = np.nanmean(np.min(eig_vals, axis=1), axis=0) - grid_expand

    if grids is None:
        assert grid_len is not None, 'grid_len is required if grids is None.'
        grids = np.linspace(lambda_min, lambda_max, num=grid_len)

    grid_len = grids.shape[0]
    if sigma_squared is None:
        sigma = 10 ** -5 * max(1, (lambda_max - lambda_min))
    else:
        sigma = sigma_squared * max(1, (lambda_max - lambda_min))

    density_each_draw = np.zeros((num_draws, grid_len))
    for i in range(num_draws):

        if np.isnan(eig_vals[i, 0]):
            raise ValueError('tridaig has nan values.')
        else:
            for j in range(grid_len):
                x = grids[j]
                vals = _kernel(eig_vals[i, :], x, sigma)
                density_each_draw[i, j] = np.sum(vals * all_weights[i, :])
    density = np.nanmean(density_each_draw, axis=0)
    norm_fact = np.sum(density) * (grids[1] - grids[0])
    density = density / norm_fact
    return density, grids


def tridiag_to_eigv(tridiag_list):
    """Preprocess the tridiagonal matrices for density estimation.

    Args:
      tridiag_list: Array of shape [num_draws, order, order] List of the
        tridiagonal matrices computed from running num_draws independent runs
        of lanczos. The output of this function can be fed directly into
        eigv_to_density.

    Returns:
      eig_vals: Array of shape [num_draws, order]. The eigenvalues of the
        tridiagonal matricies.
      all_weights: Array of shape [num_draws, order]. The weights associated with
        each eigenvalue. These weights are to be used in the kernel density
        estimate.
    """
    # Calculating the node / weights from Jacobi matrices.
    num_draws = len(tridiag_list)
    num_lanczos = tridiag_list[0].shape[0]
    eig_vals = np.zeros((num_draws, num_lanczos))
    all_weights = np.zeros((num_draws, num_lanczos))
    for i in range(num_draws):
        nodes, evecs = np.linalg.eigh(tridiag_list[i])
        index = np.argsort(nodes)
        nodes = nodes[index]
        evecs = evecs[:, index]
        eig_vals[i, :] = nodes
        all_weights[i, :] = evecs[0] ** 2
    return eig_vals, all_weights


def tridiag_to_density(tridiag_list, sigma_squared=1e-5, grid_len=10000):
    """This function estimates the smoothed density from the output of lanczos.

    Args:
      tridiag_list: Array of shape [num_draws, order, order] List of the
        tridiagonal matrices computed from running num_draws independent runs
        of lanczos.
      sigma_squared: Controls the smoothing of the density.
      grid_len: Controls the granularity of the density.

    Returns:
      density: Array of size [grid_len]. The smoothed density estimate averaged
        over all num_draws.
      grids: Array of size [grid_len]. The values the density estimate is on.
    """
    eig_vals, all_weights = tridiag_to_eigv(tridiag_list)
    density, grids = eigv_to_density(eig_vals, all_weights,
                                     grid_len=grid_len,
                                     sigma_squared=sigma_squared)
    return density, grids


def _kernel(x, x0, variance):
    """Point estimate of the Gaussian kernel.

    This function computes the Gaussian kernel for
    C exp(-(x - x0) ^2 /(2 * variance)) where C is the appropriate normalization.
    variance should be a list of length 1. Either x0 or x should be a scalar. Only
    one of the x or x0 can be a numpy array.

    Args:
      x: Can be either scalar or array of shape [order]. Points to estimate
        the kernel on.
      x0: Scalar. Mean of the kernel.
      variance: Scalar. Variance of the kernel.

    Returns:
      point_estimate: A scalar corresponding to
        C exp(-(x - x0) ^2 /(2 * variance)).
    """
    coeff = 1.0 / np.sqrt(2 * math.pi * variance)
    val = -(x0 - x) ** 2
    val = val / (2.0 * variance)
    val = np.exp(val)
    point_estimate = coeff * val
    return point_estimate

File Path: examples/classification/spectral_analysis/geometric_median.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Toy example on geometric median estimation in the paper.

python -m classification.spectral_analysis.geometric_median --img_dir "/mnt/disks/disk-2/dump/spectrum/geometric_median"
"""
import dataclasses
import logging
import math
import sys
from typing import Tuple

import fire
import numpy as np
import torch
import tqdm
from ml_swissknife import utils

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


@dataclasses.dataclass
class Data:
    beta_train: torch.Tensor
    beta_test: torch.Tensor
    Ar: torch.Tensor  # A^{1/2}.
    sensitivity: float

    def __post_init__(self):
        self.n_train, self.d = self.beta_train.size()
        self.n_test = self.beta_test.shape[0]


class Modes(metaclass=utils.ContainerMeta):
    const = "const"
    quarter = "quarter"
    sqrt = "sqrt"
    linear = "linear"
    quadratic = "quadratic"


def make_data(
    betas=None,
    n_train=100000, n_test=100000, d=10, dmin=1, mu_beta=0.2, si_beta=0.1,
    mode="linear",
    g0=1.,
):
    if betas is None:
        beta_train, beta_test = make_beta(
            n_train=n_train, n_test=n_test, d=d, dmin=dmin, mu_beta=mu_beta, si_beta=si_beta
        )
    else:
        beta_train, beta_test = betas
        n_train, d = beta_train.size()
        n_test, _ = beta_test.size()

    if mode == Modes.const:
        Ar = g0 * torch.arange(1, d + 1, device=device)
    elif mode == Modes.quarter:
        Ar = g0 * torch.arange(1, d + 1, device=device) ** -.25
    elif mode == Modes.sqrt:
        Ar = g0 * torch.arange(1, d + 1, device=device) ** -.5
    elif mode == Modes.linear:
        Ar = g0 * torch.arange(1, d + 1, device=device) ** -1.
    elif mode == Modes.quadratic:
        Ar = g0 * torch.arange(1, d + 1, device=device) ** -2.
    else:
        raise ValueError(f"Unknown mode: {mode}")

    sensitivity = 2 * g0 / n_train

    return Data(beta_train=beta_train, beta_test=beta_test, Ar=Ar, sensitivity=sensitivity)


def make_beta(n_train, n_test, d, dmin, mu_beta, si_beta):
    if d < dmin:
        raise ValueError(f"d < dmin")

    beta_train = mu_beta + torch.randn(size=(n_train, d), device=device) * si_beta
    beta_train[:, dmin:] = 0.  # Ensure init distance to opt is the same.

    beta_test = mu_beta + torch.randn(size=(n_test, d), device=device) * si_beta
    beta_test[:, dmin:] = 0.  # Same distribution as train.

    return beta_train, beta_test


def evaluate(data: Data, beta: torch.Tensor) -> Tuple:
    """Compute loss 1 / n sum_i | A^{1/2} (beta - beta_i) |_2 for train and test."""

    def compute_loss(samples):
        res = data.Ar[None, :] * (beta - samples)  # (n, d).
        return res.norm(2, dim=1).mean(dim=0).item()

    return tuple(
        compute_loss(samples=samples)
        for samples in (data.beta_train, data.beta_test)
    )


def train_one_step(data: Data, beta, lr, epsilon, delta, weight_decay):
    res = data.Ar[None, :] * (beta - data.beta_train)  # (n, d).
    grad = data.Ar * (res / res.norm(2, dim=1, keepdim=True)).mean(dim=0)

    gaussian_mechanism_variance = 2. * math.log(1.25 / delta) * data.sensitivity ** 2. / epsilon ** 2.
    grad_priv = grad + torch.randn_like(grad) * math.sqrt(gaussian_mechanism_variance)
    beta = beta - lr * (grad_priv + weight_decay * beta)
    return beta


@torch.no_grad()
def train(data: Data, num_steps, eval_steps, lr, weight_decay, epsilon, delta, tag, verbose, seed):
    utils.manual_seed(seed)

    per_step_epsilon, per_step_delta = make_per_step_privacy_spending(
        target_epsilon=epsilon, target_delta=delta, num_steps=num_steps
    )

    beta = torch.zeros(size=(1, data.d,), device=device)
    beta_avg = beta.clone()

    for global_step in range(0, num_steps):
        if global_step % eval_steps == 0:
            tr_loss, te_loss = evaluate(data=data, beta=beta_avg)
            if verbose:
                logging.warning(
                    f"tag: {tag}, global_step: {global_step}, lr: {lr:.6f}, num_steps: {num_steps}, "
                    f"train_loss: {tr_loss:.6f}, test_loss: {te_loss:.6f}"
                )

        beta = train_one_step(
            data=data,
            beta=beta,
            lr=lr, weight_decay=weight_decay,
            epsilon=per_step_epsilon, delta=per_step_delta,
        )
        beta_avg = beta_avg * global_step / (global_step + 1) + beta / (global_step + 1)

    final_tr_loss, final_te_loss = evaluate(data=data, beta=beta_avg)
    if verbose:
        logging.warning(
            f"tag: {tag}, final, lr: {lr:.6f}, num_steps: {num_steps}, "
            f"train_loss: {final_tr_loss:.6f}, te_loss: {final_te_loss:.6f}"
        )

    return beta_avg, (final_tr_loss, final_te_loss)


def make_per_step_privacy_spending(
    target_epsilon, target_delta, num_steps, threshold=1e-4,
):
    per_step_delta = target_delta / (num_steps + 1)

    def adv_composition(per_step_epsilon):
        total_epsilon = (
            math.sqrt(2 * num_steps * math.log(1 / per_step_delta)) * per_step_epsilon +
            num_steps * per_step_epsilon * (math.exp(per_step_epsilon) - 1)
        )
        return total_epsilon

    minval, maxval = 1e-6, 5
    while maxval - minval > threshold:
        midval = (maxval + minval) / 2
        eps = adv_composition(midval)
        if eps > target_epsilon:
            maxval = midval
        else:
            minval = midval
    per_step_epsilon = minval
    return per_step_epsilon, per_step_delta


def main(
    img_dir=None, eval_steps=10000, weight_decay=0, epsilon=2, delta=1e-6,
    n_train=10000, n_test=10000, dmin=1, mu_beta=1., si_beta=1, g0=3.,
    seeds=(42, 96, 10000, 999, 101),  # Some arbitrary numbers.
    modes=(Modes.const, Modes.sqrt, Modes.linear),  # A subset of all possible modes for visualization.
    verbose=False,
    quick=False,  # Use small data if True.
):
    if quick:
        dims = (10, 50,)
        num_steps_list = (10, 20,)
        lrs = (1e-4, 3e-4,)
    else:
        dims = (20, 50, 100, 200, 500, 1000, 2000)
        num_steps_list = (10, 20, 40, 80, 160, 320, 640, 1280, 2560, 5120)
        lrs = (1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3,)

    tr_losses = {mode: [] for mode in modes}
    te_losses = {mode: [] for mode in modes}
    for dim in tqdm.tqdm(dims, desc="dims"):
        betas = make_beta(n_train=n_train, n_test=n_test, d=dim, dmin=dmin, mu_beta=mu_beta, si_beta=si_beta)
        data = tuple(make_data(betas=betas, mode=mode, g0=g0) for mode in modes)

        tr_loss = {mode: [sys.maxsize] for mode in modes}
        te_loss = {mode: [sys.maxsize] for mode in modes}
        for this_data, this_mode in tqdm.tqdm(utils.zip_(data, modes), desc="modes", total=len(data)):

            # Hyperparameter tuning.
            for num_steps in num_steps_list:
                for lr in lrs:
                    kwargs = dict(
                        data=this_data,
                        num_steps=num_steps,
                        lr=lr,

                        eval_steps=eval_steps,
                        weight_decay=weight_decay,
                        epsilon=epsilon,
                        delta=delta,
                        tag=this_mode,
                        verbose=verbose,
                    )

                    tr_results = []
                    te_results = []
                    for seed in seeds:
                        _, (a, b) = train(**kwargs, seed=seed)
                        tr_results.append(a)
                        te_results.append(b)

                    if np.mean(tr_results) < np.mean(tr_loss[this_mode]):
                        tr_loss[this_mode] = tr_results
                        te_loss[this_mode] = te_results

        # update after hp tuning.
        for this_mode in modes:
            tr_losses[this_mode].append(tr_loss[this_mode])
            te_losses[this_mode].append(te_loss[this_mode])

    raw_data = dict(tr_losses=tr_losses, te_losses=te_losses, modes=modes, dims=dims)

    if img_dir is not None:
        utils.jdump(raw_data, utils.join(img_dir, 'toyplot.json'))

        plot_modes = modes
        linestyles = ("-", "--", ":", "-.")
        markers = ("o", "+", "x", "^")

        tr_plotting = dict(
            errorbars=tuple(
                dict(
                    x=dims,
                    y=np.mean(np.array(tr_losses[this_mode]), axis=1),
                    yerr=np.std(np.array(tr_losses[this_mode]), axis=1),
                    label=this_mode, marker=markers[mode_idx],
                    linestyle=linestyles[mode_idx]
                )
                for mode_idx, this_mode in enumerate(plot_modes)
            ),
            options=dict(xlabel="$d$", ylabel="train loss")
        )
        utils.plot_wrapper(
            img_path=utils.join(img_dir, 'trplot'),
            suffixes=('.png', '.pdf'),
            **tr_plotting,
        )

        te_plotting = dict(
            errorbars=tuple(
                dict(
                    x=dims,
                    y=np.mean(np.array(te_losses[this_mode]), axis=1),
                    yerr=np.std(np.array(te_losses[this_mode]), axis=1),
                    label=this_mode, marker=markers[mode_idx],
                    linestyle=linestyles[mode_idx]
                )
                for mode_idx, this_mode in enumerate(plot_modes)
            ),
            options=dict(xlabel="$d$", ylabel="test loss")
        )
        utils.plot_wrapper(
            img_path=utils.join(img_dir, 'teplot'),
            suffixes=('.png', '.pdf'),
            **te_plotting,
        )


if __name__ == "__main__":
    fire.Fire(main)

File Path: examples/classification/spectral_analysis/rebuttal_neurips_2022.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Experiments ran pre- and post-rebuttals."""
import logging
import os
from typing import Optional

import fire
import torch
import tqdm
from ml_swissknife import utils, numerical_distributed
from torch.utils.data import DataLoader, TensorDataset


def run_save_grads(
    num_train_epochs=60,  # This amounts to 4k updates, roughly.
    model_name_or_path="roberta-base",
    train_dir=None,
    per_device_train_batch_size=25,
):
    if train_dir is None:
        train_dir = utils.join("/mnt/data1/dump/", 'rebuttal_v2', f'run-{model_name_or_path}')
    command = f'''python -m classification.run_wrapper \
        --output_dir {train_dir} \
        --task_name "sst-2" \
        --model_name_or_path "{model_name_or_path}" \
        --attention_only "yes" \
        --static_lm_head "yes" \
        --num_train_epochs {num_train_epochs} \
        --eval_spectrum "no" \
        --non_private "no" \
        --eval_steps 50 \
        --randomly_initialize "no" \
        --per_device_train_batch_size {per_device_train_batch_size} \
        --batch_size 1000 \
        --clipping_mode "default" \
        --store_grads "yes"'''
    os.system(command)


def run_pca(
    # Place where grads are stored and where results will be stored.
    train_dir="/mnt/disks/disk-2/dump/privlm/roberta/sst-2",
    n=2000,  # How many checkpoints?
    k=1000,  # How many eigenvectors?
    num_power_iteration=10,
    batch_size=20,  # Batch size for processing the checkpoints in matmul.
    seed=42,  # Controls randomness in sampling the first vector in orthogonal iteration.
    start_index=0,  # The index of the first checkpoint to be selected.
    eval_steps=5,  # Evaluate PCA accuracy once this many iterations.
    save_steps=5,  # Save eigenvalue and eigenvector tensors once this many iterations.
    disable_tqdm=False,
    dtype="float",  # String repr of dtype.
):
    utils.manual_seed(seed)

    ckpt_dir = utils.join(train_dir, 'grad_trajectory')
    dump_dir = utils.join(train_dir, 'orthproj')

    all_ckpts = utils.all_ckpts(ckpt_dir, sort=True)
    tgt_ckpts = all_ckpts[start_index:start_index + n]
    dataset = torch.stack([
        torch.load(ckpt_path)["flat_grad"] for ckpt_path in tqdm.tqdm(tgt_ckpts, desc="load data")
    ]).to(utils.get_dtype(dtype))
    input_mat = DataLoader(dataset=TensorDataset(dataset), batch_size=batch_size)

    def callback(global_step, eigenvalues, eigenvectors):
        if global_step % save_steps == 0:
            utils.tsave(
                dict(eigenvalues=eigenvalues, eigenvectors=eigenvectors),
                utils.join(dump_dir, "all", f"global_step_{global_step:06d}.pt")
            )
            utils.tsave(
                dict(eigenvalues=eigenvalues),
                utils.join(dump_dir, "eigenvalues", f"global_step_{global_step:06d}.evals")
            )
        if global_step % eval_steps == 0:
            err_abs, err_rel = numerical_distributed.check_error(
                input_mat=input_mat, eigenvectors=eigenvectors, disable_tqdm=disable_tqdm
            )
            logging.warning(f"global_step: {global_step}, abs error: {err_abs:.6f}, rel error: {err_rel:.6f}")

    numerical_distributed.orthogonal_iteration(
        input_mat=input_mat,
        k=k,
        num_power_iteration=num_power_iteration,
        callback=callback,
        disable_tqdm=disable_tqdm,
    )


def run_retrain_single(
    output_dir: str,
    orthogonal_projection_path: str,
    model_name_or_path: str,
    rank: Optional[int] = None,
    seed=42,
):
    cmd = f'''python -m classification.run_wrapper \
        --output_dir {output_dir} \
        --task_name "sst-2" \
        --model_name_or_path {model_name_or_path} \
        --few_shot_type "prompt" \
        --attention_only "yes" \
        --static_lm_head "yes" \
        --per_device_train_batch_size 25 \
        --batch_size 1000 \
        --clipping_mode "default" \
        --num_train_epochs 4 \
        --eval_spectrum "no" \
        --non_private "no" \
        --eval_steps 25 \
        --randomly_initialize "no" \
        --seed {seed} \
        --orthogonal_projection_path {orthogonal_projection_path}'''
    if rank is not None:
        cmd += f' --orthogonal_projection_rank {rank}'
    os.system(cmd)


def main(task, **kwargs):
    globals()[task](**kwargs)


if __name__ == "__main__":
    fire.Fire(main)

File Path: examples/classification/spectral_analysis/rebuttal_plots_neurips_2022.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Plot 1) spectral decay, 2) retrain curves.
"""

import math

import fire
import numpy as np
import scipy.stats
import torch
from ml_swissknife import utils

from . import density


def plot1(
    ckpt_path: str,  # Path to eigenvalues.
    dump_dir="./classification/plots",
    img_name="",
    k=500,
    **kwargs,
):
    """Eigenvalues.

    Run on gvm.
    """
    # Roberta-large
    # python -m classification.spectral_analysis.rebuttal_plots_neurips_2022 --task "plot1" --ckpt_path "/mnt/data1/dump/rebuttal/run-roberta-large/orthproj/eigenvalues/global_step_000005.evals" --img_name "large" --k 100
    if img_name != "":
        img_name = f'-{img_name}'

    state_dicts = torch.load(ckpt_path)
    eigenvalues = state_dicts["eigenvalues"].numpy()
    eigenvalues = -np.sort(-eigenvalues)
    k = min(k, len(eigenvalues))

    # Linear fit.
    x = np.arange(1, k + 1)
    g = np.sqrt(eigenvalues[:k])
    logg = np.log(g)
    logx = np.log(x)

    linfit = scipy.stats.linregress(logx, logg)
    g_linfit = np.exp(logx * linfit.slope + linfit.intercept)

    print("slope:", linfit.slope)
    print("R value:", linfit.rvalue)

    plots = [
        dict(x=x, y=g, marker='+', linewidth=0, label="estimated values", markersize=8, alpha=0.8),
        dict(x=x, y=g_linfit,
             label=f"linear fit: $\log y = {linfit.slope:.2f} \log x {linfit.intercept:.2f} $ ($R^2="
                   f"{linfit.rvalue ** 2.:.3f}$)"),
    ]
    utils.plot_wrapper(
        img_path=utils.join(dump_dir, f"eigenvalue-linfit{img_name}"),
        suffixes=(".png", ".pdf"),
        plots=plots,
        options=dict(xlabel="$k$", ylabel="$\lambda(H^\\top H)^{1/2}$", xscale='log', yscale='log')
    )

    # Spectral density.
    sigma_squared = 1e-6
    evals = np.sqrt(eigenvalues[None, :k])
    den, gri = density.eigv_to_density(evals, sigma_squared=sigma_squared, grid_len=300000, grid_expand=3e-4)
    utils.plot_wrapper(
        img_path=utils.join(dump_dir, f'eigenvalue-density{img_name}'),
        suffixes=(".png", ".pdf"),
        plots=[dict(x=gri, y=den, label=f"bandwidth $\sigma={math.sqrt(sigma_squared):.5f}$")],
        options=dict(xlabel="$\lambda(H^\\top H)^{1/2}$", ylabel="Density of KDE",
                     ylim=dict(bottom=1e-10, top=2e2),
                     xscale="log", yscale='log')
    )


def plot2(
    base_dir: str,
    img_name="",
    seeds=(42, 9008, 0),
    ranks=(10, 20, 100, None),
    dump_dir="./classification/plots",
    markers=('x', '^', '+', 'o'),
    roberta_large=False,
    **kwargs,
):
    """Retrain.

    Run locally.
    """
    # Roberta-large
    # python -m classification.spectral_analysis.rebuttal_plots_neurips_2022 --task "plot2" --img_name "large" --base_dir "/mnt/data1/dump/rebuttal" --roberta_large True
    if img_name != "":
        img_name = f'-{img_name}'

    errorbars = []
    for rank, marker in utils.zip_(ranks, markers):
        results = []
        for seed in seeds:
            if roberta_large:
                output_dir = utils.join(
                    f"{base_dir}/roberta_prompt_large_retrain_{rank}_{seed}/sst-2",
                    'log_history.json'
                )
            else:
                output_dir = utils.join(
                    f"{base_dir}/roberta_prompt_retrain_{rank}_{seed}/sst-2",
                    'log_history.json'
                )
            record = utils.jload(output_dir)
            results.append([dumpi['dev']['eval_acc'] for dumpi in record])
            steps = [dumpi['step'] for dumpi in record]

        label = f"subspace rank={rank}" if rank is not None else "original"
        mu, si = utils.average_over_seed(results)
        errorbar = dict(x=steps, y=mu, yerr=si, label=label, marker=marker)
        errorbars.append(errorbar)

    img_path = utils.join(dump_dir, f'plot2{img_name}')
    utils.plot_wrapper(
        img_path=img_path,
        suffixes=('.png', '.pdf'),
        errorbars=errorbars,
        options=dict(xlabel="iteration", ylabel="SST-2 classification accuracy (dev)")
    )


def plot_all(**kwargs):
    # rebuttal roberta-base experiments.
    # python -m classification.spectral_analysis.rebuttal_plots_neurips_2022 --task "plot_all" --base_dir "/mnt/data1/dump/rebuttal" --ckpt_path "/mnt/data1/dump/rebuttal/run-roberta-base/orthproj/eigenvalues/global_step_000010.evals"
    plot1(**kwargs)
    plot2(**kwargs)


def main(task="plot_all", **kwargs):
    utils.runs_tasks(
        task=task,
        task_names=("plot_all", "plot1", "plot2"),
        task_callables=(plot_all, plot1, plot2),
        **kwargs,
    )


if __name__ == "__main__":
    fire.Fire(main)

File Path: examples/classification/src/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: examples/classification/src/common.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch

task_name2suffix_name = {"sst-2": "GLUE-SST-2", "mnli": "MNLI", "qqp": "QQP", "qnli": "QNLI"}
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
true_tags = ('y', 'yes', 't', 'true')

File Path: examples/classification/src/compiled_args.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass, field

import transformers

from .common import true_tags
from typing import Optional


@dataclass
class PrivacyArguments:
    """Arguments for differentially private training."""

    per_example_max_grad_norm: float = field(
        default=.1, metadata={
            "help": "Clipping 2-norm of per-sample gradients."
        }
    )
    noise_multiplier: float = field(
        default=None, metadata={
            "help": "Standard deviation of noise added for privacy; if `target_epsilon` is specified, "
                    "use the one searched based budget"
        }
    )
    target_epsilon: float = field(
        default=None, metadata={
            "help": "Privacy budget; if `None` use the noise multiplier specified."
        }
    )
    target_delta: float = field(
        default=None, metadata={
            "help": "Lax probability in approximate differential privacy; if `None` use 1 / len(train_data)."
        }
    )
    non_private: str = field(
        default="yes", metadata={"help": "Train non-privately if True."}
    )
    accounting_mode: str = field(
        default="rdp", metadata={"help": "One of (`rdp`, `glw`, `all`)."}
    )
    clipping_mode: str = field(
        default="default"
    )

    def __post_init__(self):
        self.non_private = self.non_private.lower() in true_tags  # noqa


@dataclass
class TrainingArguments(transformers.TrainingArguments):
    eval_epochs: int = field(default=10, metadata={"help": "Evaluate once such epochs"})
    evaluate_before_training: bool = field(default=False, metadata={"help": "Run evaluation before training."})
    lr_decay: str = field(
        default="no", metadata={"help": "Apply the usual linear decay if `yes`, otherwise no deacy."}
    )
    evaluate_test_split: bool = field(default=False, metadata={"help": "Run evaluation on the test split"})

    def __post_init__(self):
        super(TrainingArguments, self).__post_init__()
        self.lr_decay = self.lr_decay.lower() in true_tags  # noqa


@dataclass
class AuxiliaryArguments:
    eval_spectrum: str = field(default="no")
    max_spectrum_batches: int = field(default=100)
    max_lanczos_iter: int = field(default=100)

    store_grads: str = field(default="no")
    orthogonal_projection_path: Optional[str] = field(default=None)
    orthogonal_projection_rank: int = field(default=100)

    def __post_init__(self):
        self.eval_spectrum = self.eval_spectrum.lower() in true_tags  # noqa
        self.store_grads = self.store_grads.lower() in true_tags  # noqa

File Path: examples/classification/src/dataset.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Dataset utils for different data settings for GLUE."""

import dataclasses
from dataclasses import dataclass
import json
import logging
import os
import time
from typing import List, Optional, Union

from filelock import FileLock
import numpy as np
import pandas as pd
from sentence_transformers import util
import torch
import tqdm
from transformers.data.processors.utils import InputFeatures

from .processors import processors_mapping, median_mapping

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class OurInputFeatures(InputFeatures):
    """
    Inherit from Transformers' InputFeatuers.
    """

    input_ids: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    label: Optional[Union[int, float]] = None
    mask_pos: Optional[List[int]] = None  # Position of the mask token
    label_word_list: Optional[List[int]] = None  # Label word mapping (dynamic)

    def to_json_string(self):
        """Serializes this instance to a JSON string."""
        return json.dumps(dataclasses.asdict(self)) + "\n"


def input_example_to_string(example, sep_token):
    if example.text_b is None:
        return example.text_a
    else:
        # Warning: very simple hack here
        return example.text_a + ' ' + sep_token + ' ' + example.text_b


def input_example_to_tuple(example):
    if example.text_b is None:
        if pd.isna(example.text_a) or example.text_a is None:
            return ['']
            logger.warn("Empty input")
        else:
            return [example.text_a]
    else:
        return [example.text_a, example.text_b]


def tokenize_multipart_input(
    input_text_list,
    max_length,
    tokenizer,
    prompt: bool = False,
    template: Optional[str] = None,
    label_word_list=None,
    first_sent_limit: Optional[int] = None,
    other_sent_limit: Optional[int] = None,
    truncate_head=False,
    support_labels=None,

    # lxuechen: Not sure why these were included originally.
    task_name=None,
    gpt3=False,

    # lxuechen: For checking the dataset.
    early_return=False,
):
    """Tokenize (potentially multiple) sentences according to a potential pattern.

    Args:
        input_text_list: A list of strings.
        max_length: Maximum length of the overall output id list.
        tokenizer: HF tokenizer object.
        prompt (bool): Tokenize the sentences according to the pattern described in `template` if True.
        template (str): The pattern.
        label_word_list (list): A list of strings for words that are labels.
        first_sent_limit (int): Maximum length the first sentence should occupy.
        other_sent_limit (int): Maximum length the other sentence should occupy in the output.
        truncate_head (bool): If True, remove some head tokens when the list of tokenized ids is longer than the limit.
        support_labels: Only useful in gpt3 setting.

    Returns:
        A dictionary describing the current example with keys 'input_ids', 'attention_mask', 'mask_pos'.
    """

    def enc(text):
        return tokenizer.encode(text, add_special_tokens=False)

    input_ids = []
    attention_mask = []
    token_type_ids = []  # Only for BERT
    mask_pos = None  # Position of the mask token

    if prompt:
        """
        Concatenate all sentences and prompts based on the provided template.
        Template example: '*cls*It was*mask*.*sent_0**<sep>*label_0:*sent_1**<sep>**label_1*:*sent_2**<sep>*'
        *xx* represent variables:
            *cls*: cls_token
            *mask*: mask_token
            *sep*: sep_token
            *sep+*: sep_token, also means +1 for segment id
            *sent_i*: sentence i (input_text_list[i])
            *sent-_i*: same as above, but delete the last token
            *sentl_i*: same as above, but use lower case for the first word
            *sentl-_i*: same as above, but use lower case for the first word and delete the last token
            *+sent_i*: same as above, but add a space before the sentence
            *+sentl_i*: same as above, but add a space before the sentence and use lower case for the first word
            *label_i*: label_word_list[i]
            *label_x*: label depends on the example id (support_labels needed). this is only used in GPT-3's 
            in-context learning

        Use "_" to replace space.
        PAY ATTENTION TO SPACE!! DO NOT leave space before variables, for this will lead to extra space token.
        """
        assert template is not None

        special_token_mapping = {
            'cls': tokenizer.cls_token_id,
            'mask': tokenizer.mask_token_id,
            'sep': tokenizer.sep_token_id,
            'sep+': tokenizer.sep_token_id,
        }
        template_list = template.split('*')  # Get variable list in the template
        segment_id = 0  # Current segment id. Segment id +1 if encountering sep+.

        for part_id, part in enumerate(template_list):
            new_tokens = []
            segment_plus_1_flag = False
            if part in special_token_mapping:
                if part == 'cls' and 'T5' in type(tokenizer).__name__:
                    # T5 does not have cls token
                    continue
                new_tokens.append(special_token_mapping[part])
                if part == 'sep+':
                    segment_plus_1_flag = True
            elif part[:6] == 'label_':
                # Note that label_word_list already has extra space, so do not add more space ahead of it.
                label_id = int(part.split('_')[1])
                label_word = label_word_list[label_id]
                new_tokens.append(label_word)
            elif part[:7] == 'labelx_':
                instance_id = int(part.split('_')[1])
                label_id = support_labels[instance_id]
                label_word = label_word_list[label_id]
                new_tokens.append(label_word)
            elif part[:5] == 'sent_':
                sent_id = int(part.split('_')[1])
                new_tokens += enc(input_text_list[sent_id])
            elif part[:6] == '+sent_':
                # Add space
                sent_id = int(part.split('_')[1])
                new_tokens += enc(' ' + input_text_list[sent_id])
            elif part[:6] == 'sent-_':
                # Delete the last token
                sent_id = int(part.split('_')[1])
                new_tokens += enc(input_text_list[sent_id][:-1])
            elif part[:6] == 'sentl_':
                # Lower case the first token
                sent_id = int(part.split('_')[1])
                text = input_text_list[sent_id]
                text = text[:1].lower() + text[1:]
                new_tokens += enc(text)
            elif part[:7] == '+sentl_':
                # Lower case the first token and add space 
                sent_id = int(part.split('_')[1])
                text = input_text_list[sent_id]
                text = text[:1].lower() + text[1:]
                new_tokens += enc(' ' + text)
            elif part[:7] == 'sentl-_':
                # Lower case the first token and discard the last token
                sent_id = int(part.split('_')[1])
                text = input_text_list[sent_id]
                text = text[:1].lower() + text[1:]
                new_tokens += enc(text[:-1])
            elif part[:6] == 'sentu_':
                # Upper case the first token
                sent_id = int(part.split('_')[1])
                text = input_text_list[sent_id]
                text = text[:1].upper() + text[1:]
                new_tokens += enc(text)
            elif part[:7] == '+sentu_':
                # Upper case the first token and add space
                sent_id = int(part.split('_')[1])
                text = input_text_list[sent_id]
                text = text[:1].upper() + text[1:]
                new_tokens += enc(' ' + text)
            else:
                # Just natural language prompt
                part = part.replace('_', ' ')
                # handle special case when T5 tokenizer might add an extra space
                if len(part) == 1:
                    new_tokens.append(tokenizer._convert_token_to_id(part))
                else:
                    new_tokens += enc(part)

            if part[:4] == 'sent' or part[1:5] == 'sent':
                # If this part is the sentence, limit the sentence length
                sent_id = int(part.split('_')[1])
                if sent_id == 0:
                    if first_sent_limit is not None:
                        new_tokens = new_tokens[:first_sent_limit]
                else:
                    if other_sent_limit is not None:
                        new_tokens = new_tokens[:other_sent_limit]

            input_ids += new_tokens
            attention_mask += [1 for i in range(len(new_tokens))]
            token_type_ids += [segment_id for i in range(len(new_tokens))]

            if segment_plus_1_flag:
                segment_id += 1
    else:
        input_ids = [tokenizer.cls_token_id]
        attention_mask = [1]
        token_type_ids = [0]

        for sent_id, input_text in enumerate(input_text_list):
            if input_text is None:
                # Do not have text_b
                continue
            if pd.isna(input_text) or input_text is None:
                # Empty input
                input_text = ''
            input_tokens = enc(input_text) + [tokenizer.sep_token_id]
            input_ids += input_tokens
            attention_mask += [1 for i in range(len(input_tokens))]
            token_type_ids += [sent_id for i in range(len(input_tokens))]

        if 'T5' in type(tokenizer).__name__:  # T5 does not have CLS token
            input_ids = input_ids[1:]
            attention_mask = attention_mask[1:]
            token_type_ids = token_type_ids[1:]

    if early_return:
        return input_ids

    # Padding
    if first_sent_limit is not None and len(input_ids) > max_length:
        # If using sentence limit, the total length still exceeds the maximum limit, report a warning
        logger.warn("Input exceeds max_length limit: {}".format(tokenizer.decode(input_ids)))

    len_input_ids = len(input_ids)
    if len_input_ids < max_length:
        input_ids.extend([tokenizer.pad_token_id] * (max_length - len_input_ids))
        attention_mask.extend([0] * (max_length - len_input_ids))
        token_type_ids.extend([0] * (max_length - len_input_ids))

    # Truncate
    if len(input_ids) > max_length:
        if truncate_head:
            input_ids = input_ids[-max_length:]
            attention_mask = attention_mask[-max_length:]
            token_type_ids = token_type_ids[-max_length:]
        else:
            # Default is to truncate the tail
            input_ids = input_ids[:max_length]
            attention_mask = attention_mask[:max_length]
            token_type_ids = token_type_ids[:max_length]

    # Find mask token
    if prompt:
        mask_pos = [input_ids.index(tokenizer.mask_token_id)]
        # Make sure that the masked position is inside the max_length
        assert mask_pos[0] < max_length

    result = {'input_ids': input_ids, 'attention_mask': attention_mask}
    if 'BERT' in type(tokenizer).__name__:
        # Only provide token type ids for BERT
        result['token_type_ids'] = token_type_ids

    if prompt:
        result['mask_pos'] = mask_pos

    return result


class FewShotDataset(torch.utils.data.Dataset):
    """Few-shot dataset."""

    def __init__(self, args, tokenizer, cache_dir=None, mode="train", use_demo=False):
        self.args = args
        self.task_name = args.task_name
        # call .get_train_examples, .get_dev_examples, .get_test_examples here.
        self.processor = processors_mapping[args.task_name]
        self.tokenizer = tokenizer
        self.mode = mode

        # If not using demonstrations, use use_demo=True
        self.use_demo = use_demo
        if self.use_demo:
            logger.info("Use demonstrations")
        assert mode in ["train", "dev", "test"]

        # Get label list and (for prompt) label word list
        self.label_list = self.processor.get_labels()
        self.num_labels = len(self.label_list)
        if args.prompt:
            assert args.mapping is not None
            self.label_to_word = eval(args.mapping)

            for key in self.label_to_word:
                # For RoBERTa/BART/T5, tokenization also considers space, so we use space+word as label words.
                if self.label_to_word[key][0] not in ['<', '[', '.', ',']:
                    # Make sure space+word is in the vocabulary
                    assert len(tokenizer.tokenize(' ' + self.label_to_word[key])) == 1

                    self.label_to_word[key] = tokenizer._convert_token_to_id(
                        tokenizer.tokenize(' ' + self.label_to_word[key])[0]
                    )
                else:
                    self.label_to_word[key] = tokenizer._convert_token_to_id(self.label_to_word[key])
                logger.info(
                    "Label {} to word {} ({})".format(
                        key, tokenizer._convert_id_to_token(self.label_to_word[key]), self.label_to_word[key]
                    )
                )

            if len(self.label_list) > 1:
                self.label_word_list = [self.label_to_word[label] for label in self.label_list]
            else:
                # Regression task
                # '0' represents low polarity and '1' represents high polarity.
                self.label_word_list = [self.label_to_word[label] for label in ['0', '1']]
        else:
            self.label_to_word = None
            self.label_word_list = None

        # Multiple sampling: when using demonstrations, we sample different combinations of demonstrations during 
        # inference and aggregate the results by averaging the logits. The number of different samples is num_sample.
        if (mode == "train") or not self.use_demo:
            # We do not do multiple sampling when not using demonstrations or when it's the training mode 
            self.num_sample = 1
        else:
            # lxuechen: (Section 6.1) At testing time, we still sample demonstration sets from Dtrain
            # and ensemble predictions across all sets.
            self.num_sample = args.num_sample

        # If we use multiple templates, we also need to do multiple sampling during inference.
        if args.prompt and args.template_list is not None:
            logger.info("There are %d templates. Multiply num_sample by %d" % (
                len(args.template_list), len(args.template_list)))
            self.num_sample *= len(args.template_list)

        logger.info("Total num_sample for mode %s: %d" % (mode, self.num_sample))

        # Load cache
        # Cache name distinguishes mode, task name, tokenizer, and length. So if you change anything beyond these
        # elements, make sure to clear your cache.
        # --- lxuechen: Change cache name to distinguish between original GLUE.
        cached_features_file = os.path.join(
            cache_dir if cache_dir is not None else args.data_dir,
            "cached_{}_{}_{}_{}_few_shot".format(
                mode,
                tokenizer.__class__.__name__,
                str(args.max_seq_length),
                args.task_name,
            ),
        )
        # ---

        logger.info(f"Creating/loading examples from dataset file at {args.data_dir}")

        lock_path = cached_features_file + ".lock"
        with FileLock(lock_path):

            if os.path.exists(cached_features_file) and not args.overwrite_cache:
                start = time.time()
                self.support_examples, self.query_examples = torch.load(cached_features_file)
                logger.info(
                    f"Loading features from cached file {cached_features_file} [took %.3f s]", time.time() - start
                )
            else:
                logger.info(f"Creating features from dataset file at {args.data_dir}")

                # The support examples are sourced from the training set.
                self.support_examples = self.processor.get_train_examples(args.data_dir)

                if mode == "dev":
                    self.query_examples = self.processor.get_dev_examples(args.data_dir)
                elif mode == "test":
                    self.query_examples = self.processor.get_test_examples(args.data_dir)
                else:
                    self.query_examples = self.support_examples

                start = time.time()
                torch.save([self.support_examples, self.query_examples], cached_features_file)
                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.
                logger.info(
                    "Saving features into cached file %s [took %.3f s]", cached_features_file, time.time() - start
                )

        # For filtering in using demonstrations, load pre-calculated embeddings
        if self.use_demo and args.demo_filter:
            if mode == 'train':
                split_name = 'train'
            elif mode == 'dev':
                if args.task_name == 'mnli':
                    split_name = 'dev_matched'
                elif args.task_name == 'mnli-mm':
                    split_name = 'dev_mismatched'
                else:
                    split_name = 'dev'
            elif mode == 'test':
                if args.task_name == 'mnli':
                    split_name = 'test_matched'
                elif args.task_name == 'mnli-mm':
                    split_name = 'test_mismatched'
                else:
                    split_name = 'test'
            else:
                raise NotImplementedError

            self.support_emb = np.load(os.path.join(args.data_dir, "train_{}.npy".format(args.demo_filter_model)))
            self.query_emb = np.load(
                os.path.join(args.data_dir, "{}_{}.npy".format(split_name, args.demo_filter_model))
            )
            logger.info("Load embeddings (for demonstration filtering) from {}".format(
                os.path.join(args.data_dir, "{}_{}.npy".format(split_name, args.demo_filter_model)))
            )

            assert len(self.support_emb) == len(self.support_examples)
            assert len(self.query_emb) == len(self.query_examples)

        # Size is expanded by num_sample
        # --- lxuechen: Don't expand when you force not to use demo during inference.
        if args.inference_time_demo:
            self.size = len(self.query_examples) * self.num_sample
        else:
            self.size = len(self.query_examples)
        # ---

        # Prepare examples (especially for using demonstrations)
        support_indices = list(range(len(self.support_examples)))  # [0, ..., len(train)].
        self.example_idx = []
        for sample_idx in range(self.num_sample):
            for query_idx in tqdm.tqdm(
                range(len(self.query_examples)), desc=f"Queries for {self.mode}"
            ):
                # If training, exclude the current example. Else keep all.
                if self.use_demo and args.demo_filter:
                    # Demonstration filtering
                    candidate = [support_idx for support_idx in support_indices
                                 if support_idx != query_idx or mode != "train"]
                    sim_score = []
                    for support_idx in candidate:
                        sim_score.append(
                            (
                                support_idx,
                                util.pytorch_cos_sim(self.support_emb[support_idx], self.query_emb[query_idx])
                            )
                        )
                    sim_score.sort(key=lambda x: x[1], reverse=True)
                    if self.num_labels == 1:
                        # Regression task
                        limit_each_label = int(len(sim_score) // 2 * args.demo_filter_rate)
                        count_each_label = {'0': 0, '1': 0}
                        context_indices = []

                        if args.debug_mode:
                            print("Query %s: %s" % (
                                self.query_examples[query_idx].label, self.query_examples[query_idx].text_a))  # debug
                        for support_idx, score in sim_score:
                            if count_each_label[
                                '0' if float(self.support_examples[support_idx].label) <= median_mapping[
                                    args.task_name] else '1'] < limit_each_label:
                                count_each_label[
                                    '0' if float(self.support_examples[support_idx].label) <= median_mapping[
                                        args.task_name] else '1'] += 1
                                context_indices.append(support_idx)
                                if args.debug_mode:
                                    print("    %.4f %s | %s" % (score, self.support_examples[support_idx].label,
                                                                self.support_examples[support_idx].text_a))  # debug
                    else:
                        limit_each_label = int(len(sim_score) // self.num_labels * args.demo_filter_rate)
                        count_each_label = {label: 0 for label in self.label_list}
                        context_indices = []

                        if args.debug_mode:
                            print("Query %s: %s" % (
                                self.query_examples[query_idx].label, self.query_examples[query_idx].text_a))  # debug
                        for support_idx, score in sim_score:
                            if count_each_label[self.support_examples[support_idx].label] < limit_each_label:
                                count_each_label[self.support_examples[support_idx].label] += 1
                                context_indices.append(support_idx)
                                if args.debug_mode:
                                    print("    %.4f %s | %s" % (score, self.support_examples[support_idx].label,
                                                                self.support_examples[support_idx].text_a))  # debug
                else:
                    # Using demonstrations without filtering

                    # --- lxuechen: This shit code creates a list `context_indices` of size the
                    #   training set for each example. No wonder it gives RAM errors...
                    #   Patching memory error...
                    if not use_demo and not args.inference_time_demo:
                        context_indices = []
                    else:
                        context_indices = [
                            support_idx for support_idx in support_indices
                            if support_idx != query_idx or mode != "train"
                        ]
                    # ---

                # We'll subsample context_indices further later.
                self.example_idx.append((query_idx, context_indices, sample_idx))

        # If it is not training, we pre-process the data; otherwise, we process the data online.
        # For any test/eval example, we sample one context for each class label;
        # this procedure is repeated num_sample times.
        # TODO(lxuechen): When does self.features get used?
        # --- lxuechen: Don't create features if don't use demo at inference time
        if mode != "train" and args.inference_time_demo:
            # ---
            self.features = []
            _ = 0
            for query_idx, context_indices, bootstrap_idx in self.example_idx:
                # The input (query) example
                example = self.query_examples[query_idx]
                # The demonstrations
                supports = self.select_context([self.support_examples[i] for i in context_indices])

                if args.template_list is not None:
                    template = args.template_list[sample_idx % len(args.template_list)]  # Use template in order
                else:
                    template = args.template

                self.features.append(
                    self.convert_fn(
                        example=example,
                        supports=supports,
                        use_demo=self.use_demo,
                        label_list=self.label_list,
                        prompt=args.prompt,
                        template=template,
                        label_word_list=self.label_word_list,
                        verbose=True if _ == 0 else False,
                    )
                )

                _ += 1
        else:
            self.features = None

    def select_context(self, context_examples):
        """
        Select demonstrations from provided examples.
        """
        max_demo_per_label = 1
        counts = {k: 0 for k in self.label_list}
        if len(self.label_list) == 1:
            # Regression
            counts = {'0': 0, '1': 0}
        selection = []

        if self.args.gpt3_in_context_head or self.args.gpt3_in_context_tail:
            # For GPT-3's in-context learning, we sample gpt3_in_context_num demonstrations randomly. 
            order = np.random.permutation(len(context_examples))
            for i in range(min(self.args.gpt3_in_context_num, len(order))):
                selection.append(context_examples[order[i]])
        else:
            # Our sampling strategy
            order = np.random.permutation(len(context_examples))

            for i in order:
                label = context_examples[i].label
                if len(self.label_list) == 1:
                    # Regression
                    label = '0' if float(label) <= median_mapping[self.args.task_name] else '1'
                if counts[label] < max_demo_per_label:
                    selection.append(context_examples[i])
                    counts[label] += 1
                if sum(counts.values()) == len(counts) * max_demo_per_label:
                    break

            if self.use_demo or self.args.inference_time_demo:
                assert len(selection) > 0, (
                    "When `use_demo` (for training) or `inference_time_demo` (for test) is True,"
                    "you shouldn't have len=0 demo list"
                )

        return selection

    def __len__(self):
        return self.size

    def __getitem__(self, i):
        if self.features is None:
            query_idx, context_indices, bootstrap_idx = self.example_idx[i]
            # The input (query) example
            example = self.query_examples[query_idx]
            # The demonstrations
            supports = self.select_context([self.support_examples[i] for i in context_indices])

            if self.args.template_list is not None:
                template = self.args.template_list[sample_idx % len(self.args.template_list)]
            else:
                template = self.args.template

            features = self.convert_fn(
                example=example,
                supports=supports,
                use_demo=self.use_demo,
                label_list=self.label_list,
                prompt=self.args.prompt,
                template=template,
                label_word_list=self.label_word_list,
                verbose=False,
            )
        else:
            features = self.features[i]

        return features

    def get_labels(self):
        return self.label_list

    def convert_fn(
        self,
        example,
        supports,
        use_demo=False,
        label_list=None,
        prompt=False,
        template=None,
        label_word_list=None,
        verbose=False
    ):
        """
        Returns a list of processed "InputFeatures".
        """
        max_length = self.args.max_seq_length

        # Prepare labels
        label_map = {label: i for i, label in enumerate(label_list)}  # Mapping the label names to label ids
        if len(label_list) == 1:
            # Regression
            label_map = {'0': 0, '1': 1}

        # Get example's label id (for training/inference)
        if example.label is None:
            example_label = None
        elif len(label_list) == 1:
            # Regerssion
            example_label = float(example.label)
        else:
            example_label = label_map[example.label]

        # Prepare other features
        if not use_demo:
            # No using demonstrations
            inputs = tokenize_multipart_input(
                input_text_list=input_example_to_tuple(example),
                max_length=max_length,
                tokenizer=self.tokenizer,
                task_name=self.args.task_name,
                prompt=prompt,
                template=template,
                label_word_list=label_word_list,
                first_sent_limit=self.args.first_sent_limit,
                other_sent_limit=self.args.other_sent_limit,
                # --- lxuechen: Enable toggling this.
                truncate_head=self.args.truncate_head,
                # ---
            )
            features = OurInputFeatures(**inputs, label=example_label)

        else:
            # Using demonstrations

            # Max length
            if self.args.double_demo:
                # When using demonstrations, double the maximum length
                # Note that in this case, args.max_seq_length is the maximum length for a single sentence
                max_length = max_length * 2
            if self.args.gpt3_in_context_head or self.args.gpt3_in_context_tail:
                # When using GPT-3's in-context learning, take the maximum tokenization length of the model (512)
                max_length = 512

            # All input sentences, including the query and the demonstrations, are put into augmented_examples, 
            # and are numbered based on the order (starting from 0). For single sentence tasks, the input (query)
            # is the sentence 0; for sentence-pair tasks, the input (query) is the sentence 0 and 1. Note that for
            # GPT-3's
            # in-context learning, the input (query) might be at the end instead of the beginning (gpt3_in_context_head)
            augmented_example = []
            query_text = input_example_to_tuple(example)  # Input sentence list for query
            support_by_label = [[] for i in range(len(label_map))]

            if self.args.gpt3_in_context_head or self.args.gpt3_in_context_tail:
                support_labels = []
                augmented_example = query_text
                for support_example in supports:
                    augmented_example += input_example_to_tuple(support_example)
                    current_label = support_example.label
                    if len(label_list) == 1:
                        current_label = '0' if float(current_label) <= median_mapping[
                            self.args.task_name] else '1'  # Regression
                    support_labels.append(label_map[current_label])
            else:
                # Group support examples by label
                for label_name, label_id in label_map.items():
                    if len(label_list) == 1:
                        # Regression
                        for support_example in filter(
                            lambda s: ('0' if float(s.label) <= median_mapping[
                                self.args.task_name] else '1') == label_name, supports
                        ):
                            support_by_label[label_id] += input_example_to_tuple(support_example)
                    else:
                        for support_example in filter(lambda s: s.label == label_name, supports):
                            support_by_label[label_id] += input_example_to_tuple(support_example)

                augmented_example = query_text
                for label_id in range(len(label_map)):
                    augmented_example += support_by_label[label_id]

            # Tokenization (based on the template)
            inputs = tokenize_multipart_input(
                input_text_list=augmented_example,
                max_length=max_length,
                tokenizer=self.tokenizer,
                task_name=self.args.task_name,
                prompt=prompt,
                template=template,
                label_word_list=label_word_list,
                first_sent_limit=self.args.first_sent_limit,
                other_sent_limit=self.args.other_sent_limit,
                truncate_head=self.args.truncate_head,
                gpt3=self.args.gpt3_in_context_head or self.args.gpt3_in_context_tail,
                support_labels=None if not (
                    self.args.gpt3_in_context_head or self.args.gpt3_in_context_tail) else support_labels
            )
            features = OurInputFeatures(**inputs, label=example_label)

        if verbose:
            logger.info("*** Example ***")
            logger.info("guid: %s" % (example.guid))
            logger.info("features: %s" % features)
            logger.info("text: %s" % self.tokenizer.decode(features.input_ids))

        return features

File Path: examples/classification/src/label_search.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Automatic label search helpers."""

import itertools
import logging
import multiprocessing

import numpy as np
import scipy.spatial as spatial
import scipy.special as special
import scipy.stats as stats
import tqdm

logger = logging.getLogger(__name__)


def select_likely_words(train_logits, train_labels, k_likely=1000, vocab=None, is_regression=False):
    """Pre-select likely words based on conditional likelihood."""
    indices = []
    if is_regression:
        median = np.median(train_labels)
        train_labels = (train_labels > median).astype(np.int)
    num_labels = np.max(train_labels) + 1
    for idx in range(num_labels):
        label_logits = train_logits[train_labels == idx]
        scores = label_logits.mean(axis=0)
        kept = []
        for i in np.argsort(-scores):
            text = vocab[i]
            if not text.startswith("Ġ"):
                continue
            kept.append(i)
        indices.append(kept[:k_likely])
    return indices


def select_neighbors(distances, k_neighbors, valid):
    """Select k nearest neighbors based on distance (filtered to be within the 'valid' set)."""
    indices = np.argsort(distances)
    neighbors = []
    for i in indices:
        if i not in valid:
            continue
        neighbors.append(i)
    if k_neighbors > 0:
        return neighbors[:k_neighbors]
    return neighbors


def init(train_logits, train_labels):
    global logits, labels
    logits = train_logits
    labels = train_labels


def eval_pairing_acc(pairing):
    global logits, labels
    label_logits = np.take(logits, pairing, axis=-1)
    preds = np.argmax(label_logits, axis=-1)
    correct = np.sum(preds == labels)
    return correct / len(labels)


def eval_pairing_corr(pairing):
    global logits, labels
    if pairing[0] == pairing[1]:
        return -1
    label_logits = np.take(logits, pairing, axis=-1)
    label_probs = special.softmax(label_logits, axis=-1)[:, 1]
    pearson_corr = stats.pearsonr(label_probs, labels)[0]
    return pearson_corr


def find_labels(
    model,
    train_logits,
    train_labels,
    seed_labels=None,
    k_likely=1000,
    k_neighbors=None,
    top_n=-1,
    vocab=None,
    is_regression=False,
):
    # Get top indices based on conditional likelihood using the LM.
    likely_indices = select_likely_words(
        train_logits=train_logits,
        train_labels=train_labels,
        k_likely=k_likely,
        vocab=vocab,
        is_regression=is_regression)

    logger.info("Top labels (conditional) per class:")
    for i, inds in enumerate(likely_indices):
        logger.info("\t| Label %d: %s", i, ", ".join([vocab[i] for i in inds[:10]]))

    # Convert to sets.
    valid_indices = [set(inds) for inds in likely_indices]

    # If specified, further re-rank according to nearest neighbors of seed labels.
    # Otherwise, keep ranking as is (based on conditional likelihood only).
    if seed_labels:
        assert (vocab is not None)
        seed_ids = [vocab.index(l) for l in seed_labels]
        vocab_vecs = model.lm_head.decoder.weight.detach().cpu().numpy()
        seed_vecs = np.take(vocab_vecs, seed_ids, axis=0)

        # [num_labels, vocab_size]
        label_distances = spatial.distance.cdist(seed_vecs, vocab_vecs, metric="cosine")

        # Establish label candidates (as k nearest neighbors).
        label_candidates = []
        logger.info("Re-ranked by nearest neighbors:")
        for i, distances in enumerate(label_distances):
            label_candidates.append(select_neighbors(distances, k_neighbors, valid_indices[i]))
            logger.info("\t| Label: %s", seed_labels[i])
            logger.info("\t| Neighbors: %s", " ".join([vocab[idx] for idx in label_candidates[i]]))
    else:
        label_candidates = likely_indices

    # Brute-force search all valid pairings.
    pairings = list(itertools.product(*label_candidates))

    if is_regression:
        eval_pairing = eval_pairing_corr
        metric = "corr"
    else:
        eval_pairing = eval_pairing_acc
        metric = "acc"

    # Score each pairing.
    pairing_scores = []
    with multiprocessing.Pool(initializer=init, initargs=(train_logits, train_labels)) as workers:
        with tqdm.tqdm(total=len(pairings)) as pbar:
            chunksize = max(10, int(len(pairings) / 1000))
            for score in workers.imap(eval_pairing, pairings, chunksize=chunksize):
                pairing_scores.append(score)
                pbar.update()

    # Take top-n.
    best_idx = np.argsort(-np.array(pairing_scores))[:top_n]
    best_scores = [pairing_scores[i] for i in best_idx]
    best_pairings = [pairings[i] for i in best_idx]

    logger.info("Automatically searched pairings:")
    for i, indices in enumerate(best_pairings):
        logger.info("\t| %s (%s = %2.2f)", " ".join([vocab[j] for j in indices]), metric, best_scores[i])

    return best_pairings

File Path: examples/classification/src/models.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Custom models for few-shot learning specific operations."""

import logging

import torch
import torch.nn as nn
from transformers.activations import gelu
from transformers.models.albert.modeling_albert import AlbertModel, AlbertMLMHead
from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel, BertOnlyMLMHead, BertForMaskedLM
from transformers.models.distilbert import DistilBertModel, DistilBertForMaskedLM
from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaLMHead

logger = logging.getLogger(__name__)


def resize_token_type_embeddings(model, new_num_types: int, random_segment: bool):
    """
    Resize the segment (token type) embeddings for BERT
    """
    if hasattr(model, 'bert'):
        old_token_type_embeddings = model.bert.embeddings.token_type_embeddings
    else:
        raise NotImplementedError
    new_token_type_embeddings = nn.Embedding(new_num_types, old_token_type_embeddings.weight.size(1))
    if not random_segment:
        new_token_type_embeddings.weight.data[
        :old_token_type_embeddings.weight.size(0)] = old_token_type_embeddings.weight.data

    model.config.type_vocab_size = new_num_types
    if hasattr(model, 'bert'):
        model.bert.embeddings.token_type_embeddings = new_token_type_embeddings
    else:
        raise NotImplementedError


class BertForPromptFinetuning(BertForMaskedLM):

    def __init__(self, config, add_pooling_layer=False):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.bert = BertModel(config, add_pooling_layer=add_pooling_layer)
        # lxuechen: The name of this variable must be `.cls`! Otherwise, error in loading
        # and you implicitly get random weights!!!
        self.cls = BertOnlyMLMHead(config)
        self.init_weights()

        # These attributes should be assigned once the model is initialized
        self.model_args = None
        self.data_args = None
        self.label_word_list = None

        # For regression
        self.lb = None
        self.ub = None

        # For label search.
        self.return_full_softmax = None

    def get_input_embeddings(self):
        return self.bert.get_input_embeddings()

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, mask_pos=None, labels=None):
        if mask_pos is not None:
            mask_pos = mask_pos.squeeze()

        # Encode everything
        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Get <mask> token representation
        sequence_output, = outputs[:1]
        sequence_mask_output = sequence_output[torch.arange(sequence_output.size(0)), mask_pos]

        # Logits over vocabulary tokens
        prediction_mask_scores = self.cls(sequence_mask_output)

        # Exit early and only return mask logits.
        if self.return_full_softmax:
            if labels is not None:
                return torch.zeros(1, out=prediction_mask_scores.new()), prediction_mask_scores
            return prediction_mask_scores

        # Return logits for each label
        logits = []
        for label_id in range(len(self.label_word_list)):
            logits.append(prediction_mask_scores[:, self.label_word_list[label_id]].unsqueeze(-1))
        logits = torch.cat(logits, -1)

        # Regression task
        if self.config.num_labels == 1:
            logsoftmax = nn.LogSoftmax(-1)
            logits = logsoftmax(logits)  # Log prob of right polarity

        loss = None
        if labels is not None:
            if self.num_labels == 1:
                # Regression task
                loss_fct = nn.KLDivLoss(log_target=True)
                labels = torch.stack([1 - (labels.view(-1) - self.lb) / (self.ub - self.lb),
                                      (labels.view(-1) - self.lb) / (self.ub - self.lb)], -1)
                loss = loss_fct(logits.view(-1, 2), labels)
            else:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        output = (logits,)
        if self.num_labels == 1:
            # Regression output
            output = (torch.exp(logits[..., 1].unsqueeze(-1)) * (self.ub - self.lb) + self.lb,)
        return ((loss,) + output) if loss is not None else output


class DistilBertForPromptFinetuning(DistilBertForMaskedLM):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.distilbert = DistilBertModel(config)
        self.vocab_transform = nn.Linear(config.dim, config.dim)
        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)
        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)

        self.init_weights()

        # These attributes should be assigned once the model is initialized
        self.model_args = None
        self.data_args = None
        self.label_word_list = None
        self.lb = None
        self.ub = None
        self.return_full_softmax = None

    def get_input_embeddings(self):
        return self.distilbert.get_input_embeddings()

    def forward(self, input_ids=None, attention_mask=None, mask_pos=None, labels=None):
        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output, = outputs[:1]

        batch_size = input_ids.size(0)
        if mask_pos is not None:
            mask_pos = mask_pos.squeeze()  # (batch_size,), a sequence of ints.
        sequence_mask_output = sequence_output[torch.arange(batch_size), mask_pos]

        prediction_logits = self.vocab_transform(sequence_mask_output)  # (bs, dim)
        prediction_logits = gelu(prediction_logits)  # (bs, dim)
        prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, dim)
        prediction_mask_scores = self.vocab_projector(prediction_logits)  # (bs, vocab_size)

        if self.return_full_softmax:
            if labels is not None:
                return torch.zeros(1, out=prediction_mask_scores.new()), prediction_mask_scores
            return prediction_mask_scores

        logits = []
        for label_id in range(len(self.label_word_list)):
            logits.append(prediction_mask_scores[:, self.label_word_list[label_id]].unsqueeze(-1))
        logits = torch.cat(logits, -1)

        if self.config.num_labels == 1:
            logsoftmax = nn.LogSoftmax(-1)
            logits = logsoftmax(logits)  # Log prob of right polarity

        loss = None
        if labels is not None:
            if self.num_labels == 1:
                # Regression task
                loss_fct = nn.KLDivLoss(log_target=True)
                labels = torch.stack([1 - (labels.view(-1) - self.lb) / (self.ub - self.lb),
                                      (labels.view(-1) - self.lb) / (self.ub - self.lb)], -1)
                loss = loss_fct(logits.view(-1, 2), labels)
            else:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        output = (logits,)
        if self.num_labels == 1:
            output = (torch.exp(logits[..., 1].unsqueeze(-1)) * (self.ub - self.lb) + self.lb,)
        return ((loss,) + output) if loss is not None else output


class RobertaForPromptFinetuning(BertPreTrainedModel):

    def __init__(self, config, add_pooling_layer=False):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.roberta = RobertaModel(config, add_pooling_layer=add_pooling_layer)
        # lxuechen: The name of this variable must be `.lm_head`! Otherwise, error in loading,
        #   and you implicitly get random weights!!!
        self.lm_head = RobertaLMHead(config)
        self.init_weights()

        # These attributes should be assigned once the model is initialized
        self.model_args = None
        self.data_args = None
        self.label_word_list = None

        # For regression
        self.lb = None
        self.ub = None

        # For auto label search.
        self.return_full_softmax = None

    def get_input_embeddings(self):
        return self.roberta.get_input_embeddings()

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        mask_pos=None,
        labels=None,
    ):
        """
        Args:
            input_ids: (batch_size, seq_len).
            attention_mask: (batch_size, seq_len).
            mask_pos: (batch_size, 1).
            labels: (batch_size,).

        Returns:
            tuple of logits (and maybe loss).
        """
        batch_size = input_ids.size(0)

        if mask_pos is not None:
            mask_pos = mask_pos.squeeze()  # (batch_size,), a sequence of ints.

        # Encode everything
        outputs = self.roberta(input_ids, attention_mask=attention_mask)

        # --- lxuechen: pooled_output does not seem to be used!
        sequence_output, = outputs[:1]
        # Pick the entries that correspond to labels.
        sequence_mask_output = sequence_output[torch.arange(batch_size), mask_pos]
        # ---

        # # Get <mask> token representation
        # sequence_output, pooled_output = outputs[:2]
        # sequence_mask_output = sequence_output[torch.arange(sequence_output.size(0)), mask_pos]

        # Logits over vocabulary tokens
        prediction_mask_scores = self.lm_head(sequence_mask_output)

        # Exit early and only return mask logits.
        if self.return_full_softmax:
            if labels is not None:
                return torch.zeros(1, out=prediction_mask_scores.new()), prediction_mask_scores
            return prediction_mask_scores

        # Return logits for each label
        logits = []
        for label_id in range(len(self.label_word_list)):
            logits.append(prediction_mask_scores[:, self.label_word_list[label_id]].unsqueeze(-1))
        logits = torch.cat(logits, -1)

        # Regression task
        if self.config.num_labels == 1:
            logsoftmax = nn.LogSoftmax(-1)
            logits = logsoftmax(logits)  # Log prob of right polarity

        loss = None
        if labels is not None:
            if self.num_labels == 1:
                # Regression task
                loss_fct = nn.KLDivLoss(log_target=True)
                labels = torch.stack([1 - (labels.view(-1) - self.lb) / (self.ub - self.lb),
                                      (labels.view(-1) - self.lb) / (self.ub - self.lb)], -1)
                loss = loss_fct(logits.view(-1, 2), labels)
            else:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        output = (logits,)
        if self.num_labels == 1:
            # Regression output
            output = (torch.exp(logits[..., 1].unsqueeze(-1)) * (self.ub - self.lb) + self.lb,)
        return ((loss,) + output) if loss is not None else output


class AlbertForPromptFinetuning(BertPreTrainedModel):

    def __init__(self, config, add_pooling_layer=False):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.albert = AlbertModel(config, add_pooling_layer=add_pooling_layer)
        # lxuechen: The name of this variable must be `.predictions`! Otherwise, error in loading
        # and you implicitly get random weights!!!
        self.predictions = AlbertMLMHead(config)
        self.init_weights()

        # These attributes should be assigned once the model is initialized
        self.model_args = None
        self.data_args = None
        self.label_word_list = None

        # For regression
        self.lb = None
        self.ub = None

        # For auto label search.
        self.return_full_softmax = None

    def get_input_embeddings(self):
        return self.albert.get_input_embeddings()

    def forward(
        self,
        input_ids=None,
        token_type_ids=None,
        attention_mask=None,
        mask_pos=None,
        labels=None,
    ):
        """
        Args:
            input_ids: (batch_size, seq_len).
            token_type_ids: (batch_size, seq_len).
            attention_mask: (batch_size, seq_len).
            mask_pos: (batch_size, 1).
            labels: (batch_size,).
        Returns:
            tuple/dict of loss and logits.
        """
        batch_size = input_ids.size(0)

        if mask_pos is not None:
            mask_pos = mask_pos.squeeze()  # (batch_size,), a sequence of ints.

        # Encode everything
        outputs = self.albert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # --- lxuechen: pooled_output does not seem to be used!
        sequence_output, = outputs[:1]
        # Pick the entries that correspond to labels.
        sequence_mask_output = sequence_output[torch.arange(batch_size), mask_pos]
        # ---

        # # Get <mask> token representation
        # sequence_output, pooled_output = outputs[:2]
        # sequence_mask_output = sequence_output[torch.arange(sequence_output.size(0)), mask_pos]

        # Logits over vocabulary tokens
        prediction_mask_scores = self.predictions(sequence_mask_output)

        # Exit early and only return mask logits.
        if self.return_full_softmax:
            if labels is not None:
                return torch.zeros(1, out=prediction_mask_scores.new()), prediction_mask_scores
            return prediction_mask_scores

        # Return logits for each label
        # --- lxuechen: This is only slow for large number of labels.
        logits = []
        for label_id in range(len(self.label_word_list)):
            logits.append(prediction_mask_scores[:, self.label_word_list[label_id]].unsqueeze(-1))
        logits = torch.cat(logits, -1)

        # Regression task
        if self.config.num_labels == 1:
            logsoftmax = nn.LogSoftmax(-1)
            logits = logsoftmax(logits)  # Log prob of right polarity

        loss = None
        if labels is not None:
            if self.num_labels == 1:
                # Regression task
                loss_fct = nn.KLDivLoss(log_target=True)
                labels = torch.stack([1 - (labels.view(-1) - self.lb) / (self.ub - self.lb),
                                      (labels.view(-1) - self.lb) / (self.ub - self.lb)], -1)
                loss = loss_fct(logits.view(-1, 2), labels)
            else:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        output = (logits,)
        if self.num_labels == 1:
            # Regression output
            output = (torch.exp(logits[..., 1].unsqueeze(-1)) * (self.ub - self.lb) + self.lb,)
        return ((loss,) + output) if loss is not None else output

File Path: examples/classification/src/processors.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Dataset utils for different data settings for GLUE."""

import logging

logger = logging.getLogger(__name__)

import pandas as pd
from transformers.data.metrics import glue_compute_metrics
from transformers.data.processors.glue import *


class MrpcProcessor(DataProcessor):
    """Processor for the MRPC data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence1"].numpy().decode("utf-8"),
            tensor_dict["sentence2"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        logger.info("LOOKING AT {}".format(os.path.join(data_dir, "train.tsv")))
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            text_a = line[3]
            text_b = line[4]
            label = line[0]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class MnliProcessor(DataProcessor):
    """Processor for the MultiNLI data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["premise"].numpy().decode("utf-8"),
            tensor_dict["hypothesis"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev_matched.tsv")), "dev_matched")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test_matched.tsv")), "test_matched")

    def get_labels(self):
        """See base class."""
        return ["contradiction", "entailment", "neutral"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0])
            text_a = line[8]
            text_b = line[9]
            label = line[-1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class MnliMismatchedProcessor(MnliProcessor):
    """Processor for the MultiNLI Mismatched data set (GLUE version)."""

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev_mismatched.tsv")), "dev_mismatched")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test_mismatched.tsv")), "test_mismatched")


class SnliProcessor(DataProcessor):
    """Processor for the MultiNLI data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["premise"].numpy().decode("utf-8"),
            tensor_dict["hypothesis"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["contradiction", "entailment", "neutral"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0])
            text_a = line[7]
            text_b = line[8]
            label = line[-1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class ColaProcessor(DataProcessor):
    """Processor for the CoLA data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence"].numpy().decode("utf-8"),
            None,
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        test_mode = set_type == "test"
        text_index = 3
        examples = []
        for (i, line) in enumerate(lines):
            guid = "%s-%s" % (set_type, i)
            text_a = line[text_index]
            label = line[1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        return examples


class Sst2Processor(DataProcessor):
    """Processor for the SST-2 data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence"].numpy().decode("utf-8"),
            None,
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        text_index = 0
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            text_a = line[text_index]

            # --- lxuechen: Make the non-few-shot setting work.
            # Test sets might not have labels; just use a dummy thing at the moment.
            if len(line) < 2:
                dummy_label = self.get_labels()[0]
                label = dummy_label
            else:
                label = line[1]
            # ---
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        return examples


class StsbProcessor(DataProcessor):
    """Processor for the STS-B data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence1"].numpy().decode("utf-8"),
            tensor_dict["sentence2"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return [None]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0])
            text_a = line[7]
            text_b = line[8]
            label = line[-1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class QqpProcessor(DataProcessor):
    """Processor for the QQP data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["question1"].numpy().decode("utf-8"),
            tensor_dict["question2"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        test_mode = set_type == "test"
        q1_index = 3
        q2_index = 4
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0])
            try:
                text_a = line[q1_index]
                text_b = line[q2_index]
                label = line[5]
            except IndexError:
                continue
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class QnliProcessor(DataProcessor):
    """Processor for the QNLI data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["question"].numpy().decode("utf-8"),
            tensor_dict["sentence"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["entailment", "not_entailment"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0])
            text_a = line[1]
            text_b = line[2]
            label = line[-1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class RteProcessor(DataProcessor):
    """Processor for the RTE data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence1"].numpy().decode("utf-8"),
            tensor_dict["sentence2"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["entailment", "not_entailment"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0])
            text_a = line[1]
            text_b = line[2]
            label = line[-1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class WnliProcessor(DataProcessor):
    """Processor for the WNLI data set (GLUE version)."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence1"].numpy().decode("utf-8"),
            tensor_dict["sentence2"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0])
            text_a = line[1]
            text_b = line[2]
            label = line[-1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class TextClassificationProcessor(DataProcessor):
    """
    Data processor for text classification datasets (mr, sst-5, subj, trec, cr, mpqa).
    """

    def __init__(self, task_name):
        self.task_name = task_name

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence"].numpy().decode("utf-8"),
            None,
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(pd.read_csv(os.path.join(data_dir, "train.csv"), header=None).values.tolist(),
                                     "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(pd.read_csv(os.path.join(data_dir, "dev.csv"), header=None).values.tolist(), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(pd.read_csv(os.path.join(data_dir, "test.csv"), header=None).values.tolist(),
                                     "test")

    def get_labels(self):
        """See base class."""
        if self.task_name == "mr":
            return list(range(2))
        elif self.task_name == "sst-5":
            return list(range(5))
        elif self.task_name == "subj":
            return list(range(2))
        elif self.task_name == "trec":
            return list(range(6))
        elif self.task_name == "cr":
            return list(range(2))
        elif self.task_name == "mpqa":
            return list(range(2))
        else:
            raise Exception("task_name not supported.")

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            guid = "%s-%s" % (set_type, i)
            if self.task_name == "ag_news":
                examples.append(
                    InputExample(guid=guid, text_a=line[1] + '. ' + line[2], short_text=line[1] + ".", label=line[0]))
            elif self.task_name == "yelp_review_full":
                examples.append(InputExample(guid=guid, text_a=line[1], short_text=line[1], label=line[0]))
            elif self.task_name == "yahoo_answers":
                text = line[1]
                if not pd.isna(line[2]):
                    text += ' ' + line[2]
                if not pd.isna(line[3]):
                    text += ' ' + line[3]
                examples.append(InputExample(guid=guid, text_a=text, short_text=line[1], label=line[0]))
            elif self.task_name in ['mr', 'sst-5', 'subj', 'trec', 'cr', 'mpqa']:
                examples.append(InputExample(guid=guid, text_a=line[1], label=line[0]))
            else:
                raise Exception("Task_name not supported.")

        return examples


def text_classification_metrics(task_name, preds, labels):
    return {"acc": (preds == labels).mean()}


# Add your task to the following mappings

processors_mapping = {
    "cola": ColaProcessor(),
    "mnli": MnliProcessor(),
    "mnli-mm": MnliMismatchedProcessor(),
    "mrpc": MrpcProcessor(),
    "sst-2": Sst2Processor(),
    "sts-b": StsbProcessor(),
    "qqp": QqpProcessor(),
    "qnli": QnliProcessor(),
    "rte": RteProcessor(),
    "wnli": WnliProcessor(),
    "snli": SnliProcessor(),
    "mr": TextClassificationProcessor("mr"),
    "sst-5": TextClassificationProcessor("sst-5"),
    "subj": TextClassificationProcessor("subj"),
    "trec": TextClassificationProcessor("trec"),
    "cr": TextClassificationProcessor("cr"),
    "mpqa": TextClassificationProcessor("mpqa")
}

num_labels_mapping = {
    "cola": 2,
    "mnli": 3,
    "mrpc": 2,
    "sst-2": 2,
    "sts-b": 1,
    "qqp": 2,
    "qnli": 2,
    "rte": 2,
    "wnli": 2,
    "snli": 3,
    "mr": 2,
    "sst-5": 5,
    "subj": 2,
    "trec": 6,
    "cr": 2,
    "mpqa": 2
}

output_modes_mapping = {
    "cola": "classification",
    "mnli": "classification",
    "mnli-mm": "classification",
    "mrpc": "classification",
    "sst-2": "classification",
    "sts-b": "regression",
    "qqp": "classification",
    "qnli": "classification",
    "rte": "classification",
    "wnli": "classification",
    "snli": "classification",
    "mr": "classification",
    "sst-5": "classification",
    "subj": "classification",
    "trec": "classification",
    "cr": "classification",
    "mpqa": "classification"
}

# Return a function that takes (task_name, preds, labels) as inputs
compute_metrics_mapping = {
    "cola": glue_compute_metrics,
    "mnli": glue_compute_metrics,
    "mnli-mm": glue_compute_metrics,
    "mrpc": glue_compute_metrics,
    "sst-2": glue_compute_metrics,
    "sts-b": glue_compute_metrics,
    "qqp": glue_compute_metrics,
    "qnli": glue_compute_metrics,
    "rte": glue_compute_metrics,
    "wnli": glue_compute_metrics,
    "snli": text_classification_metrics,
    "mr": text_classification_metrics,
    "sst-5": text_classification_metrics,
    "subj": text_classification_metrics,
    "trec": text_classification_metrics,
    "cr": text_classification_metrics,
    "mpqa": text_classification_metrics,
}

# For regression task only: median
median_mapping = {
    "sts-b": 2.5
}

bound_mapping = {
    "sts-b": (0, 5)
}

File Path: examples/classification/src/trainer.py
Content:
# coding=utf-8
# Copyright (c) Xuechen Li. All Rights Reserved.
# Copyright 2020-present the HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
The Trainer class, to easily train a 🤗 Transformers from scratch or finetune it on a new task.
"""
import collections
import gc
import json
import os
from typing import Any, Dict, Optional, Union

import torch
import torch.nn.functional as F
import transformers
from ml_swissknife import utils
from packaging import version
from torch import nn
from torch.utils.data.dataloader import DataLoader
from torch.utils.data.dataset import Dataset
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm, trange
from transformers.file_utils import is_datasets_available, is_in_notebook, is_torch_tpu_available
from transformers.integrations import (
    is_comet_available,
    is_optuna_available,
    is_ray_available,
    is_tensorboard_available,
    is_wandb_available,
)
from transformers.modeling_outputs import SequenceClassifierOutput
from transformers.optimization import AdamW, get_linear_schedule_with_warmup
from transformers.trainer_callback import (
    DefaultFlowCallback,
    ProgressCallback,
)
from transformers.trainer_utils import EvaluationStrategy, IntervalStrategy, TrainOutput
from transformers.utils import logging

from .compiled_args import TrainingArguments

_use_native_amp = False
_use_apex = False

DEFAULT_CALLBACKS = [DefaultFlowCallback]
DEFAULT_PROGRESS_CALLBACK = ProgressCallback

if is_in_notebook():
    from transformers.utils.notebook import NotebookProgressCallback

    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback

# Check if Pytorch version >= 1.6 to switch between Native AMP and Apex
if version.parse(torch.__version__) < version.parse("1.6"):
    from transformers.file_utils import is_apex_available

    if is_apex_available():
        from apex import amp
    _use_apex = True
else:
    _use_native_amp = True

if version.parse(torch.__version__) < version.parse("1.2"):
    _use_ddp_no_sync = False
else:
    _use_ddp_no_sync = True

if is_datasets_available():
    pass

if is_torch_tpu_available():
    import torch_xla.core.xla_model as xm
    import torch_xla.debug.metrics as met
    import torch_xla.distributed.parallel_loader as pl

if is_tensorboard_available():
    from transformers.integrations import TensorBoardCallback

    DEFAULT_CALLBACKS.append(TensorBoardCallback)

if is_wandb_available():
    from transformers.integrations import WandbCallback

    DEFAULT_CALLBACKS.append(WandbCallback)

if is_comet_available():
    from transformers.integrations import CometCallback

    DEFAULT_CALLBACKS.append(CometCallback)

if is_optuna_available():
    pass

if is_ray_available():
    pass

logger = logging.get_logger(__name__)


########## The above part is copied from Transformers' trainer (3.4.0) ##########

def default_dev_objective(metrics):
    """
    Objective used for picking the best model on development sets
    """
    if "eval_mnli/acc" in metrics:
        return metrics["eval_mnli/acc"]
    elif "eval_mnli-mm/acc" in metrics:
        return metrics["eval_mnli-mm/acc"]
    elif "eval_f1" in metrics:
        return metrics["eval_f1"]
    elif "eval_mcc" in metrics:
        return metrics["eval_mcc"]
    elif "eval_pearson" in metrics:
        return metrics["eval_pearson"]
    elif "eval_acc" in metrics:
        return metrics["eval_acc"]

    raise Exception("No metric founded for {}".format(metrics))


def default_dev_objective_key(metrics):
    """Get the key (name) for the specific metric used for dev."""
    keys = (
        "eval_mnli/acc",
        "eval_mnli-mm/acc",
        "eval_f1",
        "eval_mcc",
        "eval_pearson",
        "eval_acc"
    )
    for key in keys:
        if key in metrics:
            return key
    raise Exception("No metric founded for {}".format(metrics))


class Trainer(transformers.Trainer):
    """
    Adding some functions based on Transformers' Trainer class.
    """

    def __init__(self, model_args=None, privacy_args=None, auxiliary_args=None, **kwargs):
        super(Trainer, self).__init__(**kwargs)
        self.privacy_args = privacy_args
        self.model_args = model_args
        self.auxiliary_args = auxiliary_args
        self.scaler = torch.cuda.amp.GradScaler(init_scale=128)

    # --- lxuechen: Not sure why v4.10.0 removed this function...
    def is_local_master(self) -> bool:
        if is_torch_tpu_available():
            return xm.is_master_ordinal(local=True)
        else:
            return self.args.local_rank in [-1, 0]

    # ---

    def create_optimizer_and_scheduler(self, num_training_steps: int):
        """
        Based on Transformers' default one, we add fixing layer option where the bottom n layers' parameters
        are fixed and only the top layers are further fine-tuned.
        """
        if self.optimizer is None:
            params = {}
            for n, p in self.model.named_parameters():
                if self.args.fix_layers > 0:
                    if 'encoder.layer' in n:
                        try:
                            layer_num = int(n[n.find('encoder.layer') + 14:].split('.')[0])
                        except:
                            print(n)
                            raise Exception("")
                        if layer_num >= self.args.fix_layers:
                            print('yes', n)
                            params[n] = p
                        else:
                            print('no ', n)
                    elif 'embeddings' in n:
                        print('no ', n)
                    else:
                        print('yes', n)
                        params[n] = p
                else:
                    params[n] = p
            no_decay = ["bias", "LayerNorm.weight"]
            optimizer_grouped_parameters = [
                {
                    "params": [p for n, p in params.items() if not any(nd in n for nd in no_decay)],
                    "weight_decay": self.args.weight_decay,
                },
                {
                    "params": [p for n, p in params.items() if any(nd in n for nd in no_decay)],
                    "weight_decay": 0.0,
                },
            ]
            self.optimizer = AdamW(
                optimizer_grouped_parameters,
                lr=self.args.learning_rate,
                betas=(self.args.adam_beta1, self.args.adam_beta2),
                eps=self.args.adam_epsilon,
            )
        if self.lr_scheduler is None:
            self.lr_scheduler = get_linear_schedule_with_warmup(
                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps
            )
        return self.optimizer, self.lr_scheduler

    def get_training_setup(self):
        train_dataloader = self.get_train_dataloader()
        if self.args.max_steps > 0:
            num_update_steps_per_epoch = len(train_dataloader) // self.args.gradient_accumulation_steps
            t_total = self.args.max_steps
            num_train_epochs = self.args.max_steps // num_update_steps_per_epoch + int(
                self.args.max_steps % num_update_steps_per_epoch > 0
            )

            t_total_from_num_train_epochs = (
                int(len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs)
            )
            assert t_total <= t_total_from_num_train_epochs, (
                "`num_train_epochs` give strict control (since it also controls the noise multiplier), "
                "`max_steps` should yield fewer steps"
            )
        else:
            t_total = int(len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs)
            num_train_epochs = self.args.num_train_epochs
        return dict(
            train_dataloader=train_dataloader,
            t_total=t_total,
            num_train_epochs=num_train_epochs
        )

    def train(self, model_path=None, dev_objective=None, dev_objective_key=None):
        """
        Main training entry point.

        The training logic is directly borrowed from transformers.Trainer (version 3.0.2).
        Add early stopping.
        """
        if self.args.local_rank != -1 or self.args.n_gpu > 1:
            raise ValueError("Multi-gpu and distributed training is currently not supported.")
        if self.args.fp16:
            raise ValueError("Mixed-precision training is currently not supported.")

        self.args: TrainingArguments

        self.best_dir = None
        self.objective = -float("inf")
        self.dev_objective = default_dev_objective if dev_objective is None else dev_objective
        self.dev_objective_key = default_dev_objective_key if dev_objective_key is None else dev_objective_key
        # --- lxuechen: Don't use self.state.log_history. Given implementation so convoluted...
        self.log_history = []
        # ---

        # Data loading.
        training_setup = self.get_training_setup()
        train_dataloader = training_setup["train_dataloader"]
        t_total = training_setup["t_total"]
        num_train_epochs = training_setup["num_train_epochs"]

        optimizer, scheduler = self.create_optimizer_and_scheduler(num_training_steps=t_total)

        # Check if saved optimizer or scheduler states exist
        if (
            model_path is not None
            and os.path.isfile(os.path.join(model_path, "optimizer.pt"))
            and os.path.isfile(os.path.join(model_path, "scheduler.pt"))
        ):
            # Load in optimizer and scheduler states
            optimizer.load_state_dict(
                torch.load(os.path.join(model_path, "optimizer.pt"), map_location=self.args.device)
            )
            scheduler.load_state_dict(torch.load(os.path.join(model_path, "scheduler.pt")))

        model = self.model

        if self.args.fp16 and _use_apex:
            if not transformers.is_apex_available():
                raise ImportError("Please install apex from https://www.github.com/nvidia/apex to use fp16 training.")
            model, optimizer = amp.initialize(model, optimizer, opt_level=self.args.fp16_opt_level)

        # Multi-gpu training (should be after apex fp16 initialization)
        if self.args.n_gpu > 1:
            model = torch.nn.DataParallel(model)

        # Distributed training (should be after apex fp16 initialization)
        if self.args.local_rank != -1:
            model = torch.nn.parallel.DistributedDataParallel(
                model,
                device_ids=[self.args.local_rank],
                output_device=self.args.local_rank,
                find_unused_parameters=True,
            )

        # Train
        if transformers.is_torch_tpu_available():
            total_train_batch_size = self.args.train_batch_size * xm.xrt_world_size()
        else:
            total_train_batch_size = (
                self.args.train_batch_size
                * self.args.gradient_accumulation_steps
                * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)
            )
        logger.info("***** Running training *****")
        logger.info("  Num examples = %d", self.num_examples(train_dataloader))
        logger.info("  Num Epochs = %d", num_train_epochs)
        logger.info("  Instantaneous batch size per device = %d", self.args.per_device_train_batch_size)
        logger.info("  Total train batch size (w. parallel, distributed & accumulation) = %d", total_train_batch_size)
        logger.info("  Gradient Accumulation steps = %d", self.args.gradient_accumulation_steps)
        logger.info("  Total optimization steps = %d", t_total)

        self.global_step = 0
        self.epoch = 0
        epochs_trained = 0
        steps_trained_in_current_epoch = 0
        # Check if continuing training from a checkpoint
        if model_path is not None:
            # set global_step to global_step of last saved checkpoint from model path
            try:
                self.global_step = int(model_path.split("-")[-1].split("/")[0])
                epochs_trained = self.global_step // (len(train_dataloader) // self.args.gradient_accumulation_steps)
                steps_trained_in_current_epoch = self.global_step % (
                    len(train_dataloader) // self.args.gradient_accumulation_steps
                )
                logger.info("  Continuing training from checkpoint, will skip to saved global_step")
                logger.info("  Continuing training from epoch %d", epochs_trained)
                logger.info("  Continuing training from global step %d", self.global_step)
                logger.info("  Will skip the first %d steps in the first epoch", steps_trained_in_current_epoch)
            except ValueError:
                self.global_step = 0
                logger.info("  Starting fine-tuning.")

        tr_loss = torch.tensor(0.0).to(self.args.device)
        logging_loss_scalar = 0.0

        # --- low rank analysis project ---
        callback = None  # Default; don't store gradients or perform projection.

        if self.auxiliary_args.orthogonal_projection_path is not None:
            state_dicts = torch.load(self.auxiliary_args.orthogonal_projection_path)
            # Kept on CPU during most of the time of training.
            orthogonal_projection = state_dicts.get("eigenvectors")[:, :self.auxiliary_args.orthogonal_projection_rank]

            def callback(privacy_engine):
                """Orthogonally project flattened `.summed_grad` with projection matrix then fill this back."""
                named_params = privacy_engine.named_params

                # Collect.
                flat_grad = []
                for _, param in named_params:
                    flat_grad.append(param.summed_grad.flatten())
                    param.summed_grad = None  # Save memory.
                flat_grad = torch.cat(flat_grad)

                # Project.
                P = orthogonal_projection  # noqa
                if orthogonal_projection.device != flat_grad.device or orthogonal_projection.dtype != flat_grad.dtype:
                    P = orthogonal_projection.to(flat_grad)  # noqa
                Pt_flat_g = torch.matmul(P.T, flat_grad)  # noqa
                # Matrix multiplication with very large dimension (millions in this case) results in weird issues.
                # In this case, `torch.matmul` fails due to calling some algo. Resorting to `torch.mm` for now.
                flat_grad = torch.mm(P, Pt_flat_g[:, None]).squeeze()

                # Redistribute.
                grads = utils.flat_to_shape(flat_tensor=flat_grad, shapes=[param.shape for _, param in named_params])
                for (_, param), grad in utils.zip_(named_params, grads):
                    param.summed_grad = grad

        if self.auxiliary_args.store_grads:
            store_grads_dir = utils.join(self.args.output_dir, 'grad_trajectory')
            utils.makedirs(store_grads_dir, exist_ok=True)
        else:
            store_grads_dir = None
        # ---

        # --- in case no training happens ---
        epoch = 0
        metrics = None
        # ---

        if self.args.evaluate_before_training:
            logging_loss_scalar = self.evaluate_and_log(
                tr_loss=tr_loss,
                logging_loss_scalar=logging_loss_scalar,
                scheduler=scheduler,
            )

        train_iterator = trange(epochs_trained, int(num_train_epochs), desc="Epoch", disable=not self.is_local_master())
        for epoch in train_iterator:
            # Clear gradient before entering a new epochs.
            # This is ultra important when using gradient accumulation in privacy training;
            # grads of micro batches could ooze.
            model.zero_grad(set_to_none=True)

            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                train_dataloader.sampler.set_epoch(epoch)

            if transformers.is_torch_tpu_available():
                parallel_loader = pl.ParallelLoader(train_dataloader, [self.args.device]).per_device_loader(
                    self.args.device
                )
                epoch_iterator = tqdm(parallel_loader, desc="Iteration", disable=not self.is_local_master())
            else:
                epoch_iterator = tqdm(train_dataloader, desc="Iteration", disable=False)

            for step, inputs in enumerate(epoch_iterator):

                # Skip past any already trained steps if resuming training
                if steps_trained_in_current_epoch > 0:
                    steps_trained_in_current_epoch -= 1
                    continue

                losses = self.training_step(model, inputs)
                tr_loss += losses["scalar_loss"]

                if (step + 1) % self.args.gradient_accumulation_steps == 0 or (
                    # last step in epoch but step is always smaller than gradient_accumulation_steps

                    # --- Don't do the update when this is the case. You get bad batch size for privacy ---
                    # len(epoch_iterator) <= self.args.gradient_accumulation_steps
                    # and (step + 1) == len(epoch_iterator)
                    # ---
                ):
                    if self.privacy_args.non_private:
                        # Don't double clip in private learning.
                        torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)
                        optimizer.step()
                    else:
                        if store_grads_dir is not None:
                            def callback(privacy_engine):
                                """Store clipped gradients for spectrum analysis."""
                                named_params = privacy_engine.named_params
                                flat_grad = torch.cat([param.summed_grad.flatten() for _, param in named_params])
                                flat_grad.div_(privacy_engine.batch_size)
                                torch.save(
                                    {"flat_grad": flat_grad.cpu().float()},
                                    utils.join(store_grads_dir, f'global_step_{self.global_step:06d}.ckpt')
                                )

                        vector_loss = losses.get("vector_loss")
                        self.optimizer.step(loss=vector_loss, callback=callback)

                    scheduler.step()
                    model.zero_grad(set_to_none=True)
                    self.global_step += 1
                    self.epoch = epoch + (step + 1) / len(epoch_iterator)

                    metrics = None
                    if (
                        self.args.evaluation_strategy in (IntervalStrategy.STEPS, EvaluationStrategy.STEPS) and
                        self.global_step % self.args.eval_steps == 0
                    ):
                        logging_loss_scalar = self.evaluate_and_log(
                            tr_loss=tr_loss,
                            logging_loss_scalar=logging_loss_scalar,
                            scheduler=scheduler,
                        )
                else:
                    if not self.privacy_args.non_private:
                        self.optimizer.virtual_step(loss=losses.get("vector_loss"))  # noqa

                if 0 < self.args.max_steps < self.global_step:
                    epoch_iterator.close()
                    break

            if self.args.evaluation_strategy == IntervalStrategy.EPOCH and (epoch + 1) % self.args.eval_epochs == 0:
                logging_loss_scalar = self.evaluate_and_log(
                    tr_loss=tr_loss,
                    logging_loss_scalar=logging_loss_scalar,
                    scheduler=scheduler,
                )

            if 0 < self.args.max_steps < self.global_step:
                train_iterator.close()
                break

            if self.args.tpu_metrics_debug or self.args.debug:
                # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)
                xm.master_print(met.metrics_report())

        if self.args.evaluate_after_training:
            logger.info("Evaluate after training ends.")
            self.evaluate_and_log(
                tr_loss=tr_loss,
                logging_loss_scalar=logging_loss_scalar,
                scheduler=scheduler,
            )

        logger.info("\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n")
        return TrainOutput(self.global_step, tr_loss / self.global_step, metrics=metrics), self.objective

    def compute_loss(self, model, inputs, return_outputs=False, return_vector_loss=False):
        """
        How the loss is computed by Trainer. By default, all models return the loss in the first element.

        Subclass and override for custom behavior.
        """
        labels = inputs.pop("labels")
        outputs = model(**inputs)  # This should not contain `loss`.
        logits = outputs[0]
        if isinstance(outputs, SequenceClassifierOutput):
            outputs = (logits,)
        loss = F.cross_entropy(logits, labels, reduction="none")  # (batch_size,).
        if not return_vector_loss:
            loss = loss.mean(dim=0)
        return (loss, (loss,) + outputs) if return_outputs else loss

    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> dict:
        model.train()
        inputs = self._prepare_inputs(inputs)
        loss = self.compute_loss(model, inputs, return_vector_loss=True)  # (batch_size,).

        vector_loss = loss
        scalar_loss = loss.mean(dim=0) / self.args.gradient_accumulation_steps

        if self.privacy_args.non_private:
            scalar_loss.backward()

        scalar_loss = scalar_loss.detach()
        return dict(vector_loss=vector_loss, scalar_loss=scalar_loss)

    """
    Difference compared to original implementation: return output instead of output.metrics (so there is also the 
    logits)
    """

    def evaluate(self, eval_dataset: Optional[Dataset] = None) -> Dict[str, float]:
        """
        Run evaluation and returns metrics.

        The calling script will be responsible for providing a method to compute metrics, as they are
        task-dependent (pass it to the init :obj:`compute_metrics` argument).

        You can also subclass and override this method to inject custom behavior.

        Args:
            eval_dataset (:obj:`Dataset`, `optional`):
                Pass a dataset if you wish to override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`,
                columns not accepted by the ``model.forward()`` method are automatically removed. It must implement
                the :obj:`__len__` method.

        Returns:
            A dictionary containing the evaluation loss and the potential metrics computed from the predictions.
        """
        if eval_dataset is not None and not isinstance(eval_dataset, collections.abc.Sized):
            raise ValueError("eval_dataset must implement __len__")

        eval_dataloader = self.get_eval_dataloader(eval_dataset)

        output = self.prediction_loop(eval_dataloader, description="Evaluation")

        self.log(output.metrics)

        if self.args.tpu_metrics_debug or self.args.debug:
            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)
            xm.master_print(met.metrics_report())

        return output

    def evaluate_and_log(
        self,
        tr_loss,
        logging_loss_scalar,
        scheduler,
    ):
        # lxuechen: Defaults to use .eval_dataset, which is set to 'dev'.
        output = self.evaluate()
        metrics = output.metrics

        objective = self.dev_objective(metrics)
        objective_key = self.dev_objective_key(metrics)

        # --- lxuechen: Print the metrics in a pretty format.
        print('metrics: ')
        print(json.dumps(metrics, indent=4))
        print(f'dev objective {objective_key}: {objective}')
        # ---

        if objective > self.objective:
            logger.info("Best dev result: {}".format(objective))
            self.objective = objective
            self.save_model(self.args.output_dir)

        # --- lxuechen: Combine logging and evaluation
        logs = dict(dev=metrics)

        tr_loss_scalar = tr_loss.item()
        logs["loss"] = (tr_loss_scalar - logging_loss_scalar) / self.args.logging_steps
        # backward compatibility for pytorch schedulers
        logs["learning_rate"] = (
            scheduler.get_last_lr()[0]
            if version.parse(torch.__version__) >= version.parse("1.4")
            else scheduler.get_lr()[0]
        )
        logging_loss_scalar = tr_loss_scalar

        if not self.privacy_args.non_private:
            logs["get_training_stats"] = self.optimizer.get_training_stats()
            logs["privacy_spent"] = self.optimizer.get_privacy_spent(accounting_mode="all", lenient=True)

        logs["epoch"] = self.epoch
        logs["step"] = self.global_step
        self.log_history.append(logs)

        # Write to disk!
        utils.jdump(self.log_history, os.path.join(self.args.output_dir, 'log_history.json'))
        # ---

        # ---
        # Evaluate gradient covariance spectrum for the dimension-dependence analysis project.
        if self.auxiliary_args.eval_spectrum:
            from ..spectrum import spectrum_utils

            def loss_fn(batch, model):
                batch = self._prepare_inputs(inputs=batch)
                return self.compute_loss(
                    model=model, inputs=batch, return_outputs=False, return_vector_loss=True
                )

            default_dtype = torch.get_default_dtype()
            torch.set_default_dtype(torch.float64)  # Slow but accurate.
            self.model.to(dtype=torch.float64)

            spectrum_loader = DataLoader(
                self.train_dataset,
                batch_size=self.args.train_batch_size,
                shuffle=False,  # Must not shuffle.
                collate_fn=self.data_collator,
                drop_last=self.args.dataloader_drop_last,
                num_workers=self.args.dataloader_num_workers,
                pin_memory=self.args.dataloader_pin_memory,
            )

            # No per-sample grads accumulated here, since `model.eval()` called internally.
            spectrum_outputs = spectrum_utils.make_spectrum_lanczos(
                loader=spectrum_loader,
                model=self.model,
                max_batches=self.auxiliary_args.max_spectrum_batches,
                max_lanczos_iter=self.auxiliary_args.max_lanczos_iter,
                loss_fn=loss_fn,
                return_dict=True,
                verbose=True,
            )

            state_dicts = {
                key: value.cpu().float() if torch.is_tensor(value) else value
                for key, value in spectrum_outputs.items()
            }
            utils.tsave(
                state_dicts,
                utils.join(self.args.output_dir, 'spectrum', f'global_step_{self.global_step:06d}.pt')
            )

            torch.set_default_dtype(default_dtype)
            self.model.to(dtype=default_dtype)

            del spectrum_outputs, state_dicts
            gc.collect()
            torch.cuda.empty_cache()
        # ---

        # ---
        # Store grad params.
        state_dicts = dict()
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                state_dicts[name] = param.data.cpu().float()
        utils.tsave(
            state_dicts,
            utils.join(self.args.output_dir, 'grad_params', f'global_step_{self.global_step:06d}.pt')
        )
        # ---

        return logging_loss_scalar

File Path: examples/image_classification/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: examples/image_classification/main.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""CIFAR-10 classification with Vi-T."""
import logging

import fire
import torch
import torch.nn.functional as F
import tqdm
import transformers
from ml_swissknife import utils
from torchvision import transforms

import private_transformers

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


@torch.no_grad()
def evaluate(loader, model):
    model.eval()
    xents, zeons = [], []
    for i, (images, labels) in enumerate(loader):
        images, labels = tuple(t.to(device) for t in (images, labels))
        logits = model(pixel_values=images).logits
        xents.append(F.cross_entropy(logits, labels, reduction='none'))
        zeons.append(logits.argmax(dim=-1).ne(labels).float())
    return tuple(torch.cat(lst).mean().item() for lst in (xents, zeons))


def main(
    model_name_or_path='google/vit-base-patch16-224',
    train_batch_size=1000,
    per_device_train_batch_size=50,
    test_batch_size=500,
    epochs=10,
    target_epsilon=2,
    lr=2e-3,
    max_grad_norm=0.1,
    linear_probe=True,
):
    gradient_accumulation_steps = train_batch_size // per_device_train_batch_size

    image_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
    ])
    train_loader, test_loader = utils.get_loader(
        data_name='cifar10',
        task="classification",
        train_batch_size=per_device_train_batch_size,
        test_batch_size=test_batch_size,
        data_aug=False,
        train_transform=image_transform,
        test_transform=image_transform,
    )

    config = transformers.AutoConfig.from_pretrained(model_name_or_path)
    config.num_labels = 10
    model = transformers.ViTForImageClassification.from_pretrained(
        model_name_or_path,
        config=config,
        ignore_mismatched_sizes=True  # Default pre-trained model has 1k classes; we only have 10.
    ).to(device)
    if linear_probe:
        model.requires_grad_(False)
        model.classifier.requires_grad_(True)
        logging.warning("Linear probe classification head.")
    else:
        private_transformers.freeze_isolated_params_for_vit(model)
        logging.warning("Full fine-tune up to isolated embedding parameters.")

    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)
    privacy_engine = private_transformers.PrivacyEngine(
        model,
        batch_size=train_batch_size,
        sample_size=50000,
        epochs=epochs,
        max_grad_norm=max_grad_norm,
        target_epsilon=target_epsilon,
    )
    privacy_engine.attach(optimizer)

    train_loss_meter = utils.AvgMeter()
    for epoch in range(epochs):
        optimizer.zero_grad()
        pbar = tqdm.tqdm(enumerate(train_loader, 1), total=len(train_loader))
        for global_step, (images, labels) in pbar:
            model.train()
            images, labels = tuple(t.to(device) for t in (images, labels))
            logits = model(pixel_values=images).logits
            loss = F.cross_entropy(logits, labels, reduction="none")
            train_loss_meter.step(loss.mean().item())
            if global_step % gradient_accumulation_steps == 0:
                optimizer.step(loss=loss)
                optimizer.zero_grad()
            else:
                optimizer.virtual_step(loss=loss)
            pbar.set_description(f"Train loss running average: {train_loss_meter.item():.4f}")
        avg_xent, avg_zeon = evaluate(test_loader, model)
        logging.warning(
            f"Epoch: {epoch}, average cross ent loss: {avg_xent:.4f}, average zero one loss: {avg_zeon:.4f}"
        )


if __name__ == "__main__":
    fire.Fire(main)

File Path: examples/table2text/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: examples/table2text/compiled_args.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Compilation of all the arguments."""
import logging
import os
import sys
from dataclasses import dataclass, field
from typing import Optional

import transformers

MODEL_CONFIG_CLASSES = list(transformers.MODEL_WITH_LM_HEAD_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

TRUE_TAGS = ('y', 'yes', 't', 'true')


# See all possible arguments in src/transformers/training_args.py
# or by passing the --help flag to this script.
# We now keep distinct sets of args, for a cleaner separation of concerns.
@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "The model checkpoint for weights initialization. Leave None if you want to train a model from "
                    "scratch."
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )

    static_lm_head: str = field(default='no')
    static_embedding: str = field(default='no')
    attention_only: str = field(default="no")

    def __post_init__(self):
        self.static_lm_head = self.static_lm_head.lower() in TRUE_TAGS
        self.static_embedding = self.static_embedding.lower() in TRUE_TAGS
        self.attention_only = self.attention_only.lower() in TRUE_TAGS


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """
    data_folder: Optional[str] = field(default=None, metadata={"help": "Path to folder with all the data."})

    # Useful for truncating the dataset.
    max_train_examples: Optional[int] = field(default=sys.maxsize)
    max_valid_examples: Optional[int] = field(default=sys.maxsize)
    max_eval_examples: Optional[int] = field(default=sys.maxsize)

    line_by_line: bool = field(
        default=True,
        metadata={"help": "Whether distinct lines of text in the dataset are to be handled as distinct sequences."},
    )
    task_mode: Optional[str] = field(
        default=None, metadata={"help": "The name of the task."}
    )
    format_mode: Optional[str] = field(
        default='cat', metadata={"help": "The mode of data2text format (cat, peek, nopeek)"}
    )
    max_source_length: Optional[int] = field(
        default=512, metadata={"help": "the max source length of summarization data. "}
    )
    train_max_target_length: Optional[int] = field(
        default=100, metadata={"help": "the max target length for training data. "}
    )
    val_max_target_length: Optional[int] = field(
        default=100, metadata={"help": "the max target length for dev data. "}
    )
    block_size: int = field(
        default=-1,
        metadata={
            "help": "Optional input sequence length after tokenization."
                    "The training dataset will be truncated in block of this size for training."
                    "Default to the model max input length for single sentence inputs (take into account special "
                    "tokens)."
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    max_seq_len: int = field(default=sys.maxsize)

    def __post_init__(self):
        if self.data_folder is not None:
            logging.warning(f'Overriding dataset paths using those given in `data_folder`')

            if self.task_mode == "e2e":
                self.train_data_file = os.path.join(self.data_folder, 'src1_train.txt')
                self.valid_data_file = os.path.join(self.data_folder, 'src1_valid.txt')
                self.eval_data_file = os.path.join(self.data_folder, 'src1_test.txt')

                self.train_prompt_file = os.path.join(self.data_folder, 'prompts_train.txt')
                self.val_prompt_file = os.path.join(self.data_folder, 'prompts_valid.txt')
                self.eval_prompt_file = os.path.join(self.data_folder, 'prompts_test.txt')

            elif self.task_mode == "dart":
                self.train_data_file = os.path.join(self.data_folder, 'dart-v1.1.1-full-train.json')
                self.valid_data_file = os.path.join(self.data_folder, 'dart-v1.1.1-full-dev.json')
                self.eval_data_file = os.path.join(self.data_folder, 'dart-v1.1.1-full-test.json')

                self.train_prompt_file = os.path.join(self.data_folder, 'prompts_train.txt')
                self.val_prompt_file = os.path.join(self.data_folder, 'prompts_valid.txt')
                self.eval_prompt_file = os.path.join(self.data_folder, 'prompts_test.txt')


@dataclass
class TrainingArguments(transformers.TrainingArguments):
    max_eval_batches: int = field(default=-1, metadata={"help": "Maximum number of evaluation steps to run."})
    max_generations: int = field(default=sys.maxsize)
    max_generations_train: int = field(default=10)
    max_generations_valid: int = field(default=10)
    skip_generation: str = field(default="no")

    ema_model_averaging: str = field(default="no")
    ema_model_gamma: float = field(default=0.99)
    ema_model_start_from: int = field(default=1000)
    lr_decay: str = field(default="yes")
    eval_epochs: int = field(default=10)

    evaluate_during_training: str = field(
        default="yes",
        metadata={"help": "Run evaluation during training at each logging step."},
    )
    evaluate_before_training: str = field(
        default="yes",
        metadata={"help": "Run evaluation before training."},
    )
    save_at_last: str = field(default="no", metadata={"help": "Save at the end of training."})

    def __post_init__(self):
        super(TrainingArguments, self).__post_init__()
        self.skip_generation = self.skip_generation.lower() in ('y', 'yes')
        self.ema_model_averaging = (self.ema_model_averaging.lower() in ('y', 'yes'))
        self.lr_decay = (self.lr_decay.lower() in ('y', 'yes'))
        self.evaluate_during_training = (self.evaluate_during_training in ('y', 'yes'))
        self.evaluate_before_training = (self.evaluate_before_training in ('y', 'yes'))
        self.save_at_last = (self.save_at_last in ('y', 'yes'))


@dataclass
class PrivacyArguments:
    """Arguments for differentially private training."""
    per_example_max_grad_norm: float = field(
        default=.1, metadata={
            "help": "Clipping 2-norm of per-sample gradients."
        }
    )
    noise_multiplier: float = field(
        default=None, metadata={
            "help": "Standard deviation of noise added for privacy; if `target_epsilon` is specified, "
                    "use the one searched based budget"
        }
    )
    target_epsilon: float = field(
        default=None, metadata={
            "help": "Privacy budget; if `None` use the noise multiplier specified."
        }
    )
    target_delta: float = field(
        default=None, metadata={
            "help": "Lax probability in approximate differential privacy; if `None` use 1 / len(train_data)."
        }
    )
    accounting_mode: str = field(
        default="rdp", metadata={"help": "One of `rdp`, `glw`, `all`."}
    )
    non_private: str = field(default="no")
    clipping_mode: str = field(default="default")

    def __post_init__(self):
        self.non_private = self.non_private.lower() in ('y', 'yes')


@dataclass
class AuxiliaryArguments:
    eval_spectrum: str = field(default="no")
    max_spectrum_batches: int = field(default=100)
    max_lanczos_iter: int = field(default=100)

    store_grads: str = field(default="no")
    orthogonal_projection_path: Optional[str] = field(default=None)
    orthogonal_projection_rank: int = field(default=100)

    def __post_init__(self):
        self.eval_spectrum = self.eval_spectrum.lower() in TRUE_TAGS  # noqa
        self.store_grads = self.store_grads.lower() in TRUE_TAGS  # noqa

File Path: examples/table2text/data_utils/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: examples/table2text/data_utils/data_collator.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union

import torch
from torch.nn.utils.rnn import pad_sequence

from transformers.tokenization_utils import PreTrainedTokenizer
from transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy
from transformers.tokenization_utils_fast import PreTrainedTokenizerFast


InputDataClass = NewType("InputDataClass", Any)

"""
A DataCollator is a function that takes a list of samples from a Dataset
and collate them into a batch, as a dictionary of Tensors.
"""
DataCollator = NewType("DataCollator", Callable[[List[InputDataClass]], Dict[str, torch.Tensor]])


@dataclass
class DataCollatorForData2TextLanguageModeling:
    """
    Data collator used for language modeling.
    - collates batches of tensors, honoring their tokenizer's pad_token
    - preprocesses batches for masked language modeling
    """
    tokenizer: PreTrainedTokenizer
    mlm: bool = True
    format_mode: str = 'cat'
    mlm_probability: float = 0.15

    def __call__(
        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        if isinstance(examples[0], (dict, BatchEncoding)):
            examples = [e["input_ids"] for e in examples]
        input_ids, labels, src, tgt, cate = zip(*examples)
        if self.mlm:
            inputs, labels = self.mask_tokens(batch)
            return {"input_ids": inputs, "labels": labels}
        else:
            if self.format_mode == 'cat':
                mode_input = 3
            elif self.format_mode == 'peek':
                mode_input = 1
            elif self.format_mode == 'nopeek':
                mode_input = 2
            elif self.format_mode == 'infix':
                mode_input = 4

            # mode_input = 1 # means that we take the input again.
            # mode_input = 2 # means that we do not peek at src again.
            # mode_input = 3 # means that we look at the categories, and see the input again.

            if mode_input == 1:
                # input, batch
                batch = self._tensorize_batch(input_ids)
                labels = self._tensorize_batch(labels)
                src = self._tensorize_batch(src)
                cate_batch, cate_attn = None, None
                # tgt = self._tensorize_batch(tgt)
            elif mode_input == 2:
                # nopeek.
                batch = self._tensorize_batch(tgt)
                labels = batch.clone()
                src = self._tensorize_batch(src)
                cate_batch, cate_attn = None, None
            elif mode_input == 3:
                batch = self._tensorize_batch(input_ids)
                labels = self._tensorize_batch(labels)
                src = self._tensorize_batch(cate)
                cate_batch, cate_attn = None, None
            elif mode_input == 4:
                batch = self._tensorize_batch(tgt)
                labels = batch.clone()
                src = self._tensorize_batch(src)

                cate_batch = self._tensorize_batch(cate)
                cate_attn = (cate_batch != self.tokenizer.pad_token_id)

            labels[labels == self.tokenizer.pad_token_id] = -100 # tgt
            src_attn = (src != self.tokenizer.pad_token_id) # src
            tgt_attn = (batch != self.tokenizer.pad_token_id) # tgt

            if cate_batch is None:
                return {"input_ids": batch, "labels": labels, 'src_attn': src_attn, 'tgt_attn':tgt_attn,
                        'src':src}
            else:
                return {"input_ids": batch, "labels": labels, 'src_attn': src_attn, 'tgt_attn': tgt_attn,
                        'src': src, "cate_batch":cate_batch, "cate_attn":cate_attn}

    def _tensorize_batch(
        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]
    ) -> torch.Tensor:
        # In order to accept both lists of lists and lists of Tensors
        if isinstance(examples[0], (list, tuple)):
            examples = [torch.tensor(e, dtype=torch.long) for e in examples]
        length_of_first = examples[0].size(0)
        are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)
        if are_tensors_same_length:
            return torch.stack(examples, dim=0)
        else:
            if self.tokenizer._pad_token is None:
                raise ValueError(
                    "You are attempting to pad samples but the tokenizer you are using"
                    f" ({self.tokenizer.__class__.__name__}) does not have one."
                )
            return pad_sequence(examples, batch_first=True, padding_value=self.tokenizer.pad_token_id)

    def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.
        """

        if self.tokenizer.mask_token is None:
            raise ValueError(
                "This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer."
            )

        labels = inputs.clone()
        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)
        probability_matrix = torch.full(labels.shape, self.mlm_probability)
        special_tokens_mask = [
            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()
        ]
        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)
        if self.tokenizer._pad_token is not None:
            padding_mask = labels.eq(self.tokenizer.pad_token_id)
            probability_matrix.masked_fill_(padding_mask, value=0.0)
        masked_indices = torch.bernoulli(probability_matrix).bool()
        labels[~masked_indices] = -100  # We only compute loss on masked tokens

        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])
        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)

        # 10% of the time, we replace masked input tokens with random word
        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)
        inputs[indices_random] = random_words[indices_random]

        # The rest of the time (10% of the time) we keep the masked input tokens unchanged
        return inputs, labels


@dataclass
class DataCollatorForSumLanguageModeling:
    """
    Data collator used for language modeling.
    - collates batches of tensors, honoring their tokenizer's pad_token
    - preprocesses batches for masked language modeling
    """
    tokenizer: PreTrainedTokenizer
    mlm: bool = True
    format_mode: str = 'cat'
    mlm_probability: float = 0.15

    def __call__(
        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        if isinstance(examples[0], (dict, BatchEncoding)):
            examples = [e["input_ids"] for e in examples]
        # print(examples[0])
        # print(len(examples))
        input_ids, labels, src, tgt = zip(*examples)
        # print(len(input_ids), len(labels), len(weights))
        if self.mlm:
            inputs, labels = self.mask_tokens(batch)
            return {"input_ids": inputs, "labels": labels}
        else:

            # print(self.format_mode)

            if self.format_mode == 'peek' or self.format_mode == 'cat':
                mode_input = 1
            elif self.format_mode == 'nopeek':
                assert False, 'should use format_mode = peek or cat.'
                mode_input = 2
            elif self.format_mode == 'infix':
                assert False, 'should use format_mode = peek or cat.'
                mode_input = 4

            # mode_input = 1 # means that we take the input again.
            # mode_input = 2 # means that we do not peek at src again.
            # mode_input = 3 # means that we look at the categories, and see the input again.

            # print(self.format_mode, mode_input)

            if mode_input == 1:
                # input, batch
                batch = self._tensorize_batch(input_ids)
                labels = self._tensorize_batch(labels)
                src = self._tensorize_batch(src)

            labels[labels == self.tokenizer.pad_token_id] = -100 # tgt
            src_attn = (src != self.tokenizer.pad_token_id) # src
            tgt_attn = (batch != self.tokenizer.pad_token_id) # tgt

            return {"input_ids": batch, "labels": labels, 'src_attn': src_attn, 'tgt_attn':tgt_attn,
                    'src':src}


    def _tensorize_batch(
        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]
    ) -> torch.Tensor:
        # In order to accept both lists of lists and lists of Tensors
        if isinstance(examples[0], (list, tuple)):
            examples = [torch.tensor(e, dtype=torch.long) for e in examples]
        length_of_first = examples[0].size(0)
        are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)
        if are_tensors_same_length:
            return torch.stack(examples, dim=0)
        else:
            if self.tokenizer._pad_token is None:
                raise ValueError(
                    "You are attempting to pad samples but the tokenizer you are using"
                    f" ({self.tokenizer.__class__.__name__}) does not have one."
                )
            return pad_sequence(examples, batch_first=True, padding_value=self.tokenizer.pad_token_id)

File Path: examples/table2text/data_utils/language_modeling.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import copy
import json
import os
import sys

import torch
from torch.utils.data.dataset import Dataset
from transformers.tokenization_utils import PreTrainedTokenizer
from transformers.utils import logging

logger = logging.get_logger(__name__)


class LineByLineTextDataset(Dataset):

    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"
        # Here, we do not cache the features, operating under the assumption
        # that we will soon use fast multithreaded tokenizers from the
        # `tokenizers` repo everywhere =)
        logger.info("Creating features from dataset file at %s", file_path)

        with open(file_path, encoding="utf-8") as f:
            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]

        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)
        self.examples = batch_encoding["input_ids"]

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i) -> torch.Tensor:
        return torch.tensor(self.examples[i], dtype=torch.long)


class LineByLineE2ETextDataset(Dataset):

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        bos_tok: str,
        eos_tok: str,
        max_seq_len=sys.maxsize,
        max_examples=sys.maxsize,
        **_,
    ):
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"
        # Here, we do not cache the features, operating under the assumption
        # that we will soon use fast multithreaded tokenizers from the
        # `tokenizers` repo everywhere =)
        logger.info("Creating features from dataset file at %s", file_path)

        with open(file_path, encoding="utf-8") as f:
            lines = [
                line.split('||')
                for line in f.read().splitlines() if (
                    len(line) > 0 and not line.isspace() and len(line.split('||')) == 2
                )
            ]
        src_lines, tgt_lines = list(zip(*lines))
        src_lines = list(src_lines)
        tgt_lines = list(tgt_lines)

        edited_sents = []
        for src, tgt in zip(src_lines, tgt_lines):
            sent = ' {} {} '.format(src, bos_tok) + tgt + ' {}'.format(eos_tok)
            edited_sents.append(sent)

        # --- Filter out super long sentences ---
        new_src_lines, new_tgt_lines, new_edited_sents = [], [], []
        for src_line, tgt_line, edited_sent in zip(src_lines, tgt_lines, edited_sents):
            tokenized_edited_sent = tokenizer.tokenize(edited_sent)
            if len(tokenized_edited_sent) <= max_seq_len:
                new_src_lines.append(src_line)
                new_tgt_lines.append(tgt_line)
                new_edited_sents.append(edited_sent)
            del src_line, tgt_line, edited_sent
        src_lines, tgt_lines, edited_sents = new_src_lines, new_tgt_lines, new_edited_sents
        # ---------------------------------------

        # --- Truncate the dataset if necessary; this must be after the length filtering. ---
        src_lines = src_lines[:max_examples]
        tgt_lines = tgt_lines[:max_examples]
        edited_sents = edited_sents[:max_examples]
        # ---

        batch_encoding = tokenizer(
            edited_sents,
            add_special_tokens=True,
            truncation=True,
            max_length=block_size,
            is_split_into_words=False,
        )

        self.examples = batch_encoding["input_ids"]
        self.labels = copy.deepcopy(self.examples)

        # split into category words:
        ssl_lst = []
        for ss in src_lines:
            ssl = [la.split(':')[0].strip() for la in ss.split('|')]
            ssl_lst.append(ssl)

        self.src_cat = tokenizer(
            ssl_lst,
            add_special_tokens=True,
            truncation=True,
            max_length=block_size,
            is_split_into_words=True
        )['input_ids']

        self.src_sent = []
        self.tgt_sent = []

        temp_src_len = 0
        temp_tgt_len = 0
        temp_count = 0

        separator = tokenizer(bos_tok, add_special_tokens=False)['input_ids'][0]
        for i, elem in enumerate(self.labels):
            sep_idx = elem.index(separator) + 1
            self.src_sent.append(self.examples[i][:sep_idx - 1])
            self.tgt_sent.append(self.examples[i][sep_idx - 1:])
            self.labels[i][:sep_idx] = [-100] * sep_idx  # Doesn't contribute to loss.
            temp_src_len += sep_idx - 1
            temp_tgt_len += len(elem) - (sep_idx - 1)
            temp_count += 1

        print('tgt_avg: ', temp_tgt_len / temp_count)
        print('src_avg: ', temp_src_len / temp_count)
        print('ratios: ', temp_src_len / temp_tgt_len)

        print(self.labels[0])
        print(self.examples[0])
        print(edited_sents[0])
        print(self.src_sent[0])
        print(self.tgt_sent[0])
        print(self.src_cat[0])
        assert len(self.src_cat) == len(self.examples)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return (
            torch.tensor(self.examples[i], dtype=torch.long),
            torch.tensor(self.labels[i], dtype=torch.long),
            torch.tensor(self.src_sent[i], dtype=torch.long),
            torch.tensor(self.tgt_sent[i], dtype=torch.long),
            torch.tensor(self.src_cat[i], dtype=torch.long),
        )


class LineByLineWebNLGTextDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach
    soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        bos_tok: str,
        eos_tok: str,
    ):
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"
        # Here, we do not cache the features, operating under the assumption
        # that we will soon use fast multithreaded tokenizers from the
        # `tokenizers` repo everywhere =)
        logger.info("Creating features from dataset file at %s", file_path)

        with open(file_path) as f:
            lines_dict = json.load(f)

        full_rela_lst = []
        full_src_lst = []
        full_tgt_lst = []

        for i, example in enumerate(lines_dict['entries']):
            sents = example[str(i + 1)]['lexicalisations']
            triples = example[str(i + 1)]['modifiedtripleset']

            rela_lst = []
            temp_triples = ''
            for j, tripleset in enumerate(triples):
                subj, rela, obj = tripleset['subject'], tripleset['property'], tripleset['object']
                rela_lst.append(rela)
                if j > 0:
                    temp_triples += ' | '
                temp_triples += '{} : {} : {}'.format(subj, rela, obj)

            for sent in sents:
                if sent["comment"] == 'good':
                    full_tgt_lst.append(sent["lex"])
                    full_src_lst.append(temp_triples)
                    full_rela_lst.append(rela_lst)

        assert len(full_rela_lst) == len(full_src_lst)
        assert len(full_rela_lst) == len(full_tgt_lst)

        edited_sents = []
        for src, tgt in zip(full_src_lst, full_tgt_lst):
            sent = ' {} {} '.format(src, bos_tok) + tgt + ' {}'.format(eos_tok)
            edited_sents.append(sent)

        batch_encoding = tokenizer(edited_sents, add_special_tokens=True, truncation=True, max_length=block_size,
                                   is_split_into_words=False)
        self.examples = batch_encoding["input_ids"]

        self.labels = copy.deepcopy(self.examples)

        # split into category words:
        ssl_lst = full_rela_lst

        self.src_cat = tokenizer(ssl_lst, add_special_tokens=True, truncation=True, max_length=block_size,
                                 is_split_into_words=True)['input_ids']

        self.src_sent = []
        self.tgt_sent = []
        temp_src_len = 0
        temp_tgt_len = 0
        temp_count = 0

        if True:
            separator = tokenizer(bos_tok, add_special_tokens=False)['input_ids'][0]
            for i, elem in enumerate(self.labels):
                sep_idx = elem.index(separator) + 1
                self.src_sent.append(self.examples[i][:sep_idx - 1])  # does not contain the BOS separator
                self.tgt_sent.append(self.examples[i][sep_idx - 1:])  # contains the BOS separator.
                self.labels[i][:sep_idx] = [-100] * sep_idx
                temp_src_len += sep_idx - 1
                temp_tgt_len += len(elem) - (sep_idx - 1)
                temp_count += 1

        print('tgt_avg: ', temp_tgt_len / temp_count)
        print('src_avg: ', temp_src_len / temp_count)
        print('ratios: ', temp_src_len / temp_tgt_len)

        print(self.labels[0])
        print(self.examples[0])
        print(edited_sents[0])
        print(self.src_sent[0])
        print(self.tgt_sent[0])
        print(self.src_cat[0])
        print()
        print(self.labels[1])
        print(self.examples[1])
        print(edited_sents[1])
        print(self.src_sent[1])
        print(self.tgt_sent[1])
        print(self.src_cat[1])
        assert len(self.src_cat) == len(self.examples)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return (
            torch.tensor(self.examples[i], dtype=torch.long),
            torch.tensor(self.labels[i], dtype=torch.long),
            torch.tensor(self.src_sent[i], dtype=torch.long),
            torch.tensor(self.tgt_sent[i], dtype=torch.long),
            torch.tensor(self.src_cat[i], dtype=torch.long),
        )


class LineByLineTriplesTextDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach
    soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        bos_tok: str,
        eos_tok: str,
        max_seq_len=sys.maxsize,
        max_examples=sys.maxsize,
    ):
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"
        # Here, we do not cache the features, operating under the assumption
        # that we will soon use fast multithreaded tokenizers from the
        # `tokenizers` repo everywhere =)
        logger.info("Creating features from dataset file at %s", file_path)

        with open(file_path) as f:
            lines_dict = json.load(f)

        full_rela_lst = []
        full_src_lst = []
        full_tgt_lst = []
        for example in lines_dict:
            rela_lst = []
            temp_triples = ''
            for i, tripleset in enumerate(example['tripleset']):
                subj, rela, obj = tripleset
                rela = rela.lower()
                rela_lst.append(rela)
                if i > 0:
                    temp_triples += ' | '
                temp_triples += '{} : {} : {}'.format(subj, rela, obj)

            for sent in example['annotations']:
                full_tgt_lst.append(sent['text'])
                full_src_lst.append(temp_triples)
                full_rela_lst.append(rela_lst)

        # Truncate the dataset if necessary.
        full_rela_lst = full_rela_lst[:max_examples]
        full_src_lst = full_src_lst[:max_examples]
        full_tgt_lst = full_tgt_lst[:max_examples]

        assert len(full_rela_lst) == len(full_src_lst)
        assert len(full_rela_lst) == len(full_tgt_lst)

        edited_sents = []
        for src, tgt in zip(full_src_lst, full_tgt_lst):
            sent = f' {src} {bos_tok} {tgt} {eos_tok} '
            edited_sents.append(sent)

        # --- Filter out super long sentences ---
        this_full_rela_lst = []
        this_full_src_lst = []
        this_full_tgt_lst = []
        this_edited_sents = []
        for rela, src, tgt, edited_sent in zip(full_rela_lst, full_src_lst, full_tgt_lst, edited_sents):
            tokenized_edited_sent = tokenizer.tokenize(edited_sent)
            if len(tokenized_edited_sent) <= max_seq_len:
                this_full_rela_lst.append(rela)
                this_full_src_lst.append(src)
                this_full_tgt_lst.append(tgt)
                this_edited_sents.append(edited_sent)
        full_rela_lst = this_full_rela_lst
        full_src_lst = this_full_src_lst
        full_tgt_lst = this_full_tgt_lst
        edited_sents = this_edited_sents
        # ---------------------------------------

        batch_encoding = tokenizer(edited_sents, add_special_tokens=True, truncation=True, max_length=block_size,
                                   is_split_into_words=False)
        self.examples = batch_encoding["input_ids"]

        self.labels = copy.deepcopy(self.examples)

        # split into category words:
        ssl_lst = full_rela_lst

        self.src_cat = tokenizer(ssl_lst, add_special_tokens=True, truncation=True, max_length=block_size,
                                 is_split_into_words=True)['input_ids']

        self.src_sent = []
        self.tgt_sent = []
        temp_src_len = 0
        temp_tgt_len = 0
        temp_count = 0
        if True:
            separator = tokenizer(bos_tok, add_special_tokens=False)['input_ids'][0]
            for i, elem in enumerate(self.labels):
                sep_idx = elem.index(separator) + 1
                self.src_sent.append(self.examples[i][:sep_idx - 1])  # does not contain the BOS separator
                self.tgt_sent.append(self.examples[i][sep_idx - 1:])  # contains the BOS separator.
                self.labels[i][:sep_idx] = [-100] * sep_idx

                temp_src_len += sep_idx - 1
                temp_tgt_len += len(elem) - (sep_idx - 1)
                temp_count += 1

        print('tgt_avg: ', temp_tgt_len / temp_count)
        print('src_avg: ', temp_src_len / temp_count)
        print('ratios: ', temp_src_len / temp_tgt_len)

        print(self.labels[0])
        print(self.examples[0])
        print(edited_sents[0])
        print(self.src_sent[0])
        print(self.tgt_sent[0])
        print(self.src_cat[0])
        assert len(self.src_cat) == len(self.examples)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return (
            torch.tensor(self.examples[i], dtype=torch.long),
            torch.tensor(self.labels[i], dtype=torch.long),
            torch.tensor(self.src_sent[i], dtype=torch.long),
            torch.tensor(self.tgt_sent[i], dtype=torch.long),
            torch.tensor(self.src_cat[i], dtype=torch.long),
        )

File Path: examples/table2text/decoding_utils.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utilities for generation."""
import logging
import sys
from typing import Optional

import tqdm
import transformers


def generate(
    model: transformers.PreTrainedModel,
    tokenizer: transformers.PreTrainedTokenizer,
    loader=None,
    prompt_dataset=None,
    max_length=100,
    min_length=5,
    top_k=0,
    top_p=0.9,  # Only filter with top_p.
    repetition_penalty=1,
    do_sample=False,
    num_beams=5,
    bad_words_ids=None,
    dummy_token_id=-100,  # Used as mask.
    num_return_sequences=1,
    max_generations=sys.maxsize,
    device=None,
    padding_token="[PAD]",
    **kwargs,
):
    assert not model.training, "Generation must be when `model` is in eval mode."
    if kwargs:
        logging.warning(f"Unknown kwargs: {kwargs}")

    # These are linebreaks; generating these will mess up the evaluation, since those files assume one example per-line.
    if bad_words_ids is None:
        bad_words_ids = [[628], [198]]
        if padding_token in tokenizer.get_vocab():
            bad_words_ids.append(tokenizer.encode(padding_token))

    kwargs = dict(
        model=model,
        tokenizer=tokenizer,
        max_length=max_length,
        min_length=min_length,
        top_k=top_k,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        do_sample=do_sample,
        num_beams=num_beams,
        bad_words_ids=bad_words_ids,
        dummy_token_id=dummy_token_id,
        num_return_sequences=num_return_sequences,
        max_generations=max_generations,
        device=device,
        padding_token=padding_token,
    )
    if loader is not None:
        result = _generate_with_loader(loader=loader, **kwargs)
    elif prompt_dataset is not None:
        result = _generate_with_prompt_dataset(prompt_dataset=prompt_dataset, **kwargs)
    else:
        raise ValueError(f"`loader` and `prompt_dataset` cannot both be `None`.")

    return result


def _generate_with_loader(
    loader,

    model,
    tokenizer: transformers.PreTrainedTokenizer,
    max_length,
    min_length,
    top_k,
    top_p,
    repetition_penalty,
    do_sample,
    num_beams,
    bad_words_ids,
    dummy_token_id,
    num_return_sequences,
    max_generations,
    device,
    padding_token,
):
    references = []
    full_generations = []  # Sentences including the prompt part.
    unstripped_generations = []
    generations = []

    stop_generation = False
    for batch_idx, batch in tqdm.tqdm(enumerate(loader), desc="generation"):
        if stop_generation:
            break

        batch_input_ids, batch_labels = batch["input_ids"], batch["labels"]
        # e.g., inputs_ids may be [[95, 123, 32], [198, 19, 120]], and
        # labels may be [[-100, 123, 32], [-100, -100, 120]

        for input_ids, labels in zip(batch_input_ids, batch_labels):
            if stop_generation:
                break

            # Find the first pad token and end the sentence from there!
            if padding_token in tokenizer.get_vocab():
                pad_positions, = (
                    input_ids == tokenizer.encode(padding_token, return_tensors="pt").squeeze()
                ).nonzero(as_tuple=True)
                # Some sentences might have padding; others might not.
                if pad_positions.numel() == 0:
                    first_pad_position = None
                else:
                    first_pad_position = pad_positions[0]
                reference_str: str = tokenizer.decode(input_ids[:first_pad_position], clean_up_tokenization_spaces=True)
            else:
                reference_str: str = tokenizer.decode(input_ids, clean_up_tokenization_spaces=True)
            references.append(reference_str)

            # Find the first non- -100 position. Note there are trailing -100s.
            non_prompt_positions, = (labels != dummy_token_id).nonzero(as_tuple=True)
            first_non_prompt_position = non_prompt_positions[0].item()
            prompt_len = first_non_prompt_position
            prompt_ids = input_ids[:prompt_len]

            output_ids = model.generate(
                input_ids=prompt_ids[None, ...].to(device),
                max_length=max_length + prompt_len,  # This cannot be a 0-D tensor!
                min_length=min_length,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                do_sample=do_sample,
                bad_words_ids=bad_words_ids,
                num_return_sequences=num_return_sequences,
                num_beams=num_beams,
                pad_token_id=tokenizer.eos_token_id,  # Stop the stupid logging...
            )
            output_ids = output_ids.squeeze(dim=0)  # Throw away batch dimension.

            whole_str: str = tokenizer.decode(output_ids, clean_up_tokenization_spaces=True)
            prompt_str: str = tokenizer.decode(prompt_ids, clean_up_tokenization_spaces=True)
            output_str: str = whole_str[len(prompt_str):]

            full_generations.append(whole_str)
            del whole_str, prompt_str

            # Remove potential eos_token at the end.
            eos_position: Optional[int] = output_str.find(tokenizer.eos_token)
            if eos_position == -1:  # Didn't generate eos_token; that's okay -- just skip!
                eos_position = None
            output_str = output_str[:eos_position]
            unstripped_generations.append(output_str)

            # Removing leading and trailing spaces.
            output_str = output_str.strip()

            generations.append(output_str)

            if len(generations) >= max_generations:
                stop_generation = True

    return full_generations, unstripped_generations, generations, references


def _generate_with_prompt_dataset(
    prompt_dataset,

    model,
    tokenizer,
    max_length,
    min_length,
    top_k,
    top_p,
    repetition_penalty,
    do_sample,
    num_beams,
    bad_words_ids,
    dummy_token_id,
    num_return_sequences,
    max_generations,
    device,
    padding_token,
):
    references = []
    full_generations = []  # Sentences including the prompt part.
    unstripped_generations = []
    generations = []

    stop_generation = False
    for input_ids in tqdm.tqdm(prompt_dataset, desc="generation"):
        if stop_generation:
            break

        prompt_len = len(input_ids[0])
        output_ids = model.generate(
            input_ids=input_ids.to(device),
            max_length=max_length + prompt_len,  # This cannot be a 0-D tensor!
            min_length=min_length,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            do_sample=do_sample,
            bad_words_ids=bad_words_ids,
            num_return_sequences=num_return_sequences,
            num_beams=num_beams,
            pad_token_id=tokenizer.eos_token_id,  # Stop the stupid logging...
        )
        output_ids = output_ids.squeeze(dim=0)  # Throw away batch dimension.
        input_ids = input_ids.squeeze(dim=0)

        whole_str: str = tokenizer.decode(output_ids, clean_up_tokenization_spaces=True)
        prompt_str: str = tokenizer.decode(input_ids, clean_up_tokenization_spaces=True)
        output_str: str = whole_str[len(prompt_str):]

        full_generations.append(whole_str)
        del whole_str, prompt_str

        # Remove potential eos_token at the end.
        eos_position: Optional[int] = output_str.find(tokenizer.eos_token)
        if eos_position == -1:  # Didn't generate eos_token; that's okay -- just skip!
            eos_position = None
        output_str = output_str[:eos_position]
        unstripped_generations.append(output_str)

        # Removing leading and trailing spaces.
        output_str = output_str.strip()

        generations.append(output_str)

        if len(generations) >= max_generations:
            stop_generation = True
    return full_generations, unstripped_generations, generations, references

File Path: examples/table2text/density.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Code for converting Lanczos outputs to densities."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math

import numpy as np


def eigv_to_density(eig_vals, all_weights=None, grids=None,
                    grid_len=10000, sigma_squared=None, grid_expand=1e-2):
    """Compute the smoothed spectral density from a set of eigenvalues.

    Convolves the given eigenvalues with a Gaussian kernel, weighting the values
    by all_weights (or uniform weighting if all_weights is None). Example output
    can be seen in Figure 1 of https://arxiv.org/pdf/1901.10159.pdf. Visualizing
    the estimated density can be done by calling plt.plot(grids, density). There
    is likely not a best value of sigma_squared that works for all use cases,
    so it is recommended to try multiple values in the range [1e-5,1e-1].

    Args:
      eig_vals: Array of shape [num_draws, order]
      all_weights: Array of shape [num_draws, order], if None then weights will be
        taken to be uniform.
      grids: Array of shape [grid_len], the smoothed spectrum will be plotted
        in the interval [grids[0], grids[-1]]. If None then grids will be
        computed based on max and min eigenvalues and grid length.
      grid_len: Integer specifying number of grid cells to use, only used if
        grids is None
      sigma_squared: Scalar. Controls the smoothing of the spectrum estimate.
        If None, an appropriate value is inferred.
      grid_expand: Controls the window of values that grids spans.
        grids[0] = smallest eigenvalue - grid_expand.
        grids[-1] = largest_eigenvalue + grid_expand.

    Returns:
      density: Array of shape [grid_len], the estimated density, averaged over
        all draws.
      grids: Array of shape [grid_len]. The values the density is estimated on.
    """
    if all_weights is None:
        all_weights = np.ones(eig_vals.shape) * 1.0 / float(eig_vals.shape[1])
    num_draws = eig_vals.shape[0]

    lambda_max = np.nanmean(np.max(eig_vals, axis=1), axis=0) + grid_expand
    lambda_min = np.nanmean(np.min(eig_vals, axis=1), axis=0) - grid_expand

    if grids is None:
        assert grid_len is not None, 'grid_len is required if grids is None.'
        grids = np.linspace(lambda_min, lambda_max, num=grid_len)

    grid_len = grids.shape[0]
    if sigma_squared is None:
        sigma = 10 ** -5 * max(1, (lambda_max - lambda_min))
    else:
        sigma = sigma_squared * max(1, (lambda_max - lambda_min))

    density_each_draw = np.zeros((num_draws, grid_len))
    for i in range(num_draws):

        if np.isnan(eig_vals[i, 0]):
            raise ValueError('tridaig has nan values.')
        else:
            for j in range(grid_len):
                x = grids[j]
                vals = _kernel(eig_vals[i, :], x, sigma)
                density_each_draw[i, j] = np.sum(vals * all_weights[i, :])
    density = np.nanmean(density_each_draw, axis=0)
    norm_fact = np.sum(density) * (grids[1] - grids[0])
    density = density / norm_fact
    return density, grids


def tridiag_to_eigv(tridiag_list):
    """Preprocess the tridiagonal matrices for density estimation.

    Args:
      tridiag_list: Array of shape [num_draws, order, order] List of the
        tridiagonal matrices computed from running num_draws independent runs
        of lanczos. The output of this function can be fed directly into
        eigv_to_density.

    Returns:
      eig_vals: Array of shape [num_draws, order]. The eigenvalues of the
        tridiagonal matricies.
      all_weights: Array of shape [num_draws, order]. The weights associated with
        each eigenvalue. These weights are to be used in the kernel density
        estimate.
    """
    # Calculating the node / weights from Jacobi matrices.
    num_draws = len(tridiag_list)
    num_lanczos = tridiag_list[0].shape[0]
    eig_vals = np.zeros((num_draws, num_lanczos))
    all_weights = np.zeros((num_draws, num_lanczos))
    for i in range(num_draws):
        nodes, evecs = np.linalg.eigh(tridiag_list[i])
        index = np.argsort(nodes)
        nodes = nodes[index]
        evecs = evecs[:, index]
        eig_vals[i, :] = nodes
        all_weights[i, :] = evecs[0] ** 2
    return eig_vals, all_weights


def tridiag_to_density(tridiag_list, sigma_squared=1e-5, grid_len=10000):
    """This function estimates the smoothed density from the output of lanczos.

    Args:
      tridiag_list: Array of shape [num_draws, order, order] List of the
        tridiagonal matrices computed from running num_draws independent runs
        of lanczos.
      sigma_squared: Controls the smoothing of the density.
      grid_len: Controls the granularity of the density.

    Returns:
      density: Array of size [grid_len]. The smoothed density estimate averaged
        over all num_draws.
      grids: Array of size [grid_len]. The values the density estimate is on.
    """
    eig_vals, all_weights = tridiag_to_eigv(tridiag_list)
    density, grids = eigv_to_density(eig_vals, all_weights,
                                     grid_len=grid_len,
                                     sigma_squared=sigma_squared)
    return density, grids


def _kernel(x, x0, variance):
    """Point estimate of the Gaussian kernel.

    This function computes the Gaussian kernel for
    C exp(-(x - x0) ^2 /(2 * variance)) where C is the appropriate normalization.
    variance should be a list of length 1. Either x0 or x should be a scalar. Only
    one of the x or x0 can be a numpy array.

    Args:
      x: Can be either scalar or array of shape [order]. Points to estimate
        the kernel on.
      x0: Scalar. Mean of the kernel.
      variance: Scalar. Variance of the kernel.

    Returns:
      point_estimate: A scalar corresponding to
        C exp(-(x - x0) ^2 /(2 * variance)).
    """
    coeff = 1.0 / np.sqrt(2 * math.pi * variance)
    val = -(x0 - x) ** 2
    val = val / (2.0 * variance)
    val = np.exp(val)
    point_estimate = coeff * val
    return point_estimate

File Path: examples/table2text/misc.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Miscellaneous utilities.

Mostly bespoke data loaders at the moment.
"""

from transformers import (
    DataCollatorForLanguageModeling,
    DataCollatorForPermutationLanguageModeling,
    PreTrainedTokenizer
)

from .compiled_args import DataTrainingArguments
from .data_utils.data_collator import DataCollatorForData2TextLanguageModeling
from .data_utils.language_modeling import LineByLineE2ETextDataset, LineByLineTriplesTextDataset


def get_dataset_with_path(
    data_args: DataTrainingArguments,
    tokenizer: PreTrainedTokenizer,
    file_path: str,
    max_examples: int,
    **_,
):
    if data_args.line_by_line:
        if data_args.task_mode == 'e2e':
            dataset = LineByLineE2ETextDataset(
                tokenizer=tokenizer,
                file_path=file_path,
                block_size=data_args.block_size,
                bos_tok=tokenizer.bos_token,
                eos_tok=tokenizer.eos_token,
                max_seq_len=data_args.max_seq_len,
                max_examples=max_examples,
            )
        elif data_args.task_mode == 'dart':
            dataset = LineByLineTriplesTextDataset(
                tokenizer=tokenizer,
                file_path=file_path,
                block_size=data_args.block_size,
                bos_tok=tokenizer.bos_token,
                eos_tok=tokenizer.eos_token,
                max_seq_len=data_args.max_seq_len,
                max_examples=max_examples,
            )
        else:
            raise ValueError(f"Unknown `args.task_mode`: {data_args.task_mode}")

    else:
        raise ValueError("table2text task don't support anything other than line_by_line!")
    return dataset


def get_prompt_dataset(file_path, tokenizer):
    with open(file_path, 'r') as f:
        lines = f.readlines()
    encoded_lines = [
        tokenizer.encode(line.strip(), add_special_tokens=False, return_tensors="pt")
        for line in lines
    ]
    return encoded_lines


def get_all_datasets(config, tokenizer, data_args, model_args, **_):
    kwargs = dict(data_args=data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir)
    train_dataset = get_dataset_with_path(
        **kwargs, file_path=data_args.train_data_file, max_examples=data_args.max_train_examples
    )
    valid_dataset = get_dataset_with_path(
        **kwargs, file_path=data_args.valid_data_file, max_examples=data_args.max_valid_examples
    )
    eval_dataset = get_dataset_with_path(
        **kwargs, file_path=data_args.eval_data_file, max_examples=data_args.max_eval_examples
    )

    if config.model_type == "xlnet":
        data_collator = DataCollatorForPermutationLanguageModeling(
            tokenizer=tokenizer,
            plm_probability=data_args.plm_probability,
            max_span_length=data_args.max_span_length,
        )
    else:
        if data_args.task_mode == 'e2e' or data_args.task_mode == 'dart':
            data_collator = DataCollatorForData2TextLanguageModeling(
                tokenizer=tokenizer, mlm=False, format_mode=data_args.format_mode
            )
        else:
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer, mlm=False,
            )

    return train_dataset, valid_dataset, eval_dataset, data_collator

File Path: examples/table2text/models.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
from torch import nn
from transformers import GPT2PreTrainedModel, GPT2LMHeadModel


class _View(nn.Module):
    def __init__(self, shape):
        super(_View, self).__init__()
        self.shape = shape

    def forward(self, x):
        return x.reshape(*self.shape)


class PrefixTuner(GPT2PreTrainedModel):
    """A minimalistic implementation of the core components."""

    def __init__(self, config, model_args, gpt2=None):
        super(PrefixTuner, self).__init__(config=config)

        # Instantiate a GPT-2, and DON'T optimizer it!
        if gpt2 is None:
            self.gpt2 = GPT2LMHeadModel.from_pretrained(
                model_args.model_name_or_path, config=config, cache_dir=model_args.cache_dir,
            )
        else:
            self.gpt2 = gpt2

        self.register_buffer('extra_prefix_ids', torch.arange(model_args.prefix_len))
        # TODO: Also introduce the easier net.
        self.extra_prefix_net = nn.Sequential(
            nn.Embedding(model_args.prefix_len, config.n_embd),
            nn.Linear(config.n_embd, model_args.mid_dim),
            nn.Tanh(),
            nn.Linear(model_args.mid_dim, config.n_layer * 2 * config.n_embd),
            _View((-1, model_args.prefix_len, config.n_layer * 2, config.n_head, config.n_embd // config.n_head)),
            nn.Dropout(model_args.prefix_dropout),
        )

    def make_past_key_values(self, bsz=None):
        extra_prefix_ids = self.extra_prefix_ids[None, :].expand(bsz, -1)
        past_key_values = self.extra_prefix_net(extra_prefix_ids)
        # (n_layer, batch_size, n_head, prefix_len, n_embed // n_head).
        # e.g., (2, 1, 12, 5, 64,).
        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2, dim=0)
        return past_key_values

    def state_dict(self):
        """Avoid storing GPT-2, since it's not even trained."""
        return self.extra_prefix_net.state_dict()

    def load_state_dict(self, state_dict):
        """Avoid loading GPT-2, since it's not even trained."""
        self.extra_prefix_net.load_state_dict(state_dict)

    @property
    def major_device(self):
        """Returns the device where the parameters are on."""
        return next(self.parameters()).device

    def forward(
        self,
        input_ids,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        **kwargs,
    ):
        past_key_values = self.make_past_key_values(bsz=input_ids.size(0))
        return self.gpt2(
            input_ids=input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        )

    def generate(self, input_ids, num_beams, **kwargs):
        # Additional files also changed:
        # src/transformers/generation_utils.py
        # src/transformers/models/gpt2/modeling_gpt2.py

        # --- lxuechen: This part is really error-prone!
        #   A sanity check is to optimize the model for a few updates and check if the beam-search generations changed.
        #   The confusing logic in generation_utils:
        #       1) `past` is used in `GPT2LMHeadModel:prepare_inputs_for_generation`,
        #       2) it's converted to `past_key_values` in that function,
        #       3) `past_key_values` is then updated in forward due to return_dict,
        #       4) `past` is set to `past_key_values` in `generation_utils:_update_model_kwargs_for_generation`

        # This is expansion step is important for generation, since otherwise the shapes are wrong.
        past_key_values = self.make_past_key_values(bsz=input_ids.size(0) * num_beams)
        # ---

        return self.gpt2.generate(
            input_ids=input_ids,
            num_beams=num_beams,
            past_key_values=past_key_values,

            use_cache=True,
            position_ids=None,

            # --- lxuechen: These arguments I created to make sure prefix-tuning works correctly.
            #   The logic: At beginning, past=None, and then it gets replaced with past_key_values.
            #              Can't directly give in past, since otherwise, input_ids gets truncated to the last index.
            use_past_key_values_as_past_at_init=True,
            nullify_attention_mask=True,
            # ---

            **kwargs
        )

File Path: examples/table2text/run_language_modeling.py
Content:
# coding=utf-8
# Copyright (c) Xuechen Li. All Rights Reserved.
# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet).
GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned
using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.
"""

import json
import logging
import os

import torch
from ml_swissknife import utils
from transformers import HfArgumentParser, MODEL_WITH_LM_HEAD_MAPPING, set_seed
from transformers.models.gpt2 import GPT2Tokenizer
from transformers.optimization import get_linear_schedule_with_warmup

from private_transformers import PrivacyEngine
from .compiled_args import (AuxiliaryArguments, DataTrainingArguments, ModelArguments, PrivacyArguments,
                            TrainingArguments)
from .misc import get_all_datasets, get_prompt_dataset
from .trainer import Trainer

logger = logging.getLogger(__name__)

MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def main():
    parser = HfArgumentParser(
        (ModelArguments, DataTrainingArguments, TrainingArguments, PrivacyArguments, AuxiliaryArguments)
    )
    model_args, data_args, training_args, privacy_args, auxiliary_args = parser.parse_args_into_dataclasses()

    model_args: ModelArguments
    data_args: DataTrainingArguments
    training_args: TrainingArguments
    privacy_args: PrivacyArguments
    auxiliary_args: AuxiliaryArguments

    if data_args.eval_data_file is None and training_args.do_eval:
        raise ValueError(
            "Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file "
            "or remove the --do_eval argument."
        )

    if (
        os.path.exists(training_args.output_dir)
        and os.listdir(training_args.output_dir)
        and training_args.do_train
        and not training_args.overwrite_output_dir
    ):
        raise ValueError(
            f"Output directory ({training_args.output_dir}) already exists and is not empty. Use "
            f"--overwrite_output_dir to overcome."
        )

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,
    )
    logger.warning(
        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
        training_args.local_rank,
        training_args.device,
        training_args.n_gpu,
        bool(training_args.local_rank != -1),
        training_args.fp16,
    )
    logger.info("Training/evaluation parameters %s", training_args)

    # Set seed
    set_seed(training_args.seed)

    # Debug mode
    if training_args.debug:
        import warnings
        warnings.filterwarnings("error")

    # Low rank models need special models!
    from transformers.models.gpt2 import GPT2Config, GPT2LMHeadModel

    # Config.
    config = GPT2Config.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)
    config.return_dict = True
    config.tie_word_embeddings = False

    # Tokenizer; `bos_token` and `eos_token` is the same for GPT2; both are 50256.
    tokenizer = GPT2Tokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)

    # Model.
    gpt2 = GPT2LMHeadModel.from_pretrained(
        model_args.model_name_or_path,
        config=config,
        cache_dir=model_args.cache_dir,
    )
    print(f'base gpt2 model: {model_args.model_name_or_path}')
    print(gpt2)

    # Clone the embedding into the lm_head for better initialization.
    lm_head = gpt2.get_output_embeddings()
    embedding = gpt2.get_input_embeddings()
    lm_head.weight.data.copy_(embedding.weight.data)
    print(f'Cloning initial embedding into lm_head, '
          f'checking norms... \n'
          f'\tlm_head: {lm_head.weight.norm()}, embedding: {embedding.weight.norm()}')
    torch.testing.assert_allclose(lm_head.weight, embedding.weight)
    del lm_head, embedding

    if data_args.block_size <= 0:
        data_args.block_size = tokenizer.model_max_length
    else:
        data_args.block_size = min(data_args.block_size, tokenizer.model_max_length)

    # Adjust tokenizer and model embeddings.
    print('adapt tokenizer to include [PAD]')
    print(f'before len(tokenizer) = {len(tokenizer)}')
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    print(f'after len(tokenizer) = {len(tokenizer)}')
    print('tokenizer.eos_token:', tokenizer.eos_token, tokenizer.eos_token_id)
    print('tokenizer.bos_token:', tokenizer.bos_token, tokenizer.bos_token_id)

    print('adapt the size of lm_head and input_embeddings to include [PAD]')
    print('use avg-based initialization')

    input_embeddings_before = gpt2.get_input_embeddings().weight
    lm_head_before = gpt2.get_output_embeddings().weight
    gpt2.resize_token_embeddings(len(tokenizer))

    input_embeddings_after = gpt2.get_input_embeddings().weight
    lm_head_after = gpt2.get_output_embeddings().weight
    print(
        f'before lm_head.weight.size() = {lm_head_before.size()}, '
        f'input_embeddings_before.size() = {input_embeddings_before.size()}'
    )
    print(
        f'after lm_head.weight.size() = {lm_head_after.size()}, '
        f'after input_embeddings_after.size() = {input_embeddings_after.size()}'
    )
    torch.testing.assert_allclose(lm_head_before, lm_head_after[:-1])
    print('pre-chunk equal for lm_head')
    torch.testing.assert_allclose(input_embeddings_before, input_embeddings_after[:-1])
    print('pre-chunk equal for input_embeddings')
    lm_head_after.data[-1] = lm_head_before.mean(dim=0)
    input_embeddings_after.data[-1] = input_embeddings_before.mean(dim=0)

    print('double check: ')
    print('embedding size', gpt2.get_input_embeddings().weight.size())
    print('lm_head size', gpt2.get_output_embeddings().weight.size())
    model = gpt2

    train_dataset, val_dataset, eval_dataset, data_collator = get_all_datasets(
        config=config,
        tokenizer=tokenizer,
        data_args=data_args,
        training_args=training_args,
        model_args=model_args,
    )

    # Materialize the prompts.
    generation_stuff = dict(
        train_prompts=get_prompt_dataset(file_path=data_args.train_prompt_file, tokenizer=tokenizer),
        val_prompts=get_prompt_dataset(file_path=data_args.val_prompt_file, tokenizer=tokenizer),
        eval_prompts=get_prompt_dataset(file_path=data_args.eval_prompt_file, tokenizer=tokenizer),
    )

    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        model_args=model_args,
        data_args=data_args,
        privacy_args=privacy_args,
        auxiliary_args=auxiliary_args,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        generation_stuff=generation_stuff,
    )

    # Massage the parameters.
    if model_args.attention_only:
        model.requires_grad_(False)
        for name, param in model.named_parameters():
            if 'c_attn.weight' in name:
                param.requires_grad_(True)
    else:
        model.requires_grad_(True)
        if model_args.static_lm_head:
            model.get_output_embeddings().requires_grad_(False)
        if model_args.static_embedding:
            model.get_input_embeddings().requires_grad_(False)
            model.transformer.wpe.requires_grad_(False)
    params = tuple(param for param in model.parameters() if param.requires_grad)
    names = tuple(name for name, param in model.named_parameters() if param.requires_grad)
    num_trainable_params = sum(param.numel() for param in params)
    print(f"Number of trainable params: {num_trainable_params / 1e6:.4f} million")
    print(json.dumps(names, indent=4))

    # TODO: Using a single gigantic parameter group is okay only when `weight_decay` is 0.
    #   Biases and LM parameters should not be decayed perhaps even with privacy.
    optimizer = torch.optim.AdamW(
        params=params,
        lr=training_args.learning_rate,
        betas=(training_args.adam_beta1, training_args.adam_beta2),
        eps=training_args.adam_epsilon,
    )
    trainer.optimizer = optimizer

    # Create the lr_scheduler.
    num_update_steps_per_epoch = len(trainer.get_train_dataloader()) // trainer.args.gradient_accumulation_steps
    num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
    t_total = int(num_update_steps_per_epoch * trainer.args.num_train_epochs)
    if training_args.lr_decay:
        trainer.lr_scheduler = get_linear_schedule_with_warmup(
            trainer.optimizer,
            num_warmup_steps=training_args.warmup_steps,
            num_training_steps=t_total,
        )
    else:
        trainer.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(trainer.optimizer, lambda _: 1.)

    # Hacky way to set noise_multiplier.
    if privacy_args.non_private:
        privacy_args.noise_multiplier = 0.
        privacy_args.per_example_max_grad_norm = None
    else:
        actual_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps
        privacy_engine = PrivacyEngine(
            module=model,
            batch_size=actual_batch_size,
            sample_size=len(train_dataset),
            epochs=training_args.num_train_epochs,
            max_grad_norm=privacy_args.per_example_max_grad_norm,
            noise_multiplier=privacy_args.noise_multiplier,
            target_epsilon=privacy_args.target_epsilon,
            target_delta=privacy_args.target_delta,
            accounting_mode=privacy_args.accounting_mode,
            clipping_mode=privacy_args.clipping_mode,
        )
        # Originally, these could have been null.
        privacy_args.noise_multiplier = privacy_engine.noise_multiplier
        privacy_args.target_delta = privacy_engine.target_delta

        print('privacy_args: ')
        print(json.dumps(privacy_args.__dict__, indent=4))
        privacy_engine.attach(optimizer)

    # Training.
    if training_args.do_train:
        all_args = {
            **training_args.__dict__,
            **data_args.__dict__,
            **model_args.__dict__,
            **privacy_args.__dict__,
        }
        utils.jdump(
            all_args,
            os.path.join(training_args.output_dir, 'argparse.json'),
            default=lambda x: str(x),
        )

        # For convenience, we also re-save the tokenizer to the same directory,
        # so that you can share your model easily on huggingface.co/models =)
        if trainer.is_world_master():
            tokenizer.save_pretrained(training_args.output_dir)

        logger.info("*** Train ***")
        logger.info(
            f"Training set size: {len(train_dataset)}, "
            f"per_device_train_batch_size: {training_args.per_device_train_batch_size}, "
            f"gradient_accumulation_steps: {training_args.gradient_accumulation_steps}"
        )
        # lxuechen: Especially so for the restored checkpoints. Don't resume...
        trainer.train(model_path=None)
        if training_args.save_at_last:
            trainer.save_model()

    # Evaluation
    if training_args.do_eval:
        logger.info("*** Evaluate ***")

        output = trainer.evaluate(log_results=False)
        utils.jdump(
            output,
            os.path.join(training_args.output_dir, "final_results.json"),
        )

        logger.info("***** Eval results *****")
        logger.info(output)


if __name__ == "__main__":
    main()

File Path: examples/table2text/trainer.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import inspect
import json
import os
import re
import shutil
import warnings
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import datasets
import numpy as np
import torch
import torch.nn.functional as F
from ml_swissknife import utils
from packaging import version
from torch import nn
from torch.utils.data.dataloader import DataLoader
from torch.utils.data.dataset import Dataset
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data.sampler import RandomSampler, SequentialSampler
from tqdm.auto import tqdm, trange
from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator
from transformers.file_utils import is_datasets_available, is_torch_tpu_available
from transformers.modeling_utils import PreTrainedModel
from transformers.models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING
from transformers.optimization import AdamW, get_linear_schedule_with_warmup
from transformers.tokenization_utils_base import PreTrainedTokenizerBase
from transformers.trainer_pt_utils import distributed_broadcast_scalars
from transformers.trainer_utils import (EvalPrediction, EvaluationStrategy, IntervalStrategy, PREFIX_CHECKPOINT_DIR,
                                        PredictionOutput, TrainOutput, set_seed)
from transformers.utils import logging

from . import decoding_utils
from .compiled_args import (AuxiliaryArguments, DataTrainingArguments, ModelArguments, PrivacyArguments,
                            TrainingArguments)

logger = logging.get_logger(__name__)


class Trainer:
    """
    Trainer is a simple but feature-complete training and eval loop for PyTorch,
    optimized for 🤗 Transformers.

    Args:
        model (:class:`~transformers.PreTrainedModel`, `optional`):
            The model to train, evaluate or use for predictions. If not provided, a ``model_init`` must be passed.
        args (:class:`~transformers.TrainingArguments`, `optional`):
            The arguments to tweak for training. Will default to a basic instance of
            :class:`~transformers.TrainingArguments`
            with the ``output_dir`` set to a directory named `tmp_trainer` in the current directory if not provided.
        data_collator (:obj:`DataCollator`, `optional`):
            The function to use to form a batch from a list of elements of :obj:`train_dataset` or
            :obj:`eval_dataset`. Will default to :func:`~transformers.default_data_collator` if no ``tokenizer`` is
            provided, an instance of :func:`~transformers.DataCollatorWithPadding` otherwise.
        train_dataset (:obj:`torch.utils.data.dataset.Dataset`, `optional`):
            The dataset to use for training. If it is an :obj:`datasets.Dataset`, columns not accepted by the
            ``model.forward()`` method are automatically removed.
        eval_dataset (:obj:`torch.utils.data.dataset.Dataset`, `optional`):
             The dataset to use for evaluation. If it is an :obj:`datasets.Dataset`, columns not accepted by the
            ``model.forward()`` method are automatically removed.
        tokenizer (:class:`PreTrainedTokenizerBase`, `optional`):
            The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs the
            maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an
            interrupted training or reuse the fine-tuned model.
        model_init (:obj:`Callable[[], PreTrainedModel]`, `optional`):
            A function that instantiates the model to be used. If provided, each call to
            :meth:`~transformers.Trainer.train` will start from a new instance of the model as given by this function.
        compute_metrics (:obj:`Callable[[EvalPrediction], Dict]`, `optional`):
            The function that will be used to compute metrics at evaluation. Must take a
            :class:`~transformers.EvalPrediction` and return a dictionary string to metric values.
        optimizers (:obj:`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR`, `optional`):
            A tuple containing the optimizer and the scheduler to use. Will default to an instance of
            :class:`~transformers.AdamW` on your model and a scheduler given by
            :func:`~transformers.get_linear_schedule_with_warmup` controlled by :obj:`args`.
        kwargs:
            Deprecated keyword arguments.
    """

    def __init__(
        self,
        model: Optional[PreTrainedModel] = None,
        args: Optional[TrainingArguments] = None,
        model_args: Optional[ModelArguments] = None,
        data_args: Optional[DataTrainingArguments] = None,
        privacy_args: Optional[PrivacyArguments] = None,
        auxiliary_args: Optional[AuxiliaryArguments] = None,
        data_collator: Optional[DataCollator] = None,
        train_dataset: Optional[Dataset] = None,
        eval_dataset: Optional[Dataset] = None,
        tokenizer: Optional["PreTrainedTokenizerBase"] = None,
        model_init: Callable[[], PreTrainedModel] = None,
        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),

        val_dataset: Optional[Dataset] = None,
        generation_stuff: Optional[Dict] = None,
        **kwargs,
    ):
        if args is None:
            logger.info("No `TrainingArguments` passed, using the current path as `output_dir`.")
            args = TrainingArguments("tmp_trainer")
        self.args = args
        self.model_args = model_args
        self.data_args = data_args
        self.privacy_args = privacy_args
        self.auxiliary_args = auxiliary_args

        # Seed must be set before instantiating the model when using model
        set_seed(self.args.seed)
        assert (
            model is not None or model_init is not None
        ), "You must provide a model to use `Trainer`, either by using the `model` argument or the `model_init` " \
           "argument."
        assert model_init is None
        self.model = model.to(args.device) if model is not None else None
        self.num_params = sum(
            param.numel() for param in self.model.parameters() if param.requires_grad
        )
        from transformers.modeling_utils import Conv1D
        self.num_non_embedding_params = sum(
            param.numel()
            for module in self.model.modules() if isinstance(module, (nn.LayerNorm, Conv1D))
            for param in module.parameters() if param.requires_grad
        )
        default_collator = default_data_collator if tokenizer is None else DataCollatorWithPadding(tokenizer)
        self.data_collator = data_collator if data_collator is not None else default_collator
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        self.val_dataset = val_dataset
        self.generation_stuff = generation_stuff
        self.tokenizer = tokenizer
        self.curr_best_eval = 10000000.
        self.model_init = model_init
        self.compute_metrics = compute_metrics
        self.optimizer, self.lr_scheduler = optimizers
        if model_init is not None and (self.optimizer is not None or self.lr_scheduler is not None):
            raise RuntimeError(
                "Passing a `model_init` is incompatible with providing the `optimizers` argument."
                "You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method."
            )
        self.log_history = []
        if "prediction_loss_only" in kwargs:
            warnings.warn(
                "Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future "
                "version. Use `args.prediction_loss_only` instead.",
                FutureWarning,
            )
            self.args.prediction_loss_only = kwargs.pop("prediction_loss_only")
        assert kwargs == {}, f"Unexpected keyword arguments: {list(kwargs.keys())}."

        # Will be set to True by `self._setup_loggers()` on first call to `self.log()`.
        self._loggers_initialized = False

        # Create output directory if needed
        if self.is_world_process_zero():
            os.makedirs(self.args.output_dir, exist_ok=True)
        if is_torch_tpu_available():
            # Set an xla_device flag on the model's config.
            # We'll find a more elegant and not need to do this in the future.
            self.model.config.xla_device = True
        if not callable(self.data_collator) and callable(getattr(self.data_collator, "collate_batch", None)):
            self.data_collator = self.data_collator.collate_batch
            warnings.warn(
                (
                    "The `data_collator` should now be a simple callable (function, class with `__call__`), classes "
                    + "with a `collate_batch` are deprecated and won't be supported in a future version."
                ),
                FutureWarning,
            )

        self.global_step = None
        self.epoch = None
        self.total_flos = None
        self.hp_search_backend = None
        self.use_tune_checkpoints = False
        if self.args.label_names is None:
            self.args.label_names = (
                ["start_positions, end_positions"]
                if type(self.model) in MODEL_FOR_QUESTION_ANSWERING_MAPPING.values()
                else ["labels"]
            )

    def _remove_unused_columns(self, dataset: "datasets.Dataset", description: Optional[str] = None):
        if not self.args.remove_unused_columns:
            return
        # Inspect model forward signature to keep only the arguments it accepts.
        signature = inspect.signature(self.model.forward)
        signature_columns = list(signature.parameters.keys())
        # Labels may be named label or label_ids, the default data collator handles that.
        signature_columns += ["label", "label_ids"]
        columns = [k for k in signature_columns if k in dataset.column_names]
        ignored_columns = list(set(dataset.column_names) - set(signature_columns))
        dset_description = "" if description is None else f"in the {description} set "
        logger.info(
            f"The following columns {dset_description}don't have a corresponding argument in `"
            f"{self.model.__class__.__name__}.forward` and have been ignored: {', '.join(ignored_columns)}."
        )
        dataset.set_format(type=dataset.format["type"], columns=columns)

    def _get_train_sampler(self, shuffle=True) -> Optional[torch.utils.data.sampler.Sampler]:
        if isinstance(self.train_dataset, torch.utils.data.IterableDataset):
            return None
        else:
            # Sometimes we don't want to shuffle!
            if shuffle:
                return (
                    RandomSampler(self.train_dataset)
                    if self.args.local_rank == -1
                    else DistributedSampler(self.train_dataset)
                )
            else:
                return SequentialSampler(self.train_dataset)

    def get_train_dataloader(self, train_sampler=None) -> DataLoader:
        """
        Returns the training :class:`~torch.utils.data.DataLoader`.

        Will use no sampler if :obj:`self.train_dataset` is a :obj:`torch.utils.data.IterableDataset`, a random sampler
        (adapted to distributed training if necessary) otherwise.

        Subclass and override this method if you want to inject some custom behavior.
        """
        if self.train_dataset is None:
            raise ValueError("Trainer: training requires a train_dataset.")

        if train_sampler is None:
            train_sampler = self._get_train_sampler()

        return DataLoader(
            self.train_dataset,
            batch_size=self.args.train_batch_size,
            sampler=train_sampler,
            collate_fn=self.data_collator,
            drop_last=self.args.dataloader_drop_last,
            num_workers=self.args.dataloader_num_workers,
        )

    def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.sampler.Sampler]:
        if isinstance(eval_dataset, torch.utils.data.IterableDataset):
            return None
        elif self.args.local_rank != -1:
            raise ValueError("Multi-gpu and distributed training is currently not supported.")
        else:
            return SequentialSampler(eval_dataset)

    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:
        """
        Returns the evaluation :class:`~torch.utils.data.DataLoader`.

        Will use no sampler if :obj:`self.eval_dataset` is a :obj:`torch.utils.data.IterableDataset`, a sequential
        sampler (adapted to distributed training if necessary) otherwise.

        Subclass and override this method if you want to inject some custom behavior.

        Args:
            eval_dataset (:obj:`torch.utils.data.dataset.Dataset`, `optional`):
                If provided, will override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`, columns not
                accepted by the ``model.forward()`` method are automatically removed.
        """
        if eval_dataset is None and self.eval_dataset is None:
            raise ValueError("Trainer: evaluation requires an eval_dataset.")
        elif eval_dataset is not None and is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):
            self._remove_unused_columns(eval_dataset, description="evaluation")
        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        eval_sampler = self._get_eval_sampler(eval_dataset)

        return DataLoader(
            eval_dataset,
            sampler=eval_sampler,
            batch_size=self.args.eval_batch_size,
            collate_fn=self.data_collator,
            drop_last=self.args.dataloader_drop_last,
            num_workers=self.args.dataloader_num_workers,
        )

    def create_optimizer_and_scheduler(self, num_training_steps: int):
        """
        Setup the optimizer and the learning rate scheduler.

        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.
        """
        if self.optimizer is None:
            no_decay = ["bias", "LayerNorm.weight"]
            optimizer_grouped_parameters = [
                {
                    "params": [p for n, p in self.model.named_parameters() if
                               (not any(nd in n for nd in no_decay)) and p.requires_grad],
                    "weight_decay": self.args.weight_decay,
                },
                {
                    "params": [p for n, p in self.model.named_parameters() if
                               any(nd in n for nd in no_decay) and p.requires_grad],
                    "weight_decay": 0.0,
                },
            ]

            self.optimizer = AdamW(
                optimizer_grouped_parameters,
                lr=self.args.learning_rate,
                betas=(self.args.adam_beta1, self.args.adam_beta2),
                eps=self.args.adam_epsilon,
            )
        if self.lr_scheduler is None:
            self.lr_scheduler = get_linear_schedule_with_warmup(
                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps
            )

    def num_examples(self, dataloader: DataLoader) -> int:
        """
        Helper to get number of samples in a :class:`~torch.utils.data.DataLoader` by accessing its dataset.
        """
        return len(dataloader.dataset)

    def train(self, model_path: Optional[str] = None, **kwargs):
        """
        Main training entry point.

        Args:
            model_path (:obj:`str`, `optional`):
                Local path to the model if the model to train has been instantiated from a local path. If present,
                training will resume from the optimizer/scheduler states loaded here.
        """
        if self.args.local_rank != -1 or self.args.n_gpu > 1:
            raise ValueError("Multi-gpu and distributed training is currently not supported.")
        if self.args.fp16:
            raise ValueError("Mixed-precision training is currently not supported.")

        # Model re-init
        if self.model_init is not None:
            # Seed must be set before instantiating the model when using model_init.
            set_seed(self.args.seed)
            model = self.model_init()
            self.model = model.to(self.args.device)

            # Reinitializes optimizer and scheduler
            self.optimizer, self.lr_scheduler = None, None

        # Data loader and number of training steps
        train_dataloader = self.get_train_dataloader()
        num_update_steps_per_epoch = len(train_dataloader) // self.args.gradient_accumulation_steps
        num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
        if self.args.max_steps > 0:
            t_total = self.args.max_steps
            num_train_epochs = self.args.max_steps // num_update_steps_per_epoch + int(
                self.args.max_steps % num_update_steps_per_epoch > 0
            )
        else:
            t_total = int(num_update_steps_per_epoch * self.args.num_train_epochs)
            num_train_epochs = self.args.num_train_epochs
            self.args.max_steps = t_total

        self.create_optimizer_and_scheduler(num_training_steps=t_total)

        # Check if saved optimizer or scheduler states exist
        if (
            model_path is not None
            and os.path.isfile(os.path.join(model_path, "optimizer.pt"))
            and os.path.isfile(os.path.join(model_path, "scheduler.pt"))
        ):
            # Load in optimizer and scheduler states
            self.optimizer.load_state_dict(
                torch.load(os.path.join(model_path, "optimizer.pt"), map_location=self.args.device)
            )
            self.lr_scheduler.load_state_dict(torch.load(os.path.join(model_path, "scheduler.pt")))

        model = self.model

        # Evaluate before training.
        if self.args.evaluate_before_training:
            self.evaluate(epoch=0)  # No need to report to hp search.

        # --- low rank analysis project ---
        callback = None

        if self.auxiliary_args.orthogonal_projection_path is not None:
            state_dicts = torch.load(self.auxiliary_args.orthogonal_projection_path)
            # Kept on CPU during most of the time of training.
            orthogonal_projection = state_dicts.get("eigenvectors")[:, :self.auxiliary_args.orthogonal_projection_rank]

            def callback(privacy_engine):
                """Orthogonally project flattened `.summed_grad` with projection matrix then fill this back."""
                named_params = privacy_engine.named_params

                # Collect.
                flat_grad = []
                for _, param in named_params:
                    flat_grad.append(param.summed_grad.flatten())
                    param.summed_grad = None  # Save memory.
                flat_grad = torch.cat(flat_grad)

                # Project.
                P = orthogonal_projection  # noqa
                if orthogonal_projection.device != flat_grad.device or orthogonal_projection.dtype != flat_grad.dtype:
                    P = orthogonal_projection.to(flat_grad)  # noqa
                Pt_flat_g = torch.matmul(P.t(), flat_grad)  # noqa
                # Matrix multiplication with very large dimension (millions in this case) results in weird issues.
                # In this case, `torch.matmul` fails due to calling some algo. Resorting to `torch.mm` for now.
                flat_grad = torch.mm(P, Pt_flat_g[:, None]).squeeze()

                # Redistribute.
                grads = utils.flat_to_shape(flat_tensor=flat_grad, shapes=[param.shape for _, param in named_params])
                for (_, param), grad in utils.zip_(named_params, grads):
                    param.summed_grad = grad

        if self.auxiliary_args.store_grads:
            store_grads_dir = utils.join(self.args.output_dir, 'grad_trajectory')
            utils.makedirs(store_grads_dir, exist_ok=True)
        else:
            store_grads_dir = None
        # ---

        # Train!
        total_train_batch_size = (
            self.args.train_batch_size
            * self.args.gradient_accumulation_steps
            * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)
        )
        logger.warning("***** Running training *****")
        logger.warning("  Num examples = %d", self.num_examples(train_dataloader))
        logger.warning("  Num Epochs = %d", num_train_epochs)
        logger.warning("  Instantaneous batch size per device = %d", self.args.per_device_train_batch_size)
        logger.warning("  Total train batch size (w. parallel, distributed & accumulation) = %d",
                       total_train_batch_size)
        logger.warning("  Gradient Accumulation steps = %d", self.args.gradient_accumulation_steps)
        logger.warning("  Total optimization steps = %d", t_total)

        self.global_step = 0
        self.epoch = 0
        self.total_flos = 0
        epochs_trained = 0
        steps_trained_in_current_epoch = 0
        # Check if continuing training from a checkpoint
        if model_path is not None:
            # set global_step to global_step of last saved checkpoint from model path
            try:
                self.global_step = int(model_path.split("-")[-1].split(os.path.sep)[0])
                if self.args.n_gpu > 1:
                    self.total_flos = getattr(model.module.config, "total_flos", 0)
                else:
                    self.total_flos = getattr(model.config, "total_flos", 0)

                epochs_trained = self.global_step // num_update_steps_per_epoch
                steps_trained_in_current_epoch = self.global_step % (num_update_steps_per_epoch)

                logger.info("  Continuing training from checkpoint, will skip to saved global_step")
                logger.info("  Continuing training from epoch %d", epochs_trained)
                logger.info("  Continuing training from global step %d", self.global_step)
                logger.info("  Continuing training from %d non-embedding floating-point operations", self.total_flos)
                logger.info("  Will skip the first %d steps in the first epoch", steps_trained_in_current_epoch)
            except ValueError:
                self.global_step = 0
                self.total_flos = 0
                logger.info("  Starting fine-tuning.")

        tr_loss = torch.tensor(0.0).to(self.args.device)
        logging_loss_scalar = 0.0
        disable_tqdm = self.args.disable_tqdm or not self.is_local_process_zero()
        train_pbar = trange(epochs_trained, int(np.ceil(num_train_epochs)), desc="Epoch", disable=disable_tqdm)
        for epoch in range(epochs_trained, int(np.ceil(num_train_epochs))):
            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                train_dataloader.sampler.set_epoch(epoch)

            epoch_iterator = train_dataloader

            # Reset the past mems state at the beginning of each epoch if necessary.
            if self.args.past_index >= 0:
                self._past = None

            # This extra step is crucial. The problem is that the total number of steps in one epoch might
            # not divide the number of accumulation steps, thus the accumulated .summed_grad (.grad) might overflow to
            # the next epoch, causing more gradient signal than there truly is.
            model.zero_grad(set_to_none=True)

            epoch_pbar = tqdm(epoch_iterator, desc="Iteration", disable=disable_tqdm)
            for step, inputs in enumerate(epoch_iterator):
                # Skip past any already trained steps if resuming training
                if steps_trained_in_current_epoch > 0:
                    steps_trained_in_current_epoch -= 1
                    epoch_pbar.update(1)
                    continue

                losses = self.training_step(model, inputs)
                tr_loss += losses["scalar_loss"]
                self.total_flos += self.floating_point_ops(inputs)

                if (step + 1) % self.args.gradient_accumulation_steps == 0 or (
                    # last step in epoch but step is always smaller than gradient_accumulation_steps
                    self.args.gradient_accumulation_steps >= len(epoch_iterator) == (step + 1)
                ):
                    if self.privacy_args.non_private:
                        # Don't double clip in private learning.
                        torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)
                        self.optimizer.step()
                    else:
                        if store_grads_dir is not None:
                            def callback(privacy_engine):
                                named_params = privacy_engine.named_params
                                flat_grad = torch.cat([param.summed_grad.flatten() for _, param in named_params])
                                flat_grad.div_(privacy_engine.batch_size)
                                torch.save(
                                    {"flat_grad": flat_grad.cpu().float()},
                                    utils.join(store_grads_dir, f'global_step_{self.global_step:06d}.ckpt')
                                )

                        vector_loss = losses.get("vector_loss")
                        self.optimizer.step(loss=vector_loss, callback=callback)

                    self.lr_scheduler.step()
                    model.zero_grad(set_to_none=True)

                    self.global_step += 1
                    self.epoch = epoch + (step + 1) / len(epoch_iterator)

                    if (self.args.logging_steps > 0 and self.global_step % self.args.logging_steps == 0) or (
                        self.global_step == 1 and self.args.logging_first_step
                    ):
                        logs: Dict[str, float] = {}
                        tr_loss_scalar = tr_loss.item()
                        logs["loss"] = (tr_loss_scalar - logging_loss_scalar) / self.args.logging_steps
                        # backward compatibility for pytorch schedulers
                        logs["learning_rate"] = (
                            self.lr_scheduler.get_last_lr()[0]
                            if version.parse(torch.__version__) >= version.parse("1.4")
                            else self.lr_scheduler.get_lr()[0]
                        )
                        logging_loss_scalar = tr_loss_scalar

                        self.log(logs)

                    if (
                        self.args.evaluation_strategy in (EvaluationStrategy.STEPS, IntervalStrategy.STEPS)
                        and self.global_step % self.args.eval_steps == 0
                    ):
                        self.evaluate(epoch=epoch)

                    if self.args.save_steps > 0 and self.global_step % self.args.save_steps == 0:
                        # In all cases (even distributed/parallel), self.model is always a reference
                        # to the model we want to save.
                        if hasattr(model, "module"):
                            assert (
                                model.module is self.model
                            ), f"Module {model.module} should be a reference to self.model"
                        else:
                            assert model is self.model, f"Model {model} should be a reference to self.model"
                        # Save model checkpoint
                        checkpoint_folder = f"{PREFIX_CHECKPOINT_DIR}-{self.global_step}"
                        output_dir = os.path.join(self.args.output_dir, checkpoint_folder)

                        self.store_flos()
                        self.save_model(output_dir)

                        if self.is_world_process_zero():
                            self._rotate_checkpoints(use_mtime=True)
                            torch.save(self.optimizer.state_dict(), os.path.join(output_dir, "optimizer.pt"))
                            torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, "scheduler.pt"))
                else:
                    if not self.privacy_args.non_private:
                        self.optimizer.virtual_step(loss=losses.get("vector_loss"))

                epoch_pbar.update(1)
                if self.args.max_steps > 0 and self.global_step >= self.args.max_steps:
                    break

            epoch_pbar.close()
            train_pbar.update(1)

            if (
                self.args.evaluation_strategy in (EvaluationStrategy.EPOCH, IntervalStrategy.EPOCH) and
                (epoch + 1) % self.args.eval_epochs == 0
            ):
                metrics = self.evaluate(epoch=epoch)

            if self.args.max_steps is not None and 0 < self.args.max_steps <= self.global_step:
                break

        train_pbar.close()
        if self.args.past_index and hasattr(self, "_past"):
            # Clean the state at the end of training
            delattr(self, "_past")

        logger.info("\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n")
        return TrainOutput(self.global_step, tr_loss.item() / self.global_step, metrics=dict())

    def log(self, logs: Dict[str, float], iterator: Optional[tqdm] = None) -> None:
        """
        Log :obj:`logs` on the various objects watching training.

        Subclass and override this method to inject custom behavior.

        Args:
            logs (:obj:`Dict[str, float]`):
                The values to log.
            iterator (:obj:`tqdm`, `optional`):
                A potential tqdm progress bar to write the logs on.
        """
        if self.epoch is not None:
            logs["epoch"] = self.epoch
        if self.total_flos is not None:
            if self.args.local_rank != -1:
                total_flos = distributed_broadcast_scalars([self.total_flos]).sum().item()
            else:
                total_flos = self.total_flos
            if total_flos > 0:
                logs["total_flos"] = self.total_flos
        if self.global_step is None:
            # when logging evaluation metrics without training
            self.global_step = 0
        output = {
            **logs,
            **{
                "step": self.global_step,
                'num_params': self.num_params,
                'num_non_embedding_params': self.num_non_embedding_params
            }
        }
        if self.is_world_process_zero():
            self.log_history.append(output)
        if iterator is not None:
            iterator.write(output)
        else:
            print(output)

    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:
        for k, v in inputs.items():
            if isinstance(v, torch.Tensor):
                inputs[k] = v.to(self.args.device)

        # GPT-2 don't use these; these are mostly for encoder-decoder architectures.
        inputs.pop('src_attn', None)
        inputs.pop('tgt_attn', None)
        inputs.pop('src', None)
        return inputs

    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> dict:
        model.train()
        inputs = self._prepare_inputs(inputs)
        loss = self.compute_loss(model, inputs)  # (batch_size,).

        vector_loss = loss
        scalar_loss = loss.mean(dim=0) / self.args.gradient_accumulation_steps

        if self.privacy_args.non_private:
            scalar_loss.backward()

        scalar_loss = scalar_loss.detach()
        return dict(vector_loss=vector_loss, scalar_loss=scalar_loss)

    def compute_loss(self, model, inputs):
        labels = inputs.pop('labels')
        outputs = model(**inputs)

        logits = outputs.logits
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        seq_lens = (shift_labels != -100).sum(dim=1)
        loss = F.cross_entropy(shift_logits.permute(0, 2, 1), shift_labels, reduction="none")
        loss = loss.sum(dim=1) / seq_lens  # Per token loss.

        # Save past state if it exists
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]

        return loss  # (batch_size,)

    def is_local_master(self) -> bool:
        """
        Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on
        several machines) main process.

        .. warning::

            This method is deprecated, use :meth:`~transformers.Trainer.is_local_process_zero` instead.
        """
        warnings.warn("This method is deprecated, use `Trainer.is_local_process_zero()` instead.", FutureWarning)
        return self.is_local_process_zero()

    def is_local_process_zero(self) -> bool:
        """
        Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on
        several machines) main process.
        """
        return self.args.local_rank in [-1, 0]

    def is_world_master(self) -> bool:
        """
        Whether or not this process is the global main process (when training in a distributed fashion on
        several machines, this is only going to be :obj:`True` for one process).

        .. warning::

            This method is deprecated, use :meth:`~transformers.Trainer.is_world_process_zero` instead.
        """
        warnings.warn("This method is deprecated, use `Trainer.is_world_process_zero()` instead.", FutureWarning)
        return self.is_world_process_zero()

    def is_world_process_zero(self) -> bool:
        """
        Whether or not this process is the global main process (when training in a distributed fashion on
        several machines, this is only going to be :obj:`True` for one process).
        """
        return self.args.local_rank == -1 or torch.distributed.get_rank() == 0

    def save_model(self, output_dir: Optional[str] = None):
        """
        Will save the model, so you can reload it using :obj:`from_pretrained()`.

        Will only save from the world_master process (unless in TPUs).
        """

        if is_torch_tpu_available():
            self._save_tpu(output_dir)
        elif self.is_world_process_zero():
            self._save(output_dir)

    def _save(self, output_dir: Optional[str] = None):
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        logger.info("Saving model checkpoint to %s", output_dir)
        # Save a trained model and configuration using `save_pretrained()`.
        # They can then be reloaded using `from_pretrained()`
        if not isinstance(self.model, PreTrainedModel):
            raise ValueError("Trainer.model appears to not be a PreTrainedModel")
        self.model.save_pretrained(output_dir)  # Find the models in `train_dir/checkpoint-k/pytorch_model.bin`
        if self.tokenizer is not None:
            self.tokenizer.save_pretrained(output_dir)

        # Good practice: save your training arguments together with the trained model
        torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
        json.dump(
            self.log_history, open(os.path.join(output_dir, "log_history.json"), "w"), indent=4, ensure_ascii=False
        )

    def store_flos(self):
        # Storing the number of floating-point operations that went into the model
        if self.total_flos is not None:
            if self.args.local_rank != -1:
                total_flos = distributed_broadcast_scalars([self.total_flos]).sum().item()
            else:
                total_flos = self.total_flos
            if total_flos > 0:
                self.model.config.total_flos = total_flos

    def _sorted_checkpoints(self, checkpoint_prefix=PREFIX_CHECKPOINT_DIR, use_mtime=False) -> List[str]:
        output_dir_name = os.path.basename(self.args.output_dir)
        checkpoint_prefix = f"{output_dir_name}-{PREFIX_CHECKPOINT_DIR}"

        ordering_and_checkpoint_path = []

        glob_checkpoints = [str(x) for x in Path(self.args.output_dir).glob(f"{checkpoint_prefix}-*")]

        for path in glob_checkpoints:
            if use_mtime:
                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))
            else:
                regex_match = re.match(f".*{checkpoint_prefix}-([0-9]+)", path)
                if regex_match and regex_match.groups():
                    ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))

        checkpoints_sorted = sorted(ordering_and_checkpoint_path)
        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]
        return checkpoints_sorted

    def _rotate_checkpoints(self, use_mtime=False) -> None:
        if self.args.save_total_limit is None or self.args.save_total_limit <= 0:
            return

        # Check if we should delete older checkpoint(s)
        checkpoints_sorted = self._sorted_checkpoints(use_mtime=use_mtime)
        if len(checkpoints_sorted) <= self.args.save_total_limit:
            return

        number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - self.args.save_total_limit)
        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]
        for checkpoint in checkpoints_to_be_deleted:
            logger.info("Deleting older checkpoint [{}] due to args.save_total_limit".format(checkpoint))
            shutil.rmtree(checkpoint)

    def evaluate(self, log_results=True, epoch=None) -> Dict[str, float]:
        """
        Run evaluation and returns metrics.

        The calling script will be responsible for providing a method to compute metrics, as they are
        task-dependent (pass it to the init :obj:`compute_metrics` argument).

        You can also subclass and override this method to inject custom behavior.

        Args:
            log_results:
                Store the results in `self.log_history` and print to stdout.

        Returns:
            A dictionary containing the evaluation loss and the potential metrics computed from the predictions.
        """

        eval_dataloader = self.get_eval_dataloader(self.eval_dataset)
        eval_output = self.prediction_loop(eval_dataloader, description="Evaluate eval split")

        val_dataloader = self.get_eval_dataloader(self.val_dataset)
        val_output = self.prediction_loop(val_dataloader, description="Evaluate val split")

        train_sampler = self._get_train_sampler(shuffle=False)  # Don't shuffle during evaluation!
        train_dataloader = self.get_train_dataloader(train_sampler=train_sampler)
        train_output = self.prediction_loop(train_dataloader, description="Evaluate train split")

        metrics = {
            "train": train_output.metrics,
            "eval": eval_output.metrics,
            "val": val_output.metrics,
            "epoch": epoch,
            "lr": [pg["lr"] for pg in self.optimizer.param_groups],
        }

        if hasattr(self.optimizer, 'privacy_engine'):
            pe = self.optimizer.privacy_engine
            privacy_metrics = pe.get_privacy_spent(accounting_mode="all", lenient=True)
            privacy_stats = pe.get_training_stats()
            metrics = {**metrics, **privacy_metrics, **privacy_stats}

        # Generate with beam search.
        if not self.args.skip_generation:
            self.generate_and_write_to_file()

        if log_results:
            self.log(metrics)

            # Save log history always! This must appear after the `log_history` is updated.
            json.dump(
                self.log_history,
                open(os.path.join(self.args.output_dir, "log_history.json"), "w"),
                indent=2,
                ensure_ascii=False
            )

        return metrics

    def _get_loader_by_split(self, split):
        if split == "train":
            loader = self.get_train_dataloader()
        else:
            if split == "val":
                loader = self.get_eval_dataloader(self.val_dataset)
            elif split == "eval":
                loader = self.get_eval_dataloader(self.eval_dataset)
            else:
                raise ValueError(f"Unknown split: {split}")
        return loader

    def _get_prompt_dataset_by_split(self, split):
        return {
            "train": self.generation_stuff["train_prompts"],
            "val": self.generation_stuff["val_prompts"],
            "eval": self.generation_stuff["eval_prompts"],
        }[split]

    def generate_and_write_to_file(self, num_generations_to_print=6, **decoding_kwargs):
        # Pass in the additional decoding stuff from `decoding_kwargs`.

        models = (self.model,)
        model_tags = ("model",)
        all_generations = {model_tag: {} for model_tag in model_tags}

        for this_model, this_model_tag in utils.zip_(models, model_tags):
            kwargs = dict(model=this_model, tokenizer=self.tokenizer, device=self.args.device)
            this_generations = all_generations[this_model_tag]

            for split in ("train", "val", "eval"):
                # Don't use the loader to avoid duplicated prompts!
                prompt_dataset = self._get_prompt_dataset_by_split(split)
                if split == "train":  # Don't waste compute on sanity checks.
                    max_generations = self.args.max_generations_train
                elif split in ('val', 'valid'):  # Use val and valid interchangeably.
                    max_generations = self.args.max_generations_valid
                else:
                    max_generations = self.args.max_generations

                full_generations, unstripped_generations, generations, references = decoding_utils.generate(
                    prompt_dataset=prompt_dataset, max_generations=max_generations,
                    **kwargs, **decoding_kwargs
                )
                this_generations[split] = dict(
                    full_generations=full_generations,
                    unstripped_generations=unstripped_generations,
                    generations=generations,
                    references=references,
                )

                def pretty_format(lines):
                    """A useful helper to make printted generationed look nice."""
                    return '\n'.join([repr(line) for line in lines[:num_generations_to_print]])

                # Various visuals.
                print(f" --- split {split} --- ")
                print(f" *** full generations *** ")
                print(pretty_format(full_generations))
                print(f" *** unstripped generations *** ")
                print(pretty_format(unstripped_generations))
                print(f" *** generations *** ")
                print(pretty_format(generations))
                print(f" *** references *** ")
                print(pretty_format(references))
                print(f" *** num generations: {len(generations)}, num references: {len(references)} *** ")

                # Store generations for BLEU.
                counter = self.global_step if self.global_step is not None else -1
                generations_path = os.path.join(
                    self.args.output_dir,
                    f'generations_{this_model_tag}', f'{split}', f'global_step_{counter:08d}.txt'
                )
                os.makedirs(os.path.dirname(generations_path), exist_ok=True)
                with open(generations_path, 'w') as f:
                    f.writelines([line + '\n' for line in generations])
                logger.warning(f"Wrote generations to {generations_path}")

    def prediction_loop(
        self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None
    ) -> PredictionOutput:

        prediction_loss_only = (
            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only
        )

        assert not getattr(
            self.model.config, "output_attentions", False
        ), "The prediction loop does not work with `output_attentions=True`."
        assert not getattr(
            self.model.config, "output_hidden_states", False
        ), "The prediction loop does not work with `output_hidden_states=True`."

        batch_size = dataloader.batch_size
        logger.info("***** Running %s *****", description)
        logger.info("  Num examples = %d", self.num_examples(dataloader))
        logger.info("  Batch size = %d", batch_size)

        self.model.eval()
        models = (self.model,)
        model_tags = ("model",)

        def create_record():
            return dict(
                eval_losses=[], entropy_losses=[], tok_logprobs=[], lin_logprobs=[],
            )

        records = {model_tag: create_record() for model_tag in model_tags}
        preds = label_ids = None

        if self.args.past_index >= 0:
            self._past = None

        def eval_stats(inputs, loss, logits, labels):
            if loss is not None:
                batch_size = inputs['input_ids'].size(0)
                eval_loss = [loss] * batch_size
            else:
                eval_loss = [-1]

            if logits is not None:
                logits = logits[..., :-1, :]
                labels = labels[..., 1:]

                valid_locations = (labels != -100)
                all_log_probs = logits.log_softmax(dim=-1)  # (B, L, V).
                entropy = -(all_log_probs.exp() * all_log_probs).sum(dim=-1)  # (B, L).
                entropy = entropy[valid_locations]

                logprob = F.cross_entropy(logits.permute(0, 2, 1), labels, reduction="none")  # (B, L).
            else:
                entropy, logprob = [-1], [-1]

            return eval_loss, entropy, logprob

        disable_tqdm = not self.is_local_process_zero() or self.args.disable_tqdm
        for batch_idx, inputs in tqdm(enumerate(dataloader), desc=description, disable=disable_tqdm):
            for this_model, this_model_tag in utils.zip_(models, model_tags):
                this_record = records[this_model_tag]
                loss, logits, labels = self.prediction_step(this_model, inputs, prediction_loss_only)
                eval_loss, entropy, logprob = eval_stats(inputs, loss, logits, labels)
                this_record["eval_losses"].extend(eval_loss)
                this_record["entropy_losses"].extend(entropy.tolist())
                this_record["tok_logprobs"].extend(logprob.view(-1).tolist())
                this_record["lin_logprobs"].extend(logprob.sum(dim=-1).view(-1).tolist())

            if 0 < self.args.max_eval_batches <= batch_idx + 1:
                break

        if self.args.past_index and hasattr(self, "_past"):
            # Clean the state at the end of the evaluation loop
            delattr(self, "_past")

        # lxuechen: I removed everything regarding distributed training.
        for record_key, record_value in records.items():
            this_record = records[record_key]
            for key, value in this_record.items():
                if isinstance(value, (list, tuple)):
                    this_record[key] = np.mean(value)

        metrics = records

        return PredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics)

    def prediction_step(
        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool
    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:

        has_labels = all(inputs.get(k) is not None for k in self.args.label_names)
        inputs = self._prepare_inputs(inputs)

        with torch.no_grad():
            outputs = model(**inputs)
            loss = outputs.loss
            if has_labels:  # The .mean() is to reduce in case of distributed training
                loss = loss.mean().item()
            logits = outputs.logits

            if self.args.past_index >= 0:
                self._past = outputs[self.args.past_index if has_labels else self.args.past_index - 1]

        if prediction_loss_only:
            return loss, None, None

        if has_labels:
            labels = tuple(inputs.get(name).detach() for name in self.args.label_names)
            if len(labels) == 1:
                labels = labels[0]
        else:
            labels = None

        return loss, logits, labels

    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):
        """
        For models that inherit from :class:`~transformers.PretrainedModel`, uses
        that method to compute the number of floating point operations for every backward + forward pass. If using
        another model, either implement such a method in the model or subclass and override this method.

        Args:
            model (:obj:`nn.Module`):
                The model to evaluate.
            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):
                The inputs and targets of the model.

        Returns:
            :obj:`int`: The number of floating-point operations.
        """

        if isinstance(self.model, torch.nn.DataParallel) or isinstance(
            self.model, torch.nn.parallel.DistributedDataParallel
        ):
            model = self.model.module
        else:
            model = self.model

        if hasattr(model, "floating_point_ops"):
            return model.floating_point_ops(inputs)

        else:
            return 0

File Path: private_transformers/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from . import lora_utils
from .privacy_engine import PrivacyEngine
from .transformers_support import freeze_isolated_params_for_vit

__version__ = '0.2.3'

File Path: private_transformers/accounting/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: private_transformers/accounting/accounting_manager.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import abc
import math
from typing import Dict, Optional, Union

from . import rdp_accounting

DEFAULT_ALPHAS = tuple(1 + x / 10.0 for x in range(1, 100)) + tuple(range(12, 64))  # RDP.


class AccountingManager(abc.ABC):
    def _get_sigma_with_target_epsilon(
        self,
        target_epsilon,
        target_delta,
        sample_rate,
        steps,
        threshold,
        sigma_hi_init,
        sigma_lo_init,
    ):
        """Binary search σ given ε and δ."""
        if sigma_lo_init > sigma_hi_init:
            raise ValueError("`sigma_lo` should be smaller than `sigma_hi`.")

        # Find an appropriate region for binary search.
        sigma_hi = sigma_hi_init
        sigma_lo = sigma_lo_init

        # Ensure sigma_hi isn't too small.
        while True:
            eps = self._compute_epsilon_from_sigma(sigma_hi, sample_rate, target_delta, steps)
            if eps < target_epsilon:
                break
            sigma_hi *= 2

        # Ensure sigma_lo isn't too large.
        while True:
            eps = self._compute_epsilon_from_sigma(sigma_lo, sample_rate, target_delta, steps)
            if eps > target_epsilon:
                break
            sigma_lo /= 2

        # Binary search.
        while sigma_hi - sigma_lo > threshold:
            sigma = (sigma_hi + sigma_lo) / 2
            eps = self._compute_epsilon_from_sigma(sigma, sample_rate, target_delta, steps)
            if eps < target_epsilon:
                sigma_hi = sigma
            else:
                sigma_lo = sigma

        # Conservative estimate.
        return sigma_hi

    @abc.abstractmethod
    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:
        """Override for reporting results."""
        raise NotImplementedError

    @abc.abstractmethod
    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps) -> float:
        """Override for binary sigma search."""
        raise NotImplementedError

    def compute_sigma(
        self,
        target_epsilon: float,
        target_delta: float,
        sample_rate: float,
        epochs: Optional[Union[float, int]] = None,
        steps=None,
        threshold=1e-3,
        sigma_hi_init=4,
        sigma_lo_init=0.1,
    ) -> float:
        if steps is None:
            if epochs is None:
                raise ValueError("Epochs and steps cannot both be None.")
            steps = math.ceil(epochs / sample_rate)
        return self._get_sigma_with_target_epsilon(
            target_epsilon=target_epsilon,
            target_delta=target_delta,
            sample_rate=sample_rate,
            steps=steps,
            threshold=threshold,
            sigma_hi_init=sigma_hi_init,
            sigma_lo_init=sigma_lo_init,
        )


class RDPManager(AccountingManager):
    def __init__(self, alphas):
        super(RDPManager, self).__init__()
        self._alphas = alphas

    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):
        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)["eps_rdp"]

    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:
        """Compute RDP as usual, but convert to (ε, δ)-DP based on the result by Canonne, Kamath, Steinke."""
        rdp = rdp_accounting.compute_rdp(q=sample_rate, noise_multiplier=sigma, steps=steps, orders=self._alphas)
        eps, alpha = rdp_accounting.get_privacy_spent(orders=self._alphas, rdp=rdp, delta=target_delta)
        return dict(eps_rdp=eps, alpha_rdp=alpha)


class GLWManager(AccountingManager):
    def __init__(self, eps_error=0.05):
        super(GLWManager, self).__init__()
        self._eps_error = eps_error

    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):
        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)["eps_upper"]  # Be conservative.

    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:
        if steps == 0:
            return dict(eps_low=None, eps_estimate=None, eps_upper=None)

        from prv_accountant import Accountant
        accountant = Accountant(
            noise_multiplier=sigma,
            sampling_probability=sample_rate,
            delta=target_delta,
            eps_error=self._eps_error,
            max_compositions=steps
        )
        eps_low, eps_estimate, eps_upper = accountant.compute_epsilon(num_compositions=steps)
        return dict(eps_low=eps_low, eps_estimate=eps_estimate, eps_upper=eps_upper)

File Path: private_transformers/accounting/rdp_accounting.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""
This file is adapted from the privacy accounting procedure in Opacus', which in turn is adapted from tf-privacy.
Below is the original documentation in Opacus.

*Based on Google's TF Privacy:* https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/analysis
/rdp_accountant.py.
*Here, we update this code to Python 3, and optimize dependencies.*

Functionality for computing Renyi Differential Privacy (RDP) of an additive
Sampled Gaussian Mechanism (SGM).

Example:
    Suppose that we have run an SGM applied to a function with L2-sensitivity of 1.

    Its parameters are given as a list of tuples
    ``[(q_1, sigma_1, steps_1), ..., (q_k, sigma_k, steps_k)],``
    and we wish to compute epsilon for a given target delta.

    The example code would be:

    >>> max_order = 32
    >>> orders = range(2, max_order + 1)
    >>> rdp = np.zeros_like(orders, dtype=float)
    >>> for q, sigma, steps in parameters:
    >>>     rdp += privacy_analysis.compute_rdp(q, sigma, steps, orders)
    >>> epsilon, opt_order = privacy_analysis.get_privacy_spent(orders, rdp, delta)
"""

import math
import numpy as np
from scipy import special
from typing import List, Sequence, Union


########################
# LOG-SPACE ARITHMETIC #
########################


def _log_add(logx: float, logy: float) -> float:
    r"""Adds two numbers in the log space.

    Args:
        logx: First term in log space.
        logy: Second term in log space.

    Returns:
        Sum of numbers in log space.
    """
    a, b = min(logx, logy), max(logx, logy)
    if a == -np.inf:  # adding 0
        return b
    # Use exp(a) + exp(b) = (exp(a - b) + 1) * exp(b)
    return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)


def _log_sub(logx: float, logy: float) -> float:
    r"""Subtracts two numbers in the log space.

    Args:
        logx: First term in log space. Expected to be greater than the second term.
        logy: First term in log space. Expected to be less than the first term.

    Returns:
        Difference of numbers in log space.

    Raises:
        ValueError
            If the result is negative.
    """
    if logx < logy:
        raise ValueError("The result of subtraction must be non-negative.")
    if logy == -np.inf:  # subtracting 0
        return logx
    if logx == logy:
        return -np.inf  # 0 is represented as -np.inf in the log space.

    try:
        # Use exp(x) - exp(y) = (exp(x - y) - 1) * exp(y).
        return math.log(math.expm1(logx - logy)) + logy  # expm1(x) = exp(x) - 1
    except OverflowError:
        return logx


def _compute_log_a_for_int_alpha(q: float, sigma: float, alpha: int) -> float:
    r"""Computes :math:`log(A_\alpha)` for integer ``alpha``.

    Notes:
        Note that
        :math:`A_\alpha` is real valued function of ``alpha`` and ``q``,
        and that 0 < ``q`` < 1.

        Refer to Section 3.3 of https://arxiv.org/pdf/1908.10530.pdf for details.

    Args:
        q: Sampling rate of SGM.
        sigma: The standard deviation of the additive Gaussian noise.
        alpha: The order at which RDP is computed.

    Returns:
        :math:`log(A_\alpha)` as defined in Section 3.3 of
        https://arxiv.org/pdf/1908.10530.pdf.
    """

    # Initialize with 0 in the log space.
    log_a = -np.inf

    for i in range(alpha + 1):
        log_coef_i = (
            math.log(special.binom(alpha, i))
            + i * math.log(q)
            + (alpha - i) * math.log(1 - q)
        )

        s = log_coef_i + (i * i - i) / (2 * (sigma ** 2))
        log_a = _log_add(log_a, s)

    return float(log_a)


def _compute_log_a_for_frac_alpha(q: float, sigma: float, alpha: float) -> float:
    r"""Computes :math:`log(A_\alpha)` for fractional ``alpha``.

    Notes:
        Note that
        :math:`A_\alpha` is real valued function of ``alpha`` and ``q``,
        and that 0 < ``q`` < 1.

        Refer to Section 3.3 of https://arxiv.org/pdf/1908.10530.pdf for details.

    Args:
        q: Sampling rate of SGM.
        sigma: The standard deviation of the additive Gaussian noise.
        alpha: The order at which RDP is computed.

    Returns:
        :math:`log(A_\alpha)` as defined in Section 3.3 of
        https://arxiv.org/pdf/1908.10530.pdf.
    """
    # The two parts of A_alpha, integrals over (-inf,z0] and [z0, +inf), are
    # initialized to 0 in the log space:
    log_a0, log_a1 = -np.inf, -np.inf
    i = 0

    z0 = sigma ** 2 * math.log(1 / q - 1) + 0.5

    while True:  # do ... until loop
        coef = special.binom(alpha, i)
        log_coef = math.log(abs(coef))
        j = alpha - i

        log_t0 = log_coef + i * math.log(q) + j * math.log(1 - q)
        log_t1 = log_coef + j * math.log(q) + i * math.log(1 - q)

        log_e0 = math.log(0.5) + _log_erfc((i - z0) / (math.sqrt(2) * sigma))
        log_e1 = math.log(0.5) + _log_erfc((z0 - j) / (math.sqrt(2) * sigma))

        log_s0 = log_t0 + (i * i - i) / (2 * (sigma ** 2)) + log_e0
        log_s1 = log_t1 + (j * j - j) / (2 * (sigma ** 2)) + log_e1

        if coef > 0:
            log_a0 = _log_add(log_a0, log_s0)
            log_a1 = _log_add(log_a1, log_s1)
        else:
            log_a0 = _log_sub(log_a0, log_s0)
            log_a1 = _log_sub(log_a1, log_s1)

        i += 1
        if max(log_s0, log_s1) < -30:
            break

    return _log_add(log_a0, log_a1)


def _compute_log_a(q: float, sigma: float, alpha: float) -> float:
    r"""Computes :math:`log(A_\alpha)` for any positive finite ``alpha``.

    Notes:
        Note that
        :math:`A_\alpha` is real valued function of ``alpha`` and ``q``,
        and that 0 < ``q`` < 1.

        Refer to Section 3.3 of https://arxiv.org/pdf/1908.10530.pdf
        for details.

    Args:
        q: Sampling rate of SGM.
        sigma: The standard deviation of the additive Gaussian noise.
        alpha: The order at which RDP is computed.

    Returns:
        :math:`log(A_\alpha)` as defined in the paper mentioned above.
    """
    if float(alpha).is_integer():
        return _compute_log_a_for_int_alpha(q, sigma, int(alpha))
    else:
        return _compute_log_a_for_frac_alpha(q, sigma, alpha)


def _log_erfc(x: float) -> float:
    r"""Computes :math:`log(erfc(x))` with high accuracy for large ``x``.

    Helper function used in computation of :math:`log(A_\alpha)`
    for a fractional alpha.

    Args:
        x: The input to the function

    Returns:
        :math:`log(erfc(x))`
    """
    return math.log(2) + special.log_ndtr(-x * 2 ** 0.5)


def _compute_rdp(q: float, sigma: float, alpha: float) -> float:
    r"""Computes RDP of the Sampled Gaussian Mechanism at order ``alpha``.

    Args:
        q: Sampling rate of SGM.
        sigma: The standard deviation of the additive Gaussian noise.
        alpha: The order at which RDP is computed.

    Returns:
        RDP at order ``alpha``; can be np.inf.
    """
    if q == 0:
        return 0

    # no privacy
    if sigma == 0:
        return np.inf

    if q == 1.0:
        return alpha / (2 * sigma ** 2)

    if np.isinf(alpha):
        return np.inf

    return _compute_log_a(q, sigma, alpha) / (alpha - 1)


def compute_rdp(
    q: float, noise_multiplier: float, steps: int, orders: Union[Sequence[float], float]
) -> Union[List[float], float]:
    r"""Computes Renyi Differential Privacy (RDP) guarantees of the
    Sampled Gaussian Mechanism (SGM) iterated ``steps`` times.

    Args:
        q: Sampling rate of SGM.
        noise_multiplier: The ratio of the standard deviation of the
            additive Gaussian noise to the L2-sensitivity of the function
            to which it is added. Note that this is same as the standard
            deviation of the additive Gaussian noise when the L2-sensitivity
            of the function is 1.
        steps: The number of iterations of the mechanism.
        orders: An array (or a scalar) of RDP orders.

    Returns:
        The RDP guarantees at all orders; can be ``np.inf``.
    """
    if isinstance(orders, float):
        rdp = _compute_rdp(q, noise_multiplier, orders)
    else:
        rdp = np.array([_compute_rdp(q, noise_multiplier, order) for order in orders])

    return rdp * steps


# Based on
#   https://github.com/tensorflow/privacy/blob/5f07198b66b3617b22609db983926e3ba97cd905/tensorflow_privacy/privacy/analysis/rdp_accountant.py#L237
def get_privacy_spent(orders, rdp, delta):
    """Compute epsilon given a list of RDP values and target delta.
    Args:
        orders: An array (or a scalar) of orders.
        rdp: A list (or a scalar) of RDP guarantees.
        delta: The target delta.
    Returns:
        Pair of (eps, optimal_order).
    Raises:
        ValueError: If input is malformed.
    """
    orders_vec = np.atleast_1d(orders)
    rdp_vec = np.atleast_1d(rdp)

    if delta <= 0:
        raise ValueError("Privacy failure probability bound delta must be >0.")
    if len(orders_vec) != len(rdp_vec):
        raise ValueError("Input lists must have the same length.")

    # Basic bound (see https://arxiv.org/abs/1702.07476 Proposition 3 in v3):
    #   eps = min( rdp_vec - math.log(delta) / (orders_vec - 1) )

    # Improved bound from https://arxiv.org/abs/2004.00010 Proposition 12 (in v4).
    # Also appears in https://arxiv.org/abs/2001.05990 Equation 20 (in v1).
    eps_vec = []
    for (a, r) in zip(orders_vec, rdp_vec):
        if a < 1:
            raise ValueError("Renyi divergence order must be >=1.")
        if r < 0:
            raise ValueError("Renyi divergence must be >=0.")

        if delta ** 2 + math.expm1(-r) >= 0:
            # In this case, we can simply bound via KL divergence:
            # delta <= sqrt(1-exp(-KL)).
            eps = 0  # No need to try further computation if we have eps = 0.
        elif a > 1.01:
            # This bound is not numerically stable as alpha->1.
            # Thus we have a min value of alpha.
            # The bound is also not useful for small alpha, so doesn't matter.
            eps = r + math.log1p(-1 / a) - math.log(delta * a) / (a - 1)
        else:
            # In this case we can't do anything. E.g., asking for delta = 0.
            eps = np.inf
        eps_vec.append(eps)

    idx_opt = np.argmin(eps_vec)
    return max(0, eps_vec[idx_opt]), orders_vec[idx_opt]

File Path: private_transformers/autograd_grad_sample.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
A large portion of this code is adapted from Opacus (https://github.com/pytorch/opacus),
which is licensed under Apache License 2.0.

We have modified it considerably to support ghost clipping.
"""

from typing import Tuple

import torch
import torch.nn as nn

from .settings import BackwardHookMode
from .supported_layers_grad_samplers import _supported_layers_grad_samplers

# TODO: hooks mode should be settable based on the module.
_hooks_disabled: bool = False
_hooks_mode = BackwardHookMode.default


def set_hooks_mode(mode):
    if mode not in BackwardHookMode.all():
        raise ValueError(f"Unknown mode for hooks: {mode}; expected one of {BackwardHookMode.all()}.")

    global _hooks_mode
    _hooks_mode = mode  # Set mode.

    if _hooks_mode == BackwardHookMode.ghost_grad:  # Second backward pass of ghost clipping doesn't need hooks.
        disable_hooks()
    elif _hooks_mode == BackwardHookMode.ghost_norm:  # First backward pass of ghost clipping needs to accumulate norms.
        enable_hooks()


def get_hooks_mode():
    global _hooks_mode
    return _hooks_mode


def requires_grad(module: nn.Module, recurse: bool = False) -> bool:
    """
    Checks if any parameters in a specified module require gradients.

    Args:
        module: PyTorch module whose parameters are examined
        recurse: Flag specifying if the gradient requirement check should
            be applied recursively to sub-modules of the specified module

    Returns:
        Flag indicate if any parameters require gradients
    """
    return any(p.requires_grad for p in module.parameters(recurse))


def add_hooks(model: nn.Module, loss_reduction: str = "mean"):
    r"""
    Adds hooks to model to save activations and backprop values.
    The hooks will

    1. save activations into ``param.activations`` during forward pass.
    2. compute per-sample gradients and save them in ``param.grad_sample`` during backward pass.

    Args:
        model: Model to which hooks are added.
        loss_reduction: Indicates if the loss reduction (for aggregating the gradients) is a sum or a mean operation.
            Can take values ``sum`` or ``mean``.
    """
    if hasattr(model, "autograd_grad_sample_hooks"):
        raise ValueError("Trying to add hooks twice to the same model")

    enable_hooks()

    handles = []
    for name, layer in model.named_modules():
        if type(layer) in _supported_layers_grad_samplers:
            if requires_grad(layer, recurse=False):
                handles.append(layer.register_forward_hook(_capture_activations))

                def this_backward(this_layer, grad_input, grad_output):
                    return _capture_backprops(this_layer, grad_input, grad_output, loss_reduction)

                # Starting with 1.8.0, use `register_full_backward_hook`.
                handles.append(layer.register_backward_hook(this_backward))

    model.__dict__.setdefault("autograd_grad_sample_hooks", []).extend(handles)


def remove_hooks(model: nn.Module):
    """Removes hooks added by `add_hooks()`."""
    if not hasattr(model, "autograd_grad_sample_hooks"):
        raise ValueError("Asked to remove hooks, but no hooks found")
    else:
        for handle in model.autograd_grad_sample_hooks:
            handle.remove()
        del model.autograd_grad_sample_hooks


def disable_hooks():
    """Globally disables all hooks installed by this library."""
    global _hooks_disabled
    _hooks_disabled = True


def enable_hooks():
    """Globally enables all hooks installed by this library."""
    global _hooks_disabled
    _hooks_disabled = False


def _capture_activations(layer: nn.Module, inputs: Tuple, outputs: Tuple):
    """Forward hook handler captures and saves activations."""
    if not requires_grad(layer) or not layer.training or _hooks_disabled:
        return

    if not hasattr(layer, "activations"):
        layer.activations = []

    # This improves on original Opacus and supports additional arguments on top of the (first) activation tensor.
    stored_inputs = tuple(input_i.detach() if torch.is_tensor(input_i) else input_i for input_i in inputs)
    layer.activations.append(stored_inputs)


def _capture_backprops(
    layer: nn.Module,
    inputs: Tuple[torch.Tensor],
    outputs: Tuple[torch.Tensor],
    loss_reduction: str
):
    """Backward hook handler captures grad_outputs."""
    # This improves on the original Opacus codebase and supports multiple outputs.
    backprops = tuple(output_i.detach() if torch.is_tensor(output_i) else output_i for output_i in outputs)
    _compute_grad_sample(layer, backprops, loss_reduction)


def _compute_grad_sample(layer: nn.Module, backprops: Tuple, loss_reduction: str):
    """Computes per-sample gradients with respect to the parameters."""
    if not requires_grad(layer) or not layer.training or _hooks_disabled:
        return

    if not hasattr(layer, "activations"):
        raise ValueError(f"No activations detected for {type(layer)}, run forward after add_hooks(model)")

    # Outside of the LSTM there is "batch_first" but not for the Linear inside the LSTM
    if isinstance(layer.activations, list):
        A = layer.activations.pop()
    else:
        A = layer.activations

    if not hasattr(layer, "max_batch_len"):
        assert torch.is_tensor(A[0]), f"Internal error: first input of the following layer isn't a Tensor. \n{layer}"
        layer.max_batch_len = _get_batch_size(layer, A[0])

    n = layer.max_batch_len
    if loss_reduction == "mean":
        B = tuple(B_i * n if torch.is_tensor(B_i) else B_i for B_i in backprops)
    elif loss_reduction == "sum":
        B = backprops
    else:
        raise ValueError(f"loss_reduction = {loss_reduction}. Only 'sum' and 'mean' losses are supported")

    # compute grad sample for individual layers
    compute_layer_grad_sample = _supported_layers_grad_samplers.get(type(layer))
    compute_layer_grad_sample(layer, A, B)

    if (not isinstance(layer.activations, list) or len(layer.activations) == 0) and hasattr(layer, "max_batch_len"):
        del layer.max_batch_len


def _get_batch_size(layer: nn.Module, grad_sample: torch.Tensor) -> int:
    r"""
    Computes and returns the maximum batch size which is the maximum of the dimension values
    along 'batch_dim' axis over layer.activations + [grad_sample], where layer.activations is
    a list. If layer.activations is a not a list, then return grad_sample.shape[batch_dim].
    """

    batch_dim = 0
    max_batch_len = 0
    if isinstance(layer.activations, list):
        for out in layer.activations:
            assert torch.is_tensor(out[0]), (
                f"Internal error: first input of the following layer isn't a Tensor. \n{layer}"
            )
            if out[0].shape[batch_dim] > max_batch_len:
                max_batch_len = out[0].shape[batch_dim]

    max_batch_len = max(max_batch_len, grad_sample.shape[batch_dim])
    return max_batch_len

File Path: private_transformers/lora_utils.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
LoRA layers.

This version does not have merged weights for zero latency inference. It makes the code easier to read and maintain.
Adapted from
    https://github.com/microsoft/LoRA
    https://www.microsoft.com/en-us/research/project/dp-transformers/
"""

import torch
import transformers
from torch import nn


class DPMergedLinear(nn.Module):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        lora_r=0,
        lora_alpha=1.,
        lora_dropout=0.,
    ):
        super(DPMergedLinear, self).__init__()
        self.linear = nn.Linear(in_features=in_features, out_features=out_features)
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = nn.Dropout(p=lora_dropout)
        if self.lora_r > 0:
            self.lora_A = nn.Linear(in_features=in_features, out_features=lora_r, bias=False)
            self.lora_B = nn.Linear(in_features=lora_r, out_features=out_features, bias=False)
            self.scaling = self.lora_alpha / lora_r
        self.reset_parameters()

    def forward(self, x: torch.Tensor):
        result = self.linear(x)
        if self.lora_r > 0:
            after_dropout = self.lora_dropout(x)
            after_A = self.lora_A(after_dropout)
            after_B = self.lora_B(after_A)
            result += after_B * self.scaling
        return result

    def reset_parameters(self):
        self.linear.reset_parameters()
        if self.lora_r > 0:
            self.lora_A.reset_parameters()
            self.lora_B.weight.data.zero_()

    @staticmethod
    def from_transformers_conv1d(
        original_layer,
        lora_r=0,
        lora_alpha=1.,
        lora_dropout=0.,
    ) -> "DPMergedLinear":
        lora_layer = DPMergedLinear(
            in_features=original_layer.weight.shape[0],
            out_features=original_layer.weight.shape[1],
            lora_r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
        ).to(original_layer.weight.device)
        lora_layer.linear.weight.data.copy_(original_layer.weight.T.data)
        lora_layer.linear.bias.data.copy_(original_layer.bias.data)
        return lora_layer


def convert_gpt2_attention_to_lora(
    model: transformers.GPT2PreTrainedModel,
    lora_r=0,
    lora_alpha=1.,
    lora_dropout=0.,
) -> transformers.GPT2PreTrainedModel:
    if not isinstance(model, transformers.GPT2PreTrainedModel):
        raise TypeError("Requires a GPT2 model")

    if not hasattr(model, "h") and hasattr(model, "transformer"):
        transformer = model.transformer
    else:
        transformer = model

    for h_i in transformer.h:
        new_layer = DPMergedLinear.from_transformers_conv1d(
            original_layer=h_i.attn.c_attn,
            lora_r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
        )
        h_i.attn.c_attn = new_layer

    return model


def mark_only_lora_as_trainable(model: torch.nn.Module) -> None:
    model.requires_grad_(True)
    for n, p in model.named_parameters():
        if 'lora_' not in n:
            p.requires_grad = False

File Path: private_transformers/privacy_engine.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Code for a privacy engine that plays nicely with Hugging Face transformers.

Design mostly based on Opacus with the exception that `.step` and `virtual_step`
takes in per-example losses, which should not be called with `.backward()` by
the user.
"""

import collections
import logging
import math
import types
from typing import Callable, Dict, Optional, Sequence, Union

import torch
from ml_swissknife import utils
from torch import nn

from . import autograd_grad_sample, transformers_support
from .accounting import accounting_manager
from .settings import AccountingMode, BackwardHookMode, ClippingMode, SUPPORTED_TRANSFORMERS


class PrivacyEngine(object):
    """Differentially-private optimization engine that works gracefully with Hugging Face transformers.

    Supports ghost clipping as described in
        Li, X., Tramèr, F., Liang, P., & Hashimoto, T. (2021).
        Large Language Models Can Be Strong Differentially Private Learners.
        arXiv preprint arXiv:2110.05679.

    Implicitly assumes inputs are in batch first format.
    """

    def __init__(
        self,
        module: nn.Module,
        *,
        batch_size: int,
        sample_size: int,
        max_grad_norm: float,
        epochs: Optional[Union[int, float]] = None,
        noise_multiplier: Optional[float] = None,
        target_epsilon: Optional[float] = None,
        target_delta: Optional[float] = None,
        alphas: Sequence[float] = accounting_manager.DEFAULT_ALPHAS,
        record_snr: bool = True,
        named_params: Optional[Sequence] = None,
        numerical_stability_constant=1e-6,
        clipping_mode=ClippingMode.default,
        accounting_mode="rdp",
        eps_error=0.05,
        skip_checks=False,
        **unused_kwargs,
    ):
        """Initialize the engine.

        Args:
            module: The PyTorch module for which per-sample gradient is required.
                Setting the `requires_grad` attribute of a parameter to False
                disables the per-sample gradient accumulation.
            batch_size: The expected size of Poisson-sampled batch, i.e., the lot size.
            sample_size: Size of dataset.
            max_grad_norm: The maximum 2-norm for gradient clipping.
            epochs: The number of epochs for training.
            noise_multiplier: The extra multiplier for DP-SGD noise.
            target_epsilon: The target privacy spending.
                Only used to estimate the `noise_multiplier` if it is not set.
            target_delta: The target failure probability.
                Defaults to sample_size ** -1.1 if not set.
            alphas: The RDP orders for (ε, δ)-DP conversion. Useless if not accounting in RDP.
            record_snr: Record and report the signal-to-noise ratio --
                ratio between norm of summed clipped gradient and norm of noise vector.
            named_params: Specifies which parameters need gradients;
                defaults to use parameters which require grad in module.
            numerical_stability_constant: Small constant to avoid division by 0 when clipping.
            clipping_mode: The clipping mode to use. One of 'default', 'ghost', 'per_layer', 'per_layer_percentile'.
            accounting_mode: The method of accounting privacy. One of (`rdp`, `glw`, `all`).
                Meanings of shorthands:
                    - rdp: Account loss with RDP but perform conversion to approx-DP with a procedure defined in
                        "The Discrete Gaussian for Differential Privacy". https://arxiv.org/abs/2004.00010
                    - glw: Account loss by numerically composing tradeoff functions in f-DP; defined in
                        "Numerical composition of differential privacy". https://arxiv.org/abs/2106.02848
                    - all: Report loss with all methods listed above.
            eps_error: Error threshold for upper and lower bound in the GLW accounting procedure.
            skip_checks: Skips the model type validation test if True.
        """
        utils.handle_unused_kwargs(unused_kwargs)
        del unused_kwargs
        super(PrivacyEngine, self).__init__()

        if clipping_mode not in ClippingMode.all():
            raise ValueError(f"Unknown clipping mode {clipping_mode}. Expected one of {ClippingMode.all()}.")
        if accounting_mode not in AccountingMode.all():
            raise ValueError(f"Unknown accounting mode: {accounting_mode}. Expected one of {AccountingMode.all()}.")
        if epochs <= 0.0:
            raise ValueError(f"Number of training epochs cannot be non-positive, but found epochs={epochs}")

        # Privacy parameters.
        sample_rate = batch_size / sample_size
        if target_delta is None:
            target_delta = sample_size ** -1.1
        if noise_multiplier is None:
            if target_epsilon is None or epochs is None:
                raise ValueError(
                    f"`target_epsilon` and `epochs` must be specified when `noise_multiplier` is `None`."
                )
            if accounting_mode in ("rdp", "all"):
                manager = accounting_manager.RDPManager(alphas=alphas)
            else:  # "glw"
                manager = accounting_manager.GLWManager(eps_error=eps_error)
            noise_multiplier = manager.compute_sigma(
                target_epsilon=target_epsilon, target_delta=target_delta, sample_rate=sample_rate, epochs=epochs,
            )

        self.batch_size = batch_size
        self.sample_size = sample_size
        self.sample_rate = sample_rate
        self.max_grad_norm = max_grad_norm

        self.epochs = epochs
        self.noise_multiplier = noise_multiplier
        self.effective_noise_multiplier = noise_multiplier / batch_size
        self.target_epsilon = target_epsilon
        self.target_delta = target_delta
        self.alphas = alphas
        self.eps_error = eps_error
        self.accounting_mode = accounting_mode
        self.record_snr = record_snr

        # Internals.
        self.steps = 0  # Tracks privacy spending.

        # Recording.
        self.max_clip = None
        self.min_clip = None
        self.med_clip = None
        self.signal = None
        self.noise = None
        self.snr = None
        self.noise_limit = None

        # Record parameters.
        self.module = module
        if named_params is None:
            self.named_params = tuple(
                (name, param) for (name, param) in module.named_parameters() if param.requires_grad
            )
        else:
            self.named_params = named_params
        self.num_params = sum(param.numel() for _, param in self.named_params)

        self._locked = False  # Lock the part where noisy gradients is created (in `self.step`) if True.
        self.numerical_stability_constant = numerical_stability_constant
        self.clipping_mode = clipping_mode
        if clipping_mode == ClippingMode.ghost:
            autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)  # Prepare for first backward.
        else:
            autograd_grad_sample.set_hooks_mode(BackwardHookMode.default)  # Extra guard.

        if not isinstance(module, SUPPORTED_TRANSFORMERS) and not skip_checks:
            raise ValueError(
                f"Model type {type(module)} is not supported. Please file an issue if you want this model to be added.\n"
                f"Currently supported transformers are: {SUPPORTED_TRANSFORMERS}"
            )
        transformers_support.forward_swapper(module=module)  # Fix the position embeddings broadcast issue.

    def lock(self):
        """Run this after noisy clipped gradient is created to prevent tampering with it before parameter update."""
        self._locked = True

    def unlock(self):
        """Run this after parameter update to allow creation of noisy gradient for next step"""
        self._locked = False

    def attach(self, optimizer):
        # `loss_reduction="sum"` super important.
        autograd_grad_sample.add_hooks(model=self.module, loss_reduction="sum")

        # Override zero grad.
        def dp_zero_grad(_self, *args, **kwargs):
            _self.privacy_engine.zero_grad()

        # Override step.
        def dp_step(_self, **kwargs):
            closure = kwargs.pop("closure", None)

            _self.privacy_engine.step(**kwargs)
            _self.original_step(closure=closure)
            _self.privacy_engine.unlock()  # Only enable creating new grads once parameters are updated.
            _self.privacy_engine.steps += 1

        def virtual_step(_self, **kwargs):
            _self.privacy_engine.virtual_step(**kwargs)

        def get_privacy_spent(_self, **kwargs):
            return _self.privacy_engine.get_privacy_spent(**kwargs)

        def get_training_stats(_self, **kwargs):
            return _self.privacy_engine.get_training_stats(**kwargs)

        optimizer.privacy_engine = self

        optimizer.original_step = optimizer.step
        optimizer.step = types.MethodType(dp_step, optimizer)

        optimizer.original_zero_grad = optimizer.zero_grad
        optimizer.zero_grad = types.MethodType(dp_zero_grad, optimizer)

        optimizer.virtual_step = types.MethodType(virtual_step, optimizer)

        # Make getting info easier.
        optimizer.get_privacy_spent = types.MethodType(get_privacy_spent, optimizer)
        optimizer.get_training_stats = types.MethodType(get_training_stats, optimizer)

        self.module.privacy_engine = self

        # Just to be safe, we also override `zero_grad` for module.
        self.module.original_zero_grad = self.module.zero_grad
        self.module.zero_grad = types.MethodType(dp_zero_grad, self.module)

        # For easy detaching.
        self.optimizer = optimizer

    def detach(self):
        optimizer = self.optimizer
        optimizer.step = optimizer.original_step
        optimizer.zero_grad = optimizer.original_zero_grad
        delattr(optimizer, "privacy_engine")
        delattr(optimizer, "original_step")
        delattr(optimizer, "original_zero_grad")
        delattr(optimizer, "virtual_step")
        delattr(optimizer, "get_privacy_spent")
        delattr(optimizer, "get_training_stats")

        module = self.module
        autograd_grad_sample.remove_hooks(module)
        autograd_grad_sample.set_hooks_mode("default")  # This is super important when there are multiple attaches!
        module.zero_grad(skip_grad=True)  # noqa
        module.zero_grad = module.original_zero_grad
        delattr(module, "original_zero_grad")

    @torch.no_grad()
    def step(
        self,
        loss: torch.Tensor,
        scale=1.,
        # Function that takes in named_params and does something.
        # This option was included to help with another spectrum analysis project.
        callback: Optional[Callable] = None,
    ):
        if loss.dim() != 1:
            raise ValueError(
                f"Expected `loss` to be the per-example loss 1-D tensor, but got a tensor with dims={loss.dim()}."
            )

        if self.clipping_mode == ClippingMode.ghost:
            if callback is not None:
                raise ValueError("Ghost clipping does not support `callback` in `optimizer.step`.")
            if scale != 1.:
                raise ValueError("Ghost clipping does not support mixed-precision training.")
            self._ghost_step(loss=loss)
        else:
            self._step(loss=loss, scale=scale, callback=callback)

    @torch.no_grad()
    def virtual_step(self, loss: torch.Tensor, scale=1.):
        """Virtual step function when there's gradient accumulation."""
        if self.clipping_mode == ClippingMode.ghost:
            self._ghost_virtual_step(loss=loss)
        else:
            self._virtual_step(loss=loss, scale=scale)

    def zero_grad(self, skip_grad=False):
        for name, param in self.named_params:
            if hasattr(param, "grad_sample"):
                del param.grad_sample
            if hasattr(param, "norm_sample"):
                del param.norm_sample
            if hasattr(param, "summed_grad"):
                del param.summed_grad
            if not skip_grad:
                if hasattr(param, "grad"):
                    del param.grad

    def _create_noisy_clipped_gradient(self):
        """Create noisy clipped gradient for `optimizer.step`.

        Add noise and scale by inverse batch size.

        Notes:
            In ghost clipping, `summed_grad` stores previous micro-batches; `grad` stores current micro-batch.
            In default clipping, `summed_grad` stores summed clipped gradients for all micro-batches.
        """

        signals, noises = [], []
        for name, param in self.named_params:
            assert hasattr(param, 'summed_grad'), (
                f"Internal error: PrivacyEngine should not reach here; "
                f"this means either "
                f"1) there is parameter which requires gradient, but was not used in the computational graph, "
                f"or 2) the backward hook registry failed to find the corresponding module to register."
            )
            param.grad = param.summed_grad  # Ultra important to override `.grad`.

            if self.record_snr:
                signals.append(param.grad.reshape(-1).norm(2))

            if self.noise_multiplier > 0 and self.max_grad_norm > 0:
                noise = torch.normal(
                    mean=0,
                    std=self.noise_multiplier * self.max_grad_norm,
                    size=param.size(),
                    device=param.device,
                    dtype=param.dtype,
                )
                param.grad += noise
                if self.record_snr:
                    noises.append(noise.reshape(-1).norm(2))
                del noise

            param.grad /= self.batch_size

        if self.record_snr and len(noises) > 0:
            self.signal, self.noise = tuple(torch.stack(lst).norm(2).item() for lst in (signals, noises))
            self.noise_limit = math.sqrt(self.num_params) * self.noise_multiplier * self.max_grad_norm
            self.snr = self.signal / self.noise
        else:
            self.snr = math.inf  # Undefined!

        self.lock()  # Make creating new gradients impossible, unless optimizer.step is called.

    # --- ghost clipping ---
    def _ghost_step(self, loss: torch.Tensor):
        """Run double-backward on per-example loss, then sum up all gradients and noise it."""
        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.
            logging.warning("Attempted to step, but the engine is on lock.")
            return

        self._ghost_virtual_step(loss)
        self._create_noisy_clipped_gradient()

    @torch.no_grad()
    def _ghost_virtual_step(self, loss: torch.Tensor):
        """Backward twice to accumulate summed clipped gradients in `.summed_grad`.

        We accumulate gradients in `.summed_grad` for micro-batching.
        All of this copying actually creates a new 2x memory overhead.
        """
        self._double_backward(loss)

        for name, param in self.named_params:
            if hasattr(param, 'summed_grad'):
                param.summed_grad += param.grad
            else:
                param.summed_grad = param.grad

            if hasattr(param, "grad"):
                del param.grad
            if hasattr(param, "norm_sample"):
                del param.norm_sample
            if hasattr(param, "grad_sample"):
                del param.grad_sample

    @torch.enable_grad()
    def _double_backward(self, loss: torch.Tensor):
        """Given per-example losses, backward twice to accumulate summed clipped gradients in `.grad`."""
        first_loss = loss.sum()
        first_loss.backward(retain_graph=True)

        # Prepare for second backward.
        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_grad)

        # The first backward might have accumulated things we don't need into `.grad`;
        # remove it before the second pass to avoid accumulating garbage.
        for name, param in self.named_params:
            if hasattr(param, "grad"):
                del param.grad

        coef_sample = self.get_coef_sample()
        second_loss = (coef_sample * loss).sum(dim=0)
        second_loss.backward()

        # Prepare for first backward (in the next round).
        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)

    def get_coef_sample(self) -> torch.Tensor:
        """Get per-example gradient scaling factor for clipping."""
        norm_sample = self.get_norm_sample()
        return torch.clamp_max(self.max_grad_norm / (norm_sample + self.numerical_stability_constant), 1.)

    def get_norm_sample(self) -> torch.Tensor:
        """Get per-example gradient norms."""
        norm_sample = torch.stack([param.norm_sample for name, param in self.named_params], dim=0).norm(2, dim=0)
        return norm_sample

    # --- default clipping ---
    def _step(
        self,
        loss,
        scale,
        callback,
    ):
        """Create noisy gradients.

        Should be run right before you call `optimizer.step`.

        This function does 3 things:
            1) call `loss.backward()`
            2) clip the current `.grad_sample` and add that to `.summed_grad`
            3) noise the gradients
        In mixed-precision training (with amp), the last two steps require knowing the loss scaling factor.

        Args:
            loss: The per-example loss; a 1-D tensor.
            scale: The loss up-scaling factor in amp. In full precision, this arg isn't useful.
        """
        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.
            logging.warning("Attempted to step, but the engine is on lock.")
            return

        norm_sample, coef_sample = self._accumulate_summed_grad(loss=loss, scale=scale)
        # Collect stats for debugging.
        self.max_clip = coef_sample.max().item()
        self.min_clip = coef_sample.min().item()
        self.med_clip = coef_sample.median().item()

        if callback is not None:
            callback(self)
        self._create_noisy_clipped_gradient()

    def _virtual_step(self, loss, scale):
        self._accumulate_summed_grad(loss=loss, scale=scale)

    @torch.no_grad()
    def _accumulate_summed_grad(self, loss, scale):
        """Accumulate signal by summing clipped gradients.

        Removes `.grad_sample` and `.grad` for each variable that requires grad at the end.
        """
        with torch.enable_grad():
            loss.sum(dim=0).backward()

        norm_sample = []
        for name, param in self.named_params:
            try:
                batch_size = param.grad_sample.size(0)
            except AttributeError as error:
                args = error.args
                extra_msg = f"\n *** {name} parameter has no grad_sample attribute ***"
                error.args = (args[0] + extra_msg, *args[1:])
                raise error
            norm = param.grad_sample.reshape(batch_size, -1).norm(2, dim=1)
            norm_sample.append(norm)

        # The stack operation here is prone to error, thus clarify where the error is.
        try:
            norm_sample = torch.stack(norm_sample, dim=0).norm(2, dim=0)
        except RuntimeError as runtime_error:
            args = runtime_error.args

            # Get the major shape.
            shapes = collections.defaultdict(int)
            for tensor in norm_sample:
                shapes[tensor.size()] += 1

            # Get the shape that most tensors have.
            major_shape, major_count = max(shapes.items(), key=lambda x: x[1])

            # Check which tensors don't have the major shape!
            extra_msg = f" \n*** Major shape: {major_shape}"
            for (name, param), tensor in zip(list(self.named_params), norm_sample):
                if tensor.size() != major_shape:
                    extra_msg += f", {name} wrong shape: {tensor.size()}"
            extra_msg += " ***"

            runtime_error.args = (args[0] + extra_msg, *args[1:])
            raise runtime_error

        coef_sample = torch.clamp_max(
            self.max_grad_norm * scale / (norm_sample + self.numerical_stability_constant), 1.
        )
        for name, param in self.named_params:
            if not hasattr(param, 'summed_grad'):
                param.summed_grad = 0.
            current_device = param.grad_sample.device
            param.summed_grad += torch.einsum("i,i...->...", coef_sample.to(current_device), param.grad_sample)

            # Aggressive memory saving -- delete everything except `.summed_grad` to save memory!
            if hasattr(param, "grad_sample"):
                # This must be deleted due to how `privacy_utils::supported_layers_grad_samplers.py` works!
                #   When a parameter with `.grad_sample` is reused, the per-sample gradients are accumulated!
                del param.grad_sample
            if hasattr(param, "grad"):
                del param.grad

        return norm_sample, coef_sample

    def get_privacy_spent(
        self,
        steps: Optional[int] = None,
        accounting_mode: Optional[str] = None,
        lenient=False
    ) -> Dict:
        if steps is None:
            steps = self.steps
        if accounting_mode is None:
            accounting_mode = self.accounting_mode

        privacy_results = {}  # Contains stats from all modes.
        if accounting_mode in (AccountingMode.all_, AccountingMode.rdp):
            try:
                manager = accounting_manager.RDPManager(alphas=self.alphas)
                privacy_results.update(
                    manager.compute_epsilon(
                        sigma=self.noise_multiplier,
                        sample_rate=self.sample_rate,
                        target_delta=self.target_delta,
                        steps=steps,
                    )
                )
            except Exception as err:
                logging.fatal("RDP accounting failed! Double check privacy parameters.")
                if not lenient:
                    raise err

        if accounting_mode in (AccountingMode.all_, AccountingMode.glw):
            try:
                manager = accounting_manager.GLWManager(eps_error=self.eps_error)
                privacy_results.update(
                    manager.compute_epsilon(
                        sigma=self.noise_multiplier,
                        sample_rate=self.sample_rate,
                        target_delta=self.target_delta,
                        steps=steps
                    )
                )
            except Exception as err:
                logging.fatal(
                    "Numerical composition of tradeoff functions failed! Double check privacy parameters."
                )
                if not lenient:
                    raise err

        return privacy_results

    def get_training_stats(self):
        """Get the clipping, signal, and noise statistics."""
        return {
            "med_clip": self.med_clip,
            "max_clip": self.max_clip,
            "min_clip": self.min_clip,
            "snr": self.snr,
            "signal": self.signal,
            "noise": self.noise,
            "noise_limit": self.noise_limit,
        }

    def __repr__(self):
        return (
            f"PrivacyEngine(\n"
            f"  target_epsilon={self.target_epsilon:.6f}, \n"
            f"  target_delta={self.target_delta:.6f}, \n"
            f"  noise_multiplier={self.noise_multiplier:.6f}, \n"
            f"  effective_noise_multiplier={self.effective_noise_multiplier:.6f}, \n"
            f"  epochs={self.epochs}, \n"
            f"  max_grad_norm={self.max_grad_norm}, \n"
            f"  sample_rate={self.sample_rate}, \n"
            f"  batch_size={self.batch_size}, \n"
            f"  accounting_mode={self.accounting_mode}, \n"
            f"  clipping_mode={self.clipping_mode}\n"
            f")"
        )

File Path: private_transformers/settings.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import transformers
from ml_swissknife import utils


class BackwardHookMode(metaclass=utils.ContainerMeta):
    ghost_norm = "ghost_norm"
    ghost_grad = "ghost_grad"
    default = "default"


class ClippingMode(metaclass=utils.ContainerMeta):
    default = "default"  # Global fixed.
    ghost = "ghost"  # Global fixed clipping with ghost clipping.
    per_layer = "per_layer"  # Per layer fixed clipping.
    per_layer_percentile = "per_layer_percentile"  # Clip gradient per-layer based on gradient norm percentile.


class AccountingMode(metaclass=utils.ContainerMeta):
    rdp = "rdp"
    glw = "glw"
    all_ = "all"


SUPPORTED_TRANSFORMERS = (
    transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel,
    transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModel,
    transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel,
    transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel,
    transformers.models.bert.modeling_bert.BertForSequenceClassification,
    transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification,
    transformers.models.albert.modeling_albert.AlbertForSequenceClassification,
    transformers.models.bart.modeling_bart.BartForConditionalGeneration,
    transformers.models.t5.modeling_t5.T5ForConditionalGeneration,
    transformers.models.opt.modeling_opt.OPTForCausalLM,
    transformers.models.vit.modeling_vit.ViTForImageClassification,
    transformers.models.deit.modeling_deit.DeiTForImageClassification,
    transformers.models.beit.modeling_beit.BeitForImageClassification,
)

File Path: private_transformers/supported_layers_grad_samplers.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
This module is a collection of grad samplers - methods to calculate per sample gradients
for a layer given two tensors: 1) inputs, and 2) grad_outputs.

Supports ghost clipping introduced in
Li, X., Tramèr, F., Liang, P., & Hashimoto, T. (2021).
Large Language Models Can Be Strong Differentially Private Learners. arXiv preprint arXiv:2110.05679.

A large portion of this code is adapted from Opacus (https://github.com/pytorch/opacus).

There's some memory and compute inefficiency. For a layer that requires grad, a parameter of it which doesn't require
grad still gets grads computed, but not stored. This is an unfortunate trade-off made to let code more readable.
"""

from typing import Tuple

import numpy as np
import torch
import transformers.pytorch_utils
from opt_einsum import contract
from torch import nn
from torch.functional import F
from transformers.models.opt.modeling_opt import OPTLearnedPositionalEmbedding
from transformers.models.t5.modeling_t5 import T5LayerNorm

from . import autograd_grad_sample
from .settings import BackwardHookMode


def sum_over_all_but_batch_and_last_n(tensor: torch.Tensor, n_dims: int) -> torch.Tensor:
    if tensor.dim() == n_dims + 1:
        return tensor
    else:
        dims = list(range(1, tensor.dim() - n_dims))
        return tensor.sum(dim=dims)


def _light_linear_weight_norm_sample(A, B) -> torch.Tensor:
    """Compute gradient sample norm for the weight matrix in a linear layer."""
    if A.dim() == 2 and B.dim() == 2:
        return _light_linear_weight_norm_sample_non_sequential(A, B)
    elif A.dim() == 3 and B.dim() == 3:
        return _light_linear_weight_norm_sample_sequential(A, B)
    else:
        raise ValueError(f"Unexpected input shape: {A.size()}, grad_output shape: {B.size()}")


def _light_linear_weight_norm_sample_sequential(A, B):
    """Lightweight norm computation in ghost clipping.

    Linear algebra identity trick -- Eq. 3 in the paper.
    """
    # TODO: This saves compute based on online dimension estimates. Downside is that it makes JIT impossible.
    #  Think harder about better solutions.
    (b, t, p), (_, _, d) = A.size(), B.size()
    if 2 * t ** 2 < p * d:
        return torch.sqrt((torch.bmm(A, A.transpose(-1, -2)) * torch.bmm(B, B.transpose(-1, -2))).sum(dim=(1, 2)))
    else:
        return torch.bmm(B.permute(0, 2, 1), A).flatten(start_dim=1).norm(2, dim=-1)


def _light_linear_weight_norm_sample_non_sequential(A, B):
    """The Goodfellow trick, i.e., Frobenius norm equal to product of 2-norms."""
    return A.norm(2, dim=1) * B.norm(2, dim=1)


def _light_linear_bias_norm_sample(B):
    if B.dim() == 2:
        return B.norm(2, dim=1)
    elif B.dim() == 3:
        return B.sum(dim=1).norm(2, dim=1)
    else:
        raise ValueError(f"Unexpected grad_output shape: {B.size()}")


def _create_or_extend_grad_sample(param: torch.Tensor, grad_sample: torch.Tensor) -> None:
    """Creates a ``grad_sample`` attribute in the given parameter or accumulate the existing tensor."""
    if hasattr(param, "requires_grad") and not param.requires_grad:
        return

    assert grad_sample.shape[1:] == param.shape, (
        f"Internal error: grad_sample.size()={grad_sample.size()}, param.size()={param.size()}"
    )

    # Warning: When a parameter with `grad_sample` is reused, the per-sample gradients are accumulated.
    if hasattr(param, "grad_sample"):
        param.grad_sample += grad_sample.detach()
    else:
        param.grad_sample = grad_sample.detach()


def _create_or_extend_norm_sample(param: torch.Tensor, norm_sample: torch.Tensor) -> None:
    """Creates a ``norm_sample`` attribute in the given parameter."""
    if not hasattr(param, "requires_grad") or not param.requires_grad:
        return

    assert autograd_grad_sample.get_hooks_mode() == BackwardHookMode.ghost_norm, (
        f"Internal error: Trying to extend `norm_sample` when "
        f"`_hooks_mode='{autograd_grad_sample.get_hooks_mode()}'`."
    )
    if hasattr(param, 'norm_sample'):
        raise ValueError(
            "Ghost clipping does not support parameter sharing. "
            "Parameter sharing may be due to default parameter sharing between lm_head and embedding."
            "Please use a model without parameter sharing for ghost clipping."
        )
    param.norm_sample = norm_sample


def _compute_linear_grad_sample(layer: nn.Linear, A: Tuple[torch.Tensor], B: Tuple[torch.Tensor]) -> None:
    """Computes per sample gradients for `nn.Linear` layer.

    This function is written in an unusually bespoke way to avoid using `torch.einsum`.
    """
    (A,), (B,) = A, B  # Unpack singleton tuples.

    if autograd_grad_sample.get_hooks_mode() == BackwardHookMode.ghost_norm:
        _create_or_extend_norm_sample(layer.weight, _light_linear_weight_norm_sample(A, B))

        if layer.bias is not None:
            _create_or_extend_norm_sample(layer.bias, _light_linear_bias_norm_sample(B))
    else:
        if B.dim() == 3 and A.dim() == 3:
            grad_weight = torch.bmm(B.permute(0, 2, 1), A)
            grad_bias = B.sum(dim=1)
        elif B.dim() == 2 and A.dim() == 2:
            grad_weight = B[:, :, None] * A[:, None, :]
            grad_bias = B
        else:
            raise ValueError(
                f"Expected both grad_output and input to have dimension 2 or 3, "
                f"but found len(grad_output.dim())={len(B.dim())}, len(input.dim())={len(A.dim())}"
            )
        _create_or_extend_grad_sample(layer.weight, grad_weight)

        if layer.bias is not None:
            _create_or_extend_grad_sample(layer.bias, grad_bias)


def _compute_layer_norm_grad_sample(layer: nn.LayerNorm, A: Tuple[torch.Tensor], B: Tuple[torch.Tensor]) -> None:
    """Computes per sample gradients for `nn.LayerNorm` layer."""
    (A,), (B,) = A, B  # Unpack singleton tuples.

    is_backward_ghost_norm = autograd_grad_sample.get_hooks_mode() == BackwardHookMode.ghost_norm

    grad_sample = sum_over_all_but_batch_and_last_n(
        F.layer_norm(A, layer.normalized_shape, eps=layer.eps) * B,
        layer.weight.dim(),
    )
    if is_backward_ghost_norm:
        norm_sample = grad_sample.flatten(start_dim=1).norm(2, dim=1)
        _create_or_extend_norm_sample(layer.weight, norm_sample)
    else:
        _create_or_extend_grad_sample(layer.weight, grad_sample)

    grad_sample = sum_over_all_but_batch_and_last_n(B, layer.bias.dim())
    if is_backward_ghost_norm:
        norm_sample = grad_sample.flatten(start_dim=1).norm(2, dim=1)
        _create_or_extend_norm_sample(layer.bias, norm_sample)
    else:
        _create_or_extend_grad_sample(layer.bias, grad_sample)


def _compute_embedding_grad_sample(layer: nn.Embedding, A: Tuple[torch.Tensor], B: Tuple[torch.Tensor]) -> None:
    """Computes per sample gradients for `nn.Embedding` layer."""
    # `nn.Embedding` has single input and output. Unpack singleton tuples.
    (A,), (B,) = A, B

    if autograd_grad_sample.get_hooks_mode() == BackwardHookMode.ghost_norm:
        not_AAt: torch.Tensor = ~A[:, :, None].eq(A[:, None, :])
        # Clear the contribution to the norm of the gradient for the padding token.
        #   In vanilla backpropagation, this particular embedding doesn't contribute to the gradient anyway.
        #   For more see 1.10.0 doc: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
        #       'the embedding vector at padding_idx is not updated during training, i.e. it remains as a fixed “pad”.'
        padding_idx = layer.padding_idx
        if padding_idx is not None:
            # The right way to think about the next line of code is that A_i[t, padding_idx] = 0 for all t in [T].
            #   So the entry gets cleared whenever one of A, A^t takes the padding idx.
            not_AAt.bitwise_or_((A[:, :, None] == padding_idx) | (A[:, None, :] == padding_idx))
        norm_sample = torch.sqrt((torch.bmm(B, B.transpose(-1, -2)).masked_fill(not_AAt, 0)).sum(dim=(1, 2)))
        _create_or_extend_norm_sample(layer.weight, norm_sample)
    else:
        A_dense = F.one_hot(A, num_classes=layer.weight.shape[0]).to(B)  # (batch_size, seq_len, vocab_dim,)
        grad_sample = torch.bmm(A_dense.permute(0, 2, 1), B)
        # `torch.nn.Embedding` layers don't accumulate gradient on the padding_idx position.
        #   We do the same for `grad_sample`.
        if layer.padding_idx is not None:
            # `grad_sample` has size (batch_size, num_vocab, embedding_dim).
            grad_sample[:, layer.padding_idx, :] = 0.
        _create_or_extend_grad_sample(layer.weight, grad_sample)


def _custom_compute_conv1d_grad_sample(layer: nn.Linear, A: Tuple[torch.Tensor], B: Tuple[torch.Tensor]):
    """Computes per sample gradients for `transformers.modeling_utils.Conv1D` layer."""
    # `transformers.modeling_utils.Conv1D` has single input and output. Unpack singleton tuples.
    # https://github.com/huggingface/transformers/blob/ccc089780415445768bcfd3ac4418cec20353484/src/transformers/pytorch_utils.py#L107
    (A,), (B,) = A, B

    if autograd_grad_sample.get_hooks_mode() == BackwardHookMode.ghost_norm:
        _create_or_extend_norm_sample(layer.weight, _light_linear_weight_norm_sample(A, B))

        if layer.bias is not None:
            _create_or_extend_norm_sample(layer.bias, B.sum(dim=1).norm(2, dim=1))
    else:
        _create_or_extend_grad_sample(layer.weight, torch.bmm(A.permute(0, 2, 1), B))

        if layer.bias is not None:
            _create_or_extend_grad_sample(layer.bias, B.sum(dim=1))


def _compute_t5_layer_norm_grad_sample(layer: T5LayerNorm, A: Tuple[torch.Tensor], B: Tuple[torch.Tensor]):
    # `transformers.models.t5.modeling_t5.T5LayerNorm` has single input and output. Unpack singleton tuples.
    # https://github.com/huggingface/transformers/blob/ccc089780415445768bcfd3ac4418cec20353484/src/transformers/models/t5/modeling_t5.py#L248
    (A,), (B,) = A, B

    assert A.dim() == 3 and B.dim() == 3, (
        "Internal error: T5LayerNorm receiving 2-D tensors, but expected 3-D tensors (sequential inputs)."
    )

    is_backward_ghost_norm = autograd_grad_sample.get_hooks_mode() == BackwardHookMode.ghost_norm

    grad_sample = (A * torch.rsqrt(A.pow(2).mean(-1, keepdim=True) + layer.variance_epsilon) * B).sum(dim=1)
    if is_backward_ghost_norm:
        norm_sample = grad_sample.norm(2, dim=1)
        _create_or_extend_norm_sample(layer.weight, norm_sample)
    else:
        _create_or_extend_grad_sample(layer.weight, grad_sample)


def _compute_opt_learned_positional_embedding_grad_sample(
    layer: OPTLearnedPositionalEmbedding, A: Tuple[torch.Tensor, int], B: Tuple[torch.Tensor]
):
    # `transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding` has two inputs and one output.
    # https://github.com/huggingface/transformers/blob/d0acc9537829e7d067edbb791473bbceb2ecf056/src/transformers/models/opt/modeling_opt.py#L99
    (A, past_key_values_length), (B,) = A, B  # Unpack tuples.

    attention_mask = A.long()

    # create positions depending on attention_mask
    positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1

    # cut positions if `past_key_values_length` is > 0
    positions = positions[:, past_key_values_length:] + layer.offset

    _compute_embedding_grad_sample(layer, (positions,), (B,))


def unfold2d(
    input,
    *,
    kernel_size: Tuple[int, int],
    padding: Tuple[int, int],
    stride: Tuple[int, int],
    dilation: Tuple[int, int],
):
    """
    See :meth:`~torch.nn.functional.unfold`
    """
    *shape, H, W = input.shape
    H_effective = (H + 2 * padding[0] - (kernel_size[0] + (kernel_size[0] - 1) * (dilation[0] - 1))) // stride[0] + 1
    W_effective = (W + 2 * padding[1] - (kernel_size[1] + (kernel_size[1] - 1) * (dilation[1] - 1))) // stride[1] + 1
    # F.pad's first argument is the padding of the *last* dimension
    input = F.pad(input, (padding[1], padding[1], padding[0], padding[0]))
    *shape_pad, H_pad, W_pad = input.shape
    strides = list(input.stride())
    strides = strides[:-2] + [
        W_pad * dilation[0],
        dilation[1],
        W_pad * stride[0],
        stride[1],
    ]
    out = input.as_strided(
        shape + [kernel_size[0], kernel_size[1], H_effective, W_effective], strides
    )

    return out.reshape(input.size(0), -1, H_effective * W_effective)


def _compute_conv2d_grad_sample(layer: nn.Conv2d, activations: Tuple[torch.Tensor], backprops: Tuple[torch.Tensor]):
    # `nn.Conv2d` has one input and one output. Unpack tuples.
    (activations,), (backprops,) = activations, backprops

    n = activations.shape[0]
    activations = unfold2d(
        activations, kernel_size=layer.kernel_size, padding=layer.padding, stride=layer.stride, dilation=layer.dilation
    )  # shape (n, o, q)
    backprops = backprops.reshape(n, -1, activations.shape[-1])  # shape (n, p, q)

    if autograd_grad_sample.get_hooks_mode() == BackwardHookMode.ghost_norm:
        activations = activations.permute(0, 2, 1)
        backprops = backprops.permute(0, 2, 1)
        _create_or_extend_norm_sample(layer.weight, _light_linear_weight_norm_sample(activations, backprops))
        if layer.bias is not None:
            _create_or_extend_norm_sample(layer.bias, _light_linear_bias_norm_sample(backprops))
    else:
        # n=batch_sz; o=num_out_channels; p=(num_in_channels/groups)*kernel_sz
        grad_sample = contract("noq,npq->nop", backprops, activations)
        # rearrange the above tensor and extract diagonals.
        grad_sample = grad_sample.view(
            n,
            layer.groups,
            -1,
            layer.groups,
            int(layer.in_channels / layer.groups),
            np.prod(layer.kernel_size),
        )
        grad_sample = contract("ngrg...->ngr...", grad_sample).contiguous()
        grad_weight = grad_sample.view([n] + list(layer.weight.shape))
        _create_or_extend_grad_sample(layer.weight, grad_weight)

        if layer.bias is not None:
            grad_bias = torch.sum(backprops, dim=2)
            _create_or_extend_grad_sample(layer.bias, grad_bias)


_supported_layers_grad_samplers = {
    nn.Embedding: _compute_embedding_grad_sample,
    nn.Linear: _compute_linear_grad_sample,
    nn.Conv2d: _compute_conv2d_grad_sample,
    nn.LayerNorm: _compute_layer_norm_grad_sample,
    transformers.pytorch_utils.Conv1D: _custom_compute_conv1d_grad_sample,
    transformers.models.t5.modeling_t5.T5LayerNorm: _compute_t5_layer_norm_grad_sample,
    OPTLearnedPositionalEmbedding: _compute_opt_learned_positional_embedding_grad_sample,
}

File Path: private_transformers/transformers_support.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utilities to make using PrivacyEngine easy with Hugging Face transformers."""
import types
from typing import Optional, Tuple, Union

import torch
import transformers
from torch import nn
from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPastAndCrossAttentions
from transformers.utils import logging

logger = logging.get_logger(__name__)


def forward_swapper(module):
    """Fix incompatibility between Opacus and Hugging Face.

    Root cause is adding positional embedding with broadcasting.
    """
    if isinstance(module, (transformers.OpenAIGPTLMHeadModel, transformers.OpenAIGPTDoubleHeadsModel)):
        swap_openai_gpt_model_forward(module.transformer)
    if isinstance(module, (transformers.GPT2LMHeadModel, transformers.GPT2DoubleHeadsModel)):
        swap_gpt2_model_forward(module.transformer)
    elif hasattr(module, 'roberta'):
        swap_roberta_model_forward(module.roberta)
    elif hasattr(module, 'bert'):
        swap_bert_model_forward(module.bert)
    elif hasattr(module, 'albert'):
        swap_albert_model_forward(module.albert)
    elif isinstance(module, transformers.T5ForConditionalGeneration):
        swap_t5_model_forward(module)
    elif isinstance(module, transformers.OPTForCausalLM):
        swap_opt_model_forward(module)


def swap_openai_gpt_model_forward(model: transformers.OpenAIGPTModel):
    def new_forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
        elif input_ids is not None:
            input_shape = input_ids.size()
            input_ids = input_ids.view(-1, input_shape[-1])
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        if position_ids is None:
            # Code is different from when we had a single embedding matrix  from position and token embeddings
            position_ids = self.position_ids[None, : input_shape[-1]]
            # --- lxuechen: Duplicate to make privacy work! ---
            position_ids = position_ids.repeat(input_ids.size(0), 1)
            # ---

        # Attention mask.
        if attention_mask is not None:
            # We create a 3D attention mask from a 2D tensor mask.
            # Sizes are [batch_size, 1, 1, to_seq_length]
            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
            # this attention mask is more simple than the triangular masking of causal attention
            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)

            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
            # masked positions, this operation will create a tensor which is 0.0 for
            # positions we want to attend and -10000.0 for masked positions.
            # Since we are adding it to the raw scores before the softmax, this is
            # effectively the same as removing these entirely.
            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility
            attention_mask = (1.0 - attention_mask) * -10000.0

        # Prepare head mask if needed
        head_mask = self.get_head_mask(head_mask, self.config.n_layer)

        if inputs_embeds is None:
            inputs_embeds = self.tokens_embed(input_ids)
        position_embeds = self.positions_embed(position_ids)
        if token_type_ids is not None:
            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))
            token_type_embeds = self.tokens_embed(token_type_ids)
        else:
            token_type_embeds = 0
        hidden_states = inputs_embeds + position_embeds + token_type_embeds
        hidden_states = self.drop(hidden_states)

        output_shape = input_shape + (hidden_states.size(-1),)

        all_attentions = () if output_attentions else None
        all_hidden_states = () if output_hidden_states else None
        for i, block in enumerate(self.h):
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            outputs = block(hidden_states, attention_mask, head_mask[i], output_attentions=output_attentions)
            hidden_states = outputs[0]
            if output_attentions:
                all_attentions = all_attentions + (outputs[1],)

        hidden_states = hidden_states.view(*output_shape)
        # Add last layer
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)

        return BaseModelOutput(
            last_hidden_state=hidden_states,
            hidden_states=all_hidden_states,
            attentions=all_attentions,
        )

    model.forward = types.MethodType(new_forward, model)


def swap_gpt2_model_forward(model: Union[transformers.GPT2Model, transformers.GPT2DoubleHeadsModel]):
    """Modify the forward function for `GPT2Model` so that per-sample gradients are correct.

    Main issue is that positional embedding's input should be duplicated.
    """

    def new_forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
        elif input_ids is not None:
            input_shape = input_ids.size()
            input_ids = input_ids.view(-1, input_shape[-1])
            batch_size = input_ids.shape[0]
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
            batch_size = inputs_embeds.shape[0]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        device = input_ids.device if input_ids is not None else inputs_embeds.device

        if token_type_ids is not None:
            token_type_ids = token_type_ids.view(-1, input_shape[-1])
        if position_ids is not None:
            position_ids = position_ids.view(-1, input_shape[-1])

        if past_key_values is None:
            past_length = 0
            past_key_values = tuple([None] * len(self.h))
        else:
            past_length = past_key_values[0][0].size(-2)
        if position_ids is None:
            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
            # --- lxuechen: Duplicate to make privacy work! ---
            position_ids = position_ids.repeat(batch_size, 1)
            # ---

        # GPT2Attention mask.
        if attention_mask is not None:
            assert batch_size > 0, "batch_size has to be defined and > 0"
            attention_mask = attention_mask.view(batch_size, -1)
            # We create a 3D attention mask from a 2D tensor mask.
            # Sizes are [batch_size, 1, 1, to_seq_length]
            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
            # this attention mask is more simple than the triangular masking of causal attention
            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
            attention_mask = attention_mask[:, None, None, :]

            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
            # masked positions, this operation will create a tensor which is 0.0 for
            # positions we want to attend and -10000.0 for masked positions.
            # Since we are adding it to the raw scores before the softmax, this is
            # effectively the same as removing these entirely.
            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
            attention_mask = (1.0 - attention_mask) * -10000.0

        # If a 2D ou 3D attention mask is provided for the cross-attention
        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
        if self.config.add_cross_attention and encoder_hidden_states is not None:
            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
            if encoder_attention_mask is None:
                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)
        else:
            encoder_attention_mask = None

        # Prepare head mask if needed
        # 1.0 in head_mask indicate we keep the head
        # attention_probs has shape bsz x n_heads x N x N
        # head_mask has shape n_layer x batch x n_heads x N x N
        head_mask = self.get_head_mask(head_mask, self.config.n_layer)

        if inputs_embeds is None:
            inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds

        if token_type_ids is not None:
            token_type_embeds = self.wte(token_type_ids)
            hidden_states = hidden_states + token_type_embeds

        hidden_states = self.drop(hidden_states)

        output_shape = input_shape + (hidden_states.size(-1),)

        presents = () if use_cache else None
        all_self_attentions = () if output_attentions else None
        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
        all_hidden_states = () if output_hidden_states else None
        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):

            # Model parallel
            if self.model_parallel:
                torch.cuda.set_device(hidden_states.device)
                # Ensure layer_past is on same device as hidden_states (might not be correct)
                if layer_past is not None:
                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)
                # Ensure that attention_mask is always on the same device as hidden_states
                if attention_mask is not None:
                    attention_mask = attention_mask.to(hidden_states.device)
                if isinstance(head_mask, torch.Tensor):
                    head_mask = head_mask.to(hidden_states.device)
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            if getattr(self.config, "gradient_checkpointing", False) and self.training:

                if use_cache:
                    logger.warning(
                        "`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting "
                        "`use_cache=False`..."
                    )
                    use_cache = False

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # None for past_key_value
                        return module(*inputs, use_cache, output_attentions)

                    return custom_forward

                outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(block),
                    hidden_states,
                    None,
                    attention_mask,
                    head_mask[i],
                    encoder_hidden_states,
                    encoder_attention_mask,
                )
            else:
                outputs = block(
                    hidden_states,
                    layer_past=layer_past,
                    attention_mask=attention_mask,
                    head_mask=head_mask[i],
                    encoder_hidden_states=encoder_hidden_states,
                    encoder_attention_mask=encoder_attention_mask,
                    use_cache=use_cache,
                    output_attentions=output_attentions,
                )

            hidden_states = outputs[0]
            if use_cache is True:
                presents = presents + (outputs[1],)

            if output_attentions:
                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
                if self.config.add_cross_attention:
                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)

            # Model Parallel: If it's the last layer for that device, put things on the next device
            if self.model_parallel:
                for k, v in self.device_map.items():
                    if i == v[-1] and "cuda:" + str(k) != self.last_device:
                        hidden_states = hidden_states.to("cuda:" + str(k + 1))

        hidden_states = self.ln_f(hidden_states)

        hidden_states = hidden_states.view(*output_shape)
        # Add last hidden state
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)

        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=presents,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )

    model.forward = types.MethodType(new_forward, model)


def swap_roberta_model_forward(model: transformers.RobertaModel):
    # Doing nothing is good for Roberta.
    pass


def swap_bert_model_forward(model: transformers.BertModel):
    def new_forward(
        self,
        input_ids=None,
        token_type_ids=None,
        position_ids=None,
        inputs_embeds=None,
        past_key_values_length=0
    ):
        if input_ids is not None:
            input_shape = input_ids.size()
        else:
            input_shape = inputs_embeds.size()[:-1]

        seq_length = input_shape[1]

        if position_ids is None:
            position_ids = self.position_ids[:, past_key_values_length: seq_length + past_key_values_length]

        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs
        # when its auto-generated, registered buffer helps users when tracing the model without passing
        # token_type_ids, solves issue #5664
        if token_type_ids is None:
            if hasattr(self, "token_type_ids"):
                buffered_token_type_ids = self.token_type_ids[:, :seq_length]
                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)
                token_type_ids = buffered_token_type_ids_expanded
            else:
                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)

        if inputs_embeds is None:
            inputs_embeds = self.word_embeddings(input_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        embeddings = inputs_embeds + token_type_embeddings
        if self.position_embedding_type == "absolute":
            # --- lxuechen: Duplicate to make privacy work! ---
            batch_size = input_ids.size(0)
            position_ids = position_ids.repeat(batch_size, 1)
            position_embeddings = self.position_embeddings(position_ids)
            # ---
            embeddings += position_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

    model.embeddings.forward = types.MethodType(new_forward, model.embeddings)


def swap_albert_model_forward(model: transformers.AlbertModel):
    """So far a duplicate of `swap_bert_model_forward`."""

    def new_forward(
        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0
    ):
        if input_ids is not None:
            input_shape = input_ids.size()
        else:
            input_shape = inputs_embeds.size()[:-1]

        seq_length = input_shape[1]

        if position_ids is None:
            position_ids = self.position_ids[:, past_key_values_length: seq_length + past_key_values_length]

        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs
        # when its auto-generated, registered buffer helps users when tracing the model without passing
        # token_type_ids, solves
        # issue #5664
        if token_type_ids is None:
            if hasattr(self, "token_type_ids"):
                buffered_token_type_ids = self.token_type_ids[:, :seq_length]
                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)
                token_type_ids = buffered_token_type_ids_expanded
            else:
                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)

        if inputs_embeds is None:
            inputs_embeds = self.word_embeddings(input_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        embeddings = inputs_embeds + token_type_embeddings
        if self.position_embedding_type == "absolute":
            # --- lxuechen: Duplicate to make privacy work!
            batch_size = input_ids.size(0)
            position_ids = position_ids.repeat(batch_size, 1)
            position_embeddings = self.position_embeddings(position_ids)
            # ---
            embeddings += position_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

    model.embeddings.forward = types.MethodType(new_forward, model.embeddings)


def swap_t5_model_forward(model: nn.Module):
    """Duplicates positional inputs for positional bias in T5Attention forward function."""

    def compute_bias(self, query_length, key_length, batch_size, device=None):
        """Compute binned relative position bias"""
        if device is None:
            device = self.relative_attention_bias.weight.device
        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]
        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]
        relative_position = memory_position - context_position  # shape (query_length, key_length)
        relative_position_bucket = self._relative_position_bucket(
            relative_position,  # shape (query_length, key_length)
            bidirectional=(not self.is_decoder),
            num_buckets=self.relative_attention_num_buckets,
            max_distance=self.relative_attention_max_distance,
        )
        # ---
        # lxuechen: Duplicate to make privacy work!
        # shape (batch_size, q_len x k_len)
        relative_position_bucket = relative_position_bucket.reshape(-1).unsqueeze(0).repeat(batch_size, 1)
        values = self.relative_attention_bias(relative_position_bucket)  # shape (batch_size, q_len x k_len, num_heads)
        # shape (batch_size, q_len, k_len, num_heads)
        values = values.reshape(batch_size, query_length, key_length, values.size(-1))
        values = values.permute([0, 3, 1, 2])  # shape (batch_size, num_heads, query_length, key_length)
        # ---
        return values

        # Original non-duplicated code.
        # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)
        # values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)
        # return values

    def new_forward(
        self,
        hidden_states,
        mask=None,
        key_value_states=None,
        position_bias=None,
        past_key_value=None,
        layer_head_mask=None,
        query_length=None,
        use_cache=False,
        output_attentions=False,
    ):
        batch_size, seq_length = hidden_states.shape[:2]

        real_seq_length = seq_length

        if past_key_value is not None:
            assert (
                len(past_key_value) == 2
            ), f"past_key_value should have 2 past states: keys and values. Got {len(past_key_value)} past states"
            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length

        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]

        def shape(states):
            """projection"""
            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)

        def unshape(states):
            """reshape"""
            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)

        def project(hidden_states, proj_layer, key_value_states, past_key_value):
            """projects hidden states correctly to key/query states"""
            if key_value_states is None:
                # self-attn
                # (batch_size, n_heads, seq_length, dim_per_head)
                hidden_states = shape(proj_layer(hidden_states))
            elif past_key_value is None:
                # cross-attn
                # (batch_size, n_heads, seq_length, dim_per_head)
                hidden_states = shape(proj_layer(key_value_states))

            if past_key_value is not None:
                if key_value_states is None:
                    # self-attn
                    # (batch_size, n_heads, key_length, dim_per_head)
                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)
                else:
                    # cross-attn
                    hidden_states = past_key_value
            return hidden_states

        # get query states
        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)

        # get key/value states
        key_states = project(
            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None
        )
        value_states = project(
            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None
        )

        # compute scores
        scores = torch.matmul(
            query_states, key_states.transpose(3, 2)
        )  # equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9

        if position_bias is None:
            if not self.has_relative_attention_bias:
                # lxuechen: Need batch size in dim=0.
                position_bias = torch.zeros(
                    (batch_size, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype
                )
                if self.gradient_checkpointing and self.training:
                    position_bias.requires_grad = True
            else:
                # lxuechen: Need batch size aware, due to how embeddings work.
                position_bias = self.compute_bias(
                    real_seq_length, key_length, batch_size, device=scores.device,
                )

            # if key and values are already calculated
            # we want only the last query position bias
            if past_key_value is not None:
                position_bias = position_bias[:, :, -hidden_states.size(1):, :]

            if mask is not None:
                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)

        scores += position_bias
        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(
            scores
        )  # (batch_size, n_heads, seq_length, key_length)
        attn_weights = nn.functional.dropout(
            attn_weights, p=self.dropout, training=self.training
        )  # (batch_size, n_heads, seq_length, key_length)

        # Mask heads if we want to
        if layer_head_mask is not None:
            attn_weights = attn_weights * layer_head_mask

        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)
        attn_output = self.o(attn_output)

        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None
        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)

        if output_attentions:
            outputs = outputs + (attn_weights,)
        return outputs

    for module in model.modules():
        if isinstance(module, transformers.models.t5.modeling_t5.T5Attention):
            module.forward = types.MethodType(new_forward, module)
            module.compute_bias = types.MethodType(compute_bias, module)


def swap_opt_model_forward(model):
    """Fix OPTDecoderLayer's forward function.

    OPTDecoderLayer's forward function has this weird reshape of activation in the middle.
    """

    def opt_decoder_layer_forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        layer_head_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.FloatTensor`, *optional*): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
        """

        residual = hidden_states

        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
        if self.do_layer_norm_before:
            hidden_states = self.self_attn_layer_norm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            past_key_value=past_key_value,
            attention_mask=attention_mask,
            layer_head_mask=layer_head_mask,
            output_attentions=output_attentions,
        )
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states

        # 350m applies layer norm AFTER attention
        if not self.do_layer_norm_before:
            hidden_states = self.self_attn_layer_norm(hidden_states)

        # Fully Connected
        residual = hidden_states

        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
        if self.do_layer_norm_before:
            hidden_states = self.final_layer_norm(hidden_states)

        hidden_states = self.fc1(hidden_states)
        hidden_states = self.activation_fn(hidden_states)

        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)

        hidden_states = (residual + hidden_states)

        # Original below, why reshape???

        # hidden_states_shape = hidden_states.shape
        # hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))
        # residual = hidden_states
        #
        # # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
        # if self.do_layer_norm_before:
        #     hidden_states = self.final_layer_norm(hidden_states)
        #
        # hidden_states = self.fc1(hidden_states)
        # hidden_states = self.activation_fn(hidden_states)
        #
        # hidden_states = self.fc2(hidden_states)
        # hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        #
        # hidden_states = (residual + hidden_states).view(hidden_states_shape)

        # 350m applies layer norm AFTER attention
        if not self.do_layer_norm_before:
            hidden_states = self.final_layer_norm(hidden_states)

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs

    for module in model.modules():
        if isinstance(module, transformers.models.opt.modeling_opt.OPTDecoderLayer):
            module.forward = types.MethodType(opt_decoder_layer_forward, module)


def freeze_isolated_params_for_vit(model):
    """Freeze the isolated parameters in Vi-T models.

    Supporting per-sample gradients for these parameters is possible, but takes a lot of engineering effort.
    """
    for module in model.modules():
        if isinstance(
            module,
            (transformers.models.vit.modeling_vit.ViTEmbeddings,
             transformers.models.deit.modeling_deit.DeiTEmbeddings,
             transformers.models.beit.modeling_beit.BeitEmbeddings)
        ):
            module.cls_token.requires_grad_(False)
            if module.mask_token is not None:
                module.mask_token.requires_grad_(False)
            if module.position_embeddings is not None:
                module.position_embeddings.requires_grad_(False)
        if isinstance(module, transformers.models.beit.modeling_beit.BeitRelativePositionBias):
            module.relative_position_bias_table.requires_grad_(False)
        if isinstance(module, transformers.models.beit.modeling_beit.BeitLayer):
            if module.lambda_1 is not None:
                module.lambda_1.requires_grad_(False)
            if module.lambda_2 is not None:
                module.lambda_2.requires_grad_(False)

File Path: setup.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re

import setuptools

# for simplicity we actually store the version in the __version__ attribute in the source
here = os.path.realpath(os.path.dirname(__file__))
with open(os.path.join(here, 'private_transformers', '__init__.py')) as f:
    meta_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", f.read(), re.M)
    if meta_match:
        version = meta_match.group(1)
    else:
        raise RuntimeError("Unable to find __version__ string.")

with open(os.path.join(here, 'README.md')) as f:
    readme = f.read()

setuptools.setup(
    name="private_transformers",
    version=version,
    author="Xuechen Li",
    author_email="lxuechen@cs.toronto.edu",
    description="Train Hugging Face transformers with differential privacy.",
    long_description=readme,
    url="https://github.com/lxuechen/private-transformers",
    packages=setuptools.find_packages(exclude=['examples', 'tests']),
    install_requires=[
        "torch>=1.8.0",
        "prv-accountant",
        "transformers>=4.20.1",  # v0.1.0 uses 4.16.2.
        "numpy",
        "scipy",
        "jupyterlab",
        "jupyter",
        "ml-swissknife",
        "opt_einsum",
        "pytest"
    ],
    python_requires='~=3.8',
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: Apache Software License",
    ],
)

File Path: tests/__init__.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

File Path: tests/test_privacy_engine.py
Content:
# Copyright (c) Xuechen Li. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,  software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,  either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Test gradient accumulation of privacy engine.

Compare the accumulated gradient with multiple virtual steps against processing the examples one-by-one.
You will need a GPU to run this!

python tests/test_privacy_engine.py
pytest -s tests
"""
import contextlib
import copy
import gc
import itertools
import os

import pytest
import torch
import torch.nn.functional as F
import tqdm
import transformers
from ml_swissknife import utils
from torch import optim

from private_transformers import PrivacyEngine, freeze_isolated_params_for_vit, supported_layers_grad_samplers

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.set_default_dtype(torch.float64)

# Don't mess up my disk space on cluster.
if os.path.exists("/nlp/scr/lxuechen/cache/private_transformers"):
    CACHE_DIR = "/nlp/scr/lxuechen/cache/private_transformers"
else:
    CACHE_DIR = None


def _make_classification_data(num_micro_batches=2, micro_batch_size=4, seq_len=128, num_labels=2):
    return tuple(
        dict(
            input_ids=torch.randint(low=1, high=100, size=(micro_batch_size, seq_len)),
            labels=torch.randint(low=0, high=num_labels, size=(micro_batch_size,)),
        )
        for _ in range(num_micro_batches)
    )


def _make_generation_data(num_micro_batches=4, micro_batch_size=4, seq_len=128):
    """Make a batch of plain sequences.

    Tuple of multiple micro batches.
    """
    return tuple(
        dict(input_ids=torch.randint(low=1, high=100, size=(micro_batch_size, seq_len)))
        for _ in range(num_micro_batches)
    )


def _make_encoder_decoder_data(num_micro_batches=4, micro_batch_size=4, seq_len=64, target_seq_len=64):
    """Make a batch of sequences."""
    return tuple(
        dict(
            input_ids=torch.randint(low=1, high=100, size=(micro_batch_size, seq_len)),
            decoder_input_ids=torch.randint(low=1, high=100, size=(micro_batch_size, target_seq_len)),
        )
        for _ in range(num_micro_batches)
    )


def _make_image_classification_data(
    num_micro_batches=4, micro_batch_size=4,
    num_labels=10, num_channels=3, height=224, width=224,
):
    return tuple(
        dict(
            pixel_values=torch.randn(micro_batch_size, num_channels, height, width),
            labels=torch.randint(size=(micro_batch_size,), low=0, high=num_labels),
        )
        for _ in range(num_micro_batches)
    )


def _prepare_inputs(batch: dict):
    return {key: value.to(DEVICE) for key, value in batch.items()}


@pytest.mark.parametrize(
    'clipping_mode,model_name_or_path',
    itertools.product(["ghost", "default"], ['roberta-base', 'bert-base-cased', 'albert-base-v2'])
)
def test_classification(clipping_mode: str, model_name_or_path: str):
    if clipping_mode == "ghost" and 'albert' in model_name_or_path:
        pytest.skip("Ghost clipping does not support parameter sharing which occurs in ALBERT.")

    gc.collect()
    torch.cuda.empty_cache()

    lr = 1e-4
    num_labels = 2
    num_micro_batches = 4
    micro_batch_size = 4
    seq_len = 128
    batch_size = num_micro_batches * micro_batch_size
    max_grad_norm = 1

    # Set up model -- disable dropout to remove randomness.
    config = transformers.AutoConfig.from_pretrained(
        model_name_or_path,
        num_labels=num_labels,
        attention_probs_dropout_prob=0.,
        hidden_dropout_prob=0.,
        classifier_dropout_prob=0.,  # Important for ALBERT, since otherwise randomness causes gradient difference.
        return_dict=True,
        padding_idx=-1,
        # roberta sets `pad_token_id` to 1 by default, whereas it's 0 for bert.
        #   Uncomment the following line, if you want consistency (it's not totally necessary).
        # pad_token_id=-1,
    )

    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=config)
    model.requires_grad_(True).train()

    param_names = [name for name, param in model.named_parameters() if param.requires_grad]
    num_trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)
    print(f'Number of trainable parameters: {num_trainable_params / 1e6:.4f} million')

    # Make data.
    batches = _make_classification_data(
        micro_batch_size=micro_batch_size, num_micro_batches=num_micro_batches, seq_len=seq_len, num_labels=num_labels,
    )

    # 1: Compute updates with my engine.
    clone1 = copy.deepcopy(model).to(DEVICE)
    clone1_params = [param for param in clone1.parameters() if param.requires_grad]

    optimizer = optim.Adam(params=clone1_params, lr=lr)
    privacy_engine = PrivacyEngine(
        module=clone1,
        batch_size=batch_size,
        max_grad_norm=max_grad_norm,
        noise_multiplier=0.,  # Remove noise to test gradient clipping & accumulation.
        sample_size=1000,  # Any number suffices for testing.
        epochs=1,  # Any number suffices for testing.
        numerical_stability_constant=0.,  # Important!
        clipping_mode=clipping_mode,
    )
    privacy_engine.attach(optimizer=optimizer)
    optimizer.zero_grad()  # Clears summed_grad, so don't run unless necessary.
    for i, batch in enumerate(batches, 1):
        batch = _prepare_inputs(batch)
        logits = clone1(**batch).logits
        labels = batch["labels"]
        loss = F.cross_entropy(logits, labels, reduction="none")

        del batch
        if i == len(batches):
            optimizer.step(loss=loss)
        else:
            optimizer.virtual_step(loss=loss)

    result1 = [param.grad for param in clone1_params]
    privacy_engine.detach()  # Restore`hooks_mode`.
    del clone1, loss, logits, labels, optimizer
    gc.collect()
    torch.cuda.empty_cache()

    # 2: Compute grad and clip one-by-one.
    clone2 = copy.deepcopy(model).to(DEVICE)
    clone2_params = [param for param in clone2.parameters() if param.requires_grad]

    optimizer = torch.optim.Adam(params=clone2_params, lr=lr)
    summed_grad = [torch.zeros_like(param) for param in clone2_params]
    for i, batch in tqdm.tqdm(enumerate(batches, 1), desc="over batches"):
        batch = _prepare_inputs(batch)
        for input_ids, labels in tqdm.tqdm(utils.zip_(batch["input_ids"], batch["labels"]), desc="over samples"):
            optimizer.zero_grad(set_to_none=True)  # Clear previous grad each time!
            input_ids = input_ids[None, :]
            labels = labels[None]

            logits = clone2(input_ids=input_ids, labels=labels).logits
            loss = F.cross_entropy(logits, labels, reduction="none").sum()
            loss.backward()

            with torch.no_grad():
                flat_unclipped_grad = torch.cat(tuple(param.grad.flatten() for param in clone2_params))
                factor = torch.clamp_max(max_grad_norm / flat_unclipped_grad.norm(), 1.)
                for si, pi in utils.zip_(summed_grad, clone2_params):
                    si.add_(factor * pi.grad)

    result2 = [grad / batch_size for grad in summed_grad]
    del clone2, loss, logits, labels, optimizer

    del model
    gc.collect()
    torch.cuda.empty_cache()

    for g1, g2, name in utils.zip_(result1, result2, param_names):
        try:
            torch.testing.assert_allclose(g1, g2, atol=1e-5, rtol=1e-6)
        except AssertionError as e:
            print(f"failed at {name}")
            raise e


@pytest.mark.parametrize(
    'clipping_mode,tie_word_embeddings,model_name_or_path',
    # Testing two OPT models, since the 350m one has a different LayerNorm placement.
    tuple(
        itertools.product(
            ["ghost", "default"],
            [False, True],
            ['gpt2', 'openai-gpt', 'facebook/opt-125m', 'facebook/opt-350m']
        )
    )
)
def test_generation(clipping_mode, tie_word_embeddings, model_name_or_path):
    gc.collect()
    torch.cuda.empty_cache()

    lr = 1e-4
    num_micro_batches = 4
    micro_batch_size = 4
    seq_len = 128
    batch_size = num_micro_batches * micro_batch_size
    max_grad_norm = 1

    # Catch expected failures.
    with pytest.raises(ValueError) if clipping_mode == "ghost" and tie_word_embeddings else contextlib.nullcontext():

        # Set up model.
        config = transformers.AutoConfig.from_pretrained(model_name_or_path, cache_dir=CACHE_DIR)
        config.tie_word_embeddings = tie_word_embeddings
        # Branch out due to weird inconsistency with naming.
        # OPT is AutoModelForCausalLM; GPT is AutoModelWithLMHead.
        if 'opt' in model_name_or_path:
            # Remove potential causes of randomness.
            config.activation_dropout = config.attention_dropout = config.dropout = config.layerdrop = 0.
            model = transformers.AutoModelForCausalLM.from_pretrained(
                model_name_or_path, config=config, cache_dir=CACHE_DIR
            )
        else:
            # Remove potential causes of randomness.
            config.attn_pdrop = config.embd_pdrop = config.resid_pdrop = 0.
            model = transformers.AutoModelWithLMHead.from_pretrained(
                model_name_or_path, config=config, cache_dir=CACHE_DIR
            )
        model.train()  # Needed to ensure privacy engine works.
        param_names = [name for name, param in model.named_parameters() if param.requires_grad]

        # Make data.
        batches = _make_generation_data(
            num_micro_batches=num_micro_batches, micro_batch_size=micro_batch_size, seq_len=seq_len
        )

        # 1: Compute updates with my engine.
        clone1 = copy.deepcopy(model).to(DEVICE)
        optimizer = optim.Adam(params=clone1.parameters(), lr=lr)
        privacy_engine = PrivacyEngine(
            module=clone1,
            batch_size=batch_size,
            max_grad_norm=max_grad_norm,
            noise_multiplier=0.,  # Remove noise to test gradient clipping & accumulation.
            sample_size=1000,  # Any number suffices for testing.
            epochs=1,  # Any number suffices for testing.
            numerical_stability_constant=0.,  # Important!
            clipping_mode=clipping_mode,
        )
        privacy_engine.attach(optimizer=optimizer)
        optimizer.zero_grad()  # Clears summed_grad, so don't run unless necessary.

        for i, batch in enumerate(batches, 1):
            batch = _prepare_inputs(batch)
            shifted_logits = clone1(**batch).logits[..., :-1, :].permute(0, 2, 1)
            shifted_labels = batch["input_ids"][..., 1:]
            loss = F.cross_entropy(shifted_logits, shifted_labels, reduction="none")
            loss = loss.mean(dim=1)  # Average over sequence.

            del batch
            if i == len(batches):
                optimizer.step(loss=loss)
            else:
                optimizer.virtual_step(loss=loss)
        result1 = [param.grad for param in clone1.parameters()]
        privacy_engine.detach()  # Restore`hooks_mode`.
        del clone1, loss, shifted_labels, shifted_logits, optimizer
        gc.collect()
        torch.cuda.empty_cache()

        # 2: Compute grad and clip one-by-one.
        clone2 = copy.deepcopy(model).to(DEVICE)
        optimizer = torch.optim.Adam(params=clone2.parameters(), lr=lr)
        summed_grad = [torch.zeros_like(param) for param in clone2.parameters()]
        for i, batch in tqdm.tqdm(enumerate(batches, 1), desc="over batches"):
            batch = _prepare_inputs(batch)
            for input_ids in tqdm.tqdm(batch["input_ids"], desc="over samples"):
                optimizer.zero_grad()  # Clear previous grad each time!
                input_ids = input_ids[None, :]
                shifted_logits = clone2(input_ids=input_ids).logits[..., :-1, :].permute(0, 2, 1)
                shifted_labels = input_ids[..., 1:]
                loss = F.cross_entropy(shifted_logits, shifted_labels, reduction="none")
                loss = loss.mean()
                loss.backward()

                with torch.no_grad():
                    flat_unclipped_grad = torch.cat(tuple(param.grad.flatten() for param in clone2.parameters()))
                    factor = torch.clamp_max(max_grad_norm / flat_unclipped_grad.norm(), 1.)
                    for si, pi in utils.zip_(summed_grad, list(clone2.parameters())):
                        si.add_(factor * pi.grad)
        result2 = [si / batch_size for si in summed_grad]
        del clone2, loss, shifted_labels, shifted_logits, optimizer

        gc.collect()
        torch.cuda.empty_cache()

        wrong_names = []
        for r1, r2, param_name in utils.zip_(result1, result2, param_names):
            if not torch.allclose(r1, r2, atol=1e-5, rtol=1e-6):
                wrong_names.append(param_name)
        if len(wrong_names) > 0:
            raise AssertionError(
                f"The following parameters have wrong gradients: \n{wrong_names}"
            )


@pytest.mark.parametrize(
    'clipping_mode,tie_word_embeddings,model_name_or_path',
    tuple(itertools.product(["ghost", "default"], [True], ['facebook/bart-base', 't5-base']))
)
def test_conditional_generation(clipping_mode: str, tie_word_embeddings, model_name_or_path):
    if 't5' in model_name_or_path:
        torch.set_default_dtype(torch.float32)  # Unfortunately, can't run double precision on T5.
    gc.collect()
    torch.cuda.empty_cache()

    lr = 1e-4
    num_micro_batches = 4
    micro_batch_size = 4
    seq_len = 128
    batch_size = num_micro_batches * micro_batch_size
    max_grad_norm = 1

    # Catch expected failures.
    with pytest.raises(ValueError) if clipping_mode == "ghost" and tie_word_embeddings else contextlib.nullcontext():

        # Set up model.
        config = transformers.AutoConfig.from_pretrained(model_name_or_path, cache_dir=CACHE_DIR)
        if 'bart' in model_name_or_path:
            config.tie_word_embeddings = tie_word_embeddings
            config.dropout = config.attention_dropout = config.activation_dropout = config.classifier_dropout = 0
        else:  # t5
            config.dropout_rate = 0.0

        model = transformers.AutoModelWithLMHead.from_pretrained(model_name_or_path, config=config, cache_dir=CACHE_DIR)
        if 'bart' in model_name_or_path:
            # TODO: Write per-sample grad hooks to enable optimizing the learned positional embedding layer in BART.
            model.model.encoder.embed_positions.requires_grad_(False)
            model.model.decoder.embed_positions.requires_grad_(False)
        model.train()  # Needed to ensure privacy engine works.

        # Make data.
        batches = _make_encoder_decoder_data(
            num_micro_batches=num_micro_batches, micro_batch_size=micro_batch_size, seq_len=seq_len
        )

        # 1: Compute updates with my engine.
        clone1 = copy.deepcopy(model).to(DEVICE)
        clone1_params = [param for param in clone1.parameters() if param.requires_grad]
        optimizer = optim.Adam(params=clone1.parameters(), lr=lr)
        privacy_engine = PrivacyEngine(
            module=clone1,
            batch_size=batch_size,
            max_grad_norm=max_grad_norm,
            noise_multiplier=0.,  # Remove noise to test gradient clipping & accumulation.
            sample_size=1000,  # Any number suffices for testing.
            epochs=1,  # Any number suffices for testing.
            numerical_stability_constant=0.,  # Important!
            clipping_mode=clipping_mode,
        )
        privacy_engine.attach(optimizer=optimizer)
        optimizer.zero_grad()  # Clears summed_grad, so don't run unless necessary.

        for i, batch in enumerate(batches, 1):
            batch = _prepare_inputs(batch)
            shifted_logits = clone1(**batch).logits[..., :-1, :].permute(0, 2, 1)
            shifted_labels = batch["decoder_input_ids"][..., 1:]
            loss = F.cross_entropy(shifted_logits, shifted_labels, reduction="none")
            loss = loss.mean(dim=1)  # Average over sequence.

            del batch
            if i == len(batches):
                optimizer.step(loss=loss)
            else:
                optimizer.virtual_step(loss=loss)
        # Collect grads.
        result1 = torch.cat([param.grad.flatten() for param in clone1_params])
        privacy_engine.detach()  # Restore`hooks_mode`.
        del clone1, loss, shifted_labels, shifted_logits, optimizer
        gc.collect()
        torch.cuda.empty_cache()

        # 2: Compute grad and clip one-by-one.
        clone2 = copy.deepcopy(model).to(DEVICE)
        clone2_params = [param for param in clone2.parameters() if param.requires_grad]
        optimizer = torch.optim.Adam(params=clone2.parameters(), lr=lr)
        summed_grad = [torch.zeros_like(param) for param in clone2_params]
        for i, batch in tqdm.tqdm(enumerate(batches, 1), desc="over batches"):
            batch = _prepare_inputs(batch)
            for input_ids, decoder_input_ids in tqdm.tqdm(
                utils.zip_(batch["input_ids"], batch["decoder_input_ids"]),
                desc="over samples"
            ):
                optimizer.zero_grad()  # Clear previous grad each time!
                input_ids = input_ids[None, :]
                decoder_input_ids = decoder_input_ids[None, :]
                outputs = clone2(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
                shifted_logits = outputs.logits[..., :-1, :].permute(0, 2, 1)
                shifted_labels = decoder_input_ids[..., 1:]
                loss = F.cross_entropy(shifted_logits, shifted_labels, reduction="none")
                loss = loss.mean()
                loss.backward()

                with torch.no_grad():
                    flat_unclipped_grad = torch.cat(tuple(param.grad.flatten() for param in clone2_params))
                    factor = torch.clamp_max(max_grad_norm / flat_unclipped_grad.norm(), 1.)
                    for si, pi in utils.zip_(summed_grad, [param for param in clone2_params]):
                        si.add_(factor * pi.grad)
        # Collect grads.
        result2 = torch.cat([si.flatten() for si in summed_grad]) / batch_size
        del clone2, loss, shifted_labels, shifted_logits, optimizer

        gc.collect()
        torch.cuda.empty_cache()

        if 't5' in model_name_or_path:
            # Loosen tolerance, since T5 only runs in half or single.
            torch.testing.assert_allclose(result1, result2, atol=1e-2, rtol=1e-3)
        else:
            torch.testing.assert_allclose(result1, result2, atol=1e-5, rtol=1e-6)

    if 't5' in model_name_or_path:
        torch.set_default_dtype(torch.float64)  # Revert to double precision for other tests.


def test_t5_layer_norm(batch_size=16, hidden_size=128, seq_len=4):
    t5_layer_norm = transformers.models.t5.modeling_t5.T5LayerNorm(hidden_size=hidden_size).to(DEVICE)
    l1, l2 = tuple(copy.deepcopy(t5_layer_norm) for _ in range(2))

    inputs = torch.randn(batch_size, seq_len, hidden_size, device=DEVICE)
    targets = torch.randn(batch_size, seq_len, hidden_size, device=DEVICE)

    grad1 = []
    for i, t in utils.zip_(inputs, targets):
        i, t = i[None, :], t[None, :]
        l1.zero_grad()
        (.5 * (l1(i) - t) ** 2.).sum().backward()
        grad1.append(l1.weight.grad.detach().clone())
    grad1 = torch.stack(grad1)

    l2.zero_grad()
    outputs = l2(inputs)
    grad_outputs = (outputs - targets)
    supported_layers_grad_samplers._compute_t5_layer_norm_grad_sample(l2, (inputs,), (grad_outputs,))
    grad2 = l2.weight.grad_sample
    torch.testing.assert_allclose(grad1, grad2, atol=1e-5, rtol=1e-4)


@pytest.mark.parametrize(
    'clipping_mode,model_name_or_path,num_labels',
    tuple(
        itertools.product(
            ["ghost", "default"],
            ['google/vit-base-patch16-224', 'facebook/deit-tiny-patch16-224', 'microsoft/beit-base-patch16-224'],
            [10]
        )
    )
)
def test_image_classification(clipping_mode: str, model_name_or_path: str, num_labels: int):
    lr = 1e-4
    num_micro_batches = 4
    micro_batch_size = 4
    batch_size = num_micro_batches * micro_batch_size
    max_grad_norm = 1

    config = transformers.AutoConfig.from_pretrained(model_name_or_path)
    config.hidden_dropout_prob = config.attention_probs_dropout_prob = config.drop_path_rate = 0.
    config.num_labels = num_labels
    model = transformers.AutoModelForImageClassification.from_pretrained(
        model_name_or_path,
        config=config,
        ignore_mismatched_sizes=True  # Default pre-trained model has 1k classes; we only have 10.
    )
    freeze_isolated_params_for_vit(model)
    model.train()

    batches = _make_image_classification_data(
        num_micro_batches=num_micro_batches, micro_batch_size=micro_batch_size, num_labels=num_labels,
    )
    param_names = [name for name, param in model.named_parameters() if param.requires_grad]

    # 1: Compute updates with my engine.
    clone1 = copy.deepcopy(model).to(DEVICE)
    clone1_params = [param for param in clone1.parameters() if param.requires_grad]

    optimizer = optim.Adam(params=clone1_params, lr=lr)
    privacy_engine = PrivacyEngine(
        module=clone1,
        batch_size=batch_size,
        max_grad_norm=max_grad_norm,
        noise_multiplier=0.,  # Remove noise to test gradient clipping & accumulation.
        sample_size=1000,  # Any number suffices for testing.
        epochs=1,  # Any number suffices for testing.
        numerical_stability_constant=0.,  # Important!
        clipping_mode=clipping_mode,
    )
    privacy_engine.attach(optimizer=optimizer)
    optimizer.zero_grad()  # Clears summed_grad, so don't run unless necessary.
    for i, batch in enumerate(batches, 1):
        batch = _prepare_inputs(batch)
        logits = clone1(**batch).logits
        loss = F.cross_entropy(logits, batch["labels"], reduction="none")

        del batch
        if i == len(batches):
            optimizer.step(loss=loss)
        else:
            optimizer.virtual_step(loss=loss)

    result1 = [param.grad for param in clone1_params]
    privacy_engine.detach()  # Restore`hooks_mode`.
    del clone1, loss, logits, optimizer
    gc.collect()
    torch.cuda.empty_cache()

    # 2: Compute grad and clip one-by-one.
    clone2 = copy.deepcopy(model).to(DEVICE)
    clone2_params = [param for param in clone2.parameters() if param.requires_grad]

    optimizer = torch.optim.Adam(params=clone2_params, lr=lr)
    summed_grad = [torch.zeros_like(param) for param in clone2_params]
    for i, batch in tqdm.tqdm(enumerate(batches, 1), desc="over batches"):
        for pixel_values, labels in tqdm.tqdm(utils.zip_(batch["pixel_values"], batch["labels"]), desc="over samples"):
            optimizer.zero_grad(set_to_none=True)  # Clear previous grad each time!
            batch = {"pixel_values": pixel_values[None, ...], "labels": labels[None, ...]}
            batch = _prepare_inputs(batch)
            logits = clone2(**batch).logits
            loss = F.cross_entropy(logits, batch["labels"], reduction="none").sum()
            loss.backward()

            with torch.no_grad():
                flat_unclipped_grad = torch.cat(tuple(param.grad.flatten() for param in clone2_params))
                factor = torch.clamp_max(max_grad_norm / flat_unclipped_grad.norm(), 1.)
                for si, pi in utils.zip_(summed_grad, clone2_params):
                    si.add_(factor * pi.grad)

    result2 = [grad / batch_size for grad in summed_grad]
    del clone2, loss, logits, labels, optimizer

    del model
    gc.collect()
    torch.cuda.empty_cache()

    wrong_names = []
    for r1, r2, param_name in utils.zip_(result1, result2, param_names):
        if not torch.allclose(r1, r2, atol=1e-5, rtol=1e-6):
            wrong_names.append(param_name)
    if len(wrong_names) > 0:
        raise AssertionError(
            f"The following parameters have wrong gradients: \n{wrong_names}"
        )

Output:
{
    "experimental_code": "import collections\nimport logging\nimport math\nimport types\nfrom typing import Callable, Dict, Optional, Sequence, Union\n\nimport torch\nfrom ml_swissknife import utils\nfrom torch import nn\n\nfrom . import autograd_grad_sample, transformers_support\nfrom .accounting import accounting_manager\nfrom .settings import AccountingMode, BackwardHookMode, ClippingMode, SUPPORTED_TRANSFORMERS\n\nlogger = logging.get_logger(__name__)\n\n\nclass PrivacyEngine(object):\n    \"\"\"Differentially-private optimization engine that works gracefully with Hugging Face transformers.\n\n    Supports ghost clipping as described in\n        Li, X., Tramèr, F., Liang, P., & Hashimoto, T. (2021).\n        Large Language Models Can Be Strong Differentially Private Learners.\n        arXiv preprint arXiv:2110.05679.\n\n    Implicitly assumes inputs are in batch first format.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: nn.Module,\n        *,\n        batch_size: int,\n        sample_size: int,\n        max_grad_norm: float,\n        epochs: Optional[Union[int, float]] = None,\n        noise_multiplier: Optional[float] = None,\n        target_epsilon: Optional[float] = None,\n        target_delta: Optional[float] = None,\n        alphas: Sequence[float] = accounting_manager.DEFAULT_ALPHAS,\n        record_snr: bool = True,\n        named_params: Optional[Sequence] = None,\n        numerical_stability_constant=1e-6,\n        clipping_mode=ClippingMode.default,\n        accounting_mode=\"rdp\",\n        eps_error=0.05,\n        skip_checks=False,\n        **unused_kwargs,\n    ):\n        \"\"\"Initialize the engine.\n\n        Args:\n            module: The PyTorch module for which per-sample gradient is required.\n                Setting the `requires_grad` attribute of a parameter to False\n                disables the per-sample gradient accumulation.\n            batch_size: The expected size of Poisson-sampled batch, i.e., the lot size.\n            sample_size: Size of dataset.\n            max_grad_norm: The maximum 2-norm for gradient clipping.\n            epochs: The number of epochs for training.\n            noise_multiplier: The extra multiplier for DP-SGD noise.\n            target_epsilon: The target privacy spending.\n                Only used to estimate the `noise_multiplier` if it is not set.\n            target_delta: The target failure probability.\n                Defaults to sample_size ** -1.1 if not set.\n            alphas: The RDP orders for (ε, δ)-DP conversion. Useless if not accounting in RDP.\n            record_snr: Record and report the signal-to-noise ratio --\n                ratio between norm of summed clipped gradient and norm of noise vector.\n            named_params: Specifies which parameters need gradients;s\n                defaults to use parameters which require grad in module.\n            numerical_stability_constant: Small constant to avoid division by 0 when clipping.\n            clipping_mode: The clipping mode to use. One of 'default', 'ghost', 'per_layer', 'per_layer_percentile'.\n            accounting_mode: The method of accounting privacy. One of (`rdp`, `glw`, `all`).\n                Meanings of shorthands:\n                    - rdp: Account loss with RDP but perform conversion to approx-DP with a procedure defined in\n                        \"The Discrete Gaussian for Differential Privacy\". https://arxiv.org/abs/2004.00010\n                    - glw: Account loss by numerically composing tradeoff functions in f-DP; defined in\n                        \"Numerical composition of differential privacy\". https://arxiv.org/abs/2106.02848\n                    - all: Report loss with all methods listed above.\n            eps_error: Error threshold for upper and lower bound in the GLW accounting procedure.\n            skip_checks: Skips the model type validation test if True.\n        \"\"\"\n        utils.handle_unused_kwargs(unused_kwargs)\n        del unused_kwargs\n        super(PrivacyEngine, self).__init__()\n\n        if clipping_mode not in ClippingMode.all():\n            raise ValueError(f\"Unknown clipping mode {clipping_mode}. Expected one of {ClippingMode.all()}.\")\n        if accounting_mode not in AccountingMode.all():\n            raise ValueError(f\"Unknown accounting mode: {accounting_mode}. Expected one of {AccountingMode.all()}.\")\n        if epochs <= 0.0:\n            raise ValueError(f\"Number of training epochs cannot be non-positive, but found epochs={epochs}\")\n\n        # Privacy parameters.\n        sample_rate = batch_size / sample_size\n        if target_delta is None:\n            target_delta = sample_size ** -1.1\n        if noise_multiplier is None:\n            if target_epsilon is None or epochs is None:\n                raise ValueError(\n                    f\"`target_epsilon` and `epochs` must be specified when `noise_multiplier` is `None`.\"\n                )\n            if accounting_mode in (\"rdp\", \"all\"):\n                manager = accounting_manager.RDPManager(alphas=alphas)\n            else:  # \"glw\"\n                manager = accounting_manager.GLWManager(eps_error=eps_error)\n            noise_multiplier = manager.compute_sigma(\n                target_epsilon=target_epsilon, target_delta=target_delta, sample_rate=sample_rate, epochs=epochs,\n            )\n\n        self.batch_size = batch_size\n        self.sample_size = sample_size\n        self.sample_rate = sample_rate\n        self.max_grad_norm = max_grad_norm\n\n        self.epochs = epochs\n        self.noise_multiplier = noise_multiplier\n        self.effective_noise_multiplier = noise_multiplier / batch_size\n        self.target_epsilon = target_epsilon\n        self.target_delta = target_delta\n        self.alphas = alphas\n        self.eps_error = eps_error\n        self.accounting_mode = accounting_mode\n        self.record_snr = record_snr\n\n        # Internals.\n        self.steps = 0  # Tracks privacy spending.\n\n        # Recording.\n        self.max_clip = None\n        self.min_clip = None\n        self.med_clip = None\n        self.signal = None\n        self.noise = None\n        self.snr = None\n        self.noise_limit = None\n\n        # Record parameters.\n        self.module = module\n        if named_params is None:\n            self.named_params = tuple(\n                (name, param) for (name, param) in module.named_parameters() if param.requires_grad\n            )\n        else:\n            self.named_params = named_params\n        self.num_params = sum(param.numel() for _, param in self.named_params)\n\n        self._locked = False  # Lock the part where noisy gradients is created (in `self.step`) if True.\n        self.numerical_stability_constant = numerical_stability_constant\n        self.clipping_mode = clipping_mode\n        if clipping_mode == ClippingMode.ghost:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)  # Prepare for first backward.\n        else:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.default)  # Extra guard.\n\n        if not isinstance(module, SUPPORTED_TRANSFORMERS) and not skip_checks:\n            raise ValueError(\n                f\"Model type {type(module)} is not supported. Please file an issue if you want this model to be added.\\n\"\n                f\"Currently supported transformers are: {SUPPORTED_TRANSFORMERS}\"\n            )\n        transformers_support.forward_swapper(module=module)  # Fix the position embeddings broadcast issue.\n\n    def lock(self):\n        \"\"\"Run this after noisy clipped gradient is created to prevent tampering with it before parameter update.\"\"\"\n        self._locked = True\n\n    def unlock(self):\n        \"\"\"Run this after parameter update to allow creation of noisy gradient for next step\"\"\"\n        self._locked = False\n\n    def attach(self, optimizer):\n        # `loss_reduction=\"sum\"` super important.\n        autograd_grad_sample.add_hooks(model=self.module, loss_reduction=\"sum\")\n\n        # Override zero grad.\n        def dp_zero_grad(_self, *args, **kwargs):\n            _self.privacy_engine.zero_grad()\n\n        # Override step.\n        def dp_step(_self, **kwargs):\n            closure = kwargs.pop(\"closure\", None)\n\n            _self.privacy_engine.step(**kwargs)\n            _self.original_step(closure=closure)\n            _self.privacy_engine.unlock()  # Only enable creating new grads once parameters are updated.\n            _self.privacy_engine.steps += 1\n\n        def virtual_step(_self, **kwargs):\n            _self.privacy_engine.virtual_step(**kwargs)\n\n        def get_privacy_spent(_self, **kwargs):\n            return _self.privacy_engine.get_privacy_spent(**kwargs)\n\n        def get_training_stats(_self, **kwargs):\n            return _self.privacy_engine.get_training_stats(**kwargs)\n\n        optimizer.privacy_engine = self\n\n        optimizer.original_step = optimizer.step\n        optimizer.step = types.MethodType(dp_step, optimizer)\n\n        optimizer.original_zero_grad = optimizer.zero_grad\n        optimizer.zero_grad = types.MethodType(dp_zero_grad, optimizer)\n\n        optimizer.virtual_step = types.MethodType(virtual_step, optimizer)\n\n        # Make getting info easier.\n        optimizer.get_privacy_spent = types.MethodType(get_privacy_spent, optimizer)\n        optimizer.get_training_stats = types.MethodType(get_training_stats, optimizer)\n\n        self.module.privacy_engine = self\n\n        # Just to be safe, we also override `zero_grad` for module.\n        self.module.original_zero_grad = self.module.zero_grad\n        self.module.zero_grad = types.MethodType(dp_zero_grad, self.module)\n\n        # For easy detaching.\n        self.optimizer = optimizer\n\n    def detach(self):\n        optimizer = self.optimizer\n        optimizer.step = optimizer.original_step\n        optimizer.zero_grad = optimizer.original_zero_grad\n        delattr(optimizer, \"privacy_engine\")\n        delattr(optimizer, \"original_step\")\n        delattr(optimizer, \"original_zero_grad\")\n        delattr(optimizer, \"virtual_step\")\n        delattr(optimizer, \"get_privacy_spent\")\n        delattr(optimizer, \"get_training_stats\")\n\n        module = self.module\n        autograd_grad_sample.remove_hooks(module)\n        autograd_grad_sample.set_hooks_mode(\"default\")  # This is super important when there are multiple attaches!\n        module.zero_grad(skip_grad=True)  # noqa\n        module.zero_grad = module.original_zero_grad\n        delattr(module, \"original_zero_grad\")\n\n    @torch.no_grad()\n    def step(\n        self,\n        loss: torch.Tensor,\n        scale=1.,\n        # Function that takes in named_params and does something.\n        # This option was included to help with another spectrum analysis project.\n        callback: Optional[Callable] = None,\n    ):\n        if loss.dim() != 1:\n            raise ValueError(\n                f\"Expected `loss` to be the per-example loss 1-D tensor, but got a tensor with dims={loss.dim()}.\n            )\n\n        if self.clipping_mode == ClippingMode.ghost:\n            if callback is not None:\n                raise ValueError(\"Ghost clipping does not support `callback` in `optimizer.step`.\")\n            if scale != 1.:\n                raise ValueError(\"Ghost clipping does not support mixed-precision training.\")\n            self._ghost_step(loss=loss)\n        else:\n            self._step(loss=loss, scale=scale, callback=callback)\n\n    @torch.no_grad()\n    def virtual_step(self, loss: torch.Tensor, scale=1.):\n        \"\"\"Virtual step function when there's gradient accumulation.\"\"\"\n        if self.clipping_mode == ClippingMode.ghost:\n            self._ghost_virtual_step(loss=loss)\n        else:\n            self._virtual_step(loss=loss, scale=scale)\n\n    def zero_grad(self, skip_grad=False):\n        for name, param in self.named_params:\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"summed_grad\"):\n                del param.summed_grad\n            if not skip_grad:\n                if hasattr(param, \"grad\"):\n                    del param.grad\n\n    def _create_noisy_clipped_gradient(self):\n        \"\"\"Create noisy clipped gradient for `optimizer.step`.\n\n        Add noise and scale by inverse batch size.\n\n        Notes:\n            In ghost clipping, `summed_grad` stores previous micro-batches; `grad` stores current micro-batch.\n            In default clipping, `summed_grad` stores summed clipped gradients for all micro-batches.\n        \"\"\"\n\n        signals, noises = [], []\n        for name, param in self.named_params:\n            assert hasattr(param, 'summed_grad'), (\n                f\"Internal error: PrivacyEngine should not reach here; \"\n                f\"this means either \"\n                f\"1) there is parameter which requires gradient, but was not used in the computational graph, \"\n                f\"or 2) the backward hook registry failed to find the corresponding module to register.\"\n            )\n            param.grad = param.summed_grad  # Ultra important to override `.grad`.\n\n            if self.record_snr:\n                signals.append(param.grad.reshape(-1).norm(2))\n\n            if self.noise_multiplier > 0 and self.max_grad_norm > 0:\n                noise = torch.normal(\n                    mean=0,\n                    std=self.noise_multiplier * self.max_grad_norm,\n                    size=param.size(),\n                    device=param.device,\n                    dtype=param.dtype,\n                )\n                param.grad += noise\n                if self.record_snr:\n                    noises.append(noise.reshape(-1).norm(2))\n                del noise\n\n            param.grad /= self.batch_size\n\n        if self.record_snr and len(noises) > 0:\n            self.signal, self.noise = tuple(torch.stack(lst).norm(2).item() for lst in (signals, noises))\n            self.noise_limit = math.sqrt(self.num_params) * self.noise_multiplier * self.max_grad_norm\n            self.snr = self.signal / self.noise\n        else:\n            self.snr = math.inf  # Undefined!\n\n        self.lock()  # Make creating new gradients impossible, unless optimizer.step is called.\n\n    # --- ghost clipping ---\n    def _ghost_step(self, loss: torch.Tensor):\n        \"\"\"Run double-backward on per-example loss, then sum up all gradients and noise it.\"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        self._ghost_virtual_step(loss)\n        self._create_noisy_clipped_gradient()\n\n    @torch.no_grad()\n    def _ghost_virtual_step(self, loss: torch.Tensor):\n        \"\"\"Backward twice to accumulate summed clipped gradients in `.summed_grad`.\n\n        We accumulate gradients in `.summed_grad` for micro-batching.\n        All of this copying actually creates a new 2x memory overhead.\n        \"\"\"\n        self._double_backward(loss)\n\n        for name, param in self.named_params:\n            if hasattr(param, 'summed_grad'):\n                param.summed_grad += param.grad\n            else:\n                param.summed_grad = param.grad\n\n            if hasattr(param, \"grad\"):\n                del param.grad\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n\n    @torch.enable_grad()\n    def _double_backward(self, loss: torch.Tensor):\n        \"\"\"Given per-example losses, backward twice to accumulate summed clipped gradients in `.grad`.\"\"\"\n        first_loss = loss.sum()\n        first_loss.backward(retain_graph=True)\n\n        # Prepare for second backward.\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_grad)\n\n        # The first backward might have accumulated things we don't need into `.grad`;\n        # remove it before the second pass to avoid accumulating garbage.\n        for name, param in self.named_params:\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        coef_sample = self.get_coef_sample()\n        second_loss = (coef_sample * loss).sum(dim=0)\n        second_loss.backward()\n\n        # Prepare for first backward (in the next round).\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)\n\n    def get_coef_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient scaling factor for clipping.\"\"\"\n        norm_sample = self.get_norm_sample()\n        return torch.clamp_max(self.max_grad_norm / (norm_sample + self.numerical_stability_constant), 1.)\n\n    def get_norm_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient norms.\"\"\"\n        norm_sample = torch.stack([param.norm_sample for name, param in self.named_params], dim=0).norm(2, dim=0)\n        return norm_sample\n\n    # --- default clipping ---\n    def _step(\n        self,\n        loss,\n        scale,\n        callback,\n    ):\n        \"\"\"Create noisy gradients.\n\n        Should be run right before you call `optimizer.step`.\n\n        This function does 3 things:\n            1) call `loss.backward()`\n            2) clip the current `.grad_sample` and add that to `.summed_grad`\n            3) noise the gradients\n        In mixed-precision training (with amp), the last two steps require knowing the loss scaling factor.\n\n        Args:\n            loss: The per-example loss; a 1-D tensor.\n            scale: The loss up-scaling factor in amp. In full precision, this arg isn't useful.\n        \"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        norm_sample, coef_sample = self._accumulate_summed_grad(loss=loss, scale=scale)\n        # Collect stats for debugging.\n        self.max_clip = coef_sample.max().item()\n        self.min_clip = coef_sample.min().item()\n        self.med_clip = coef_sample.median().item()\n\n        if callback is not None:\n            callback(self)\n        self._create_noisy_clipped_gradient()\n\n    def _virtual_step(self, loss, scale):\n        self._accumulate_summed_grad(loss=loss, scale=scale)\n\n    @torch.no_grad()\n    def _accumulate_summed_grad(self, loss, scale):\n        \"\"\"Accumulate signal by summing clipped gradients.\n\n        Removes `.grad_sample` and `.grad` for each variable that requires grad at the end.\n        \"\"\"\n        with torch.enable_grad():\n            loss.sum(dim=0).backward()\n\n        norm_sample = []\n        for name, param in self.named_params:\n            try:\n                batch_size = param.grad_sample.size(0)\n            except AttributeError as error:\n                args = error.args\n                extra_msg = f\"\\n *** {name} parameter has no grad_sample attribute ***\"\n                error.args = (args[0] + extra_msg, *args[1:])\n                raise error\n            norm = param.grad_sample.reshape(batch_size, -1).norm(2, dim=1)\n            norm_sample.append(norm)\n\n        # The stack operation here is prone to error, thus clarify where the error is.\n        try:\n            norm_sample = torch.stack(norm_sample, dim=0).norm(2, dim=0)\n        except RuntimeError as runtime_error:\n            args = runtime_error.args\n\n            # Get the major shape.\n            shapes = collections.defaultdict(int)\n            for tensor in norm_sample:\n                shapes[tensor.size()] += 1\n\n            # Get the shape that most tensors have.\n            major_shape, major_count = max(shapes.items(), key=lambda x: x[1])\n\n            # Check which tensors don't have the major shape!\n            extra_msg = f\" \\n*** Major shape: {major_shape}\"\n            for (name, param), tensor in zip(list(self.named_params), norm_sample):\n                if tensor.size() != major_shape:\n                    extra_msg += f\", {name} wrong shape: {tensor.size()}\"\n            extra_msg += \" ***\"\n\n            runtime_error.args = (args[0] + extra_msg, *args[1:])\n            raise runtime_error\n\n        coef_sample = torch.clamp_max(\n            self.max_grad_norm * scale / (norm_sample + self.numerical_stability_constant), 1.\n        )\n        for name, param in self.named_params:\n            if not hasattr(param, 'summed_grad'):\n                param.summed_grad = 0.\n            current_device = param.grad_sample.device\n            param.summed_grad += torch.einsum(\"i,i...->...\", coef_sample.to(current_device), param.grad_sample)\n\n            # Aggressive memory saving -- delete everything except `.summed_grad` to save memory!\n            if hasattr(param, \"grad_sample\"):\n                # This must be deleted due to how `privacy_utils::supported_layers_grad_samplers.py` works!\n                #   When a parameter with `.grad_sample` is reused, the per-sample gradients are accumulated!\n                del param.grad_sample\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        return norm_sample, coef_sample\n\n    def get_privacy_spent(\n        self,\n        steps: Optional[int] = None,\n        accounting_mode: Optional[str] = None,\n        lenient=False\n    ) -> Dict:\n        if steps is None:\n            steps = self.steps\n        if accounting_mode is None:\n            accounting_mode = self.accounting_mode\n\n        privacy_results = {}  # Contains stats from all modes.\n        if accounting_mode in (AccountingMode.all_, AccountingMode.rdp):\n            try:\n                manager = accounting_manager.RDPManager(alphas=self.alphas)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps,\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\"RDP accounting failed! Double check privacy parameters.\")\n                if not lenient:\n                    raise err\n\n        if accounting_mode in (AccountingMode.all_, AccountingMode.glw):\n            try:\n                manager = accounting_manager.GLWManager(eps_error=self.eps_error)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\n                    \"Numerical composition of tradeoff functions failed! Double check privacy parameters.\"\n                )\n                if not lenient:\n                    raise err\n\n        return privacy_results\n\n    def get_training_stats():\n        \"\"\"Get the clipping, signal, and noise statistics.\"\"\"\n        return {\n            \"med_clip\": self.med_clip,\n            \"max_clip\": self.max_clip,\n            \"min_clip\": self.min_clip,\n            \"snr\": self.snr,\n            \"signal\": self.signal,\n            \"noise\": self.noise,\n            \"noise_limit\": self.noise_limit,\n        }\n\n    def __repr__(self):\n        return (\n            f\"PrivacyEngine(\\n\"\n            f\"  target_epsilon={self.target_epsilon:.6f}, \\n\"\n            f\"  target_delta={self.target_delta:.6f}, \\n\"\n            f\"  noise_multiplier={self.noise_multiplier:.6f}, \\n\"\n            f\"  effective_noise_multiplier={self.effective_noise_multiplier:.6f}, \\n\"\n            f\"  epochs={self.epochs}, \\n\"\n            f\"  max_grad_norm={self.max_grad_norm}, \\n\"\n            f\"  sample_rate={self.sample_rate}, \\n\"\n            f\"  batch_size={self.batch_size}, \\n\"\n            f\"  accounting_mode={self.accounting_mode}, \\n\"\n            f\"  clipping_mode={self.clipping_mode}\\n\"\n            f\")\"\n        )\n\n\n# File: private_transformers/accounting/accounting_manager.py\nimport abc\nimport math\nfrom typing import Dict, Optional, Union\n\nfrom . import rdp_accounting\n\nDEFAULT_ALPHAS = tuple(1 + x / 10.0 for x in range(1, 100)) + tuple(range(12, 64))  # RDP.\n\n\nclass AccountingManager(abc.ABC):\n    def _get_sigma_with_target_epsilon(\n        self,\n        target_epsilon,\n        target_delta,\n        sample_rate,\n        steps,\n        threshold,\n        sigma_hi_init,\n        sigma_lo_init,\n    ):\n        \"\"\"Binary search σ given ε and δ.\"\"\"\n        if sigma_lo_init > sigma_hi_init:\n            raise ValueError(\"`sigma_lo` should be smaller than `sigma_hi`.\")\n\n        # Find an appropriate region for binary search.\n        sigma_hi = sigma_hi_init\n        sigma_lo = sigma_lo_init\n\n        # Ensure sigma_hi isn't too small.\n        while True:\n            eps = self._compute_epsilon_from_sigma(sigma_hi, sample_rate, target_delta, steps)\n            if eps < target_epsilon:\n                break\n            sigma_hi *= 2\n\n        # Ensure sigma_lo isn't too large.\n        while True:\n            eps = self._compute_epsilon_from_sigma(sigma_lo, sample_rate, target_delta, steps)\n            if eps > target_epsilon:\n                break\n            sigma_lo /= 2\n\n        # Binary search.\n        while sigma_hi - sigma_lo > threshold:\n            sigma = (sigma_hi + sigma_lo) / 2\n            eps = self._compute_epsilon_from_sigma(sigma, sample_rate, target_delta, steps)\n            if eps < target_epsilon:\n                sigma_hi = sigma\n            else:\n                sigma_lo = sigma\n\n        # Conservative estimate.\n        return sigma_hi\n\n    @abc.abstractmethod\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        \"\"\"Override for reporting results.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps) -> float:\n        \"\"\"Override for binary sigma search.\"\"\"\n        raise NotImplementedError\n\n    def compute_sigma(\n        self,\n        target_epsilon: float,\n        target_delta: float,\n        sample_rate: float,\n        epochs: Optional[Union[float, int]] = None,\n        steps=None,\n        threshold=1e-3,\n        sigma_hi_init=4,\n        sigma_lo_init=0.1,\n    ) -> float:\n        if steps is None:\n            if epochs is None:\n                raise ValueError(\"Epochs and steps cannot both be None.\")\n            steps = math.ceil(epochs / sample_rate)\n        return self._get_sigma_with_target_epsilon(\n            target_epsilon=target_epsilon,\n            target_delta=target_delta,\n            sample_rate=sample_rate,\n            steps=steps,\n            threshold=threshold,\n            sigma_hi_init=sigma_hi_init,\n            sigma_lo_init=sigma_lo_init,\n        )\n\n\nclass RDPManager(AccountingManager):\n    def __init__(self, alphas):\n        super(RDPManager, self).__init__()\n        self._alphas = alphas\n\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):\n        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)[\"eps_rdp\"]\n\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        \"\"\"Compute RDP as usual, but convert to (ε, δ)-DP based on the result by Canonne, Kamath, Steinke.\"\"\"\n        rdp = rdp_accounting.compute_rdp(q=sample_rate, noise_multiplier=sigma, steps=steps, orders=self._alphas)\n        eps, alpha = rdp_accounting.get_privacy_spent(orders=self._alphas, rdp=rdp, delta=target_delta)\n        return dict(eps_rdp=eps, alpha_rdp=alpha)\n\n\nclass GLWManager(AccountingManager):\n    def __init__(self, eps_error=0.05):\n        super(GLWManager, self).__init__()\n        self._eps_error = eps_error\n\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):\n        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)[\"eps_upper\"]  # Be conservative.\n\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        if steps == 0:\n            return dict(eps_low=None, eps_estimate=None, eps_upper=None)\n\n        from prv_accountant import Accountant\n        accountant = Accountant(\n            noise_multiplier=sigma,\n            sampling_probability=sample_rate,\n            delta=target_delta,\n            eps_error=self._eps_error,\n            max_compositions=steps\n        )\n        eps_low, eps_estimate, eps_upper = accountant.compute_epsilon(num_compositions=steps)\n        return dict(eps_low=eps_low, eps_estimate=eps_estimate, eps_upper=eps_upper)\n\n\n# File: private_transformers/autograd_grad_sample.py\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .settings import BackwardHookMode\nfrom .supported_layers_grad_samplers import _supported_layers_grad_samplers\n\n_hooks_disabled: bool = False\n_hooks_mode = BackwardHookMode.default\n\n\ndef set_hooks_mode(mode):\n    if mode not in BackwardHookMode.all():\n        raise ValueError(f\"Unknown mode for hooks: {mode}; expected one of {BackwardHookMode.all()}.\")\n\n    global _hooks_mode\n    _hooks_mode = mode  # Set mode.\n\n    if _hooks_mode == BackwardHookMode.ghost_grad:  # Second backward pass of ghost clipping doesn't need hooks.\n        disable_hooks()\n    elif _hooks_mode == BackwardHookMode.ghost_norm:  # First backward pass of ghost clipping needs to accumulate norms.\n        enable_hooks()\n\n\ndef get_hooks_mode():\n    global _hooks_mode\n    return _hooks_mode\n\n\ndef requires_grad(module: nn.Module, recurse: bool = False) -> bool:\n    \"\"\"\n    Checks if any parameters in a specified module require gradients.\n\n    Args:\n        module: PyTorch module whose parameters are examined\n        recurse: Flag specifying if the gradient requirement check should\n            be applied recursively to sub-modules of the specified module\n\n    Returns:\n        Flag indicate if any parameters require gradients\n    \"\"\"\n    return any(p.requires_grad for p in module.parameters(recurse))\n\n\ndef add_hooks(model: nn.Module, loss_reduction: str = \"mean\"):\n    r\"\"\"\n    Adds hooks to model to save activations and backprop values.\n    The hooks will\n\n    1. save activations into ``param.activations`` during forward pass.\n    2. compute per-sample gradients and save them in ``param.grad_sample`` during backward pass.\n\n    Args:\n        model: Model to which hooks are added.\n        loss_reduction: Indicates if the loss reduction (for aggregating the gradients) is a sum or a mean operation.\n            Can take values ``sum`` or ``mean``.\n    \"\"\"\n    if hasattr(model, \"autograd_grad_sample_hooks\"):\n        raise ValueError(\"Trying to add hooks twice to the same model\")\n\n    enable_hooks()\n\n    handles = []\n    for name, layer in model.named_modules():\n        if type(layer) in _supported_layers_grad_samplers:\n            if requires_grad(layer, recurse=False):\n                handles.append(layer.register_forward_hook(_capture_activations))\n\n                def this_backward(this_layer, grad_input, grad_output):\n                    return _capture_backprops(this_layer, grad_input, grad_output, loss_reduction)\n\n                # Starting with 1.8.0, use `register_full_backward_hook`.\n                handles.append(layer.register_backward_hook(this_backward))\n\n    model.__dict__.setdefault(\"autograd_grad_sample_hooks\", []).extend(handles)\n\n\ndef remove_hooks(model: nn.Module):\n    \"\"\"Removes hooks added by `add_hooks()`.\"\"\"\n    if not hasattr(model, \"autograd_grad_sample_hooks\"):\n        raise ValueError(\"Asked to remove hooks, but no hooks found\")\n    else:\n        for handle in model.autograd_grad_sample_hooks:\n            handle.remove()\n        del model.autograd_grad_sample_hooks\n\n\ndef disable_hooks():\n    \"\"\"Globally disables all hooks installed by this library.\"\"\"\n    global _hooks_disabled\n    _hooks_disabled = True\n\n\ndef enable_hooks():\n    \"\"\"Globally enables all hooks installed by this library.\"\"\"\n    global _hooks_disabled\n    _hooks_disabled = False\n\n\ndef _capture_activations(layer: nn.Module, inputs: Tuple, outputs: Tuple):\n    \"\"\"Forward hook handler captures and saves activations.\"\"\"\n    if not requires_grad(layer) or not layer.training or _hooks_disabled:\n        return\n\n    if not hasattr(layer, \"activations\"):\n        layer.activations = []\n\n    # This improves on original Opacus and supports additional arguments on top of the (first) activation tensor.\n    stored_inputs = tuple(input_i.detach() if torch.is_tensor(input_i) else input_i for input_i in inputs)\n    layer.activations.append(stored_inputs)\n\n\ndef _capture_backprops(\n    layer: nn.Module,\n    inputs: Tuple[torch.Tensor],\n    outputs: Tuple[torch.Tensor],\n    loss_reduction: str\n):\n    \"\"\"Backward hook handler captures grad_outputs.\"\"\"\n    # This improves on the original Opacus codebase and supports multiple outputs.\n    backprops = tuple(output_i.detach() if torch.is_tensor(output_i) else output_i for output_i in outputs)\n    _compute_grad_sample(layer, backprops, loss_reduction)\n\n\ndef _compute_grad_sample(layer: nn.Module, backprops: Tuple, loss_reduction: str):\n    \"\"\"Computes per-sample gradients with respect to the parameters.\"\"\"\n    if not requires_grad(layer) or not layer.training or _hooks_disabled:\n        return\n\n    if not hasattr(layer, \"activations\"):\n        raise ValueError(f\"No activations detected for {type(layer)}, run forward after add_hooks(model)\")\n\n    # Outside of the LSTM there is \"batch_first\" but not for the Linear inside the LSTM\n    if isinstance(layer.activations, list):\n        A = layer.activations.pop()\n    else:\n        A = layer.activations\n\n    if not hasattr(layer, \"max_batch_len\"):\n        assert torch.is_tensor(A[0]), f\"Internal error: first input of the following layer isn't a Tensor. \\n{layer}\"\n        layer.max_batch_len = _get_batch_size(layer, A[0])\n\n    n = layer.max_batch_len\n    if loss_reduction == \"mean\":\n        B = tuple(B_i * n if torch.is_tensor(B_i) else B_i for B_i in backprops)\n    elif loss_reduction == \"sum\":\n        B = backprops\n    else:\n        raise ValueError(f\"loss_reduction = {loss_reduction}. Only 'sum' and 'mean' losses are supported\")\n\n    # compute grad sample for individual layers\n    compute_layer_grad_sample = _supported_layers_grad_samplers.get(type(layer))\n    compute_layer_grad_sample(layer, A, B)\n\n    if (not isinstance(layer.activations, list) or len(layer.activations) == 0) and hasattr(layer, \"max_batch_len\"):\n        del layer.max_batch_len\n\n\ndef _get_batch_size(layer: nn.Module, grad_sample: torch.Tensor) -> int:\n    r\"\"\"\n    Computes and returns the maximum batch size which is the maximum of the dimension values\n    along 'batch_dim' axis over layer.activations + [grad_sample], where layer.activations is\n    a list. If layer.activations is a not a list, then return grad_sample.shape[batch_dim].\n    \"\"\"\n\n    batch_dim = 0\n    max_batch_len = 0\n    if isinstance(layer.activations, list):\n        for out in layer.activations:\n            assert torch.is_tensor(out[0]), (\n                f\"Internal error: first input of the following layer isn't a Tensor. \\n{layer}\"\n            )\n            if out[0].shape[batch_dim] > max_batch_len:\n                max_batch_len = out[0].shape[batch_dim]\n\n    max_batch_len = max(max_batch_len, grad_sample.shape[batch_dim])\n    return max_batch_len\n\n\n# File: examples/classification/run_classification.py\nimport collections\nimport copy\nimport dataclasses\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport gc\nimport json\nimport logging\nimport os\nfrom typing import Callable, Dict, Optional\n\nfrom filelock import FileLock\nimport numpy as np\nfrom ml_swissknife import utils\nimport torch\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction\nfrom transformers import GlueDataTrainingArguments as DataTrainingArguments\nfrom transformers import GlueDataset\nfrom transformers import HfArgumentParser, set_seed\n\nfrom private_transformers import PrivacyEngine\nfrom .src.common import true_tags\nfrom .src.compiled_args import PrivacyArguments, TrainingArguments, AuxiliaryArguments\nfrom .src.dataset import FewShotDataset\nfrom .src.models import (\n    BertForPromptFinetuning, RobertaForPromptFinetuning, AlbertForPromptFinetuning, DistilBertForPromptFinetuning,\n    resize_token_type_embeddings\n)\nfrom .src.processors import num_labels_mapping, output_modes_mapping, compute_metrics_mapping, bound_mapping\nfrom .src.trainer import Trainer\n\nlogger = logging.getLogger(__name__)\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n    )\n    # Few-shot type\n    #   - finetune: standard fine-tuning\n    #   - prompt: prompt-based fine-tuning\n    #   - prompt-demo: prompt-based fine-tuning with demonstrations\n    few_shot_type: str = field(\n        default='prompt-demo',\n        metadata={\"help\": \"Few-shot learning model type. Choice: finetune, prompt, prompt-demo\"}\n    )\n\n    # Only for BERT-type model\n    random_segment: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to reinitialize the token type embeddings (only for BERT).\"}\n    )\n\n    static_embedding: str = field(\n        default=\"no\"\n    )\n    static_lm_head: str = field(\n        default=\"no\"\n    )\n    attention_only: str = field(\n        default=\"no\"\n    )\n\n    randomly_initialize: str = field(\n        default=\"no\",\n        metadata={\"help\": \"Randomly initialize the model; useful only for ablation studies.\"}\n    )\n\n    def __post_init__(self):\n        self.static_embedding = self.static_embedding.lower() in true_tags  # noqa\n        self.static_lm_head = self.static_lm_head.lower() in true_tags  # noqa\n        self.attention_only = self.attention_only.lower() in true_tags  # noqa\n        self.randomly_initialize = self.randomly_initialize.lower() in true_tags  # noqa\n\n\n@dataclass\nclass DynamicDataTrainingArguments(DataTrainingArguments):\n    \"\"\"\n    Arguments for dynamic training.\n    \"\"\"\n    num_k: Optional[int] = field(\n        default=16,\n        metadata={\"help\": \"Number of training instances per class\"}\n    )\n\n    num_sample: Optional[int] = field(\n        default=16,\n        metadata={\"help\": \"Number of samples (for inference) in fine-tuning with demonstrations\"}\n    )\n\n    num_demo: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"Number of demonstrations from each class\"}\n    )\n\n    auto_demo: bool = field(\n        default=True,\n        metadata={\"help\": \"Automatically generate template for using demonstrations\"}\n    )\n\n    # For prompting\n    template: str = field(\n        default=None,\n        metadata={\"help\": \"Template\"}\n    )\n\n    mapping: str = field(\n        default=None,\n        metadata={\"help\": \"Label word mapping\"}\n    )\n\n    template_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to a txt file that stores all the templates, one per line. Do not set this when prompt_path \"\n                    \"is used\"}\n    )\n\n    mapping_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to a txt file that stores all the label word mappings, one per line. Do not set this when \"\n                    \"prompt_path is used\"}\n    )\n\n    prompt_path: str = field(\n        default=None,\n        metadata={\"help\": \"Path to a txt file that stores all the prompts (templates and mappings), one per line\"}\n    )\n\n    template_id: int = field(\n        default=None,\n        metadata={\"help\": \"Template id if using template_path\"}\n    )\n\n    mapping_id: int = field(\n        default=None,\n        metadata={\"help\": \"Mapping id if using template_path\"}\n    )\n\n    prompt_id: int = field(\n        default=None,\n        metadata={\"help\": \"Prompt id if using prompt_path\"}\n    )\n\n    top_n_template: int = field(\n        default=None,\n        metadata={\"help\": \"Use top-n template in the template path\"}\n    )\n\n    # For logging\n    tag: str = field(\n        default='',\n        metadata={\"help\": \"Set the tag and find the result easier in the log.\"}\n    )\n\n    # For filtering when using demonstrations\n    demo_filter: bool = field(\n        default=False,\n        metadata={\"help\": \"Only use similar instances in demonstrations\"}\n    )\n\n    demo_filter_rate: float = field(\n        default=0.5,\n        metadata={\"help\": \"Only use top-x\\% similar instances in demonstrations\"}\n    )\n\n    demo_filter_model: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Model name for demonstration filter embeddings. Will load embeddings based on the model name.\"}\n    )\n\n    debug_mode: bool = field(\n        default=False,\n        metadata={\"help\": \"Debug mode\"}\n    )\n\n    # For max length\n    double_demo: bool = field(\n        default=False,\n        metadata={\"help\": \"Use double length for using demonstrations\"}\n    )\n\n    first_sent_limit: int = field(\n        default=None,\n        metadata={\"help\": \"Limit the length of the first sentence (i.e., sent_0)\"}\n    )\n\n    other_sent_limit: int = field(\n        default=None,\n        metadata={\"help\": \"Limit the length of sentences other than the first sentence\"}\n    )\n\n    use_full_length: bool = field(\n        default=None,\n        metadata={\"help\": \"Use the full length (512)\"}\n    )\n\n    # GPT-3's in-context learning\n    gpt3_in_context_head: bool = field(\n        default=False,\n        metadata={\"help\": \"GPT-3's in-context learning (context at the beginning)\"}\n    )\n\n    gpt3_in_context_tail: bool = field(\n        default=False,\n        metadata={\"help\": \"GPT-3's in-context learning (context at the end)\"}\n    )\n\n    gpt3_in_context_num: int = field(\n        default=32,\n        metadata={\"help\": \"Number of context examples\"}\n    )\n\n    truncate_head: bool = field(\n        default=False,\n        metadata={\"help\": \"When exceeding the maximum length, truncate the head instead of the tail.\"}\n    )\n\n    # Do not set up the following fields. They are set up automatically.\n    prompt: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use prompt-based fine-tuning\"}\n    )\n    template_list: tuple = field(\n        default=None,\n        metadata={\"help\": \"(DO NOT List of templates (only initialized after the program starts.\"}\n    )\n\n    inference_time_demo: bool = field(\n        default=False,\n        metadata={\"help\": \"Do not use demonstrations during inference time; \"\n                          \"the original paper attaches to each test example a few training examples as demo -- \"\n                          \"apparently this breaks privacy. We turn this off by default here.\"}\n    )\n\n\n@dataclass\nclass DynamicTrainingArguments(TrainingArguments):\n    # For ensemble\n    array_id: int = field(\n        default=-1,\n        metadata={\"help\": \"Array ID (contains seed and hyper-paramter search) to idenfity the model\"}\n    )\n\n    model_id: int = field(\n        default=-1,\n        metadata={\"help\": \"Model ID (contains template information) to identify the model\"}\n    )\n\n    save_logit: bool = field(\n        default=False,\n        metadata={\"help\": \"Save test file logit with name $TASK-$MODEL_ID-$ARRAY_ID.npy\"}\n    )\n\n    save_logit_dir: str = field(\n        default=None,\n        metadata={\"help\": \"Where to save the prediction result\"}\n    )\n\n    # Regularization\n    fix_layers: int = field(\n        default=0,\n        metadata={\"help\": \"Fix bottom-n layers when optimizing\"}\n    )\n\n    # Training\n    save_at_last: bool = field(\n        default=False,\n        metadata={\"help\": \"Instead of saving the best (dev performance) checkpoint, save the last checkpoint\"}\n    )\n\n    # Turn off train/test\n    no_train: bool = field(\n        default=False,\n        metadata={\"help\": \"No training\"}\n    )\n    no_predict: bool = field(\n        default=False,\n        metadata={\"help\": \"No test\"}\n    )\n\n    evaluate_after_training: bool = field(\n        default=True, metadata={\"help\": \"Always run evaluation after training ends.\"}\n    )\n\n    def __post_init__(self):\n        super(DynamicTrainingArguments, self).__post_init__()\n\n\ndef main():\n    parser = HfArgumentParser(\n        (ModelArguments, DynamicDataTrainingArguments, DynamicTrainingArguments, PrivacyArguments, AuxiliaryArguments)\n    )\n    model_args, data_args, training_args, privacy_args, auxiliary_args = parser.parse_args_into_dataclasses()\n\n    if not os.path.exists(training_args.output_dir):\n        print(f\"output_dir doesn't exists, mkdir now: {training_args.output_dir}\")\n        os.makedirs(training_args.output_dir)\n\n    if 'prompt' in model_args.few_shot_type:\n        data_args.prompt = True\n\n    if training_args.no_train:\n        training_args.do_train = False\n    if training_args.no_predict:\n        training_args.do_predict = False\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n    )\n\n    # TODO: Hacky mapping creation. Refactor this in the future.\n    #  Currently gets replace if mapping_id and mapping_path is set.\n    if data_args.task_name == \"sst-2\":\n        data_args.mapping = \"{'0':'terrible','1':'great'}\"\n    elif data_args.task_name == \"mnli\":\n        data_args.mapping = \"{'contradiction': 'no', 'entailment': 'yes', 'neutral': 'maybe'}\"\n    elif data_args.task_name == \"qnli\":\n        data_args.mapping = \"{'not_entailment': 'no', 'entailment': 'yes'}\"\n    elif data_args.task_name == \"qqp\":\n        data_args.mapping = \"{'1': 'yes', '0': 'no'}\"  # 1 -- equivalent, 0 -- not equivalent.\n    else:\n        raise ValueError(f\"Unknown task: {data_args.task_name}\")\n\n    # Load prompt/template/mapping file\n    if data_args.prompt:\n        if data_args.prompt_path is not None:\n            assert data_args.prompt_id is not None\n            prompt_list = []\n            with open(data_args.prompt_path) as f:\n                for line in f:\n                    line = line.strip()\n                    template, mapping = line.split('\\t')\n                    prompt_list.append((template, mapping))\n\n            data_args.template, data_args.mapping = prompt_list[data_args.prompt_id]\n            logger.info(\n                \"Specify load the %d-th prompt: %s | %s\" % (data_args.prompt_id, data_args.template, data_args.mapping))\n        else:\n            if data_args.template_path is not None:\n                with open(data_args.template_path) as f:\n                    data_args.template_list = []\n                    for line in f:\n                        line = line.strip()\n                        if len(line) > 0:\n                            data_args.template_list.append(line)\n\n                # Load top-n templates\n                if data_args.top_n_template is not None:\n                    data_args.template_list = data_args.template_list[:data_args.top_n_template]\n                logger.info(\"Load top-%d templates from %s\" % (len(data_args.template_list), data_args.template_path))\n\n                # ... or load i-th template\n                if data_args.template_id is not None:\n                    data_args.template = data_args.template_list[data_args.template_id]\n                    data_args.template_list = None\n                    logger.info(\"Specify load the %d-th template: %s\" % (data_args.template_id, data_args.template))\n\n            if data_args.mapping_path is not None:\n                assert data_args.mapping_id is not None  # Only can use one label word mapping\n                with open(data_args.mapping_path) as f:\n                    mapping_list = []\n                    for line in f:\n                        line = line.strip()\n                        mapping_list.append(line)\n\n                data_args.mapping = mapping_list[data_args.mapping_id]\n                logger.info(\"Specify using the %d-th mapping: %s\" % (data_args.mapping_id, data_args.mapping))\n\n    # Check save path\n    if (\n        os.path.exists(training_args.output_dir)\n        and os.listdir(training_args.output_dir)\n        and training_args.do_train\n        and not training_args.overwrite_output_dir\n    ):\n        raise ValueError(f\"Output directory ({training_args.output_dir}) already exists.\")\n\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        training_args.local_rank,\n        training_args.device,\n        training_args.n_gpu,\n        bool(training_args.local_rank != -1),\n        training_args.fp16,\n    )\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n\n    # Set seed\n    set_seed(training_args.seed)\n\n    try:\n        num_labels = num_labels_mapping[data_args.task_name]\n        output_mode = output_modes_mapping[data_args.task_name]\n        logger.info(\n            \"Task name: {}, number of labels: {}, output mode: {}\".format(data_args.task_name, num_labels, output_mode))\n    except KeyError:\n        raise ValueError(\"Task not found: %s\" % (data_args.task_name))\n\n    # Automatically generate template for using demonstrations\n    if data_args.auto_demo and model_args.few_shot_type == 'prompt-demo':\n        # GPT-3's in-context learning\n        if data_args.gpt3_in_context_head or data_args.gpt3_in_context_tail:\n            logger.info(\"Automatically convert the template to GPT-3's in-context learning.\")\n            assert data_args.template_list is None\n\n            old_template = data_args.template\n            new_template = old_template + ''\n            old_template = old_template.replace('*cls*', '')\n            # Single sentence or sentence pair?\n            sent_num = 1\n            if \"_1\" in old_template:\n                sent_num = 2\n            for instance_id in range(data_args.gpt3_in_context_num):\n                sub_template = old_template + ''\n                # Replace sent_id\n                for sent_id in range(sent_num):\n                    sub_template = sub_template.replace(\"_{}*\".format(sent_id),\n                                                        \"_{}*\".format(sent_num + sent_num * instance_id + sent_id))\n                # Replace mask\n                sub_template = sub_template.replace(\"*mask*\", \"*labelx_{}*\".format(instance_id))\n                if data_args.gpt3_in_context_tail:\n                    new_template = new_template + sub_template  # Put context at the end\n                else:\n                    new_template = sub_template + new_template  # Put context at the beginning\n            logger.info(\"| {} => {}\".format(data_args.template, new_template))\n            data_args.template = new_template\n        else:\n            logger.info(\"Automatically convert the template to using demonstrations.\")\n            if data_args.template_list is not None:\n                for i in range(len(data_args.template_list)):\n                    old_template = data_args.template_list[i]\n                    new_template = old_template + ''\n                    old_template = old_template.replace('*cls*', '')\n                    # Single sentence or sentence pair?\n                    sent_num = 1\n                    if \"_1\" in old_template:\n                        sent_num = 2\n                    for label_id in range(num_labels):\n                        sub_template = old_template + ''\n                        # Replace sent id\n                        for sent_id in range(sent_num):\n                            sub_template = sub_template.replace(\"_{}*\".format(sent_id),\n                                                                \"_{}*\".format(sent_num + sent_num * label_id + sent_id))\n                        # Replace mask\n                        sub_template = sub_template.replace(\"*mask*\", \"*label_{}*\".format(label_id))\n                        new_template = new_template + sub_template\n                    logger.info(\"| {} => {}\".format(data_args.template_list[i], new_template))\n                    data_args.template_list[i] = new_template\n            else:\n                old_template = data_args.template\n                new_template = old_template + ''\n                old_template = old_template.replace('*cls*', '')\n                # Single sentence or sentence pair?\n                sent_num = 1\n                if \"_1\" in old_template:\n                    sent_num = 2\n                for label_id in range(num_labels):\n                    sub_template = old_template + ''\n                    # Replace sent id\n                    for sent_id in range(sent_num):\n                        sub_template = sub_template.replace(\"_{}\".format(sent_id),\n                                                            \"_{}\".format(sent_num + sent_num * label_id + sent_id))\n                    # Replace mask\n                    sub_template = sub_template.replace(\"*mask*\", \"*label_{}*\".format(label_id))\n                    new_template = new_template + sub_template\n                logger.info(\"| {} => {}\".format(data_args.template, new_template))\n                data_args.template = new_template\n\n    # Create config\n    config = AutoConfig.from_pretrained(\n        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n        num_labels=num_labels,\n        finetuning_task=data_args.task_name,\n        cache_dir=model_args.cache_dir,\n    )\n\n    if 'prompt' in model_args.few_shot_type:\n        if config.model_type == 'roberta':\n            model_fn = RobertaForPromptFinetuning\n        elif config.model_type == 'bert':\n            model_fn = BertForPromptFinetuning\n        elif config.model_type == 'albert':\n            model_fn = AlbertForPromptFinetuning\n        elif config.model_type == 'distilbert':\n            model_fn = DistilBertForPromptFinetuning\n        else:\n            raise NotImplementedError\n    elif model_args.few_shot_type == 'finetune':\n        model_fn = AutoModelForSequenceClassification\n    else:\n        raise NotImplementedError\n    special_tokens = []\n\n    # Create tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n        additional_special_tokens=special_tokens,\n        cache_dir=model_args.cache_dir, use_fast=False\n    )\n    print(f' | tokenizer: {tokenizer}, size: {len(tokenizer)} \\n\\n\\n')\n\n    # Get our special datasets.\n    if model_args.few_shot_type == \"finetune\":\n        assert data_args.num_sample == 1\n        train_dataset = GlueDataset(data_args, tokenizer, mode=\"train\")\n        eval_dataset = (\n            GlueDataset(data_args, tokenizer, mode=\"dev\")\n            if training_args.do_eval else None\n        )\n        test_dataset = (\n            GlueDataset(data_args, tokenizer, mode=\"test\")\n            if training_args.do_predict or training_args.evaluate_test_split\n            else None\n        )\n\n        if eval_dataset is not None:\n            eval_dataset.num_sample = 1\n        if test_dataset is not None:\n            test_dataset.num_sample = 1\n    else:\n        use_demo = \"demo\" in model_args.few_shot_type\n        train_dataset = FewShotDataset(data_args, tokenizer=tokenizer, mode=\"train\", use_demo=use_demo)\n        eval_dataset = (\n            FewShotDataset(data_args, tokenizer=tokenizer, mode=\"dev\", use_demo=use_demo)\n            if training_args.do_eval else None\n        )\n        test_dataset = (\n            FewShotDataset(data_args, tokenizer=tokenizer, mode=\"test\", use_demo=use_demo)\n            if training_args.do_predict or training_args.evaluate_test_split else None\n        )\n    print(f\" *** dataset sizes: \")\n    for _tag, _ds in zip((\"train\", \"valid\", \"test\"), (train_dataset, eval_dataset, test_dataset)):\n        if _ds is not None:\n            print(f'{_tag}: {len(_ds)}')\n    print(f\" ***\")\n\n    set_seed(training_args.seed)\n\n    model = model_fn.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n    )\n    print(\" | model type: \")\n    print(type(model))\n\n    if model_args.attention_only:\n        model.requires_grad_(False)\n        for name, param in model.named_parameters():\n            if 'query' in name or 'value' in name or 'classifier' in name or 'lm_head' in name:\n                param.requires_grad_(True)\n        if model_args.static_lm_head and hasattr(model, 'lm_head'):\n            model.lm_head.requires_grad_(False)\n    else:\n        model.requires_grad_(True)\n        if model_args.static_embedding:\n            model.get_input_embeddings().requires_grad_(False)\n\n    if model_args.randomly_initialize:\n        # Only reinit the params which require gradients.\n        model_old = copy.deepcopy(model)  # Copy pretrained model.\n        model.init_weights()\n\n        params = tuple(model.parameters())\n        params_old = tuple(model_old.parameters())\n        for param, param_old in utils.zip_(params, params_old):\n            if not param.requires_grad:\n                param.data.copy_(param_old.data)\n\n        del model_old\n        gc.collect()\n        torch.cuda.empty_cache()\n    print(f\"attention_only: {model_args.attention_only} | randomly_initialize: {model_args.randomly_initialize}\")\n\n    named_params = [(name, param) for name, param in model.named_parameters() if param.requires_grad]\n    print('Params to update: ')\n    print(json.dumps([name for name, param in named_params], indent=4))\n    num_differentiable_params = utils.count_parameters(model, only_differentiable=True)\n    print(f'Number of differentiable params: {num_differentiable_params / 1e6:.3f} million')\n\n    # For BERT, increase the size of the segment (token type) embeddings\n    if config.model_type == 'bert':\n        model.resize_token_embeddings(len(tokenizer))\n        resize_token_type_embeddings(model, new_num_types=10, random_segment=model_args.random_segment)\n\n    # Pass dataset and argument information to the model\n    if data_args.prompt:\n        model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()\n        print(f\" | Classification label_word_list: {model.label_word_list}\")\n        print(f\"   converted words: {tokenizer.convert_ids_to_tokens(model.label_word_list)}\")\n    if output_modes_mapping[data_args.task_name] == 'regression':\n        # lower / upper bounds\n        model.lb, model.ub = bound_mapping[data_args.task_name]\n        print(f\" | Regression lb: {model.lb}, ub: {model.ub}\")\n    model.model_args = model_args\n    model.data_args = data_args\n    model.tokenizer = tokenizer\n\n    # Build metric\n    def build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:\n        def compute_metrics_fn(p: EvalPrediction):\n            # Note: the eval dataloader is sequential, so the examples are in order.\n            # We average the logits over each sample for using demonstrations.\n            predictions = p.predictions\n            num_logits = predictions.shape[-1]\n            logits = predictions.reshape([eval_dataset.num_sample, -1, num_logits])\n            logits = logits.mean(axis=0)\n\n            if num_logits == 1:\n                preds = np.squeeze(logits)\n            else:\n                preds = np.argmax(logits, axis=1)\n\n            # Just for sanity, assert label ids are the same.\n            label_ids = p.label_ids.reshape([eval_dataset.num_sample, -1])\n            label_ids_avg = label_ids.mean(axis=0)\n            label_ids_avg = label_ids_avg.astype(p.label_ids.dtype)\n            assert (label_ids_avg - label_ids[0]).mean() < 1e-2\n            label_ids = label_ids[0]\n\n            return compute_metrics_mapping[task_name](task_name, preds, label_ids)\n\n        return compute_metrics_fn\n\n    # Initialize our Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        model_args=model_args,\n        privacy_args=privacy_args,\n        auxiliary_args=auxiliary_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=build_compute_metrics_fn(data_args.task_name)\n    )\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)],\n         'weight_decay': training_args.weight_decay},\n        {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = trainer.optimizer = torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=training_args.learning_rate,\n        betas=(training_args.adam_beta1, training_args.adam_beta2),\n        eps=training_args.adam_epsilon,\n    )\n    if training_args.lr_decay:  # Default linear decay.\n        training_setup = trainer.get_training_setup()\n        t_total = training_setup[\"t_total\"]\n        # `trainer.optimizer` is not None here, so no optimizer is created.\n        trainer.create_optimizer_and_scheduler(num_training_steps=t_total)\n    else:\n        trainer.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(trainer.optimizer, lambda _: 1.)\n\n    if privacy_args.non_private:\n        privacy_args.noise_multiplier = 0.\n        privacy_args.per_example_max_grad_norm = None\n    else:\n        total_train_batch_size = training_args.gradient_accumulation_steps * training_args.per_device_train_batch_size\n        privacy_engine = PrivacyEngine(\n            module=model,\n            batch_size=total_train_batch_size,\n            sample_size=len(train_dataset),\n            epochs=training_args.num_train_epochs,\n            max_grad_norm=privacy_args.per_example_max_grad_norm,\n            noise_multiplier=privacy_args.noise_multiplier,\n            target_epsilon=privacy_args.target_epsilon,\n            target_delta=privacy_args.target_delta,\n            accounting_mode=privacy_args.accounting_mode,\n            clipping_mode=privacy_args.clipping_mode,\n            skip_checks=True,\n        )\n        # Originally, it could have been null.\n        privacy_args.noise_multiplier = privacy_engine.noise_multiplier\n        privacy_args.target_delta = privacy_engine.target_delta\n\n        print('privacy_args: ')\n        print(json.dumps(privacy_args.__dict__, indent=4))\n        privacy_engine.attach(optimizer)\n\n    # Training\n    if training_args.do_train:\n        # Write argparse.\n        utils.jdump(\n            {**training_args.__dict__, **model_args.__dict__, **data_args.__dict__, **privacy_args.__dict__},\n            os.path.join(training_args.output_dir, 'argparse.json'),\n            default=lambda x: str(x),\n        )\n        print(data_args.mapping)\n        print(data_args.template)\n\n        # Don't reload.\n        trainer.train(model_path=None)\n        # Use the early stop, so do not save the model in the end (unless specify save_at_last)\n        if training_args.save_at_last:\n            trainer.save_model(training_args.output_dir)\n\n        if trainer.is_world_process_zero():\n            tokenizer.save_pretrained(training_args.output_dir)\n            torch.save(model_args, os.path.join(training_args.output_dir, \"model_args.bin\"))\n            torch.save(data_args, os.path.join(training_args.output_dir, \"data_args.bin\"))\n\n    if training_args.do_eval or training_args.do_predict:\n        # Reload the best checkpoint (for eval or predict).\n        logger.info(\"*** Loading best checkpoint ***\")\n        model = model_fn.from_pretrained(training_args.output_dir)\n        model = model.to(training_args.device)\n        trainer.model = model\n        if data_args.prompt:\n            model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()\n        if output_modes_mapping[data_args.task_name] == 'regression':\n            # lower / upper bounds\n            model.lb, model.ub = bound_mapping[data_args.task_name]\n        model.model_args = model_args\n        model.data_args = data_args\n        model.tokenizer = tokenizer\n\n    # Evaluation\n    final_result = {'time': str(datetime.today())}\n\n    eval_results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Validate ***\")\n\n        eval_datasets = []\n        eval_task_names = []\n        eval_splits = []\n        for split, dataset in zip(('dev', 'test'), (eval_dataset, test_dataset)):\n            if split == \"test\" and not training_args.evaluate_test_split:\n                continue\n\n            eval_datasets.append(dataset)\n            eval_task_names.append(data_args.task_name)\n            eval_splits.append(split)\n\n            # --- lxuechen: This block depends on `split`.\n            if data_args.task_name == \"mnli\":\n                mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n                eval_task_names.append(mnli_mm_data_args.task_name)\n                eval_splits.append(split)\n                if model_args.few_shot_type == \"finetune\":\n                    mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode=split)\n                    mnli_mm_dataset.num_sample = 1\n                    eval_datasets.append(mnli_mm_dataset)\n                else:\n                    eval_datasets.append(\n                        FewShotDataset(\n                            mnli_mm_data_args, tokenizer=tokenizer, mode=split,\n                            use_demo=('demo' in model_args.few_shot_type),\n                        )\n                    )\n            # ---\n\n        results_json = collections.defaultdict(dict)\n        for eval_dataset, eval_task_name, eval_split in zip(eval_datasets, eval_task_names, eval_splits):\n            trainer.compute_metrics = build_compute_metrics_fn(eval_dataset.args.task_name)\n            output = trainer.evaluate(eval_dataset=eval_dataset)\n            eval_result = output.metrics\n\n            # --- lxuechen: My evaluation procedure.\n            if eval_result is not None:\n                if not privacy_args.non_private:\n                    privacy_spent = privacy_engine.get_privacy_spent(accounting_mode=\"all\", lenient=True)\n                    to_record_dict = {**eval_result, **privacy_spent}\n                else:\n                    to_record_dict = eval_result\n\n                if training_args.evaluate_test_split:\n                    results_json[eval_split][eval_task_name] = to_record_dict\n                else:\n                    results_json[eval_task_name] = to_record_dict\n            # ---\n\n        output_path = os.path.join(training_args.output_dir, \"final_results.json\")\n        utils.jdump(results_json, output_path)\n\n    test_results = {}\n    if training_args.do_predict:\n        logging.info(\"*** Test ***\")\n\n        test_datasets = [test_dataset]\n        if data_args.task_name == \"mnli\":\n            mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n            if model_args.few_shot_type == \"finetune\":\n                mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode=\"test\")\n                mnli_mm_dataset.num_sample = 1\n                test_datasets.append(mnli_mm_dataset)\n            else:\n                test_datasets.append(\n                    FewShotDataset(\n                        mnli_mm_data_args,\n                        tokenizer=tokenizer, mode=\"test\", use_demo=('demo' in model_args.few_shot_type)\n                    )\n                )\n\n        for test_dataset in test_datasets:\n            trainer.compute_metrics = build_compute_metrics_fn(test_dataset.args.task_name)\n            output = trainer.evaluate(eval_dataset=test_dataset)\n            test_result = output.metrics\n\n            output_test_file = os.path.join(\n                training_args.output_dir, f\"test_results_{test_dataset.args.task_name}.txt\"\n            )\n            if trainer.is_world_process_zero():\n                with open(output_test_file, \"w\") as writer:\n                    logger.info(\"***** Test results {} *****\".format(test_dataset.args.task_name))\n                    for key, value in test_result.items():\n                        logger.info(\"  %s = %s\", key, value)\n                        writer.write(\"%s = %s\\n\" % (key, value))\n                        final_result[test_dataset.args.task_name + '_test_' + key] = value\n\n                if training_args.save_logit:\n                    predictions = output.predictions\n                    num_logits = predictions.shape[-1]\n                    logits = predictions.reshape([test_dataset.num_sample, -1, num_logits]).mean(axis=0)\n                    np.save(os.path.join(training_args.save_logit_dir,\n                                         \"{}-{}-{}.npy\".format(test_dataset.task_name, training_args.model_id,\n                                                               training_args.array_id)), logits)\n\n            test_results.update(test_result)\n\n    with FileLock('log.lock'):\n        with open('log', 'a') as f:\n            final_result.update(vars(model_args))\n            final_result.update(vars(training_args))\n            final_result.update(vars(data_args))\n            if 'evaluation_strategy' in final_result:\n                final_result.pop('evaluation_strategy')\n            f.write(str(final_result) + '\\n')\n\n    return eval_results\n\n\nif __name__ == \"__main__\":\n    main()\n\n",
    "experimental_info": "- **Hyperparameter Optimization (HPO) for learning rate (η) and number of steps (T) via `r = η × T`**: The `PrivacyEngine` is designed to calculate the `noise_multiplier` (sigma) given a target epsilon and number of steps/epochs (`steps = math.ceil(epochs / sample_rate)`), which serves as the building block for the proposed linear scaling rule. The method describes an HPO strategy involving fitting a linear approximation to empirical points (ε, r(ε)), which is a high-level conceptual strategy not directly implemented as a single function within the provided code, but facilitated by the `PrivacyEngine`'s ability to calculate `sigma` based on target `epsilon`.\n- **Full batch gradient computation (DP-GD)**: Achieved through gradient accumulation. The total effective batch size for one update (`batch_size`) is determined by a base batch size scaled by a factor related to the task (`batch_size = int(base_batch_size * factor)`). `gradient_accumulation_steps` are calculated as `batch_size // per_device_train_batch_size`.\n- **Unit clipping (C=1)**: The maximum per-example gradient 2-norm, `per_example_max_grad_norm`, is set to `0.1` in `examples/classification/run_wrapper.py`, not 1.\n- **Zero initialization of linear classifiers**: The code typically loads pre-trained models. For language models, the `lm_head` is explicitly initialized by *cloning* the embedding weights, not zeroing them. For classification tasks, the new classifier head might be randomly initialized by `from_pretrained`.\n- **SGD with momentum (ρ=0.9)**: The optimization is performed using `torch.optim.AdamW`. The `adam_beta1` parameter, which controls the momentum, defaults to `0.9` in `transformers.TrainingArguments`, aligning with the specified momentum.\n- **Privacy Loss Variable (PLV) accounting**: Implemented using `accounting_mode=\"glw\"` (Generalized L-W, also known as f-DP accounting), configurable via `PrivacyArguments`. A `target_epsilon` of `8` and a `target_delta` of `sample_size ** -1.1` (default if not explicitly set) are used.\n- **Noise Multiplier**: Automatically computed via a binary search in `PrivacyEngine`'s `compute_sigma` method if a `target_epsilon` is provided.\n- **Clipping Mode**: The `clipping_mode` can be configured as \"ghost\" or \"default\". \"ghost\" clipping is utilized for improved efficiency.\n- **Learning Rate Schedule**: A linear learning rate decay (`lr_decay=\"yes\"`) is applied by default."
}
