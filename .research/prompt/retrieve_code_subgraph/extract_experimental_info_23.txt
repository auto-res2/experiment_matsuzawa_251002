
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The methodology comprises theoretical analysis, simulation studies, and benchmark experiments. The theoretical analysis investigates the effect of reshuffling on the empirical loss surface by deriving the limiting distribution of the validation loss, modeling it as a zero-mean Gaussian process and providing a bound on the expected regret. The simulation study uses a univariate quadratic loss surface with a squared exponential kernel for the noise process to systematically vary curvature, noise correlation, and reshuffling extent. Benchmark experiments utilize random search and state-of-the-art Bayesian Optimization (BO) variants (HEBO, SMAC3) to evaluate different resampling strategies (holdout, M-fold CV, M-fold holdout, 5x 5-fold CV) with and without reshuffling.

# Repository Content
File Path: analyze/__init__.py
Content:

File Path: analyze/post_selector.py
Content:
from abc import ABC, abstractmethod

import numpy as np


class PostSelector(ABC):
    """
    Abstract class for a post selector.
    """

    def __init__(
        self,
        id: str,
        result_analyzer: "ResultAnalyzer",
        supported_valid_types: list = [
            "holdout",
            "cv",
            "cv_repeated",
            "repeatedholdout",
        ],
        supported_reshufflings: list = [True, False],
    ):
        self.id = id
        self.result_analyzer = result_analyzer
        self.supported_valid_types = supported_valid_types
        self.supported_reshufflings = supported_reshufflings

    def select(self, iteration: int, metric: str, **kwargs) -> int:
        """
        Function to select a configuration.
        Performs some checks and then calls the _select function.
        """
        if self.result_analyzer.valid_type not in self.supported_valid_types:
            raise ValueError(
                f"Valid type {self.result_analyzer.valid_type} not supported by post selector {self.id}"
            )
        if self.result_analyzer.reshuffle not in self.supported_reshufflings:
            raise ValueError(
                f"Reshuffling {self.result_analyzer.reshuffle} not supported by post selector {self.id}"
            )

        return self._select(iteration=iteration, metric=metric, **kwargs)

    @abstractmethod
    def _select(self, iteration: int, metric: str, **kwargs) -> int:
        """
        Function to select a configuration.
        """
        pass

    def reset(self):
        """
        Reset the post selector.
        Calls the _reset function.
        """
        self._reset()

    @abstractmethod
    def _reset(self):
        """
        Reset the post selector.
        """
        pass


class PostSelectorNaive(PostSelector):
    """
    Selects the configuration with the lowest validation score.
    """

    def __init__(self, result_analyzer: "ResultAnalyzer"):
        super().__init__(id="naive", result_analyzer=result_analyzer)

    def _select(self, iteration: int, metric: str, **kwargs) -> int:
        """
        Selects the configuration with the lowest validation score.
        """
        valid = self.result_analyzer.results_raw[metric]["valid"].values[
            : iteration + 1
        ]
        selected = np.argmin(valid)
        return selected

    def _reset(self):
        """
        Reset the post selector.
        """
        pass

File Path: analyze/result_analyzer.py
Content:
import argparse
import concurrent.futures
import json
import os
import warnings
from typing import Dict, List, Optional

import numdifftools
import numpy as np
import optuna
import pandas as pd
import scipy.optimize as opt
import torch
import tqdm
from hebo.models.model_factory import get_model
from scipy.stats import kendalltau

from analyze.post_selector import PostSelector, PostSelectorNaive
from analyze.utils import compute_overtuning
from reshufflebench.algorithms import CatBoost, FunnelMLP, LogReg, XGBoost
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    NumpyArrayEncoder,
    check_y_predict_proba,
    int_or_none,
    load_list_of_1d_arrays,
    load_list_of_list_of_1d_arrays,
    load_list_of_list_of_pd_arrays,
    load_list_of_pd_arrays,
    load_single_array,
    str2bool,
)


class ResultAnalyzer(object):
    """
    Class to analyze the results of a study.
    """

    def __init__(
        self,
        results_path: str,
        seed: int,
        n_repeats: Optional[int] = None,
        check_files: bool = True,
    ):
        self.results_path = os.path.abspath(results_path)
        self.seed = seed
        self.file_name = f"study_seed_{self.seed}"
        with open(
            os.path.join(self.results_path, f"{self.file_name}_params.json"), "r"
        ) as f:
            self.params = json.load(f)
        if self.params["valid_type"] in ["cv", "repeatedholdout"]:
            self.params.update(
                {
                    "metric_to_cv_metric": {
                        v: k for k, v in self.params["cv_metric_to_metric"].items()
                    }
                }
            )

        if self.params["valid_type"] == "holdout" and n_repeats is not None:
            raise ValueError("n_repeats must be None for validation type holdout")

        if self.params["valid_type"] in ["cv", "repeatedholdout"]:
            if n_repeats is None:
                raise ValueError(
                    "n_repeats must not be None for validation type cv or repeatedholdout"
                )
            if n_repeats < 1 or n_repeats > self.params["n_repeats"]:
                raise ValueError(
                    f"n_repeats must be between 1 and {self.params['n_repeats']}"
                )

        self.n_repeats = n_repeats
        self.check_files = check_files

        resampling = (
            f"{self.params['valid_type']}_{str(self.params['valid_frac']).replace('.', '')}_{self.params['reshuffle']}"
            if self.params["valid_type"] == "holdout"
            else (
                f"{self.params['valid_type']}_{self.params['n_splits']}_{self.n_repeats}_{self.params['reshuffle']}"
                if self.params["valid_type"] == "cv"
                else f"{self.params['valid_type']}_{str(self.params['valid_frac']).replace('.', '')}_{self.n_repeats}_{self.params['reshuffle']}"
            )
        )
        self.params.update({"resampling": resampling})

        self.valid_type = (
            "holdout"
            if self.params["valid_type"] == "holdout"
            else (
                "repeatedholdout"
                if self.params["valid_type"] == "repeatedholdout"
                else (
                    "cv"
                    if self.params["valid_type"] == "cv" and self.n_repeats == 1
                    else "cv_repeated"
                )
            )
        )
        self.reshuffle = self.params["reshuffle"]

        self.adjust_results_for_n_repeats_needed = (
            self.valid_type in ["cv", "cv_repeated", "repeatedholdout"]
            and self.n_repeats != self.params["n_repeats"]
        )

        files = os.listdir(self.results_path)
        self.files = [file for file in files if str(self.seed) in file]
        if self.valid_type in ["cv", "cv_repeated", "repeatedholdout"]:
            self.expected_files_raw = [
                # "y_train_hist",
                "y_valid_hist",
                "y_add_valid_hist",
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_add_valid_proba_hist",
                "y_pred_test_proba_hist",
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
                "y_valid_train",
                "y_add_valid_use",
                "y_test",
                # "cv_splits_hist_train",
                "cv_splits_hist_valid",
                "cv_splits_add_valid_hist_valid",
            ]
            self.expected_files = [
                f"{self.file_name}_{file}.parquet" for file in self.expected_files_raw
            ]
        else:
            self.expected_files_raw = [
                # "y_train_hist",
                "y_valid_hist",
                "y_add_valid_hist",
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_add_valid_proba_hist",
                "y_pred_test_proba_hist",
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
                # "y_valid_train",
                "y_add_valid_use",
                "y_test",
            ]
            self.expected_files = [
                f"{self.file_name}_{file}.parquet" for file in self.expected_files_raw
            ]
        self.expected_files.append(f"{self.file_name}.db")
        self.expected_files.append(f"{self.file_name}_params.json")
        if self.check_files:
            if set(self.files) != set(self.expected_files):
                raise ValueError(f"Expected files does not match found files")

        self.study = None
        self.results = None

        self.y_train_hist = None
        self.y_valid_hist = None
        self.y_add_valid_hist = None
        self.y_pred_train_proba_hist = None
        self.y_pred_valid_proba_hist = None
        self.y_pred_add_valid_proba_hist = None
        self.y_pred_test_proba_hist = None
        self.y_pred_valid_train_proba_hist = None
        self.y_pred_test_proba_retrained_hist = None
        self.y_valid_train = None
        self.y_test = None

        self.cv_splits_hist_train = None
        self.cv_splits_hist_valid = None
        self.cv_splits_add_valid_hist_valid = None

        self.results_raw = {}
        self.results_post_selection = {}

        self.kendalls_tau_valid_test = {}
        self.curvature = {}
        self.test_type_internal = None

    @property
    def test_type(self):
        """
        Return the test type based on the internal test type.
        """
        if self.test_type_internal is not None:
            if self.test_type_internal == "test_retrained":
                return "test_retrained"
            elif self.test_type_internal == "test":
                if self.valid_type == "holdout":
                    return "test"
                else:
                    return "test_ensemble"
        else:
            raise ValueError("test_type_internal is None")

    def adjust_results_for_n_repeats(self) -> None:
        """
        Adjust the results for the number of repeats in the case that the number of repeats is smaller than the number of repeats that was used for the study when using cv or repeatedholdout.
        This allows for post hoc simulation of n-times repeated cv or holdout with n ranging from 1 to the number of repeats that was used for the study.
        """
        if self.n_repeats == self.params["n_repeats"]:
            pass
        else:
            # subset to the indices that are needed
            if self.valid_type in ["cv", "cv_repeated"]:
                all_indices = list(
                    range(self.params["n_splits"] * self.params["n_repeats"])
                )
                indices = all_indices[: self.params["n_splits"] * self.n_repeats]
            elif self.valid_type == "repeatedholdout":
                all_indices = list(range(self.params["n_repeats"]))
                indices = all_indices[: self.n_repeats]
            field_to_subset = [
                "y_valid_hist",
                "y_add_valid_hist",
                "y_pred_valid_proba_hist",
                "y_pred_add_valid_proba_hist",
                "y_pred_test_proba_hist",
                "cv_splits_hist_valid",
                "cv_splits_add_valid_hist_valid",
            ]
            for field in field_to_subset:
                subset = []
                for i in range(len(getattr(self, field))):
                    subset.append([getattr(self, field)[i][j] for j in indices])
                setattr(self, field, subset)

            # correct results for each metric based on the relevant subset
            # e.g. user_attrs_accuracies_add_valid, user_attrs_accuracies_test, user_attrs_accuracies_train, user_attrs_accuracies_valid,
            #      user_attrs_accuracy_add_valid, user_attrs_accuracy_test, user_attrs_accuracy_test_ensemble, user_attrs_accuracy_train, user_attrs_accuracy_valid
            for metric in self.params["metrics"]:
                cv_metric = self.params["metric_to_cv_metric"][metric]
                columns_cv_metric = [
                    f"user_attrs_{cv_metric}_add_valid",
                    f"user_attrs_{cv_metric}_test",
                    f"user_attrs_{cv_metric}_train",
                    f"user_attrs_{cv_metric}_valid",
                ]
                for column in columns_cv_metric:
                    self.results[column] = (
                        self.results[column]
                        .apply(eval)
                        .apply(lambda x: [x[j] for j in indices])
                        .apply(json.dumps, cls=NumpyArrayEncoder)
                    )

                columns_metric = [
                    f"user_attrs_{metric}_add_valid",
                    f"user_attrs_{metric}_test",
                    f"user_attrs_{metric}_train",
                    f"user_attrs_{metric}_valid",
                ]

                for i, column in enumerate(columns_metric):
                    cv_values = self.results[columns_cv_metric[i]].apply(eval)
                    if not all(cv_values.apply(len) == len(indices)):
                        raise ValueError(f"Not all cv values have the expected length")
                    self.results[column] = cv_values.apply(np.mean)

                test_ensemble_column = f"user_attrs_{metric}_test_ensemble"
                test_ensemble_values = []
                for i in range(len(self.results)):
                    if len(self.y_pred_test_proba_hist[i]) != len(indices):
                        raise ValueError(f"y_pred_test_proba_hist has the wrong length")
                    predictions_proba_test_ensemble = np.mean(
                        self.y_pred_test_proba_hist[i], axis=0
                    )
                    row_sums = predictions_proba_test_ensemble.sum(
                        axis=1, keepdims=True
                    )
                    predictions_proba_test_ensemble = (
                        predictions_proba_test_ensemble / row_sums
                    )
                    check_y_predict_proba(predictions_proba_test_ensemble)
                    predictions_test_ensemble = np.argmax(
                        predictions_proba_test_ensemble, axis=1
                    )
                    test_ensemble_values.append(
                        compute_metric(
                            y_true=self.y_test,
                            y_pred=predictions_test_ensemble,
                            y_pred_proba=predictions_proba_test_ensemble,
                            metric=metric,
                            labels=self.params["labels"],
                            multiclass=self.params["multiclass"],
                        )
                    )
                self.results[test_ensemble_column] = test_ensemble_values

    def load(self, additional_file_types: str = "none") -> None:
        """
        Load the results.
        If additional_file_types is "none", load no additional files.
        If additional_file_types is "valid", load only additional files that are needed for validation.
        If additional_file_types is "all", load all additional files.
        Note that if self.adjust_results_for_n_repeats_needed is True, additional_file_types must be "all" and will be set to "all" automatically.
        If self.adjust_results_for_n_repeats_needed is True, the results will be adjusted for the number of repeats, see self.adjust_results_for_n_repeats.
        """
        if additional_file_types not in ["none", "valid", "all"]:
            raise ValueError(f"Unknown additional_file_types: {additional_file_types}")

        if self.adjust_results_for_n_repeats_needed:
            additional_file_types = "all"

        self.study = optuna.study.load_study(
            study_name=self.params["study_name"], storage=self.params["storage"]
        )
        self.results = self.study.trials_dataframe()
        if (
            len(self.results[self.results["state"] == "COMPLETE"])
            != self.params["n_trials"]
        ):
            raise ValueError(
                f"Number of trials does not match expected number of trials"
            )
        if not all(self.results["number"].values == range(self.params["n_trials"])):
            raise ValueError(f"Trial numbers are not in order")

        if additional_file_types in ["all", "valid"]:
            files = self.expected_files_raw
            if additional_file_types == "valid":
                files = [
                    file
                    for file in files
                    if file
                    in [
                        "y_valid_hist",
                        "y_pred_valid_proba_hist",
                        "cv_splits_hist_valid",
                    ]
                ]
            for file in files:
                try:
                    if self.valid_type == "holdout":
                        if file in [
                            "y_train_hist",
                            "y_valid_hist",
                            "y_add_valid_hist",
                        ]:
                            setattr(
                                self,
                                file,
                                load_list_of_1d_arrays(
                                    os.path.join(
                                        self.results_path,
                                        f"{self.file_name}_{file}.parquet",
                                    )
                                ),
                            )
                        elif file in ["y_valid_train", "y_add_valid_use", "y_test"]:
                            setattr(
                                self,
                                file,
                                load_single_array(
                                    os.path.join(
                                        self.results_path,
                                        f"{self.file_name}_{file}.parquet",
                                    )
                                ),
                            )
                        else:
                            setattr(
                                self,
                                file,
                                load_list_of_pd_arrays(
                                    os.path.join(
                                        self.results_path,
                                        f"{self.file_name}_{file}.parquet",
                                    )
                                ),
                            )
                    else:
                        if file in [
                            "y_train_hist",
                            "y_valid_hist",
                            "y_add_valid_hist",
                            "cv_splits_hist_train",
                            "cv_splits_hist_valid",
                            "cv_splits_add_valid_hist_valid",
                        ]:
                            setattr(
                                self,
                                file,
                                load_list_of_list_of_1d_arrays(
                                    os.path.join(
                                        self.results_path,
                                        f"{self.file_name}_{file}.parquet",
                                    )
                                ),
                            )
                        elif file in ["y_valid_train", "y_add_valid_use", "y_test"]:
                            setattr(
                                self,
                                file,
                                load_single_array(
                                    os.path.join(
                                        self.results_path,
                                        f"{self.file_name}_{file}.parquet",
                                    )
                                ),
                            )
                        elif file in [
                            "y_pred_valid_train_proba_hist",
                            "y_pred_test_proba_retrained_hist",
                        ]:
                            setattr(
                                self,
                                file,
                                load_list_of_pd_arrays(
                                    os.path.join(
                                        self.results_path,
                                        f"{self.file_name}_{file}.parquet",
                                    )
                                ),
                            )
                        else:
                            setattr(
                                self,
                                file,
                                load_list_of_list_of_pd_arrays(
                                    os.path.join(
                                        self.results_path,
                                        f"{self.file_name}_{file}.parquet",
                                    )
                                ),
                            )
                except FileNotFoundError:
                    warnings.warn(f"File {file} not found")

        if self.adjust_results_for_n_repeats_needed:
            self.adjust_results_for_n_repeats()

    def create_results_table_raw(self) -> None:
        """
        Create a results table with the raw results.
        """
        for metric in self.params["metrics"]:
            results = self.results.copy()
            params = [
                column for column in results.columns if column.startswith("params_")
            ]
            columns = [
                f"user_attrs_{metric}_train",
                f"user_attrs_{metric}_valid",
                f"user_attrs_{metric}_test",
                f"user_attrs_{metric}_valid_train",
                f"user_attrs_{metric}_test_retrained",
            ]
            if self.valid_type in ["cv", "cv_repeated", "repeatedholdout"]:
                columns.append(f"user_attrs_{metric}_test_ensemble")
                columns.append(
                    f"user_attrs_{self.params['metric_to_cv_metric'][metric]}_valid"
                )
            if OPTIMIZER == "hebo":
                columns.append("user_attrs_hebo_fallback_triggered")
            elif OPTIMIZER == "smac":
                columns.append("user_attrs_smac_fallback_triggered")
            results = results[columns]
            if self.valid_type == "holdout":
                results.rename(
                    columns={
                        f"user_attrs_{metric}_train": "train",
                        f"user_attrs_{metric}_valid": "valid",
                        f"user_attrs_{metric}_test": "test",
                        f"user_attrs_{metric}_valid_train": "valid_train",
                        f"user_attrs_{metric}_test_retrained": "test_retrained",
                    },
                    inplace=True,
                )
            else:
                results.rename(
                    columns={
                        f"user_attrs_{metric}_train": "train",
                        f"user_attrs_{metric}_valid": "valid",
                        f"user_attrs_{metric}_test": "test",
                        f"user_attrs_{metric}_valid_train": "valid_train",
                        f"user_attrs_{metric}_test_retrained": "test_retrained",
                        f"user_attrs_{metric}_test_ensemble": "test_ensemble",
                        f"user_attrs_{self.params['metric_to_cv_metric'][metric]}_valid": "cv_valid",
                    },
                    inplace=True,
                )
            if OPTIMIZER == "hebo":
                results.rename(
                    columns={
                        "user_attrs_hebo_fallback_triggered": "hebo_fallback_triggered"
                    },
                    inplace=True,
                )
            elif OPTIMIZER == "smac":
                results.rename(
                    columns={
                        "user_attrs_smac_fallback_triggered": "smac_fallback_triggered"
                    },
                    inplace=True,
                )
            if self.params["metrics_direction"][metric] == "maximize":
                for column in list(
                    set(results.columns)
                    - {
                        "hebo_fallback_triggered",
                        "early_stopping_triggered",
                        "smac_fallback_triggered",
                    }
                ):
                    if column == "cv_valid":
                        results[column] = (
                            results[column].apply(eval).apply(lambda x: [-y for y in x])
                        )
                    else:
                        results[column] = results[column].apply(lambda x: -x)
            else:
                if "cv_valid" in results.columns:
                    results["cv_valid"] = results["cv_valid"].apply(eval)
            results["iteration"] = range(1, len(results) + 1)
            results["seed"] = self.seed
            results["classifier"] = self.params["classifier"]
            results["data_id"] = self.params["data_id"]
            results["train_valid_size"] = self.params["train_valid_size"]
            results["resampling"] = self.params["resampling"]
            results["metric"] = metric
            # add params
            for param in params:
                results[param] = self.results[param]
            self.results_raw[metric] = results

    def calculate_curvature(self) -> None:
        """
        Fit a GP on observed values and calculate some curvature metrics at the empirical optimum.
        """
        for metric in self.params["metrics"]:
            dat = self.results_raw[metric]
            relevant_columns_valid = [
                column for column in dat.columns if "params_" in column
            ] + ["valid"]
            dat_valid = dat.loc[:, relevant_columns_valid]
            dat_valid.rename(columns={"valid": "y"}, inplace=True)
            X = dat_valid.drop(columns=["y"])
            y = dat_valid["y"].values.reshape(-1, 1)
            X.rename(
                columns={column: column.replace("params_", "") for column in X.columns},
                inplace=True,
            )
            if self.params["classifier"] == "logreg":
                classifier = LogReg(self.params["seed"])
                space = classifier.get_hebo_search_space()
                bounds = [
                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras
                ]
            elif self.params["classifier"] == "funnel_mlp":
                classifier = FunnelMLP(self.params["seed"])
                n_train_samples = int(
                    0.8 * self.params["train_valid_size"]
                )  # Holdout 80/20 or 5-fold CV variants
                space = classifier.get_hebo_search_space(
                    n_train_samples=n_train_samples
                )
                bounds = [
                    (space.paras[name].lb, space.paras[name].ub)
                    for name in space.paras
                    if not space.paras[name].is_categorical
                ]
            elif self.params["classifier"] == "xgboost":
                classifier = XGBoost(self.params["seed"])
                space = classifier.get_hebo_search_space()
                bounds = [
                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras
                ]
            elif self.params["classifier"] == "catboost":
                classifier = CatBoost(self.params["seed"])
                space = classifier.get_hebo_search_space()
                bounds = [
                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras
                ]
            # https://github.com/huawei-noah/HEBO/blob/c1c7d72b996a7d11eb2b86e25f21a174b0cc7bd4/HEBO/hebo/optimizers/hebo.py#L117
            X, Xe = space.transform(X)
            model_config = {
                "lr": 0.01,
                "num_epochs": 100,
                "verbose": False,
                "noise_lb": 8e-4,
                "pred_likeli": False,
            }
            if space.num_categorical > 0:
                model_config["num_uniqs"] = [
                    len(space.paras[name].categories) for name in space.enum_names
                ]
            # try:
            #    if y.min() <= 0:
            #        y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))
            #    else:
            #        y = torch.FloatTensor(power_transform(y / y.std(), method='box-cox'))
            #        if y.std() < 0.5:
            #            y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))
            #    if y.std() < 0.5:
            #        raise RuntimeError('Power transformation failed')
            #    model = get_model("gp", space.num_numeric, space.num_categorical, 1, **model_config)
            #    model.fit(X, Xe, y)
            # except:
            y = torch.FloatTensor(y).clone()
            model = get_model(
                "gp", space.num_numeric, space.num_categorical, 1, **model_config
            )
            model.fit(X, Xe, y)

            empirical_argmin = model.predict(X, Xe)[0].argmin()
            X_argmin = X[empirical_argmin, :].unsqueeze(0)
            Xe_argmin = Xe[empirical_argmin, :].unsqueeze(0)

            def posterior_mean_wrapper(x, model, Xe_argmin):
                x_tensor = torch.FloatTensor(x).unsqueeze(0).requires_grad_(True)
                return model.predict(x_tensor, Xe_argmin)[0][0, 0].detach().numpy()

            x0 = X[empirical_argmin, :].numpy()
            result = opt.minimize(
                posterior_mean_wrapper,
                x0,
                args=(model, Xe_argmin),
                bounds=bounds,
                method="Nelder-Mead",
            )

            x_optimal = result.x
            hessian_function = numdifftools.Hessian(posterior_mean_wrapper)
            hessian_optimal = hessian_function(x_optimal, model, Xe_argmin)

            def make_psd(matrix):
                eigenvalues, eigenvectors = np.linalg.eigh(matrix)
                already_is_psd = np.all(eigenvalues >= 0)
                eigenvalues[eigenvalues < 0] = 0
                return (
                    already_is_psd,
                    eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T,
                )

            already_is_psd, hessian_optimal = make_psd(hessian_optimal)

            det_hessian = np.linalg.det(hessian_optimal)
            trace_hessian = np.trace(hessian_optimal)
            eigenvalues_hessian = np.linalg.eigvals(hessian_optimal)
            smallest_eigenvalue_hessian = np.min(eigenvalues_hessian)
            biggest_eigenvalue_hessian = np.max(eigenvalues_hessian)

            curvature_pd = pd.DataFrame(
                {
                    "det_hessian": [det_hessian],
                    "trace_hessian": [trace_hessian],
                    "smallest_eigenvalue_hessian": [smallest_eigenvalue_hessian],
                    "biggest_eigenvalue_hessian": [biggest_eigenvalue_hessian],
                    "already_is_psd": [already_is_psd],
                    "gp_noise": [
                        model.noise.item()
                    ],  # homoscedastic observation variance (noise) from the GP
                    "seed": [self.seed],
                    "classifier": [self.params["classifier"]],
                    "data_id": [self.params["data_id"]],
                    "train_valid_size": [self.params["train_valid_size"]],
                    "resampling": [self.params["resampling"]],
                    "metric": [metric],
                }
            )
            self.curvature.update({metric: curvature_pd})

    def calculate_kendalls_tau_valid_test(self) -> None:
        """
        Calculate Kendall's Tau between valid and test_type.
        """
        for metric in self.params["metrics"]:
            kendalls_tau = []
            for local in [False, True]:
                # if local, select the top 20% based on valid and calculate Kendall's Tau between valid and test_type
                if local:
                    kendalls_tau_tmp = []
                    # for i in range(len(self.results_raw[metric])):
                    for i in range(9, len(self.results_raw[metric]), 10):
                        results = self.results_raw[metric].copy()
                        results = results.iloc[: i + 1]
                        q20 = results["valid"].quantile(0.2)
                        results = results[results["valid"] <= q20]
                        kendalls_tau_tmp.append(
                            kendalltau(
                                results["valid"].values, results[self.test_type].values
                            )[0]
                        )

                else:
                    kendalls_tau_tmp = []
                    # for i in range(len(self.results_raw[metric])):
                    for i in range(9, len(self.results_raw[metric]), 10):
                        results = self.results_raw[metric].copy()
                        results = results.iloc[: i + 1]
                        kendalls_tau_tmp.append(
                            kendalltau(
                                results["valid"].values, results[self.test_type].values
                            )[0]
                        )
                kendalls_tau.append(kendalls_tau_tmp)
            kendalls_tau_pd = pd.DataFrame(
                {
                    "kendalls_tau": kendalls_tau,
                    "test_type": self.test_type,
                    "local": [False, True],
                    "seed": self.seed,
                    "classifier": self.params["classifier"],
                    "data_id": self.params["data_id"],
                    "train_valid_size": self.params["train_valid_size"],
                    "resampling": self.params["resampling"],
                    "metric": metric,
                }
            )
            kendalls_tau_pd = kendalls_tau_pd.explode("kendalls_tau")
            kendalls_tau_pd["iteration"] = (
                # list(range(1, len(self.results_raw[metric]))) * 2
                list(range(9 + 1, len(self.results_raw[metric]) + 1, 10))
                * 2
            )
            self.kendalls_tau_valid_test.update({metric: kendalls_tau_pd})

    def create_results_table_post_selection(
        self, post_selector: PostSelector, **kwargs
    ) -> None:
        """
        Create a results table with the results after post selection.
        """
        results_post_selection = {"test": {}, "test_retrained": {}}
        for metric in self.params["metrics"]:
            post_selector.reset()
            results = self.results_raw[metric].copy()
            selected = []
            if len(results) != self.params["n_trials"] or len(results) != N_TRIALS:
                raise ValueError(
                    f"Number of trials does not match expected number of trials"
                )
            iterations = list(range(len(results)))
            for i in tqdm.tqdm(iterations):
                selected.append(
                    post_selector.select(iteration=i, metric=metric, **kwargs)
                )
            results = results.iloc[selected]
            results.reset_index(inplace=True, drop=True)
            results["method"] = (
                f"{results['resampling'].values[0]}_post_{post_selector.id}"
            )
            results["orig_iteration"] = results["iteration"]
            results["iteration"] = [x + 1 for x in iterations]

            for test_type in ["test", "test_retrained"]:
                self.test_type_internal = test_type
                results_tmp = results.copy()
                results_tmp["overtuning"] = compute_overtuning(
                    list(results_tmp[self.test_type])
                )
                results_tmp = results_tmp.loc[
                    :,
                    [
                        "iteration",
                        "orig_iteration",
                        "valid",
                        self.test_type,
                        "overtuning",
                        "seed",
                        "classifier",
                        "data_id",
                        "train_valid_size",
                        "resampling",
                        "metric",
                        "method",
                    ],
                ]
                results_tmp.rename(columns={self.test_type: "test"}, inplace=True)
                if test_type == "test":
                    results_post_selection["test"][metric] = results_tmp
                elif test_type == "test_retrained":
                    results_post_selection["test_retrained"][metric] = results_tmp
        self.results_post_selection.update({post_selector.id: results_post_selection})


class ResultAnalyzerSimulateRepeatedHoldout(ResultAnalyzer):
    """
    Class to analyze the results of a study.
    Simulates repeated holdout (Monte Carlo CV) from the results of repeated CV.
    """

    def __init__(
        self, results_path: str, seed: int, n_repeats: int, check_files: bool = True
    ):
        super().__init__(
            results_path=results_path,
            seed=seed,
            n_repeats=n_repeats,
            check_files=check_files,
        )
        self.adjust_results_for_n_repeats_needed = True  # always adjust results for n_repeats due to simulation of repeated holdout

    # Note: could be faster if adjust is not done during loading but separately with the option to reset the results
    def adjust_results_for_n_repeats(self) -> None:
        """
        Simulates Monte Carlo CV from the results of repeated CV.
        """
        indices = [
            0 + i * self.params["n_splits"] for i in range(self.n_repeats)
        ]  # [0, 5, 10, 15, 20] etc.

        field_to_subset = [
            "y_valid_hist",
            "y_add_valid_hist",
            "y_pred_valid_proba_hist",
            "y_pred_add_valid_proba_hist",
            "y_pred_test_proba_hist",
            "cv_splits_hist_valid",
            "cv_splits_add_valid_hist_valid",
        ]
        for field in field_to_subset:
            subset = []
            for i in range(len(getattr(self, field))):
                subset.append([getattr(self, field)[i][j] for j in indices])
            setattr(self, field, subset)

        # correct results for each metric based on the relevant subset
        # e.g. user_attrs_accuracies_add_valid, user_attrs_accuracies_test, user_attrs_accuracies_train, user_attrs_accuracies_valid,
        #      user_attrs_accuracy_add_valid, user_attrs_accuracy_test, user_attrs_accuracy_test_ensemble, user_attrs_accuracy_train, user_attrs_accuracy_valid
        for metric in self.params["metrics"]:
            cv_metric = self.params["metric_to_cv_metric"][metric]
            columns_cv_metric = [
                f"user_attrs_{cv_metric}_add_valid",
                f"user_attrs_{cv_metric}_test",
                f"user_attrs_{cv_metric}_train",
                f"user_attrs_{cv_metric}_valid",
            ]
            for column in columns_cv_metric:
                self.results[column] = (
                    self.results[column]
                    .apply(eval)
                    .apply(lambda x: [x[j] for j in indices])
                    .apply(json.dumps, cls=NumpyArrayEncoder)
                )

            columns_metric = [
                f"user_attrs_{metric}_add_valid",
                f"user_attrs_{metric}_test",
                f"user_attrs_{metric}_train",
                f"user_attrs_{metric}_valid",
            ]

            for i, column in enumerate(columns_metric):
                cv_values = self.results[columns_cv_metric[i]].apply(eval)
                if not all(cv_values.apply(len) == len(indices)):
                    raise ValueError(f"Not all cv values have the expected length")
                self.results[column] = cv_values.apply(np.mean)

            test_ensemble_column = f"user_attrs_{metric}_test_ensemble"
            test_ensemble_values = []
            for i in range(len(self.results)):
                if len(self.y_pred_test_proba_hist[i]) != len(indices):
                    raise ValueError(f"y_pred_test_proba_hist has the wrong length")
                predictions_proba_test_ensemble = np.mean(
                    self.y_pred_test_proba_hist[i], axis=0
                )
                row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)
                predictions_proba_test_ensemble = (
                    predictions_proba_test_ensemble / row_sums
                )
                check_y_predict_proba(predictions_proba_test_ensemble)
                predictions_test_ensemble = np.argmax(
                    predictions_proba_test_ensemble, axis=1
                )
                test_ensemble_values.append(
                    compute_metric(
                        y_true=self.y_test,
                        y_pred=predictions_test_ensemble,
                        y_pred_proba=predictions_proba_test_ensemble,
                        metric=metric,
                        labels=self.params["labels"],
                        multiclass=self.params["multiclass"],
                    )
                )
            self.results[test_ensemble_column] = test_ensemble_values


def analyze_results_basic(
    results_path: str,
    results_subfolder: str,
    seed: int,
    n_repeats: Optional[int] = None,
    check_files: bool = True,
) -> [pd.DataFrame, pd.DataFrame]:
    """
    Analyze the results of a study.
    Create a results table with the raw results and further calculates some estimates of the local curvature in the (validation) optimum and
    Kendall's Tau between valid and test_type for test type "test" and "test_retrained".
    """
    results_raw_tmp = {}
    curvature_tmp = {}
    kendalls_tau_valid_test_tmp = {"test": [], "test_retrained": []}
    analyzer = ResultAnalyzer(
        results_path=os.path.join(results_path, results_subfolder),
        seed=seed,
        n_repeats=n_repeats,
        check_files=check_files,
    )
    analyzer.load(additional_file_types="none")
    analyzer.create_results_table_raw()
    analyzer.calculate_curvature()
    for metric in METRICS:
        results_raw_tmp[metric] = analyzer.results_raw[metric]
        curvature_tmp[metric] = analyzer.curvature[metric]

    for test_type in TEST_TYPES:
        analyzer.test_type_internal = test_type
        analyzer.calculate_kendalls_tau_valid_test()
        for metric in METRICS:
            kendalls_tau_valid_test_tmp[test_type].append(
                analyzer.kendalls_tau_valid_test[metric]
            )
    return (
        results_raw_tmp,
        curvature_tmp,
        kendalls_tau_valid_test_tmp,
    )


def analyze_results_post(
    results_path: str,
    results_subfolder: str,
    seed: int,
    type: str,
    n_repeats: Optional[int] = None,
    check_files: bool = True,
) -> Dict[str, List[pd.DataFrame]]:
    """
    Analyze the results of a study after post selection.
    For test type "test" and "test_retrained", create a results table with the results after post selection.
    """
    results_tmp = {"test": {}, "test_retrained": {}}
    additional_files = (
        "none" if type in ["post_naive"] else ("valid" if type in [] else "all")
    )
    analyzer = ResultAnalyzer(
        results_path=os.path.join(results_path, results_subfolder),
        seed=seed,
        n_repeats=n_repeats,
        check_files=check_files,
    )
    analyzer.load(additional_file_types=additional_files)
    analyzer.create_results_table_raw()
    post_selectors = {
        "post_naive": PostSelectorNaive(analyzer),
    }
    post_selector = post_selectors[type]

    if (
        analyzer.valid_type in post_selector.supported_valid_types
        and analyzer.reshuffle in post_selector.supported_reshufflings
    ):
        analyzer.create_results_table_post_selection(post_selector=post_selector)
        for test_type in TEST_TYPES:
            for metric in METRICS:
                results_tmp[test_type][metric] = analyzer.results_post_selection[
                    post_selector.id
                ][test_type][metric]

    return results_tmp


def analyze_results_post_naive_simulate_repeatedholdout(
    results_path: str,
    results_subfolder: str,
    seed: int,
    type: str,
    n_repeats: int,
    check_files: bool = True,
) -> Dict[str, List[pd.DataFrame]]:
    """
    Analyze the results of a study after post selection.
    Simulate repeated holdout (Monte Carlo CV) from the results of repeated CV.
    For test type "test" and "test_retrained", create a results table with the results after naive post selection.
    """
    if type != "post_naive_simulate_repeatedholdout":
        raise ValueError(f"Unsupported type: {type}")
    results_tmp = {"test": {}, "test_retrained": {}}
    additional_files = "valid"
    analyzer = ResultAnalyzerSimulateRepeatedHoldout(
        results_path=os.path.join(results_path, results_subfolder),
        seed=seed,
        n_repeats=n_repeats,
        check_files=check_files,
    )
    analyzer.load(additional_file_types=additional_files)
    analyzer.create_results_table_raw()
    post_selector = PostSelectorNaive(analyzer)
    post_selector.id += f"_simulate_repeatedholdout_{n_repeats}"

    if (
        analyzer.valid_type in post_selector.supported_valid_types
        and analyzer.reshuffle in post_selector.supported_reshufflings
    ):
        analyzer.create_results_table_post_selection(post_selector=post_selector)
        for test_type in TEST_TYPES:
            for metric in METRICS:
                results_tmp[test_type][metric] = analyzer.results_post_selection[
                    post_selector.id
                ][test_type][metric]

    return results_tmp


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--max_workers", type=int, default=10)
    parser.add_argument(
        "--type",
        type=str,
        default="basic",
        choices=[
            "basic",
            "post_naive",
            "post_naive_simulate_repeatedholdout",
        ],
    )
    parser.add_argument("--results_subfolder", type=str)
    parser.add_argument("--n_repeats", type=int_or_none)
    parser.add_argument("--check_files", type=str2bool, default=True)
    args = parser.parse_args()

    results_path = os.path.abspath("../results")
    if "hebo" in args.results_subfolder or "smac" in args.results_subfolder:
        if "hebo" in args.results_subfolder:
            OPTIMIZER = "hebo"
        elif "smac" in args.results_subfolder:
            OPTIMIZER = "smac"
        METRICS = ["auc"]
        N_TRIALS = 250
    else:
        OPTIMIZER = "random"
        METRICS = ["accuracy", "balanced_accuracy", "logloss", "auc"]
        if "tabpfn" in args.results_subfolder or "default" in args.results_subfolder:
            N_TRIALS = 1
        else:
            N_TRIALS = 500
    SEEDS = list(range(42, 52))
    TEST_TYPES = ["test", "test_retrained"]

    results_subfolders = os.listdir(results_path)
    if args.results_subfolder not in results_subfolders:
        raise ValueError(f"Unknown results_subfolder: {args.results_subfolder}")
    if not os.path.exists(os.path.abspath("../csvs/raw")):
        os.makedirs(os.path.abspath("../csvs/raw"))

    partial_file_name = args.results_subfolder
    if args.n_repeats is not None:
        if "cv" in partial_file_name:
            resampling_abbreviation = [
                string for string in partial_file_name.split("_") if ("cv" in string)
            ][0]
        elif "repeatedholdout" in partial_file_name:
            splits = partial_file_name.split("_")
            resampling_abbreviation = [s for s in splits if "repeatedholdout" in s][0]
            resampling_abbreviation += (
                "_" + splits[splits.index(resampling_abbreviation) + 1]
            )
        partial_file_name = partial_file_name.replace(
            resampling_abbreviation, f"{resampling_abbreviation}ur{args.n_repeats}"
        )

    if args.type == "basic":
        results_raw = {"raw": []}
        curvature = {"curvature": []}
        kendalls_tau_valid_test = {"test": [], "test_retrained": []}
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=args.max_workers
        ) as executor:
            tasks = []
            for seed in SEEDS:
                tasks.append(
                    executor.submit(
                        analyze_results_basic,
                        results_path,
                        args.results_subfolder,
                        seed,
                        args.n_repeats,
                        args.check_files,
                    )
                )

            for future in concurrent.futures.as_completed(tasks):
                try:
                    (
                        results_raw_tmp,
                        curvature_tmp,
                        kendalls_tau_valid_test_tmp,
                    ) = future.result()
                    if results_raw_tmp:
                        results_raw["raw"].append(pd.concat(results_raw_tmp))
                    if curvature_tmp:
                        curvature["curvature"].append(pd.concat(curvature_tmp))
                    for test_type in TEST_TYPES:
                        if kendalls_tau_valid_test_tmp[test_type]:
                            kendalls_tau_valid_test[test_type].append(
                                pd.concat(kendalls_tau_valid_test_tmp[test_type])
                            )
                except Exception as e:
                    print(f"Exception occurred: {e}")

        if results_raw["raw"]:
            results_raw_tmp = pd.concat(results_raw["raw"])
            results_raw_tmp["optimizer"] = OPTIMIZER
            results_raw_tmp.to_csv(
                f"../csvs/raw/{partial_file_name}_raw.csv",
                index=False,
            )

        if curvature["curvature"]:
            curvature_tmp = pd.concat(curvature["curvature"])
            curvature_tmp["optimizer"] = OPTIMIZER
            curvature_tmp.to_csv(
                f"../csvs/raw/{partial_file_name}_curvature.csv",
                index=False,
            )

        for test_type in TEST_TYPES:
            if kendalls_tau_valid_test[test_type]:
                kendalls_tau_valid_test_tmp = pd.concat(
                    kendalls_tau_valid_test[test_type]
                )
                kendalls_tau_valid_test_tmp["optimizer"] = OPTIMIZER
                kendalls_tau_valid_test_tmp.to_csv(
                    f"../csvs/raw/{partial_file_name}_kendalls_tau_valid_test_{test_type}.csv",
                    index=False,
                )
    elif args.type in [
        "post_naive",
    ]:
        results = {"test": [], "test_retrained": []}
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=args.max_workers
        ) as executor:
            tasks = []
            for seed in SEEDS:
                tasks.append(
                    executor.submit(
                        analyze_results_post,
                        results_path,
                        args.results_subfolder,
                        seed,
                        args.type,
                        args.n_repeats,
                        args.check_files,
                    )
                )

            for future in concurrent.futures.as_completed(tasks):
                try:
                    results_tmp = future.result()
                    for test_type in TEST_TYPES:
                        if results_tmp[test_type]:
                            results[test_type].append(pd.concat(results_tmp[test_type]))
                except Exception as e:
                    print(f"Exception occurred: {e}")

        for test_type in TEST_TYPES:
            if results[test_type]:
                results_tmp = pd.concat(results[test_type])
                results_tmp["optimizer"] = OPTIMIZER
                results_tmp.to_csv(
                    f"../csvs/raw/{partial_file_name}_{args.type}_{test_type}.csv",
                    index=False,
                )
    elif args.type == "post_naive_simulate_repeatedholdout":
        results = {"test": [], "test_retrained": []}
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=args.max_workers
        ) as executor:
            tasks = []
            for seed in SEEDS:
                tasks.append(
                    executor.submit(
                        analyze_results_post_naive_simulate_repeatedholdout,
                        results_path,
                        args.results_subfolder,
                        seed,
                        args.type,
                        args.n_repeats,
                        args.check_files,
                    )
                )

            for future in concurrent.futures.as_completed(tasks):
                try:
                    results_tmp = future.result()
                    for test_type in TEST_TYPES:
                        if results_tmp[test_type]:
                            results[test_type].append(pd.concat(results_tmp[test_type]))
                except Exception as e:
                    print(f"Exception occurred: {e}")

        for test_type in TEST_TYPES:
            if results[test_type]:
                results_tmp = pd.concat(results[test_type])
                results_tmp["optimizer"] = OPTIMIZER
                results_tmp.to_csv(
                    f"../csvs/raw/{partial_file_name}_{args.type}_{args.n_repeats}_{test_type}.csv",
                    index=False,
                )

File Path: analyze/utils.py
Content:
from typing import List


def compute_overtuning(test_values: List[float]) -> List[float]:
    """
    Compute the overtuning metric.
    """
    best_test_value = float("inf")
    overtuning = []
    for i in range(len(test_values)):
        test_value = test_values[i]
        if test_value < best_test_value:
            best_test_value = test_value
        overtuning.append(-(best_test_value - test_value))
    return overtuning

File Path: collect_results.py
Content:
if __name__ == "__main__":
    import argparse
    import os

    import pandas as pd

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--valid_type",
        type=str,
        choices=["cv", "cv_repeated", "holdout", "repeatedholdout"],
        required=True,
    )

    args = parser.parse_args()

    valid_abbrev = {
        "cv": ["_cv5r1ur1_", "_cv5r5ur1_"],
        "cv_repeated": ["_cv5r5ur5_"],
        "holdout": ["_holdout02_"],
        "repeatedholdout": ["_repeatedholdout02_5ur5_", "_simulate_repeatedholdout_"],
    }
    valid = valid_abbrev[args.valid_type]

    files = os.listdir("csvs/raw")
    files = [
        file
        for file in files
        if file.endswith(".csv") and any([v in file for v in valid])
    ]

    results_raw = [file for file in files if "results" in file and "raw" in file]
    curvature = [file for file in files if "curvature" in file]
    kendalls_tau_valid_test = [file for file in files if "kendalls_tau" in file]
    results_post = [file for file in files if "results" in file and "post" in file]

    if results_raw:
        results_raw_csv = pd.concat(
            [pd.read_csv(f"csvs/raw/{file}") for file in results_raw]
        )
        results_raw_csv.to_csv(f"csvs/results_{args.valid_type}.csv", index=False)

    if curvature:
        curvature_csv = pd.concat(
            [pd.read_csv(f"csvs/raw/{file}") for file in curvature]
        )
        curvature_csv.to_csv(f"csvs/curvature_{args.valid_type}.csv", index=False)

    if kendalls_tau_valid_test:
        for type in ["test.csv", "test_retrained.csv"]:
            kendalls_tau_valid_test_csv = pd.concat(
                [
                    pd.read_csv(f"csvs/raw/{file}")
                    for file in kendalls_tau_valid_test
                    if type in file
                ]
            )
            kendalls_tau_valid_test_csv.to_csv(
                f"csvs/kendalls_tau_{args.valid_type}_valid_test_{type}", index=False
            )

    if results_post:
        for type in ["test.csv", "test_retrained.csv"]:
            results_post_csv = pd.concat(
                [
                    pd.read_csv(f"csvs/raw/{file}")
                    for file in results_post
                    if type in file
                ]
            )
            results_post_csv.to_csv(
                f"csvs/results_{args.valid_type}_post_{type}", index=False
            )

File Path: create_analyses.py
Content:
if __name__ == "__main__":
    import argparse
    import os

    from reshufflebench.utils import str2bool

    parser = argparse.ArgumentParser()
    parser.add_argument("--max_workers", type=int, default=10)
    parser.add_argument(
        "--optimizer",
        type=str,
        choices=["random", "hebo", "smac"],
        required=True,
    )
    parser.add_argument(
        "--valid_type",
        type=str,
        choices=["cv", "cv_repeated", "holdout", "repeatedholdout"],
        required=True,
    )
    parser.add_argument("--n_repeats", type=int, default=None)
    parser.add_argument(
        "--type",
        type=str,
        default="basic",
        choices=["basic", "post_naive", "post_naive_simulate_repeatedholdout"],
    )
    parser.add_argument(
        "--reshuffle", type=str, default="Both", choices=["True", "False", "Both"]
    )
    parser.add_argument("--check_files", type=str2bool, default=False)
    args = parser.parse_args()

    results_subfolders = os.listdir("results")
    if args.optimizer in ["hebo", "smac"]:
        valid_abbrev = {
            "cv": "_cv5r1_",
            "cv_repeated": "_cv5r5_",
            "holdout": "_holdout02_",
            "repeatedholdout": "_repeatedholdout02_",
        }
        valid = valid_abbrev[args.valid_type]
        results_subfolders = [
            results_subfolder
            for results_subfolder in results_subfolders
            if args.optimizer in results_subfolder and valid in results_subfolder
        ]
    else:
        valid_abbrev = {
            "cv": "_cv5r5_",
            "cv_repeated": "_cv5r5_",
            "holdout": "_holdout02_",
            "repeatedholdout": "_cv5r5_",  # repeatedholdout simulated from repeated cv
        }
        valid = valid_abbrev[args.valid_type]
        results_subfolders = [
            results_subfolder
            for results_subfolder in results_subfolders
            if "hebo" not in results_subfolder
            and "smac" not in results_subfolder
            and valid in results_subfolder
        ]

    file = "run_analyses.sh"
    with open(file, "+w") as f:
        for results_subfolder in results_subfolders:
            if args.reshuffle == "True" or args.reshuffle == "Both":
                if "reshuffle" in results_subfolder:
                    f.write(
                        f"python3 result_analyzer.py --max_workers {args.max_workers} --type {args.type} --results_subfolder {results_subfolder} --n_repeats {args.n_repeats} --check_files {args.check_files} \n"
                    )
            if args.reshuffle == "False" or args.reshuffle == "Both":
                if "reshuffle" not in results_subfolder:
                    f.write(
                        f"python3 result_analyzer.py --max_workers {args.max_workers} --type {args.type} --results_subfolder {results_subfolder} --n_repeats {args.n_repeats} --check_files {args.check_files} \n"
                    )

    os.chmod(file, 0o755)

File Path: create_experiments.py
Content:
if __name__ == "__main__":
    import argparse
    import os

    from reshufflebench.utils import str2bool

    data_ids = [23517, 1169, 41147, 4135, 1461, 1590, 41150, 41162, 42733, 42742]
    train_valid_sizes = [500, 1000, 5000]
    reshuffle = [True, False]
    seeds = range(42, 52)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--classifier",
        type=str,
        choices=[
            "catboost",
            "funnel_mlp",
            "logreg",
            "tabpfn",
            "xgboost",
            "xgboost_large",
        ],
        required=True,
    )
    parser.add_argument("--default", type=str2bool, required=True)
    parser.add_argument(
        "--optimizer",
        type=str,
        choices=["random", "hebo", "smac"],
        required=True,
    )
    parser.add_argument(
        "--valid_type",
        type=str,
        choices=["cv", "holdout", "repeatedholdout"],
        required=True,
    )
    parser.add_argument("--n_repeats", type=int, default=None)
    args = parser.parse_args()

    if args.optimizer in ["hebo", "smac"]:
        n_trials = 250
        if args.valid_type == "cv":
            n_repeats = args.n_repeats
        elif args.valid_type == "repeatedholdout":
            n_repeats = 5
    else:
        n_trials = 500
        if args.valid_type == "cv":
            n_repeats = 5
        elif args.valid_type == "repeatedholdout":
            n_repeats = 5

    file = "run_experiments.sh"
    with open(file, "+w") as f:
        for data_id in data_ids:
            for train_valid_size in train_valid_sizes:
                for reshuffle_ in reshuffle:
                    for seed in seeds:
                        if args.valid_type == "cv":
                            f.write(
                                f"python3 main.py --classifier {args.classifier} --default {args.default} --optimizer {args.optimizer} --data_id {data_id} --valid_type {args.valid_type} --train_valid_size {train_valid_size} --reshuffle {reshuffle_} --n_splits 5 --n_repeats {n_repeats} --test_size 5000 --add_valid_size 5000 --seed {seed} --n_trials {n_trials} \n"
                            )
                        elif args.valid_type == "holdout":
                            f.write(
                                f"python3 main.py --classifier {args.classifier} --default {args.default} --optimizer {args.optimizer} --data_id {data_id} --valid_type {args.valid_type} --train_valid_size {train_valid_size} --reshuffle {reshuffle_} --valid_frac 0.2 --test_size 5000 --add_valid_size 5000 --seed {seed} --n_trials {n_trials} \n"
                            )
                        else:
                            f.write(
                                f"python3 main.py --classifier {args.classifier} --default {args.default} --optimizer {args.optimizer} --data_id {data_id} --valid_type {args.valid_type} --train_valid_size {train_valid_size} --reshuffle {reshuffle_} --valid_frac 0.2 --n_repeats {n_repeats} --test_size 5000 --add_valid_size 5000 --seed {seed} --n_trials {n_trials} \n"
                            )

    os.chmod(file, 0o755)

File Path: main.py
Content:
if __name__ == "__main__":
    import argparse

    from reshufflebench.algorithms import (
        CatBoost,
        Featureless,
        FunnelMLP,
        LogReg,
        TabPFN,
        XGBoost,
        XGBoostLarge,
    )
    from reshufflebench.learner import (
        LearnerHeboCV,
        LearnerHeboHoldout,
        LearnerHeboRepeatedHoldout,
        LearnerRandomCV,
        LearnerRandomHoldout,
        LearnerRandomRepeatedHoldout,
        LearnerSmacCV,
        LearnerSmacHoldout,
        LearnerSmacRepeatedHoldout,
    )
    from reshufflebench.utils import str2bool

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--classifier",
        type=str,
        default="catboost",
        choices=[
            "catboost",
            "funnel_mlp",
            "logreg",
            "tabpfn",
            "xgboost",
            "xgboost_large",
            "featureless",
        ],
    )
    parser.add_argument("--default", type=str2bool, default=False)
    parser.add_argument(
        "--optimizer",
        type=str,
        default="random",
        choices=["random", "hebo", "smac"],
    )
    parser.add_argument(
        "--data_id",
        type=int,
        default=11111,
        choices=[
            23517,
            1169,
            41147,
            4135,
            1461,
            1590,
            41150,
            41162,
            42733,
            42742,
            99999,
            11111,
        ],
    )
    parser.add_argument(
        "--valid_type",
        type=str,
        default="holdout",
        choices=["cv", "holdout", "repeatedholdout"],
    )
    parser.add_argument(
        "--train_valid_size",
        type=int,
        default=500,
        choices=[500, 1000, 5000],
    )
    parser.add_argument("--reshuffle", type=str2bool, default=True)
    parser.add_argument("--n_splits", type=int, default=5, choices=[5])
    # n_repeats = 1 or 5 for cv, 1 for holdout and 5 for repeatedholdout
    parser.add_argument("--n_repeats", type=int, default=1, choices=[1, 5])
    parser.add_argument("--valid_frac", type=float, default=0.2, choices=[0.2])
    parser.add_argument("--test_size", type=int, default=5000)
    parser.add_argument("--add_valid_size", type=int, default=5000)
    parser.add_argument("--n_trials", type=int, default=10)
    parser.add_argument("--seed", type=int, default=42)

    args = parser.parse_args()

    classifiers = {
        "catboost": CatBoost(seed=args.seed, default=args.default),
        "funnel_mlp": FunnelMLP(seed=args.seed, default=args.default),
        "logreg": LogReg(seed=args.seed, default=args.default),
        "tabpfn": TabPFN(seed=args.seed),
        "xgboost": XGBoost(seed=args.seed, default=args.default),
        "xgboost_large": XGBoostLarge(seed=args.seed, default=args.default),
        "featureless": Featureless(seed=args.seed),
    }
    classifier = classifiers[args.classifier]

    if args.n_trials > 500:
        raise ValueError(
            "n_trials must be <= 500 - or you must adjust seeds in codebase"
        )

    if args.optimizer == "random":
        if args.valid_type == "cv":
            learner = LearnerRandomCV(
                classifier=classifier,
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                n_splits=args.n_splits,
                n_repeats=args.n_repeats,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
        elif args.valid_type == "holdout":
            learner = LearnerRandomHoldout(
                classifier=classifier,
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                valid_frac=args.valid_frac,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
        else:
            learner = LearnerRandomRepeatedHoldout(
                classifier=classifier,
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                valid_frac=args.valid_frac,
                n_repeats=args.n_repeats,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
    elif args.optimizer == "hebo":
        if args.valid_type == "cv":
            learner = LearnerHeboCV(
                classifier=classifier,
                metric="auc",
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                n_splits=args.n_splits,
                n_repeats=args.n_repeats,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
        elif args.valid_type == "holdout":
            learner = LearnerHeboHoldout(
                classifier=classifier,
                metric="auc",
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                valid_frac=args.valid_frac,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
        else:
            learner = LearnerHeboRepeatedHoldout(
                classifier=classifier,
                metric="auc",
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                valid_frac=args.valid_frac,
                n_repeats=args.n_repeats,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
    elif args.optimizer == "smac":
        if args.valid_type == "cv":
            learner = LearnerSmacCV(
                classifier=classifier,
                metric="auc",
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                n_splits=args.n_splits,
                n_repeats=args.n_repeats,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
        elif args.valid_type == "holdout":
            learner = LearnerSmacHoldout(
                classifier=classifier,
                metric="auc",
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                valid_frac=args.valid_frac,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
        else:
            learner = LearnerSmacRepeatedHoldout(
                classifier=classifier,
                metric="auc",
                data_id=args.data_id,
                train_valid_size=args.train_valid_size,
                reshuffle=args.reshuffle,
                valid_frac=args.valid_frac,
                n_repeats=args.n_repeats,
                test_size=args.test_size,
                add_valid_size=args.add_valid_size,
                n_trials=args.n_trials,
                seed=args.seed,
            )
    else:
        raise ValueError("Invalid optimizer")
    learner.run()

File Path: reshufflebench/__init__.py
Content:

File Path: reshufflebench/algorithms/__init__.py
Content:
from .catboost_catboost import CatBoost
from .featureless import Featureless
from .sklearn_funnel_mlp import FunnelMLP
from .sklearn_logreg import LogReg
from .tabpfn_tabpfn import TabPFN
from .xgboost_xgboost import XGBoost
from .xgboost_xgboost_large import XGBoostLarge

__all__ = [
    "CatBoost",
    "FunnelMLP",
    "LogReg",
    "TabPFN",
    "XGBoost",
    "XGBoostLarge",
    "Featureless",
]

File Path: reshufflebench/algorithms/catboost_catboost.py
Content:
import json
from typing import List, Optional

import numpy as np
from catboost import CatBoostClassifier
from ConfigSpace import ConfigurationSpace, Float, Integer
from hebo.design_space import DesignSpace
from optuna.distributions import FloatDistribution, IntDistribution
from optuna.trial import Trial

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.utils import NumpyArrayEncoder


# search space from https://github.com/naszilla/tabzilla/blob/d689be5603f0a5fc8de30758d68b7122bcf46719/TabZilla/models/tree_models.py#L112
class CatBoost(Classifier):
    def __init__(self, seed: int, default: bool = False):
        if default:
            classifier_id = "catboost_default"
        else:
            classifier_id = "catboost"

        super().__init__(
            classifier_id=classifier_id,
            impute_x_cat=False,
            impute_x_num=False,
            encode_x=False,
            scale_x=False,
            seed=seed,
            default=default,
        )
        self.learning_rate = None
        self.depth = None
        self.l2_leaf_reg = None
        self.actual_iterations = []
        self.in_refit_mode = False

    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        hebo_params = [
            {
                "name": "learning_rate",
                "type": "pow",
                "lb": 0.01,
                "ub": 0.3,
                "base": np.e,
            },
            {"name": "depth", "type": "pow_int", "lb": 2, "ub": 12, "base": np.e},
            {"name": "l2_leaf_reg", "type": "pow", "lb": 0.5, "ub": 30, "base": np.e},
        ]
        hebo_search_space = DesignSpace().parse(hebo_params)
        return hebo_search_space

    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        cs = ConfigurationSpace(seed=self.seed)
        learning_rate = Float(name="learning_rate", bounds=[0.01, 0.3], log=True)
        depth = Integer(name="depth", bounds=[2, 12], log=True)
        l2_leaf_reg = Float(name="l2_leaf_reg", bounds=[0.5, 30], log=True)
        cs.add([learning_rate, depth, l2_leaf_reg])
        return cs

    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        internal_optuna_search_space = {
            "learning_rate": FloatDistribution(low=0.01, high=0.3, step=None, log=True),
            "depth": IntDistribution(low=2, high=12, step=1, log=True),
            "l2_leaf_reg": FloatDistribution(low=0.5, high=30, step=None, log=True),
        }
        return internal_optuna_search_space

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        if self.default:
            self.learning_rate = 0.03
            self.depth = 6
            self.l2_leaf_reg = 3
        else:
            self.learning_rate = trial.suggest_float(
                "learning_rate", low=0.01, high=0.3, log=True
            )
            self.depth = trial.suggest_int("depth", low=2, high=12, log=True)
            self.l2_leaf_reg = trial.suggest_float(
                "l2_leaf_reg", low=0.5, high=30, log=True
            )

        classifier = CatBoostClassifier(
            iterations=2000,
            od_type="Iter",
            od_wait=20,
            learning_rate=self.learning_rate,
            depth=self.depth,
            l2_leaf_reg=self.l2_leaf_reg,
            verbose=False,
            allow_writing_files=False,
            thread_count=1,
            # thread_count = 16 for some catboost runs with hebo, thread_count was set to 16 and HPO was run on a whole node with 16 threads, 128GB RAM to stay below 7 days of runtime
        )

        self.classifier = classifier
        self.in_refit_mode = False

    def update_actual_iterations(self) -> None:
        """
        Update the actual iterations used for refitting based on the best iteration(s).
        """
        if self.classifier is None:
            raise ValueError("Classifier is None")

        self.actual_iterations.append(self.classifier.get_best_iteration())

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        """
        if len(self.actual_iterations) == 0:
            raise ValueError("No actual iterations stored")

        if len(self.actual_iterations) == 1:
            actual_iterations = self.actual_iterations[0]
        else:
            actual_iterations = int(
                round(sum(self.actual_iterations) / len(self.actual_iterations))
            )

        if actual_iterations < 1:
            actual_iterations = 1

        classifier = CatBoostClassifier(
            iterations=actual_iterations,
            learning_rate=self.learning_rate,
            depth=self.depth,
            l2_leaf_reg=self.l2_leaf_reg,
            verbose=False,
            allow_writing_files=False,
            thread_count=1,
            # thread_count = 16 for some catboost runs with hebo, thread_count was set to 16 and HPO was run on a whole node with 16 threads, 128GB RAM to stay below 7 days of runtime
        )

        self.classifier = classifier
        self.in_refit_mode = True

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Train the classifier.
        If in refit mode no early stopping is used but the number of iterations are set to the average of the best iterations, see update_actual_iterations and construct_classifier_refit.
        """
        # check that if cat_features is not None and not empty, then the respective columns are of dtype object
        if cat_features is not None and len(cat_features) > 0:
            if x_train[:, cat_features].dtype != np.dtype("object"):
                raise ValueError(
                    "If cat_features is not None and not empty, then the respective columns of x_train must be of dtype object"
                )
            if x_valid is not None:
                if x_valid[:, cat_features].dtype != np.dtype("object"):
                    raise ValueError(
                        "If cat_features is not None and not empty, then the respective columns of x_valid must be of dtype object"
                    )
        if self.in_refit_mode:
            trial.set_user_attr(
                "actual_iterations",
                json.dumps(self.actual_iterations, cls=NumpyArrayEncoder),
            )
            self.classifier.fit(X=x_train, y=y_train, cat_features=cat_features)
        else:
            self.classifier.fit(
                X=x_train,
                y=y_train,
                eval_set=(x_valid, y_valid),
                cat_features=cat_features,
            )
            self.update_actual_iterations()

    def reset(self) -> None:
        """
        Reset the classifier.
        """
        super().reset()
        self.learning_rate = None
        self.depth = None
        self.l2_leaf_reg = None
        self.actual_iterations = []
        self.in_refit_mode = False

File Path: reshufflebench/algorithms/classifier.py
Content:
import warnings
from abc import ABC, abstractmethod
from typing import List, Optional

import numpy as np
import pandas as pd
from optuna.trial import Trial
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, TargetEncoder

from reshufflebench.utils import check_y_predict_proba


class Classifier(ABC):
    """
    Abstract class for a classifier.
    """

    def __init__(
        self,
        classifier_id: str,
        impute_x_cat: bool,
        impute_x_num: bool,
        encode_x: bool,
        scale_x: bool,
        seed: int,
        default: bool = False,
    ):
        self.classifier_id = classifier_id
        self.impute_x_cat = impute_x_cat
        self.impute_x_num = impute_x_num
        self.encode_x = encode_x
        self.scale_x = scale_x
        self.seed = seed
        self.default = default
        self.error_on_fit = False
        self.labels = None
        self.labels_fraction = None
        self.majority_class = None
        self.imputer_cat = None
        self.imputer_num = None
        self.encoder = None
        self.scaler = None
        self.classifier = None
        self.preprocessor = None

    @abstractmethod
    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        pass

    @abstractmethod
    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        pass

    @abstractmethod
    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        pass

    def construct_pipeline(
        self,
        trial: Trial,
        refit: bool,
        cat_features: List[int],
        num_features: List[int],
        **kwargs
    ) -> None:
        """
        Construct the classifier pipeline based on the trial.
        First constructs the classifier, then the preprocessor.
        The preprocessor consists of imputing, encoding and scaling.
        If impute_x_cat is True, then categorical features are imputed with a constant value "__missing__".
        If not, then categorical features are passed through.
        If impute_x_num is True, then numerical features are imputed with the mean.
        If not, then numerical features are passed through.
        If encode_x is True, then categorical features are target encoded.
        Conditional on encode_x being True, if scale_x is True, then numerical features are scaled with a standard scaler.
        This means that all features are scaled to have a mean of zero and a standard deviance of 1 because the categorical features are target encoded and count as numerical features.
        Conditional on encode_x being True, if scale_x is False, then numerical features are passed through.
        If encode_x is False, then categorical features are passed through.
        Conditional on encode_x being False, if scale_x is True, then numerical features are scaled with a standard scaler.
        In this case the categorical features are still categorical and are not target encoded and therefore also not scaled as they do not count as numerical features.
        Otherwise, if scale_x is False, then numerical features are passed through.
        """
        if refit:
            self.construct_classifier_refit(trial, **kwargs)
        else:
            self.construct_classifier(trial, **kwargs)

        if self.impute_x_cat:
            self.imputer_cat = ColumnTransformer(
                transformers=[
                    (
                        "imputer_cat",
                        SimpleImputer(strategy="constant", fill_value="__missing__"),
                        cat_features,
                    )
                ],
                remainder="passthrough",
            )
            # cat_features are now the first features in the preprocessed data
            cat_features = list(range(len(cat_features)))
            # num features are the last features due to the passthrough
            num_features = list(
                range(len(cat_features), len(cat_features) + len(num_features))
            )
        else:
            self.imputer_cat = ColumnTransformer(
                transformers=[("passthrough", "passthrough", slice(0, None))],
                remainder="passthrough",
            )
        if self.impute_x_num:
            self.imputer_num = ColumnTransformer(
                transformers=[
                    (
                        "imputer_num",
                        SimpleImputer(strategy="mean"),
                        num_features,
                    )
                ],
                remainder="passthrough",
            )
            # num features are now the first features in the preprocessed data
            num_features = list(range(len(num_features)))
            # cat_features are the last features due to the passthrough
            cat_features = list(
                range(len(num_features), len(num_features) + len(cat_features))
            )
        else:
            self.imputer_num = ColumnTransformer(
                transformers=[("passthrough", "passthrough", slice(0, None))],
                remainder="passthrough",
            )
        if self.encode_x:
            self.encoder = ColumnTransformer(
                transformers=[
                    (
                        "encoder",
                        TargetEncoder(cv=5, shuffle=True, random_state=self.seed),
                        cat_features,
                    )
                ],
                remainder="passthrough",
            )
            if self.scale_x:
                self.scaler = ColumnTransformer(
                    transformers=[
                        (
                            "scaler",
                            StandardScaler(),
                            slice(0, None),
                        )
                    ],
                    remainder="passthrough",
                )
            else:
                self.scaler = ColumnTransformer(
                    transformers=[("passthrough", "passthrough", slice(0, None))],
                    remainder="passthrough",
                )
        else:
            self.encoder = ColumnTransformer(
                transformers=[("passthrough", "passthrough", slice(0, None))],
                remainder="passthrough",
            )
            if self.scale_x:
                self.scaler = ColumnTransformer(
                    transformers=[
                        (
                            "scaler",
                            StandardScaler(),
                            num_features,
                        )
                    ],
                    remainder="passthrough",
                )
            else:
                self.scaler = ColumnTransformer(
                    transformers=[("passthrough", "passthrough", slice(0, None))],
                    remainder="passthrough",
                )

        self.preprocessor = Pipeline(
            [
                ("imputer_cat", self.imputer_cat),
                ("imputer_num", self.imputer_num),
                ("encoder", self.encoder),
                ("scaler", self.scaler),
            ]
        )

    @abstractmethod
    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        pass

    @abstractmethod
    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        """
        pass

    def fit(
        self,
        trial: Trial,
        x_train: pd.DataFrame,
        y_train: np.array,
        x_valid: Optional[pd.DataFrame] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Train the classifier pipeline.
        Performs the preprocessing and then calls _fit to train the classifier.
        """
        x_train = self.preprocessor.fit_transform(x_train, y_train)
        if x_valid is not None:
            x_valid = self.preprocessor.transform(x_valid)
        if cat_features is not None and len(cat_features) > 0:
            # if encode_x is True, then cat_features are now encoded and numeric
            if self.encode_x:
                preprocessed_cat_features = None
            # if encode_x is False, then cat_features are still categorical, and we need to find the indices of the cat_features among the preprocessed features
            else:
                preprocessed_features = self.preprocessor.get_feature_names_out(
                    self.preprocessor.feature_names_in_
                )
                preprocessed_features = [
                    feature.split("__")[-1] for feature in preprocessed_features
                ]
                if set(self.preprocessor.feature_names_in_).difference(
                    preprocessed_features
                ):
                    raise ValueError(
                        "Preprocessed features do not match input features"
                    )
                preprocessed_cat_features = []
                for cat_feature in cat_features:
                    cat_feature_name = self.preprocessor.feature_names_in_[cat_feature]
                    preprocessed_cat_feature = [
                        index
                        for index, feature_name in enumerate(preprocessed_features)
                        if cat_feature_name == feature_name
                    ]
                    preprocessed_cat_features.extend(preprocessed_cat_feature)
                if len(preprocessed_cat_features) != len(cat_features):
                    raise ValueError(
                        "Preprocessed categorical features do not match input categorical features"
                    )
        else:
            preprocessed_cat_features = None

        try:
            self._fit(
                trial,
                x_train=x_train,
                y_train=y_train,
                x_valid=x_valid,
                y_valid=y_valid,
                cat_features=preprocessed_cat_features,
            )
        except Exception as e:
            warnings.warn("Exception occurred while fitting classifier: " + str(e))
            self.error_on_fit = True
            self.labels = np.unique(y_train).tolist()
            self.labels_fraction = np.bincount(y_train) / len(y_train)
            self.majority_class = np.bincount(y_train).argmax()

    @abstractmethod
    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Train the classifier.
        """
        pass

    def predict(self, x: pd.DataFrame) -> np.array:
        """
        Predict the class labels.
        """
        if self.error_on_fit:
            warnings.warn(
                "Classifier was not fitted successfully. Predicting majority class."
            )
            return np.repeat(self.majority_class, len(x))
        else:
            x = self.preprocessor.transform(x)
            y_pred_proba = self.classifier.predict_proba(x)
            y_pred = np.argmax(y_pred_proba, axis=1)
            return y_pred

    def predict_proba(self, x: pd.DataFrame) -> np.array:
        """
        Predict the class probabilities.
        """
        if self.error_on_fit:
            warnings.warn(
                "Classifier was not fitted successfully. Predicting class probabilities according to occurrence in training data."
            )
            y_pred_proba = np.tile(self.labels_fraction, (len(x), 1))
        else:
            x = self.preprocessor.transform(x)
            y_pred_proba = self.classifier.predict_proba(x)
        check_y_predict_proba(y_pred_proba)
        return y_pred_proba

    def reset(self) -> None:
        """
        Reset the classifier and preprocessor and set error_on_fit to False, labels to None and majority_class to None.
        """
        self.error_on_fit = False
        self.labels = None
        self.labels_fraction = None
        self.majority_class = None
        self.preprocessor = None
        self.classifier = None

File Path: reshufflebench/algorithms/featureless.py
Content:
from typing import List, Optional

import numpy as np
from optuna.trial import Trial

from reshufflebench.algorithms.classifier import Classifier


class Featureless(Classifier):
    def __init__(self, seed: int):
        super().__init__(
            classifier_id="featureless",
            impute_x_cat=False,
            impute_x_num=False,
            encode_x=False,
            scale_x=False,
            seed=seed,
        )

    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        return None

    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        return None

    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        return None

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        classifier = None

        self.classifier = classifier

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        Nothing done here.
        """
        pass

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Fit the classifier.
        """
        raise ValueError("No classifier to fit, will trigger fallback")

File Path: reshufflebench/algorithms/sklearn_funnel_mlp.py
Content:
from typing import List, Optional

import numpy as np
from ConfigSpace import Categorical, ConfigurationSpace, Float, Integer
from hebo.design_space import DesignSpace
from optuna.distributions import (
    CategoricalDistribution,
    FloatDistribution,
    IntDistribution,
)
from optuna.trial import Trial
from sklearn.neural_network import MLPClassifier

from reshufflebench.algorithms.classifier import Classifier


class FunnelMLP(Classifier):
    def __init__(self, seed: int, default: bool = False):
        if default:
            classifier_id = "funnel_mlp_default"
        else:
            classifier_id = "funnel_mlp"

        super().__init__(
            classifier_id=classifier_id,
            impute_x_cat=True,
            impute_x_num=True,
            encode_x=True,
            scale_x=True,
            seed=seed,
            default=default,
        )

    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        n_train_samples = kwargs["n_train_samples"]
        # maximum batch size is the largest power of 2 that is smaller than the number of training samples
        max_batch_size_log2 = int(np.log2(n_train_samples))
        hebo_params = [
            {"name": "num_layers", "type": "int", "lb": 1, "ub": 5},
            {
                "name": "max_units",
                "type": "cat",
                "categories": [2**i for i in range(6, 10)],
            },
            {
                "name": "learning_rate",
                "type": "pow",
                "lb": 1e-4,
                "ub": 1e-1,
                "base": np.e,
            },
            {
                "name": "batch_size",
                "type": "cat",
                "categories": [2**i for i in range(4, max_batch_size_log2 + 1)],
            },
            {"name": "momentum", "type": "num", "lb": 0.1, "ub": 0.99},
            {"name": "alpha", "type": "pow", "lb": 1e-6, "ub": 1e-1, "base": np.e},
        ]
        hebo_search_space = DesignSpace().parse(hebo_params)
        return hebo_search_space

    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        n_train_samples = kwargs["n_train_samples"]
        # maximum batch size is the largest power of 2 that is smaller than the number of training samples
        max_batch_size_log2 = int(np.log2(n_train_samples))
        cs = ConfigurationSpace(seed=self.seed)
        num_layers = Integer(name="num_layers", bounds=[1, 5])
        max_units = Categorical(name="max_units", items=[2**i for i in range(6, 10)])
        learning_rate = Float(name="learning_rate", bounds=[1e-4, 1e-1], log=True)
        batch_size = Categorical(
            name="batch_size", items=[2**i for i in range(4, max_batch_size_log2 + 1)]
        )
        momentum = Float(name="momentum", bounds=[0.1, 0.99])
        alpha = Float(name="alpha", bounds=[1e-6, 1e-1], log=True)
        cs.add([num_layers, max_units, learning_rate, batch_size, momentum, alpha])
        return cs

    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        n_train_samples = kwargs["n_train_samples"]
        # maximum batch size is the largest power of 2 that is smaller than the number of training samples
        max_batch_size_log2 = int(np.log2(n_train_samples))
        internal_optuna_search_space = {
            "num_layers": IntDistribution(low=1, high=5, step=1),
            "max_units": CategoricalDistribution(choices=[2**i for i in range(6, 10)]),
            "learning_rate": FloatDistribution(
                low=1e-4, high=1e-1, step=None, log=True
            ),
            "batch_size": CategoricalDistribution(
                choices=[2**i for i in range(4, max_batch_size_log2 + 1)]
            ),
            "momentum": FloatDistribution(low=0.1, high=0.99, step=None),
            "alpha": FloatDistribution(low=1e-6, high=1e-1, step=None, log=True),
        }
        return internal_optuna_search_space

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        # search space somewhat similar to lcbench but no early stopping
        # missing dropout but included alpha
        n_train_samples = kwargs["n_train_samples"]
        if self.default:
            num_layers = 3
            max_units = 128
            learning_rate = 0.001
            batch_size = 32
            momentum = 0.9
            alpha = 0.0001
        else:
            num_layers = trial.suggest_int("num_layers", low=1, high=5)
            max_units = trial.suggest_categorical(
                "max_units", [2**i for i in range(6, 10)]
            )
            learning_rate = trial.suggest_float(
                "learning_rate", low=1e-4, high=1e-1, log=True
            )
            # maximum batch size is the largest power of 2 that is smaller than the number of training samples
            max_batch_size_log2 = int(np.log2(n_train_samples))
            batch_size = trial.suggest_categorical(
                "batch_size", [2**i for i in range(4, max_batch_size_log2 + 1)]
            )
            momentum = trial.suggest_float("momentum", low=0.1, high=0.99)
            alpha = trial.suggest_float("alpha", low=1e-6, high=1e-1, log=True)

        # hidden_layer_size is determined by num_layers and max_units
        # we start with max_units and half the number of units for each layer to create a funnel
        hidden_layer_sizes = [int(max_units / (2**i)) for i in range(num_layers)]

        classifier = MLPClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            activation="relu",
            solver="sgd",
            alpha=alpha,
            batch_size=batch_size,
            learning_rate="constant",
            learning_rate_init=learning_rate,
            max_iter=100,
            momentum=momentum,
            nesterovs_momentum=True,
            n_iter_no_change=100,
            verbose=False,
        )

        self.classifier = classifier

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        Nothing done here.
        """
        pass

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Fit the classifier.
        """
        self.classifier.fit(x_train, y_train)

File Path: reshufflebench/algorithms/sklearn_logreg.py
Content:
from typing import List, Optional

import numpy as np
from ConfigSpace import ConfigurationSpace, Float
from hebo.design_space import DesignSpace
from optuna.distributions import FloatDistribution
from optuna.trial import Trial
from sklearn.linear_model import LogisticRegression

from reshufflebench.algorithms.classifier import Classifier


class LogReg(Classifier):
    def __init__(self, seed: int, default: bool = False):
        if default:
            classifier_id = "logreg_default"
        else:
            classifier_id = "logreg"
        super().__init__(
            classifier_id=classifier_id,
            impute_x_cat=True,
            impute_x_num=True,
            encode_x=True,
            scale_x=True,
            seed=seed,
            default=default,
        )

    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        hebo_params = [
            {"name": "C", "type": "pow", "lb": 1e-6, "ub": 1e4, "base": np.e},
            {"name": "l1_ratio", "type": "num", "lb": 0.0, "ub": 1.0},
        ]
        hebo_search_space = DesignSpace().parse(hebo_params)
        return hebo_search_space

    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        cs = ConfigurationSpace(seed=self.seed)
        C = Float(name="C", bounds=[1e-6, 1e4], log=True)
        l1_ratio = Float(name="l1_ratio", bounds=[0.0, 1.0])
        cs.add([C, l1_ratio])
        return cs

    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        internal_optuna_search_space = {
            "C": FloatDistribution(low=1e-6, high=1e4, step=None, log=True),
            "l1_ratio": FloatDistribution(low=0.0, high=1.0, step=None),
        }
        return internal_optuna_search_space

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        if self.default:
            c = 1
            l1_ratio = 0
        else:
            c = trial.suggest_float("C", low=1e-6, high=1e4, log=True)
            l1_ratio = trial.suggest_float("l1_ratio", low=0.0, high=1.0)

        classifier = LogisticRegression(
            penalty="elasticnet",
            C=c,
            l1_ratio=l1_ratio,
            solver="saga",
            max_iter=1000,
        )

        self.classifier = classifier

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        Nothing done here.
        """
        pass

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Fit the classifier.
        """
        self.classifier.fit(x_train, y_train)

File Path: reshufflebench/algorithms/tabpfn_tabpfn.py
Content:
from typing import List, Optional

import numpy as np
import torch
from optuna.trial import Trial
from tabpfn import TabPFNClassifier

from reshufflebench.algorithms.classifier import Classifier


class TabPFN(Classifier):
    def __init__(self, seed: int):
        super().__init__(
            classifier_id="tabpfn",
            impute_x_cat=True,
            impute_x_num=False,
            encode_x=True,
            scale_x=False,
            seed=seed,
        )

    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        return None

    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        return None

    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        return None

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        if torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
        # seed explicitly set to ensure reproducibility of TabPFN
        classifier = TabPFNClassifier(
            device=device, N_ensemble_configurations=32, seed=self.seed
        )

        self.classifier = classifier

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        Nothing done here.
        """
        pass

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Fit the classifier.
        """
        self.classifier.fit(x_train, y_train, overwrite_warning=True)

File Path: reshufflebench/algorithms/xgboost_xgboost.py
Content:
import json
from typing import List, Optional

import numpy as np
from ConfigSpace import ConfigurationSpace, Float, Integer
from hebo.design_space import DesignSpace
from optuna.distributions import FloatDistribution, IntDistribution
from optuna.trial import Trial
from xgboost import XGBClassifier

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.utils import NumpyArrayEncoder


# search space from https://github.com/naszilla/tabzilla/blob/d689be5603f0a5fc8de30758d68b7122bcf46719/TabZilla/models/tree_models.py#L20
class XGBoost(Classifier):
    def __init__(self, seed: int, default: bool = False):
        if default:
            classifier_id = "xgboost_default"
        else:
            classifier_id = "xgboost"
        super().__init__(
            classifier_id=classifier_id,
            impute_x_cat=True,
            impute_x_num=False,
            encode_x=True,
            scale_x=False,
            seed=seed,
            default=default,
        )

        self.max_depth = None
        self.alpha = None
        self.lambda_ = None
        self.eta = None
        self.actual_iterations = []
        self.in_refit_mode = False

    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        hebo_params = [
            {
                "name": "max_depth",
                "type": "pow_int",
                "lb": 2,
                "ub": 12,
                "base": np.e,
            },
            {"name": "alpha", "type": "pow", "lb": 1e-8, "ub": 1.0, "base": np.e},
            {"name": "lambda", "type": "pow", "lb": 1e-8, "ub": 1.0, "base": np.e},
            {"name": "eta", "type": "pow", "lb": 0.01, "ub": 0.3, "base": np.e},
        ]
        hebo_search_space = DesignSpace().parse(hebo_params)
        return hebo_search_space

    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        cs = ConfigurationSpace(seed=self.seed)
        max_depth = Integer(name="max_depth", bounds=[2, 12], log=True)
        alpha = Float(name="alpha", bounds=[1e-8, 1.0], log=True)
        lambda_ = Float(name="lambda", bounds=[1e-8, 1.0], log=True)
        eta = Float(name="eta", bounds=[0.01, 0.3], log=True)
        cs.add([max_depth, alpha, lambda_, eta])
        return cs

    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        internal_optuna_search_space = {
            "max_depth": IntDistribution(low=2, high=12, step=1, log=True),
            "alpha": FloatDistribution(low=1e-8, high=1.0, step=None, log=True),
            "lambda": FloatDistribution(low=1e-8, high=1.0, step=None, log=True),
            "eta": FloatDistribution(low=0.01, high=0.3, step=None, log=True),
        }
        return internal_optuna_search_space

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        if self.default:
            self.max_depth = 6
            self.alpha = 0.0
            self.lambda_ = 1.0
            self.eta = 0.3
        else:
            self.max_depth = trial.suggest_int("max_depth", low=2, high=12, log=True)
            self.alpha = trial.suggest_float("alpha", low=1e-8, high=1.0, log=True)
            self.lambda_ = trial.suggest_float("lambda", low=1e-8, high=1.0, log=True)
            self.eta = trial.suggest_float("eta", low=0.01, high=0.3, log=True)

        classifier = XGBClassifier(
            n_estimators=2000,
            early_stopping_rounds=20,
            max_depth=self.max_depth,
            reg_alpha=self.alpha,
            reg_lambda=self.lambda_,
            learning_rate=self.eta,
            verbosity=0,
            n_jobs=1,
        )

        self.classifier = classifier
        self.in_refit_mode = False

    def update_actual_iterations(self) -> None:
        """
        Update the actual iterations used for refitting based on the best iteration(s).
        """
        if self.classifier is None:
            raise ValueError("Classifier is None")

        self.actual_iterations.append(self.classifier.best_iteration)

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        """
        if len(self.actual_iterations) == 0:
            raise ValueError("No actual iterations stored")

        if len(self.actual_iterations) == 1:
            actual_iterations = self.actual_iterations[0]
        else:
            actual_iterations = int(
                round(sum(self.actual_iterations) / len(self.actual_iterations))
            )

        if actual_iterations < 1:
            actual_iterations = 1

        classifier = XGBClassifier(
            n_estimators=actual_iterations,
            max_depth=self.max_depth,
            reg_alpha=self.alpha,
            reg_lambda=self.lambda_,
            learning_rate=self.eta,
            verbosity=0,
            n_jobs=1,
        )

        self.classifier = classifier
        self.in_refit_mode = True

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Train the classifier.
        If in refit mode no early stopping is used but the number of iterations are set to the average of the best iterations, see update_actual_iterations and construct_classifier_refit.
        """
        if self.in_refit_mode:
            trial.set_user_attr(
                "actual_iterations",
                json.dumps(self.actual_iterations, cls=NumpyArrayEncoder),
            )
            self.classifier.fit(X=x_train, y=y_train, verbose=False)
        else:
            self.classifier.fit(
                X=x_train,
                y=y_train,
                eval_set=[(x_valid, y_valid)],
                verbose=False,
            )
            self.update_actual_iterations()

    def reset(self) -> None:
        """
        Reset the classifier.
        """
        super().reset()
        self.max_depth = None
        self.alpha = None
        self.lambda_ = None
        self.eta = None
        self.actual_iterations = []
        self.in_refit_mode = False

File Path: reshufflebench/algorithms/xgboost_xgboost_large.py
Content:
import json
from typing import List, Optional

import numpy as np
from ConfigSpace import ConfigurationSpace, Float, Integer
from hebo.design_space import DesignSpace
from optuna.distributions import FloatDistribution, IntDistribution
from optuna.trial import Trial
from xgboost import XGBClassifier

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.utils import NumpyArrayEncoder


# search space from https://github.com/LeoGrin/tabular-benchmark/blob/main/src/configs/model_configs/xgb_config.py
class XGBoostLarge(Classifier):
    def __init__(self, seed: int, default: bool = False):
        if default:
            classifier_id = "xgboost_large_default"
        else:
            classifier_id = "xgboost_large"
        super().__init__(
            classifier_id=classifier_id,
            impute_x_cat=True,
            impute_x_num=False,
            encode_x=True,
            scale_x=False,
            seed=seed,
            default=default,
        )

        self.max_depth = None
        self.alpha = None
        self.lambda_ = None
        self.eta = None

        self.min_child_weight = None
        self.subsample = None
        self.colsample_bylevel = None
        self.colsample_bytree = None
        self.gamma = None

        self.actual_iterations = []
        self.in_refit_mode = False

    def get_hebo_search_space(self, **kwargs):
        """
        Get the HEBO search space.
        """
        hebo_params = [
            {
                "name": "max_depth",
                "type": "int",
                "lb": 1,
                "ub": 11,
            },
            {"name": "alpha", "type": "pow", "lb": 1e-8, "ub": 1e2, "base": np.e},
            {"name": "lambda", "type": "pow", "lb": 1, "ub": 4, "base": np.e},
            {"name": "eta", "type": "pow", "lb": 1e-5, "ub": 0.7, "base": np.e},
            {
                "name": "min_child_weight",
                "type": "pow_int",
                "lb": 1,
                "ub": 100,
                "base": np.e,
            },
            {"name": "subsample", "type": "num", "lb": 0.5, "ub": 1.0},
            {"name": "colsample_bylevel", "type": "num", "lb": 0.5, "ub": 1.0},
            {"name": "colsample_bytree", "type": "num", "lb": 0.5, "ub": 1.0},
            {"name": "gamma", "type": "pow", "lb": 1e-8, "ub": 7, "base": np.e},
        ]
        hebo_search_space = DesignSpace().parse(hebo_params)
        return hebo_search_space

    def get_configspace_search_space(self, **kwargs):
        """
        Get the configspace search space.
        """
        cs = ConfigurationSpace(seed=self.seed)
        max_depth = Integer(name="max_depth", bounds=[1, 11], log=True)
        alpha = Float(name="alpha", bounds=[1e-8, 1e2], log=True)
        lambda_ = Float(name="lambda", bounds=[1, 4], log=True)
        eta = Float(name="eta", bounds=[1e-5, 0.7], log=True)
        min_child_weight = Integer(name="min_child_weight", bounds=[1, 100], log=True)
        subsample = (Float(name="subsample", bounds=[0.5, 1.0]),)
        colsample_bylevel = (Float(name="colsample_bylevel", bounds=[0.5, 1.0]),)
        colsample_bytree = (Float(name="colsample_bytree", bounds=[0.5, 1.0]),)
        gamma = Float(name="gamma", bounds=[1e-8, 7], log=True)
        cs.add(
            [
                max_depth,
                alpha,
                lambda_,
                eta,
                min_child_weight,
                subsample,
                colsample_bylevel,
                colsample_bytree,
                gamma,
            ]
        )
        return cs

    def get_internal_optuna_search_space(self, **kwargs):
        """
        Get the internal Optuna search space.
        """
        internal_optuna_search_space = {
            "max_depth": IntDistribution(low=1, high=11, step=1),
            "alpha": FloatDistribution(low=1e-8, high=1e2, step=None, log=True),
            "lambda": FloatDistribution(low=1, high=4, step=None, log=True),
            "eta": FloatDistribution(low=1e-5, high=0.7, step=None, log=True),
            "min_child_weight": IntDistribution(low=1, high=100, step=1, log=True),
            "subsample": FloatDistribution(low=0.5, high=1.0, step=None),
            "colsample_bylevel": FloatDistribution(low=0.5, high=1.0, step=None),
            "colsample_bytree": FloatDistribution(low=0.5, high=1.0, step=None),
            "gamma": FloatDistribution(low=1e-8, high=7, step=None, log=True),
        }
        return internal_optuna_search_space

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier based on the trial.
        """
        if self.default:
            self.max_depth = 6
            self.alpha = 0.0
            self.lambda_ = 1.0
            self.eta = 0.3

            self.min_child_weight = 1
            self.subsample = 1.0
            self.colsample_bylevel = 1.0
            self.colsample_bytree = 1.0
            self.gamma = 0.0

        else:
            self.max_depth = trial.suggest_int("max_depth", low=1, high=11)
            self.alpha = trial.suggest_float("alpha", low=1e-8, high=1e2, log=True)
            self.lambda_ = trial.suggest_float("lambda", low=1, high=4, log=True)
            self.eta = trial.suggest_float("eta", low=1e-5, high=0.7, log=True)

            self.min_child_weight = trial.suggest_int(
                "min_child_weight", low=1, high=100, log=True
            )
            self.subsample = trial.suggest_float("subsample", low=0.5, high=1.0)
            self.colsample_bylevel = trial.suggest_float(
                "colsample_bylevel", low=0.5, high=1.0
            )
            self.colsample_bytree = trial.suggest_float(
                "colsample_bytree", low=0.5, high=1.0
            )
            self.gamma = trial.suggest_float("gamma", low=1e-8, high=7, log=True)

        classifier = XGBClassifier(
            n_estimators=2000,
            early_stopping_rounds=20,
            max_depth=self.max_depth,
            reg_alpha=self.alpha,
            reg_lambda=self.lambda_,
            learning_rate=self.eta,
            min_child_weight=self.min_child_weight,
            subsample=self.subsample,
            colsample_bylevel=self.colsample_bylevel,
            colsample_bytree=self.colsample_bytree,
            gamma=self.gamma,
            verbosity=0,
            n_jobs=1,
        )

        self.classifier = classifier
        self.in_refit_mode = False

    def update_actual_iterations(self) -> None:
        """
        Update the actual iterations used for refitting based on the best iteration(s).
        """
        if self.classifier is None:
            raise ValueError("Classifier is None")

        self.actual_iterations.append(self.classifier.best_iteration)

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        """
        Construct the classifier for refitting.
        """
        if len(self.actual_iterations) == 0:
            raise ValueError("No actual iterations stored")

        if len(self.actual_iterations) == 1:
            actual_iterations = self.actual_iterations[0]
        else:
            actual_iterations = int(
                round(sum(self.actual_iterations) / len(self.actual_iterations))
            )

        if actual_iterations < 1:
            actual_iterations = 1

        classifier = XGBClassifier(
            n_estimators=actual_iterations,
            max_depth=self.max_depth,
            reg_alpha=self.alpha,
            reg_lambda=self.lambda_,
            learning_rate=self.eta,
            min_child_weight=self.min_child_weight,
            subsample=self.subsample,
            colsample_bylevel=self.colsample_bylevel,
            colsample_bytree=self.colsample_bytree,
            gamma=self.gamma,
            verbosity=0,
            n_jobs=1,
        )

        self.classifier = classifier
        self.in_refit_mode = True

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        """
        Train the classifier.
        If in refit mode no early stopping is used but the number of iterations are set to the average of the best iterations, see update_actual_iterations and construct_classifier_refit.
        """
        if self.in_refit_mode:
            trial.set_user_attr(
                "actual_iterations",
                json.dumps(self.actual_iterations, cls=NumpyArrayEncoder),
            )
            self.classifier.fit(X=x_train, y=y_train, verbose=False)
        else:
            self.classifier.fit(
                X=x_train,
                y=y_train,
                eval_set=[(x_valid, y_valid)],
                verbose=False,
            )
            self.update_actual_iterations()

    def reset(self) -> None:
        """
        Reset the classifier.
        """
        super().reset()
        self.max_depth = None
        self.alpha = None
        self.lambda_ = None
        self.eta = None
        self.actual_iterations = []
        self.in_refit_mode = False

File Path: reshufflebench/learner/__init__.py
Content:
from .learner_hebo_cv import LearnerHeboCV
from .learner_hebo_holdout import LearnerHeboHoldout
from .learner_hebo_repeatedholdout import LearnerHeboRepeatedHoldout
from .learner_random_cv import LearnerRandomCV
from .learner_random_holdout import LearnerRandomHoldout
from .learner_random_repeatedholdout import LearnerRandomRepeatedHoldout
from .learner_smac_cv import LearnerSmacCV
from .learner_smac_holdout import LearnerSmacHoldout
from .learner_smac_repeatedholdout import LearnerSmacRepeatedHoldout

__all__ = [
    "LearnerHeboCV",
    "LearnerHeboHoldout",
    "LearnerHeboRepeatedHoldout",
    "LearnerRandomCV",
    "LearnerRandomHoldout",
    "LearnerRandomRepeatedHoldout",
    "LearnerSmacCV",
    "LearnerSmacHoldout",
    "LearnerSmacRepeatedHoldout",
]

File Path: reshufflebench/learner/learner_hebo.py
Content:
import json
import os
import random
from abc import ABC, abstractmethod
from typing import Union

import numpy as np
import openml
import optuna
import pandas as pd
from optuna.trial import Trial
from sklearn.datasets import make_classification, make_hastie_10_2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.samplers import HeboSampler
from reshufflebench.utils import unify_missing_values


class LearnerHebo(ABC):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        valid_type: str,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: Union[float, None],
        n_splits: Union[int, None],
        n_repeats: Union[int, None],
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
        results_path: str,
        cv_metric_to_metric: Union[dict, None],
        write_results_to_disk: bool = False,
        bootstrap_test: bool = True,
    ):
        self.metrics = ["auc"]
        self.metrics_direction = {
            "auc": "maximize",
        }
        assert metric in self.metrics
        self.metric = metric
        self.cv_metric_to_metric = cv_metric_to_metric

        self.classifier = classifier
        self.data_id = data_id
        self.valid_type = valid_type
        self.train_valid_size = train_valid_size
        self.reshuffle = reshuffle
        self.valid_frac = valid_frac
        self.valid_size = (
            int(self.train_valid_size * self.valid_frac)
            if self.valid_frac is not None
            else None
        )
        self.n_splits = n_splits
        self.n_repeats = n_repeats
        self.test_size = test_size
        self.add_valid_size = add_valid_size
        self.n_trials = n_trials
        self.seed = seed
        self.results_path = results_path
        self.file_name = f"study_seed_{self.seed}"
        self.write_results_to_disk = write_results_to_disk
        self.bootstrap_test = bootstrap_test

        self.label_encoder = None

        self.has_missings = False
        self.labels = None
        self.n_classes = None
        self.multiclass = False
        self.cat_features = None
        self.num_features = None
        self.x_valid_train = None
        self.x_test = None
        self.x_train = None
        self.x_valid = None
        self.y_valid_train = None
        self.y_test = None
        self.y_train = None
        self.y_valid = None
        self.y_train_hist = None
        self.y_valid_hist = None
        self.y_pred_train_proba_hist = None
        self.y_pred_valid_proba_hist = None
        self.y_pred_test_proba_hist = None
        self.y_pred_valid_train_proba_hist = None
        self.y_pred_test_proba_retrained_hist = None

        self.cv = None
        self.cv_splits = None
        self.cv_splits_hist_train = None
        self.cv_splits_hist_valid = None
        self.train_size = None

        self.x_add_valid_use = None
        self.y_add_valid_use = None
        self.x_add_valid = None
        self.y_add_valid = None
        self.y_add_valid_hist = None
        self.y_pred_add_valid_proba_hist = None

        self.cv_add_valid = None
        self.cv_splits_add_valid = None
        self.cv_splits_add_valid_hist_valid = None

        self.study = None
        self.study_name = None
        self.storage = None

    def prepare_data(self) -> None:
        """
        Prepare the data for the optimization.
        """

        if self.data_id == 99999:
            # generate data for testing
            x, y = make_hastie_10_2(
                n_samples=self.train_valid_size
                + self.test_size
                + self.add_valid_size
                + 1000,
                random_state=0,
            )
            x = pd.DataFrame(x)
            x.columns = [f"feature_{i}" for i in range(x.shape[1])]
            y = y.astype(int)
            y = pd.Series(y, dtype="category")
            y.columns = ["target"]
            categorical_indicator = [False] * x.shape[1]
        elif self.data_id == 11111:
            # generate data for testing
            x, y = make_classification(
                n_samples=self.train_valid_size
                + self.test_size
                + self.add_valid_size
                + 1000,
                n_features=20,
                n_informative=10,
                n_redundant=2,
                n_repeated=2,
                n_classes=2,
                n_clusters_per_class=4,
                weights=None,
                flip_y=0.1,
                class_sep=1.0,
                hypercube=True,
                shift=0.0,
                scale=1.0,
                shuffle=True,
                random_state=0,
            )
            x = pd.DataFrame(x)
            x.columns = [f"feature_{i}" for i in range(x.shape[1])]
            y = y.astype(int)
            y = pd.Series(y, dtype="category")
            y.columns = ["target"]
            categorical_indicator = [False] * x.shape[1]
        else:
            # download the OpenML data for the dataset
            dataset = openml.datasets.get_dataset(
                self.data_id,
                download_data=True,
                download_qualities=False,
                download_features_meta_data=False,
            )

            # get the pandas dataframe
            x, y, categorical_indicator, _ = dataset.get_data(
                dataset_format="dataframe",
                target=dataset.default_target_attribute,
            )

        # check if dataset is large enough for train_valid_size, test_size and add_valid_size
        if (
            x.shape[0]
            < self.train_valid_size + self.test_size + self.add_valid_size + 1000
        ):
            raise ValueError(
                "Dataset too small for train_valid_size, test_size and add_valid_size"
            )

        # check if feature names contain "__"
        if any("__" in col for col in x.columns):
            raise ValueError(
                'Feature names contain "__" which is not allowed due to preprocessing'
            )

        # convert categorical columns to object dtype
        for col in range(len(categorical_indicator)):
            if categorical_indicator[col]:
                x[x.columns[col]] = x[x.columns[col]].astype("object")
        self.cat_features = [
            x.columns.get_loc(col) for col in x.columns if x[col].dtype.name == "object"
        ]
        self.num_features = list(set(range(x.shape[1])) - set(self.cat_features))

        # check if missing values are present
        # if yes bring missing values into a standardized format
        if x.isnull().values.any():
            self.has_missings = True
            x = unify_missing_values(x)

        if y.isnull().values.any():
            raise ValueError("Missing values in target column")

        # map the labels to integers
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(y)
        self.labels = np.unique(y).tolist()
        self.n_classes = len(self.label_encoder.classes_)
        if self.n_classes > 2:
            self.multiclass = True

        # split the data into used and unused data
        x_not_use, x_use, y_not_use, y_use = train_test_split(
            x,
            y,
            test_size=self.train_valid_size + self.test_size,
            random_state=self.seed,
            stratify=y,
        )
        # split the used data into valid_train and test
        x_valid_train, x_test, y_valid_train, y_test = train_test_split(
            x_use,
            y_use,
            test_size=self.test_size,
            random_state=self.seed,
            stratify=y_use,
        )
        # from the unused data sample add_valid_size data to use as additional validation data
        _, x_add_valid_use, _, y_add_valid_use = train_test_split(
            x_not_use,
            y_not_use,
            test_size=self.add_valid_size,
            random_state=self.seed,
            stratify=y_not_use,
        )

        self.x_valid_train = x_valid_train
        self.x_add_valid_use = x_add_valid_use
        self.x_test = x_test
        self.y_train_hist = []
        self.y_valid_hist = []
        self.y_add_valid_hist = []
        self.y_pred_train_proba_hist = []
        self.y_pred_valid_proba_hist = []
        self.y_pred_test_proba_hist = []
        self.y_pred_add_valid_proba_hist = []
        self.y_pred_valid_train_proba_hist = []
        self.y_pred_test_proba_retrained_hist = []
        self.y_valid_train = y_valid_train
        self.y_add_valid_use = y_add_valid_use
        self.y_test = y_test

    @abstractmethod
    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        pass

    def prepare_study(self) -> None:
        """
        Prepare the study for the optimization.
        """
        # if results_path does not exist, create it
        if not os.path.exists(self.results_path):
            try:
                os.makedirs(self.results_path)
            except FileExistsError:
                pass
        study_name = os.path.join(self.results_path, self.file_name)
        storage = f"sqlite:///{study_name}.db"
        # raise warning if study already exists
        if os.path.exists(f"{study_name}.db"):
            print(f"WARNING: Study {study_name} already exists. Overwriting it.")
            # remove the study
            os.remove(f"{study_name}.db")
        sampler = HeboSampler(
            seed=self.seed,
            hebo_search_space=self.classifier.get_hebo_search_space(
                n_train_samples=self.train_size
            ),
            internal_optuna_search_space=self.classifier.get_internal_optuna_search_space(
                n_train_samples=self.train_size
            ),
        )
        self.study = optuna.create_study(
            storage=storage,
            sampler=sampler,
            study_name=study_name,
            direction="maximize",
        )
        self.study_name = study_name
        self.storage = storage

    @abstractmethod
    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        """
        pass

    @abstractmethod
    def store_results(self) -> None:
        """
        Store additional results.
        """
        pass

    def run(self) -> None:
        """
        Finish all preparations.
        Run the optimization.
        Stores the results in parquet along the study database.
        """
        self.prepare_data()

        self.prepare_resampling()

        self.prepare_study()

        # write a file with the parameters
        params = {
            "classifier": self.classifier.classifier_id,
            "data_id": self.data_id,
            "valid_type": self.valid_type,
            "train_valid_size": self.train_valid_size,
            "reshuffle": self.reshuffle,
            "n_splits": self.n_splits,
            "n_repeats": self.n_repeats,
            "valid_frac": self.valid_frac,
            "test_size": self.test_size,
            "n_trials": self.n_trials,
            "file_name": self.file_name,
            "seed": self.seed,
            "has_missings": self.has_missings,
            "labels": self.labels,
            "n_classes": self.n_classes,
            "multiclass": self.multiclass,
            "metric": self.metric,
            "metrics": self.metrics,
            "metrics_direction": self.metrics_direction,
            "cv_metric_to_metric": self.cv_metric_to_metric,
            "results_path": self.results_path,
            "study_name": self.study_name,
            "storage": self.storage,
        }
        with open(
            os.path.join(self.results_path, f"{self.file_name}_params.json"), "w"
        ) as f:
            json.dump(params, f, indent=4)

        np.random.seed(self.seed)
        random.seed(self.seed)
        self.study.optimize(self.objective, n_trials=self.n_trials)

        self.store_results()

File Path: reshufflebench/learner/learner_hebo_cv.py
Content:
import json
import os

import numpy as np
from optuna.trial import Trial
from sklearn.model_selection import StratifiedKFold

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_hebo import LearnerHebo
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    NumpyArrayEncoder,
    bootstrap_test_performance,
    check_y_predict_proba,
    construct_x_and_y_add_valid,
    save_list_of_list_of_1d_arrays,
    save_list_of_list_of_pd_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerHeboCV(LearnerHebo):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        n_splits: int,
        n_repeats: int,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        # Note: add_valid size is the size of the total additional validation set, not the size of the additional validation set per fold
        #       this is somewhat in contrast to LearnerRandomHoldout, where add_valid_size is the size of the additional validation set
        #       this is also in contrast to valid_size being the size of the validation set per fold in this class
        if train_valid_size % n_splits != 0:
            raise ValueError("train_valid_size must be divisible by n_splits")
        if add_valid_size % n_splits != 0:
            raise ValueError("add_valid_size must be divisible by n_splits")
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_hebo_{classifier.classifier_id}_{data_id}_cv{n_splits}r{n_repeats}_reshuffle_{train_valid_size}_{test_size}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_hebo_{classifier.classifier_id}_{data_id}_cv{n_splits}r{n_repeats}_{train_valid_size}_{test_size}",
                )
            )

        cv_metric_to_metric = {
            "accuracies": "accuracy",
            "balanced_accuracies": "balanced_accuracy",
            "loglosses": "logloss",
            "aucs": "auc",
        }

        super().__init__(
            classifier=classifier,
            metric=metric,
            data_id=data_id,
            valid_type="cv",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=None,
            n_splits=n_splits,
            n_repeats=n_repeats,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=cv_metric_to_metric,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        # we use StratifiedKFold for the cross-validation and manually repeat it n_repeats times because this is somewhat easier to handle here than using RepeatedStratifiedKFold
        self.cv = [
            StratifiedKFold(
                n_splits=self.n_splits,
                shuffle=True,
                random_state=self.seed + (i * 1000),
            )
            for i in range(self.n_repeats)
        ]
        self.cv_splits = []
        for cv in self.cv:
            self.cv_splits.append(
                list(cv.split(self.x_valid_train, self.y_valid_train))
            )
        self.train_size = int(
            self.x_valid_train.shape[0] / self.n_splits * (self.n_splits - 1)
        )
        self.valid_size = int(self.x_valid_train.shape[0] / self.n_splits)
        self.cv_splits_hist_train = []
        self.cv_splits_hist_valid = []

        # partition add_valid_use data into n_splits folds and repeat it n_repeats times
        self.cv_add_valid = [
            StratifiedKFold(
                n_splits=self.n_splits,
                shuffle=True,
                random_state=self.seed + (i * 1000),
            )
            for i in range(self.n_repeats)
        ]
        self.cv_splits_add_valid = []
        for cv in self.cv_add_valid:
            self.cv_splits_add_valid.append(
                list(cv.split(self.x_add_valid_use, self.y_add_valid_use))
            )
        self.cv_splits_add_valid_hist_valid = []

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in ["y_valid_train", "y_test"]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
            ]:
                save_list_of_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store cv_splits_hist_train and cv_splits_hist_valid
            for file in [
                # "cv_splits_hist_train",
                "cv_splits_hist_valid"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then cv_splits_hist_train and cv_splits_hist_valid are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

            # store cv_splits_add_valid_hist_valid
            data = self.cv_splits_add_valid_hist_valid
            # if we do not reshuffle, then cv_splits_add_valid_hist_valid is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_cv_splits_add_valid_hist_valid.parquet",
                ),
                data=data,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        Note: Only one metric is used for HEBO.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if trial.study.sampler.fallback_triggered:
            trial.set_user_attr("hebo_fallback_triggered", True)
        else:
            trial.set_user_attr("hebo_fallback_triggered", False)

        if self.reshuffle:
            self.cv = [
                StratifiedKFold(
                    n_splits=self.n_splits,
                    shuffle=True,
                    random_state=self.seed + (trial.number * 500000) + (i * 1000),
                )
                for i in range(self.n_repeats)
            ]
            self.cv_splits = []
            for cv in self.cv:
                self.cv_splits.append(
                    list(cv.split(self.x_valid_train, self.y_valid_train))
                )

            # partition add_valid_use data into n_splits folds and repeat it n_repeats times
            self.cv_add_valid = [
                StratifiedKFold(
                    n_splits=self.n_splits,
                    shuffle=True,
                    random_state=self.seed + (trial.number * 500000) + (i * 1000),
                )
                for i in range(self.n_repeats)
            ]
            self.cv_splits_add_valid = []
            for cv in self.cv_add_valid:
                self.cv_splits_add_valid.append(
                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))
                )

        cv_splits_hist_train_tmp = []
        cv_splits_hist_valid_tmp = []
        cv_splits_add_valid_hist_valid_tmp = []
        y_train_hist_tmp = []
        y_valid_hist_tmp = []
        y_add_valid_hist_tmp = []

        # for each repeat and each fold fit the classifier and predict on the train, valid, test and add_valid set and compute the metric
        predictions = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        predictions_proba = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        cv_metric_train = []
        cv_metric_valid = []
        cv_metric_add_valid = []
        cv_metric_test = []

        for repeat in range(self.n_repeats):
            for fold in range(self.n_splits):
                train_index, valid_index = self.cv_splits[repeat][fold]
                cv_splits_hist_train_tmp.append(train_index)
                cv_splits_hist_valid_tmp.append(valid_index)
                x_train = self.x_valid_train.iloc[train_index]
                x_valid = self.x_valid_train.iloc[valid_index]
                y_train = self.y_valid_train[train_index]
                y_valid = self.y_valid_train[valid_index]
                y_train_hist_tmp.append(y_train)
                y_valid_hist_tmp.append(y_valid)

                add_valid_index = self.cv_splits_add_valid[repeat][fold][1]
                cv_splits_add_valid_hist_valid_tmp.append(
                    np.concatenate([valid_index, add_valid_index])
                )
                x_add_valid, y_add_valid = construct_x_and_y_add_valid(
                    x_valid=x_valid,
                    y_valid=y_valid,
                    x_add_valid=self.x_add_valid_use.iloc[add_valid_index],
                    y_add_valid=self.y_add_valid_use[add_valid_index],
                )
                y_add_valid_hist_tmp.append(y_add_valid)

                self.classifier.fit(
                    trial=trial,
                    x_train=x_train,
                    y_train=y_train,
                    x_valid=x_valid,
                    y_valid=y_valid,
                    cat_features=self.cat_features,
                )
                for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
                    if data == "x_test":
                        predictions[data].append(
                            self.classifier.predict(getattr(self, data))
                        )
                        predictions_proba[data].append(
                            self.classifier.predict_proba(getattr(self, data))
                        )
                    else:
                        predictions[data].append(self.classifier.predict(eval(data)))
                        predictions_proba[data].append(
                            self.classifier.predict_proba(eval(data))
                        )
                cv_metric_train.append(
                    compute_metric(
                        y_train,
                        y_pred=predictions["x_train"][-1],
                        y_pred_proba=predictions_proba["x_train"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metric_valid.append(
                    compute_metric(
                        y_valid,
                        y_pred=predictions["x_valid"][-1],
                        y_pred_proba=predictions_proba["x_valid"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metric_add_valid.append(
                    compute_metric(
                        y_add_valid,
                        y_pred=predictions["x_add_valid"][-1],
                        y_pred_proba=predictions_proba["x_add_valid"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metric_test.append(
                    compute_metric(
                        self.y_test,
                        y_pred=predictions["x_test"][-1],
                        y_pred_proba=predictions_proba["x_test"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )

        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)
        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)
        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)
        self.y_train_hist.append(y_train_hist_tmp)
        self.y_valid_hist.append(y_valid_hist_tmp)
        self.y_add_valid_hist.append(y_add_valid_hist_tmp)
        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        cv_metric = dict(
            (value, key) for key, value in self.cv_metric_to_metric.items()
        )[self.metric]
        trial.set_user_attr(
            f"{cv_metric}_train",
            json.dumps(cv_metric_train, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_valid",
            json.dumps(cv_metric_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_add_valid",
            json.dumps(cv_metric_add_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_test",
            json.dumps(cv_metric_test, cls=NumpyArrayEncoder),
        )

        # compute the mean of the metric over the folds and repeats
        metric_train = np.mean(cv_metric_train)
        metric_valid = np.mean(cv_metric_valid)
        metric_add_valid = np.mean(cv_metric_add_valid)
        metric_test = np.mean(cv_metric_test)
        trial.set_user_attr(f"{self.metric}_train", metric_train)
        trial.set_user_attr(f"{self.metric}_valid", metric_valid)
        trial.set_user_attr(f"{self.metric}_add_valid", metric_add_valid)
        trial.set_user_attr(f"{self.metric}_test", metric_test)

        # compute the metric on the test set also in ensemble style
        predictions_proba_test_ensemble = np.mean(predictions_proba["x_test"], axis=0)
        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)
        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums
        check_y_predict_proba(predictions_proba_test_ensemble)
        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)
        metric_test_ensemble = compute_metric(
            self.y_test,
            y_pred=predictions_test_ensemble,
            y_pred_proba=predictions_proba_test_ensemble,
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_ensemble", metric_test_ensemble)

        if self.bootstrap_test:
            # bootstrap the ensemble style test performance
            metric_test_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions_test_ensemble,
                y_pred_proba=predictions_proba_test_ensemble,
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap", metric_test_bootstrap
            )
            average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                metric_test_bootstrap
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap_average",
                average_metric_test_bootstrap,
            )

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metric
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metric_valid_train = compute_metric(
            self.y_valid_train,
            y_pred=predictions["x_valid_train"],
            y_pred_proba=predictions_proba["x_valid_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_valid_train", metric_valid_train)

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metric
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metric_test_retrained = compute_metric(
            self.y_test,
            y_pred=predictions["x_test_retrained"],
            y_pred_proba=predictions_proba["x_test_retrained"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_retrained", metric_test_retrained)

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            metric_test_retrained_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap",
                metric_test_retrained_bootstrap,
            )
            average_metric_test_retrained_bootstrap = sum(
                metric_test_retrained_bootstrap
            ) / len(metric_test_retrained_bootstrap)
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap_average",
                average_metric_test_retrained_bootstrap,
            )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation metric
        # Note: Here, we assume maximization but HeboSampler assumes minimization and will correct for it
        if self.metrics_direction[self.metric] == "minimize":
            return -metric_valid
        else:
            return metric_valid

File Path: reshufflebench/learner/learner_hebo_holdout.py
Content:
import os

from optuna.trial import Trial
from sklearn.model_selection import train_test_split

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_hebo import LearnerHebo
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    bootstrap_test_performance,
    construct_x_and_y_add_valid,
    save_list_of_1d_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerHeboHoldout(LearnerHebo):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: float,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_hebo_{classifier.classifier_id}_{data_id}_holdout{str(valid_frac).replace('.', '')}_reshuffle_{str(train_valid_size)}_{str(test_size)}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_hebo_{classifier.classifier_id}_{data_id}_holdout{str(valid_frac).replace('.', '')}_{str(train_valid_size)}_{str(test_size)}",
                )
            )
        super().__init__(
            classifier=classifier,
            metric=metric,
            data_id=data_id,
            valid_type="holdout",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=valid_frac,
            n_splits=None,
            n_repeats=None,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=None,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(
            self.x_valid_train,
            self.y_valid_train,
            test_size=self.valid_size,
            random_state=self.seed,
            stratify=self.y_valid_train,
        )
        self.train_size = self.x_train.shape[0]

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in [
                # "y_valid_train",
                "y_test"
            ]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        Note: Only one metric is used for HEBO.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if trial.study.sampler.fallback_triggered:
            trial.set_user_attr("hebo_fallback_triggered", True)
        else:
            trial.set_user_attr("hebo_fallback_triggered", False)

        if self.reshuffle:
            self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(
                self.x_valid_train,
                self.y_valid_train,
                test_size=self.valid_size,
                random_state=self.seed + (trial.number * 500000),
                stratify=self.y_valid_train,
            )

        self.y_train_hist.append(self.y_train)
        self.y_valid_hist.append(self.y_valid)

        self.x_add_valid, self.y_add_valid = construct_x_and_y_add_valid(
            self.x_valid, self.y_valid, self.x_add_valid_use, self.y_add_valid_use
        )
        self.y_add_valid_hist.append(self.y_add_valid)

        # fit the classifier
        self.classifier.fit(
            trial=trial,
            x_train=self.x_train,
            y_train=self.y_train,
            x_valid=self.x_valid,
            y_valid=self.y_valid,
            cat_features=self.cat_features,
        )

        # predict on the train, valid, test and add_valid set and compute the metric
        predictions = {}
        predictions_proba = {}

        for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
            predictions[data] = self.classifier.predict(getattr(self, data))
            predictions_proba[data] = self.classifier.predict_proba(getattr(self, data))
        metric_train = compute_metric(
            self.y_train,
            y_pred=predictions["x_train"],
            y_pred_proba=predictions_proba["x_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        metric_valid = compute_metric(
            self.y_valid,
            y_pred=predictions["x_valid"],
            y_pred_proba=predictions_proba["x_valid"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        metric_add_valid = compute_metric(
            self.y_add_valid,
            y_pred=predictions["x_add_valid"],
            y_pred_proba=predictions_proba["x_add_valid"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        metric_test = compute_metric(
            self.y_test,
            y_pred=predictions["x_test"],
            y_pred_proba=predictions_proba["x_test"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_train", metric_train)
        trial.set_user_attr(f"{self.metric}_valid", metric_valid)
        trial.set_user_attr(f"{self.metric}_add_valid", metric_add_valid)
        trial.set_user_attr(f"{self.metric}_test", metric_test)

        if self.bootstrap_test:
            # bootstrap the test performance
            metric_test_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test"],
                y_pred_proba=predictions_proba["x_test"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(f"{self.metric}_test_bootstrap", metric_test_bootstrap)
            average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                metric_test_bootstrap
            )
            trial.set_user_attr(
                f"{self.metric}_test_bootstrap_average", average_metric_test_bootstrap
            )

        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metric
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metric_valid_train = compute_metric(
            self.y_valid_train,
            y_pred=predictions["x_valid_train"],
            y_pred_proba=predictions_proba["x_valid_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_valid_train", metric_valid_train)

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metric
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metric_test_retrained = compute_metric(
            self.y_test,
            y_pred=predictions["x_test_retrained"],
            y_pred_proba=predictions_proba["x_test_retrained"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_retrained", metric_test_retrained)

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            metric_test_retrained_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap",
                metric_test_retrained_bootstrap,
            )
            average_metric_test_retrained_bootstrap = sum(
                metric_test_retrained_bootstrap
            ) / len(metric_test_retrained_bootstrap)
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap_average",
                average_metric_test_retrained_bootstrap,
            )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation metric
        # Note: Here, we assume maximization but HeboSampler assumes minimization and will correct for it
        if self.metrics_direction[self.metric] == "minimize":
            return -metric_valid
        else:
            return metric_valid

File Path: reshufflebench/learner/learner_hebo_repeatedholdout.py
Content:
import json
import os

import numpy as np
from optuna.trial import Trial
from sklearn.model_selection import StratifiedShuffleSplit

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_hebo import LearnerHebo
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    NumpyArrayEncoder,
    bootstrap_test_performance,
    check_y_predict_proba,
    construct_x_and_y_add_valid,
    save_list_of_list_of_1d_arrays,
    save_list_of_list_of_pd_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerHeboRepeatedHoldout(LearnerHebo):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: float,
        n_repeats: int,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        # Note: add_valid size is the size of the total additional validation set, not the size of the additional validation set per fold
        #       this is somewhat in contrast to LearnerRandomHoldout, where add_valid_size is the size of the additional validation set
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_hebo_{classifier.classifier_id}_{data_id}_repeatedholdout{str(valid_frac).replace('.', '')}_{n_repeats}_reshuffle_{train_valid_size}_{test_size}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_hebo_{classifier.classifier_id}_{data_id}_repeatedholdout{str(valid_frac).replace('.', '')}_{n_repeats}_{train_valid_size}_{test_size}",
                )
            )

        cv_metric_to_metric = {
            "accuracies": "accuracy",
            "balanced_accuracies": "balanced_accuracy",
            "loglosses": "logloss",
            "aucs": "auc",
        }

        super().__init__(
            classifier=classifier,
            metric=metric,
            data_id=data_id,
            valid_type="repeatedholdout",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=valid_frac,
            n_splits=None,
            n_repeats=n_repeats,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=cv_metric_to_metric,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        self.cv = StratifiedShuffleSplit(
            n_splits=self.n_repeats, test_size=self.valid_size, random_state=self.seed
        )
        self.cv_splits = list(self.cv.split(self.x_valid_train, self.y_valid_train))
        self.train_size = self.y_valid_train.shape[0] - self.valid_size
        self.cv_splits_hist_train = []
        self.cv_splits_hist_valid = []

        # split add_valid_use data and repeat it n_repeats times
        self.cv_add_valid = StratifiedShuffleSplit(
            n_splits=self.n_repeats,
            test_size=int(self.add_valid_size / self.n_repeats),
            random_state=self.seed,
        )
        self.cv_splits_add_valid = list(
            self.cv_add_valid.split(self.x_add_valid_use, self.y_add_valid_use)
        )
        self.cv_splits_add_valid_hist_valid = []

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in ["y_valid_train", "y_test"]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
            ]:
                save_list_of_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store cv_splits_hist_train and cv_splits_hist_valid
            for file in [
                # "cv_splits_hist_train",
                "cv_splits_hist_valid"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then cv_splits_hist_train and cv_splits_hist_valid are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

            # store cv_splits_add_valid_hist_valid
            data = self.cv_splits_add_valid_hist_valid
            # if we do not reshuffle, then cv_splits_add_valid_hist_valid is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_cv_splits_add_valid_hist_valid.parquet",
                ),
                data=data,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        Note: Only one metric is used for HEBO.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if trial.study.sampler.fallback_triggered:
            trial.set_user_attr("hebo_fallback_triggered", True)
        else:
            trial.set_user_attr("hebo_fallback_triggered", False)

        if self.reshuffle:
            self.cv = StratifiedShuffleSplit(
                n_splits=self.n_repeats,
                test_size=self.valid_size,
                random_state=self.seed + (trial.number * 500000),
            )
            self.cv_splits = list(self.cv.split(self.x_valid_train, self.y_valid_train))

            # split add_valid_use data and repeat it n_repeats times
            self.cv_add_valid = StratifiedShuffleSplit(
                n_splits=self.n_repeats,
                test_size=int(self.add_valid_size / self.n_repeats),
                random_state=self.seed + (trial.number * 500000),
            )
            self.cv_splits_add_valid = list(
                self.cv_add_valid.split(self.x_add_valid_use, self.y_add_valid_use)
            )

        cv_splits_hist_train_tmp = []
        cv_splits_hist_valid_tmp = []
        cv_splits_add_valid_hist_valid_tmp = []
        y_train_hist_tmp = []
        y_valid_hist_tmp = []
        y_add_valid_hist_tmp = []

        # for each repeat fit the classifier and predict on the train, valid, test and add_valid set and compute the metric
        predictions = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        predictions_proba = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        cv_metric_train = []
        cv_metric_valid = []
        cv_metric_add_valid = []
        cv_metric_test = []

        for repeat in range(self.n_repeats):
            train_index, valid_index = self.cv_splits[repeat]
            cv_splits_hist_train_tmp.append(train_index)
            cv_splits_hist_valid_tmp.append(valid_index)
            x_train = self.x_valid_train.iloc[train_index]
            x_valid = self.x_valid_train.iloc[valid_index]
            y_train = self.y_valid_train[train_index]
            y_valid = self.y_valid_train[valid_index]
            y_train_hist_tmp.append(y_train)
            y_valid_hist_tmp.append(y_valid)

            add_valid_index = self.cv_splits_add_valid[repeat][1]
            cv_splits_add_valid_hist_valid_tmp.append(
                np.concatenate([valid_index, add_valid_index])
            )
            x_add_valid, y_add_valid = construct_x_and_y_add_valid(
                x_valid=x_valid,
                y_valid=y_valid,
                x_add_valid=self.x_add_valid_use.iloc[add_valid_index],
                y_add_valid=self.y_add_valid_use[add_valid_index],
            )
            y_add_valid_hist_tmp.append(y_add_valid)

            self.classifier.fit(
                trial=trial,
                x_train=x_train,
                y_train=y_train,
                x_valid=x_valid,
                y_valid=y_valid,
                cat_features=self.cat_features,
            )
            for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
                if data == "x_test":
                    predictions[data].append(
                        self.classifier.predict(getattr(self, data))
                    )
                    predictions_proba[data].append(
                        self.classifier.predict_proba(getattr(self, data))
                    )
                else:
                    predictions[data].append(self.classifier.predict(eval(data)))
                    predictions_proba[data].append(
                        self.classifier.predict_proba(eval(data))
                    )
            cv_metric_train.append(
                compute_metric(
                    y_train,
                    y_pred=predictions["x_train"][-1],
                    y_pred_proba=predictions_proba["x_train"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )
            cv_metric_valid.append(
                compute_metric(
                    y_valid,
                    y_pred=predictions["x_valid"][-1],
                    y_pred_proba=predictions_proba["x_valid"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )
            cv_metric_add_valid.append(
                compute_metric(
                    y_add_valid,
                    y_pred=predictions["x_add_valid"][-1],
                    y_pred_proba=predictions_proba["x_add_valid"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )
            cv_metric_test.append(
                compute_metric(
                    self.y_test,
                    y_pred=predictions["x_test"][-1],
                    y_pred_proba=predictions_proba["x_test"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )

        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)
        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)
        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)
        self.y_train_hist.append(y_train_hist_tmp)
        self.y_valid_hist.append(y_valid_hist_tmp)
        self.y_add_valid_hist.append(y_add_valid_hist_tmp)
        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        cv_metric = dict(
            (value, key) for key, value in self.cv_metric_to_metric.items()
        )[self.metric]
        trial.set_user_attr(
            f"{cv_metric}_train",
            json.dumps(cv_metric_train, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_valid",
            json.dumps(cv_metric_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_add_valid",
            json.dumps(cv_metric_add_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_test",
            json.dumps(cv_metric_test, cls=NumpyArrayEncoder),
        )

        # compute the mean of the metric over the folds and repeats
        metric_train = np.mean(cv_metric_train)
        metric_valid = np.mean(cv_metric_valid)
        metric_add_valid = np.mean(cv_metric_add_valid)
        metric_test = np.mean(cv_metric_test)
        trial.set_user_attr(f"{self.metric}_train", metric_train)
        trial.set_user_attr(f"{self.metric}_valid", metric_valid)
        trial.set_user_attr(f"{self.metric}_add_valid", metric_add_valid)
        trial.set_user_attr(f"{self.metric}_test", metric_test)

        # compute the metric on the test set also in ensemble style
        predictions_proba_test_ensemble = np.mean(predictions_proba["x_test"], axis=0)
        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)
        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums
        check_y_predict_proba(predictions_proba_test_ensemble)
        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)
        metric_test_ensemble = compute_metric(
            self.y_test,
            y_pred=predictions_test_ensemble,
            y_pred_proba=predictions_proba_test_ensemble,
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_ensemble", metric_test_ensemble)

        if self.bootstrap_test:
            # bootstrap the ensemble style test performance
            metric_test_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions_test_ensemble,
                y_pred_proba=predictions_proba_test_ensemble,
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap", metric_test_bootstrap
            )
            average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                metric_test_bootstrap
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap_average",
                average_metric_test_bootstrap,
            )

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metric
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metric_valid_train = compute_metric(
            self.y_valid_train,
            y_pred=predictions["x_valid_train"],
            y_pred_proba=predictions_proba["x_valid_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_valid_train", metric_valid_train)

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metric
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metric_test_retrained = compute_metric(
            self.y_test,
            y_pred=predictions["x_test_retrained"],
            y_pred_proba=predictions_proba["x_test_retrained"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_retrained", metric_test_retrained)

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            metric_test_retrained_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap",
                metric_test_retrained_bootstrap,
            )
            average_metric_test_retrained_bootstrap = sum(
                metric_test_retrained_bootstrap
            ) / len(metric_test_retrained_bootstrap)
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap_average",
                average_metric_test_retrained_bootstrap,
            )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation metric
        # Note: Here, we assume maximization but HeboSampler assumes minimization and will correct for it
        if self.metrics_direction[self.metric] == "minimize":
            return -metric_valid
        else:
            return metric_valid

File Path: reshufflebench/learner/learner_random.py
Content:
import json
import os
import random
from abc import ABC, abstractmethod
from typing import Union

import numpy as np
import openml
import optuna
import pandas as pd
from optuna.trial import Trial
from sklearn.datasets import make_classification, make_hastie_10_2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.utils import unify_missing_values


class LearnerRandom(ABC):
    def __init__(
        self,
        classifier: Classifier,
        data_id: int,
        valid_type: str,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: Union[float, None],
        n_splits: Union[int, None],
        n_repeats: Union[int, None],
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
        results_path: str,
        cv_metric_to_metric: Union[dict, None],
        write_results_to_disk: bool = True,
        bootstrap_test: bool = False,
    ):
        self.metrics = ["accuracy", "balanced_accuracy", "logloss", "auc"]
        self.metrics_direction = {
            "accuracy": "maximize",
            "balanced_accuracy": "maximize",
            "logloss": "minimize",
            "auc": "maximize",
        }
        self.cv_metric_to_metric = cv_metric_to_metric

        self.classifier = classifier
        self.data_id = data_id
        self.valid_type = valid_type
        self.train_valid_size = train_valid_size
        self.reshuffle = reshuffle
        self.valid_frac = valid_frac
        self.valid_size = (
            int(self.train_valid_size * self.valid_frac)
            if self.valid_frac is not None
            else None
        )
        self.n_splits = n_splits
        self.n_repeats = n_repeats
        self.test_size = test_size
        self.add_valid_size = add_valid_size
        self.n_trials = n_trials
        self.seed = seed
        self.results_path = results_path
        self.file_name = f"study_seed_{self.seed}"
        self.write_results_to_disk = write_results_to_disk
        self.bootstrap_test = bootstrap_test

        self.label_encoder = None

        self.has_missings = False
        self.labels = None
        self.n_classes = None
        self.multiclass = False
        self.cat_features = None
        self.num_features = None
        self.x_valid_train = None
        self.x_test = None
        self.x_train = None
        self.x_valid = None
        self.y_valid_train = None
        self.y_test = None
        self.y_train = None
        self.y_valid = None
        self.y_train_hist = None
        self.y_valid_hist = None
        self.y_pred_train_proba_hist = None
        self.y_pred_valid_proba_hist = None
        self.y_pred_test_proba_hist = None
        self.y_pred_valid_train_proba_hist = None
        self.y_pred_test_proba_retrained_hist = None

        self.cv = None
        self.cv_splits = None
        self.cv_splits_hist_train = None
        self.cv_splits_hist_valid = None
        self.train_size = None

        self.x_add_valid_use = None
        self.y_add_valid_use = None
        self.x_add_valid = None
        self.y_add_valid = None
        self.y_add_valid_hist = None
        self.y_pred_add_valid_proba_hist = None

        self.cv_add_valid = None
        self.cv_splits_add_valid = None
        self.cv_splits_add_valid_hist_valid = None

        self.study = None
        self.study_name = None
        self.storage = None

    def prepare_data(self) -> None:
        """
        Prepare the data for the optimization.
        """

        if self.data_id == 99999:
            # generate data for testing
            x, y = make_hastie_10_2(
                n_samples=self.train_valid_size
                + self.test_size
                + self.add_valid_size
                + 1000,
                random_state=0,
            )
            x = pd.DataFrame(x)
            x.columns = [f"feature_{i}" for i in range(x.shape[1])]
            y = y.astype(int)
            y = pd.Series(y, dtype="category")
            y.columns = ["target"]
            categorical_indicator = [False] * x.shape[1]
        elif self.data_id == 11111:
            # generate data for testing
            x, y = make_classification(
                n_samples=self.train_valid_size
                + self.test_size
                + self.add_valid_size
                + 1000,
                n_features=20,
                n_informative=10,
                n_redundant=2,
                n_repeated=2,
                n_classes=2,
                n_clusters_per_class=4,
                weights=None,
                flip_y=0.1,
                class_sep=1.0,
                hypercube=True,
                shift=0.0,
                scale=1.0,
                shuffle=True,
                random_state=0,
            )
            x = pd.DataFrame(x)
            x.columns = [f"feature_{i}" for i in range(x.shape[1])]
            y = y.astype(int)
            y = pd.Series(y, dtype="category")
            y.columns = ["target"]
            categorical_indicator = [False] * x.shape[1]
        else:
            # download the OpenML data for the dataset
            dataset = openml.datasets.get_dataset(
                self.data_id,
                download_data=True,
                download_qualities=False,
                download_features_meta_data=False,
            )

            # get the pandas dataframe
            x, y, categorical_indicator, _ = dataset.get_data(
                dataset_format="dataframe",
                target=dataset.default_target_attribute,
            )

        # check if dataset is large enough for train_valid_size, test_size and add_valid_size
        if (
            x.shape[0]
            < self.train_valid_size + self.test_size + self.add_valid_size + 1000
        ):
            raise ValueError(
                "Dataset too small for train_valid_size, test_size and add_valid_size"
            )

        # check if feature names contain "__"
        if any("__" in col for col in x.columns):
            raise ValueError(
                'Feature names contain "__" which is not allowed due to preprocessing'
            )

        # convert categorical columns to object dtype
        for col in range(len(categorical_indicator)):
            if categorical_indicator[col]:
                x[x.columns[col]] = x[x.columns[col]].astype("object")
        self.cat_features = [
            x.columns.get_loc(col) for col in x.columns if x[col].dtype.name == "object"
        ]
        self.num_features = list(set(range(x.shape[1])) - set(self.cat_features))

        # check if missing values are present
        # if yes bring missing values into a standardized format
        if x.isnull().values.any():
            self.has_missings = True
            x = unify_missing_values(x)

        if y.isnull().values.any():
            raise ValueError("Missing values in target column")

        # map the labels to integers
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(y)
        self.labels = np.unique(y).tolist()
        self.n_classes = len(self.label_encoder.classes_)
        if self.n_classes > 2:
            self.multiclass = True

        # split the data into used and unused data
        x_not_use, x_use, y_not_use, y_use = train_test_split(
            x,
            y,
            test_size=self.train_valid_size + self.test_size,
            random_state=self.seed,
            stratify=y,
        )
        # split the used data into valid_train and test
        x_valid_train, x_test, y_valid_train, y_test = train_test_split(
            x_use,
            y_use,
            test_size=self.test_size,
            random_state=self.seed,
            stratify=y_use,
        )
        # from the unused data sample add_valid_size data to use as additional validation data
        _, x_add_valid_use, _, y_add_valid_use = train_test_split(
            x_not_use,
            y_not_use,
            test_size=self.add_valid_size,
            random_state=self.seed,
            stratify=y_not_use,
        )

        self.x_valid_train = x_valid_train
        self.x_add_valid_use = x_add_valid_use
        self.x_test = x_test
        self.y_train_hist = []
        self.y_valid_hist = []
        self.y_add_valid_hist = []
        self.y_pred_train_proba_hist = []
        self.y_pred_valid_proba_hist = []
        self.y_pred_test_proba_hist = []
        self.y_pred_add_valid_proba_hist = []
        self.y_pred_valid_train_proba_hist = []
        self.y_pred_test_proba_retrained_hist = []
        self.y_valid_train = y_valid_train
        self.y_add_valid_use = y_add_valid_use
        self.y_test = y_test

    @abstractmethod
    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        pass

    def prepare_study(self) -> None:
        """
        Prepare the study for the optimization.
        """
        # if results_path does not exist, create it
        if not os.path.exists(self.results_path):
            try:
                os.makedirs(self.results_path)
            except FileExistsError:
                pass
        study_name = os.path.join(self.results_path, self.file_name)
        storage = f"sqlite:///{study_name}.db"
        # raise warning if study already exists
        if os.path.exists(f"{study_name}.db"):
            print(f"WARNING: Study {study_name} already exists. Overwriting it.")
            # remove the study
            os.remove(f"{study_name}.db")
        self.study = optuna.create_study(
            storage=storage,
            sampler=optuna.samplers.RandomSampler(seed=self.seed),
            study_name=study_name,
            direction="maximize",
        )
        self.study_name = study_name
        self.storage = storage

    @abstractmethod
    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        """
        pass

    @abstractmethod
    def store_results(self) -> None:
        """
        Store additional results.
        """
        pass

    def run(self) -> None:
        """
        Finish all preparations.
        Run the optimization.
        Stores the results in parquet along the study database.
        """
        self.prepare_data()

        self.prepare_resampling()

        self.prepare_study()

        # write a file with the parameters
        params = {
            "classifier": self.classifier.classifier_id,
            "data_id": self.data_id,
            "valid_type": self.valid_type,
            "train_valid_size": self.train_valid_size,
            "reshuffle": self.reshuffle,
            "n_splits": self.n_splits,
            "n_repeats": self.n_repeats,
            "valid_frac": self.valid_frac,
            "test_size": self.test_size,
            "n_trials": self.n_trials,
            "file_name": self.file_name,
            "seed": self.seed,
            "has_missings": self.has_missings,
            "labels": self.labels,
            "n_classes": self.n_classes,
            "multiclass": self.multiclass,
            "metrics": self.metrics,
            "metrics_direction": self.metrics_direction,
            "cv_metric_to_metric": self.cv_metric_to_metric,
            "results_path": self.results_path,
            "study_name": self.study_name,
            "storage": self.storage,
        }
        with open(
            os.path.join(self.results_path, f"{self.file_name}_params.json"), "w"
        ) as f:
            json.dump(params, f, indent=4)

        np.random.seed(self.seed)
        random.seed(self.seed)
        self.study.optimize(self.objective, n_trials=self.n_trials)

        self.store_results()

File Path: reshufflebench/learner/learner_random_cv.py
Content:
import json
import os

import numpy as np
from optuna.trial import Trial
from sklearn.model_selection import StratifiedKFold

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_random import LearnerRandom
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    NumpyArrayEncoder,
    bootstrap_test_performance,
    check_y_predict_proba,
    construct_x_and_y_add_valid,
    save_list_of_list_of_1d_arrays,
    save_list_of_list_of_pd_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerRandomCV(LearnerRandom):
    def __init__(
        self,
        classifier: Classifier,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        n_splits: int,
        n_repeats: int,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        # Note: add_valid size is the size of the total additional validation set, not the size of the additional validation set per fold
        #       this is somewhat in contrast to LearnerRandomHoldout, where add_valid_size is the size of the additional validation set
        #       this is also in contrast to valid_size being the size of the validation set per fold in this class
        if train_valid_size % n_splits != 0:
            raise ValueError("train_valid_size must be divisible by n_splits")
        if add_valid_size % n_splits != 0:
            raise ValueError("add_valid_size must be divisible by n_splits")
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_{classifier.classifier_id}_{data_id}_cv{n_splits}r{n_repeats}_reshuffle_{train_valid_size}_{test_size}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_{classifier.classifier_id}_{data_id}_cv{n_splits}r{n_repeats}_{train_valid_size}_{test_size}",
                )
            )

        cv_metric_to_metric = {
            "accuracies": "accuracy",
            "balanced_accuracies": "balanced_accuracy",
            "loglosses": "logloss",
            "aucs": "auc",
        }

        super().__init__(
            classifier=classifier,
            data_id=data_id,
            valid_type="cv",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=None,
            n_splits=n_splits,
            n_repeats=n_repeats,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=cv_metric_to_metric,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        # we use StratifiedKFold for the cross-validation and manually repeat it n_repeats times because this is somewhat easier to handle here than using RepeatedStratifiedKFold
        self.cv = [
            StratifiedKFold(
                n_splits=self.n_splits,
                shuffle=True,
                random_state=self.seed + (i * 1000),
            )
            for i in range(self.n_repeats)
        ]
        self.cv_splits = []
        for cv in self.cv:
            self.cv_splits.append(
                list(cv.split(self.x_valid_train, self.y_valid_train))
            )
        self.train_size = int(
            self.x_valid_train.shape[0] / self.n_splits * (self.n_splits - 1)
        )
        self.valid_size = int(self.x_valid_train.shape[0] / self.n_splits)
        self.cv_splits_hist_train = []
        self.cv_splits_hist_valid = []

        # partition add_valid_use data into n_splits folds and repeat it n_repeats times
        self.cv_add_valid = [
            StratifiedKFold(
                n_splits=self.n_splits,
                shuffle=True,
                random_state=self.seed + (i * 1000),
            )
            for i in range(self.n_repeats)
        ]
        self.cv_splits_add_valid = []
        for cv in self.cv_add_valid:
            self.cv_splits_add_valid.append(
                list(cv.split(self.x_add_valid_use, self.y_add_valid_use))
            )
        self.cv_splits_add_valid_hist_valid = []

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in ["y_valid_train", "y_test"]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
            ]:
                save_list_of_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store cv_splits_hist_train and cv_splits_hist_valid
            for file in [
                # "cv_splits_hist_train",
                "cv_splits_hist_valid"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then cv_splits_hist_train and cv_splits_hist_valid are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

            # store cv_splits_add_valid_hist_valid
            data = self.cv_splits_add_valid_hist_valid
            # if we do not reshuffle, then cv_splits_add_valid_hist_valid is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_cv_splits_add_valid_hist_valid.parquet",
                ),
                data=data,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if self.reshuffle:
            self.cv = [
                StratifiedKFold(
                    n_splits=self.n_splits,
                    shuffle=True,
                    random_state=self.seed + (trial.number * 500000) + (i * 1000),
                )
                for i in range(self.n_repeats)
            ]
            self.cv_splits = []
            for cv in self.cv:
                self.cv_splits.append(
                    list(cv.split(self.x_valid_train, self.y_valid_train))
                )

            # partition add_valid_use data into n_splits folds and repeat it n_repeats times
            self.cv_add_valid = [
                StratifiedKFold(
                    n_splits=self.n_splits,
                    shuffle=True,
                    random_state=self.seed + (trial.number * 500000) + (i * 1000),
                )
                for i in range(self.n_repeats)
            ]
            self.cv_splits_add_valid = []
            for cv in self.cv_add_valid:
                self.cv_splits_add_valid.append(
                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))
                )

        cv_splits_hist_train_tmp = []
        cv_splits_hist_valid_tmp = []
        cv_splits_add_valid_hist_valid_tmp = []
        y_train_hist_tmp = []
        y_valid_hist_tmp = []
        y_add_valid_hist_tmp = []

        # for each repeat and each fold fit the classifier and predict on the train, valid, test and add_valid set and compute the metrics
        predictions = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        predictions_proba = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        cv_metrics_train = dict([(metric, []) for metric in self.metrics])
        cv_metrics_valid = dict([(metric, []) for metric in self.metrics])
        cv_metrics_add_valid = dict([(metric, []) for metric in self.metrics])
        cv_metrics_test = dict([(metric, []) for metric in self.metrics])

        for repeat in range(self.n_repeats):
            for fold in range(self.n_splits):
                train_index, valid_index = self.cv_splits[repeat][fold]
                cv_splits_hist_train_tmp.append(train_index)
                cv_splits_hist_valid_tmp.append(valid_index)
                x_train = self.x_valid_train.iloc[train_index]
                x_valid = self.x_valid_train.iloc[valid_index]
                y_train = self.y_valid_train[train_index]
                y_valid = self.y_valid_train[valid_index]
                y_train_hist_tmp.append(y_train)
                y_valid_hist_tmp.append(y_valid)

                add_valid_index = self.cv_splits_add_valid[repeat][fold][1]
                cv_splits_add_valid_hist_valid_tmp.append(
                    np.concatenate([valid_index, add_valid_index])
                )
                x_add_valid, y_add_valid = construct_x_and_y_add_valid(
                    x_valid=x_valid,
                    y_valid=y_valid,
                    x_add_valid=self.x_add_valid_use.iloc[add_valid_index],
                    y_add_valid=self.y_add_valid_use[add_valid_index],
                )
                y_add_valid_hist_tmp.append(y_add_valid)

                self.classifier.fit(
                    trial=trial,
                    x_train=x_train,
                    y_train=y_train,
                    x_valid=x_valid,
                    y_valid=y_valid,
                    cat_features=self.cat_features,
                )
                for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
                    if data == "x_test":
                        predictions[data].append(
                            self.classifier.predict(getattr(self, data))
                        )
                        predictions_proba[data].append(
                            self.classifier.predict_proba(getattr(self, data))
                        )
                    else:
                        predictions[data].append(self.classifier.predict(eval(data)))
                        predictions_proba[data].append(
                            self.classifier.predict_proba(eval(data))
                        )
                for metric in self.metrics:
                    cv_metrics_train[metric].append(
                        compute_metric(
                            y_train,
                            y_pred=predictions["x_train"][-1],
                            y_pred_proba=predictions_proba["x_train"][-1],
                            metric=metric,
                            labels=self.labels,
                            multiclass=self.multiclass,
                        )
                    )
                    cv_metrics_valid[metric].append(
                        compute_metric(
                            y_valid,
                            y_pred=predictions["x_valid"][-1],
                            y_pred_proba=predictions_proba["x_valid"][-1],
                            metric=metric,
                            labels=self.labels,
                            multiclass=self.multiclass,
                        )
                    )
                    cv_metrics_add_valid[metric].append(
                        compute_metric(
                            y_add_valid,
                            y_pred=predictions["x_add_valid"][-1],
                            y_pred_proba=predictions_proba["x_add_valid"][-1],
                            metric=metric,
                            labels=self.labels,
                            multiclass=self.multiclass,
                        )
                    )
                    cv_metrics_test[metric].append(
                        compute_metric(
                            self.y_test,
                            y_pred=predictions["x_test"][-1],
                            y_pred_proba=predictions_proba["x_test"][-1],
                            metric=metric,
                            labels=self.labels,
                            multiclass=self.multiclass,
                        )
                    )

        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)
        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)
        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)
        self.y_train_hist.append(y_train_hist_tmp)
        self.y_valid_hist.append(y_valid_hist_tmp)
        self.y_add_valid_hist.append(y_add_valid_hist_tmp)
        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        for cv_metric in self.cv_metric_to_metric.keys():
            metric = self.cv_metric_to_metric[cv_metric]
            trial.set_user_attr(
                f"{cv_metric}_train",
                json.dumps(cv_metrics_train[metric], cls=NumpyArrayEncoder),
            )
            trial.set_user_attr(
                f"{cv_metric}_valid",
                json.dumps(cv_metrics_valid[metric], cls=NumpyArrayEncoder),
            )
            trial.set_user_attr(
                f"{cv_metric}_add_valid",
                json.dumps(cv_metrics_add_valid[metric], cls=NumpyArrayEncoder),
            )
            trial.set_user_attr(
                f"{cv_metric}_test",
                json.dumps(cv_metrics_test[metric], cls=NumpyArrayEncoder),
            )

        # compute the mean of the metrics over the folds and repeats
        metrics_train = {}
        metrics_valid = {}
        metrics_add_valid = {}
        metrics_test = {}

        for metric in self.metrics:
            metrics_train[metric] = np.mean(cv_metrics_train[metric])
            metrics_valid[metric] = np.mean(cv_metrics_valid[metric])
            metrics_add_valid[metric] = np.mean(cv_metrics_add_valid[metric])
            metrics_test[metric] = np.mean(cv_metrics_test[metric])
            trial.set_user_attr(f"{metric}_train", metrics_train[metric])
            trial.set_user_attr(f"{metric}_valid", metrics_valid[metric])
            trial.set_user_attr(f"{metric}_add_valid", metrics_add_valid[metric])
            trial.set_user_attr(f"{metric}_test", metrics_test[metric])

        # compute the metrics on the test set also in ensemble style
        metrics_test_ensemble = {}
        predictions_proba_test_ensemble = np.mean(predictions_proba["x_test"], axis=0)
        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)
        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums
        check_y_predict_proba(predictions_proba_test_ensemble)
        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)
        for metric in self.metrics:
            metrics_test_ensemble[metric] = compute_metric(
                self.y_test,
                y_pred=predictions_test_ensemble,
                y_pred_proba=predictions_proba_test_ensemble,
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(
                f"{metric}_test_ensemble", metrics_test_ensemble[metric]
            )

        if self.bootstrap_test:
            # bootstrap the ensemble style test performance
            for metric in self.metrics:
                metric_test_bootstrap = bootstrap_test_performance(
                    y_test=self.y_test,
                    y_pred=predictions_test_ensemble,
                    y_pred_proba=predictions_proba_test_ensemble,
                    metric=metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                    seed=self.seed,
                )
                trial.set_user_attr(
                    f"{metric}_test_ensemble_bootstrap", metric_test_bootstrap
                )
                average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                    metric_test_bootstrap
                )
                trial.set_user_attr(
                    f"{metric}_test_ensemble_bootstrap_average",
                    average_metric_test_bootstrap,
                )

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metrics
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metrics_valid_train = {}
        for metric in self.metrics:
            metrics_valid_train[metric] = compute_metric(
                self.y_valid_train,
                y_pred=predictions["x_valid_train"],
                y_pred_proba=predictions_proba["x_valid_train"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(f"{metric}_valid_train", metrics_valid_train[metric])

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metrics
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metrics_test_retrained = {}
        for metric in self.metrics:
            metrics_test_retrained[metric] = compute_metric(
                self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(
                f"{metric}_test_retrained", metrics_test_retrained[metric]
            )

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            for metric in self.metrics:
                metric_test_retrained_bootstrap = bootstrap_test_performance(
                    y_test=self.y_test,
                    y_pred=predictions["x_test_retrained"],
                    y_pred_proba=predictions_proba["x_test_retrained"],
                    metric=metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                    seed=self.seed,
                )
                trial.set_user_attr(
                    f"{metric}_test_retrained_bootstrap",
                    metric_test_retrained_bootstrap,
                )
                average_metric_test_retrained_bootstrap = sum(
                    metric_test_retrained_bootstrap
                ) / len(metric_test_retrained_bootstrap)
                trial.set_user_attr(
                    f"{metric}_test_retrained_bootstrap_average",
                    average_metric_test_retrained_bootstrap,
                )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation accuracy (mean over folds and repeats)
        return metrics_valid["accuracy"]

File Path: reshufflebench/learner/learner_random_holdout.py
Content:
import os

from optuna.trial import Trial
from sklearn.model_selection import train_test_split

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_random import LearnerRandom
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    bootstrap_test_performance,
    construct_x_and_y_add_valid,
    save_list_of_1d_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerRandomHoldout(LearnerRandom):
    def __init__(
        self,
        classifier: Classifier,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: float,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_{classifier.classifier_id}_{data_id}_holdout{str(valid_frac).replace('.', '')}_reshuffle_{str(train_valid_size)}_{str(test_size)}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_{classifier.classifier_id}_{data_id}_holdout{str(valid_frac).replace('.', '')}_{str(train_valid_size)}_{str(test_size)}",
                )
            )
        super().__init__(
            classifier=classifier,
            data_id=data_id,
            valid_type="holdout",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=valid_frac,
            n_splits=None,
            n_repeats=None,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=None,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(
            self.x_valid_train,
            self.y_valid_train,
            test_size=self.valid_size,
            random_state=self.seed,
            stratify=self.y_valid_train,
        )
        self.train_size = self.x_train.shape[0]

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in [
                # "y_valid_train",
                "y_test"
            ]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if self.reshuffle:
            self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(
                self.x_valid_train,
                self.y_valid_train,
                test_size=self.valid_size,
                random_state=self.seed + (trial.number * 500000),
                stratify=self.y_valid_train,
            )

        self.y_train_hist.append(self.y_train)
        self.y_valid_hist.append(self.y_valid)

        self.x_add_valid, self.y_add_valid = construct_x_and_y_add_valid(
            self.x_valid, self.y_valid, self.x_add_valid_use, self.y_add_valid_use
        )
        self.y_add_valid_hist.append(self.y_add_valid)

        # fit the classifier
        self.classifier.fit(
            trial=trial,
            x_train=self.x_train,
            y_train=self.y_train,
            x_valid=self.x_valid,
            y_valid=self.y_valid,
            cat_features=self.cat_features,
        )

        # predict on the train, valid, test and add_valid set and compute the metrics
        predictions = {}
        predictions_proba = {}
        metrics_train = {}
        metrics_valid = {}
        metrics_add_valid = {}
        metrics_test = {}

        for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
            predictions[data] = self.classifier.predict(getattr(self, data))
            predictions_proba[data] = self.classifier.predict_proba(getattr(self, data))
        for metric in self.metrics:
            metrics_train[metric] = compute_metric(
                self.y_train,
                y_pred=predictions["x_train"],
                y_pred_proba=predictions_proba["x_train"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            metrics_valid[metric] = compute_metric(
                self.y_valid,
                y_pred=predictions["x_valid"],
                y_pred_proba=predictions_proba["x_valid"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            metrics_add_valid[metric] = compute_metric(
                self.y_add_valid,
                y_pred=predictions["x_add_valid"],
                y_pred_proba=predictions_proba["x_add_valid"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            metrics_test[metric] = compute_metric(
                self.y_test,
                y_pred=predictions["x_test"],
                y_pred_proba=predictions_proba["x_test"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(f"{metric}_train", metrics_train[metric])
            trial.set_user_attr(f"{metric}_valid", metrics_valid[metric])
            trial.set_user_attr(f"{metric}_add_valid", metrics_add_valid[metric])
            trial.set_user_attr(f"{metric}_test", metrics_test[metric])

        if self.bootstrap_test:
            # bootstrap the test performance
            for metric in self.metrics:
                metric_test_bootstrap = bootstrap_test_performance(
                    y_test=self.y_test,
                    y_pred=predictions["x_test"],
                    y_pred_proba=predictions_proba["x_test"],
                    metric=metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                    seed=self.seed,
                )
                trial.set_user_attr(f"{metric}_test_bootstrap", metric_test_bootstrap)
                average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                    metric_test_bootstrap
                )
                trial.set_user_attr(
                    f"{metric}_test_bootstrap_average", average_metric_test_bootstrap
                )

        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metrics
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metrics_valid_train = {}
        for metric in self.metrics:
            metrics_valid_train[metric] = compute_metric(
                self.y_valid_train,
                y_pred=predictions["x_valid_train"],
                y_pred_proba=predictions_proba["x_valid_train"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(f"{metric}_valid_train", metrics_valid_train[metric])

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metrics
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metrics_test_retrained = {}
        for metric in self.metrics:
            metrics_test_retrained[metric] = compute_metric(
                self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(
                f"{metric}_test_retrained", metrics_test_retrained[metric]
            )

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            for metric in self.metrics:
                metric_test_retrained_bootstrap = bootstrap_test_performance(
                    y_test=self.y_test,
                    y_pred=predictions["x_test_retrained"],
                    y_pred_proba=predictions_proba["x_test_retrained"],
                    metric=metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                    seed=self.seed,
                )
                trial.set_user_attr(
                    f"{metric}_test_retrained_bootstrap",
                    metric_test_retrained_bootstrap,
                )
                average_metric_test_retrained_bootstrap = sum(
                    metric_test_retrained_bootstrap
                ) / len(metric_test_retrained_bootstrap)
                trial.set_user_attr(
                    f"{metric}_test_retrained_bootstrap_average",
                    average_metric_test_retrained_bootstrap,
                )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation accuracy
        return metrics_valid["accuracy"]

File Path: reshufflebench/learner/learner_random_repeatedholdout.py
Content:
import json
import os

import numpy as np
from optuna.trial import Trial
from sklearn.model_selection import StratifiedShuffleSplit

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_random import LearnerRandom
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    NumpyArrayEncoder,
    bootstrap_test_performance,
    check_y_predict_proba,
    construct_x_and_y_add_valid,
    save_list_of_list_of_1d_arrays,
    save_list_of_list_of_pd_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerRandomRepeatedHoldout(LearnerRandom):
    def __init__(
        self,
        classifier: Classifier,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: float,
        n_repeats: int,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        # Note: add_valid size is the size of the total additional validation set, not the size of the additional validation set per repeat
        #       this is somewhat in contrast to LearnerRandomHoldout, where add_valid_size is the size of the additional validation set
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_{classifier.classifier_id}_{data_id}_repeatedholdout{str(valid_frac).replace('.', '')}r{n_repeats}_reshuffle_{train_valid_size}_{test_size}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_{classifier.classifier_id}_{data_id}_repeatedholdout{str(valid_frac).replace('.', '')}r{n_repeats}_{train_valid_size}_{test_size}",
                )
            )

        cv_metric_to_metric = {
            "accuracies": "accuracy",
            "balanced_accuracies": "balanced_accuracy",
            "loglosses": "logloss",
            "aucs": "auc",
        }

        super().__init__(
            classifier=classifier,
            data_id=data_id,
            valid_type="repeatedholdout",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=valid_frac,
            n_splits=None,
            n_repeats=n_repeats,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=cv_metric_to_metric,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        self.cv = StratifiedShuffleSplit(
            n_splits=self.n_repeats, test_size=self.valid_size, random_state=self.seed
        )
        self.cv_splits = list(self.cv.split(self.x_valid_train, self.y_valid_train))
        self.train_size = self.y_valid_train.shape[0] - self.valid_size
        self.cv_splits_hist_train = []
        self.cv_splits_hist_valid = []

        # split add_valid_use data and repeat it n_repeats times
        self.cv_add_valid = StratifiedShuffleSplit(
            n_splits=self.n_repeats,
            test_size=int(self.add_valid_size / self.n_repeats),
            random_state=self.seed,
        )
        self.cv_splits_add_valid = list(
            self.cv_add_valid.split(self.x_add_valid_use, self.y_add_valid_use)
        )
        self.cv_splits_add_valid_hist_valid = []

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in ["y_valid_train", "y_test"]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
            ]:
                save_list_of_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store cv_splits_hist_train and cv_splits_hist_valid
            for file in [
                # "cv_splits_hist_train",
                "cv_splits_hist_valid"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then cv_splits_hist_train and cv_splits_hist_valid are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

            # store cv_splits_add_valid_hist_valid
            data = self.cv_splits_add_valid_hist_valid
            # if we do not reshuffle, then cv_splits_add_valid_hist_valid is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_cv_splits_add_valid_hist_valid.parquet",
                ),
                data=data,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if self.reshuffle:
            self.cv = StratifiedShuffleSplit(
                n_splits=self.n_repeats,
                test_size=self.valid_size,
                random_state=self.seed + (trial.number * 500000),
            )
            self.cv_splits = list(self.cv.split(self.x_valid_train, self.y_valid_train))

            # split add_valid_use data and repeat it n_repeats times
            self.cv_add_valid = StratifiedShuffleSplit(
                n_splits=self.n_repeats,
                test_size=int(self.add_valid_size / self.n_repeats),
                random_state=self.seed + (trial.number * 500000),
            )
            self.cv_splits_add_valid = list(
                self.cv_add_valid.split(self.x_add_valid_use, self.y_add_valid_use)
            )

        cv_splits_hist_train_tmp = []
        cv_splits_hist_valid_tmp = []
        cv_splits_add_valid_hist_valid_tmp = []
        y_train_hist_tmp = []
        y_valid_hist_tmp = []
        y_add_valid_hist_tmp = []

        # for each repeat fit the classifier and predict on the train, valid, test and add_valid set and compute the metrics
        predictions = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        predictions_proba = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        cv_metrics_train = dict([(metric, []) for metric in self.metrics])
        cv_metrics_valid = dict([(metric, []) for metric in self.metrics])
        cv_metrics_add_valid = dict([(metric, []) for metric in self.metrics])
        cv_metrics_test = dict([(metric, []) for metric in self.metrics])

        for repeat in range(self.n_repeats):
            train_index, valid_index = self.cv_splits[repeat]
            cv_splits_hist_train_tmp.append(train_index)
            cv_splits_hist_valid_tmp.append(valid_index)
            x_train = self.x_valid_train.iloc[train_index]
            x_valid = self.x_valid_train.iloc[valid_index]
            y_train = self.y_valid_train[train_index]
            y_valid = self.y_valid_train[valid_index]
            y_train_hist_tmp.append(y_train)
            y_valid_hist_tmp.append(y_valid)

            add_valid_index = self.cv_splits_add_valid[repeat][1]
            cv_splits_add_valid_hist_valid_tmp.append(
                np.concatenate([valid_index, add_valid_index])
            )
            x_add_valid, y_add_valid = construct_x_and_y_add_valid(
                x_valid=x_valid,
                y_valid=y_valid,
                x_add_valid=self.x_add_valid_use.iloc[add_valid_index],
                y_add_valid=self.y_add_valid_use[add_valid_index],
            )
            y_add_valid_hist_tmp.append(y_add_valid)

            self.classifier.fit(
                trial=trial,
                x_train=x_train,
                y_train=y_train,
                x_valid=x_valid,
                y_valid=y_valid,
                cat_features=self.cat_features,
            )
            for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
                if data == "x_test":
                    predictions[data].append(
                        self.classifier.predict(getattr(self, data))
                    )
                    predictions_proba[data].append(
                        self.classifier.predict_proba(getattr(self, data))
                    )
                else:
                    predictions[data].append(self.classifier.predict(eval(data)))
                    predictions_proba[data].append(
                        self.classifier.predict_proba(eval(data))
                    )
            for metric in self.metrics:
                cv_metrics_train[metric].append(
                    compute_metric(
                        y_train,
                        y_pred=predictions["x_train"][-1],
                        y_pred_proba=predictions_proba["x_train"][-1],
                        metric=metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metrics_valid[metric].append(
                    compute_metric(
                        y_valid,
                        y_pred=predictions["x_valid"][-1],
                        y_pred_proba=predictions_proba["x_valid"][-1],
                        metric=metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metrics_add_valid[metric].append(
                    compute_metric(
                        y_add_valid,
                        y_pred=predictions["x_add_valid"][-1],
                        y_pred_proba=predictions_proba["x_add_valid"][-1],
                        metric=metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metrics_test[metric].append(
                    compute_metric(
                        self.y_test,
                        y_pred=predictions["x_test"][-1],
                        y_pred_proba=predictions_proba["x_test"][-1],
                        metric=metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )

        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)
        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)
        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)
        self.y_train_hist.append(y_train_hist_tmp)
        self.y_valid_hist.append(y_valid_hist_tmp)
        self.y_add_valid_hist.append(y_add_valid_hist_tmp)
        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        for cv_metric in self.cv_metric_to_metric.keys():
            metric = self.cv_metric_to_metric[cv_metric]
            trial.set_user_attr(
                f"{cv_metric}_train",
                json.dumps(cv_metrics_train[metric], cls=NumpyArrayEncoder),
            )
            trial.set_user_attr(
                f"{cv_metric}_valid",
                json.dumps(cv_metrics_valid[metric], cls=NumpyArrayEncoder),
            )
            trial.set_user_attr(
                f"{cv_metric}_add_valid",
                json.dumps(cv_metrics_add_valid[metric], cls=NumpyArrayEncoder),
            )
            trial.set_user_attr(
                f"{cv_metric}_test",
                json.dumps(cv_metrics_test[metric], cls=NumpyArrayEncoder),
            )

        # compute the mean of the metrics over the repeats
        metrics_train = {}
        metrics_valid = {}
        metrics_add_valid = {}
        metrics_test = {}

        for metric in self.metrics:
            metrics_train[metric] = np.mean(cv_metrics_train[metric])
            metrics_valid[metric] = np.mean(cv_metrics_valid[metric])
            metrics_add_valid[metric] = np.mean(cv_metrics_add_valid[metric])
            metrics_test[metric] = np.mean(cv_metrics_test[metric])
            trial.set_user_attr(f"{metric}_train", metrics_train[metric])
            trial.set_user_attr(f"{metric}_valid", metrics_valid[metric])
            trial.set_user_attr(f"{metric}_add_valid", metrics_add_valid[metric])
            trial.set_user_attr(f"{metric}_test", metrics_test[metric])

        # compute the metrics on the test set also in ensemble style
        metrics_test_ensemble = {}
        predictions_proba_test_ensemble = np.mean(predictions_proba["x_test"], axis=0)
        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)
        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums
        check_y_predict_proba(predictions_proba_test_ensemble)
        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)
        for metric in self.metrics:
            metrics_test_ensemble[metric] = compute_metric(
                self.y_test,
                y_pred=predictions_test_ensemble,
                y_pred_proba=predictions_proba_test_ensemble,
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(
                f"{metric}_test_ensemble", metrics_test_ensemble[metric]
            )

        if self.bootstrap_test:
            # bootstrap the ensemble style test performance
            for metric in self.metrics:
                metric_test_bootstrap = bootstrap_test_performance(
                    y_test=self.y_test,
                    y_pred=predictions_test_ensemble,
                    y_pred_proba=predictions_proba_test_ensemble,
                    metric=metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                    seed=self.seed,
                )
                trial.set_user_attr(
                    f"{metric}_test_ensemble_bootstrap", metric_test_bootstrap
                )
                average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                    metric_test_bootstrap
                )
                trial.set_user_attr(
                    f"{metric}_test_ensemble_bootstrap_average",
                    average_metric_test_bootstrap,
                )

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metrics
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metrics_valid_train = {}
        for metric in self.metrics:
            metrics_valid_train[metric] = compute_metric(
                self.y_valid_train,
                y_pred=predictions["x_valid_train"],
                y_pred_proba=predictions_proba["x_valid_train"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(f"{metric}_valid_train", metrics_valid_train[metric])

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metrics
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metrics_test_retrained = {}
        for metric in self.metrics:
            metrics_test_retrained[metric] = compute_metric(
                self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=metric,
                labels=self.labels,
                multiclass=self.multiclass,
            )
            trial.set_user_attr(
                f"{metric}_test_retrained", metrics_test_retrained[metric]
            )

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            for metric in self.metrics:
                metric_test_retrained_bootstrap = bootstrap_test_performance(
                    y_test=self.y_test,
                    y_pred=predictions["x_test_retrained"],
                    y_pred_proba=predictions_proba["x_test_retrained"],
                    metric=metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                    seed=self.seed,
                )
                trial.set_user_attr(
                    f"{metric}_test_retrained_bootstrap",
                    metric_test_retrained_bootstrap,
                )
                average_metric_test_retrained_bootstrap = sum(
                    metric_test_retrained_bootstrap
                ) / len(metric_test_retrained_bootstrap)
                trial.set_user_attr(
                    f"{metric}_test_retrained_bootstrap_average",
                    average_metric_test_retrained_bootstrap,
                )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation accuracy (mean over repeats)
        return metrics_valid["accuracy"]

File Path: reshufflebench/learner/learner_smac.py
Content:
import json
import os
import random
from abc import ABC, abstractmethod
from typing import Union

import numpy as np
import openml
import optuna
import pandas as pd
from optuna.trial import Trial
from sklearn.datasets import make_classification, make_hastie_10_2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.samplers import SmacSampler
from reshufflebench.utils import unify_missing_values


class LearnerSmac(ABC):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        valid_type: str,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: Union[float, None],
        n_splits: Union[int, None],
        n_repeats: Union[int, None],
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
        results_path: str,
        cv_metric_to_metric: Union[dict, None],
        write_results_to_disk: bool = False,
        bootstrap_test: bool = True,
    ):
        self.metrics = ["auc"]
        self.metrics_direction = {
            "auc": "maximize",
        }
        assert metric in self.metrics
        self.metric = metric
        self.cv_metric_to_metric = cv_metric_to_metric

        self.classifier = classifier
        self.data_id = data_id
        self.valid_type = valid_type
        self.train_valid_size = train_valid_size
        self.reshuffle = reshuffle
        self.valid_frac = valid_frac
        self.valid_size = (
            int(self.train_valid_size * self.valid_frac)
            if self.valid_frac is not None
            else None
        )
        self.n_splits = n_splits
        self.n_repeats = n_repeats
        self.test_size = test_size
        self.add_valid_size = add_valid_size
        self.n_trials = n_trials
        self.seed = seed
        self.results_path = results_path
        self.file_name = f"study_seed_{self.seed}"
        self.write_results_to_disk = write_results_to_disk
        self.bootstrap_test = bootstrap_test

        self.label_encoder = None

        self.has_missings = False
        self.labels = None
        self.n_classes = None
        self.multiclass = False
        self.cat_features = None
        self.num_features = None
        self.x_valid_train = None
        self.x_test = None
        self.x_train = None
        self.x_valid = None
        self.y_valid_train = None
        self.y_test = None
        self.y_train = None
        self.y_valid = None
        self.y_train_hist = None
        self.y_valid_hist = None
        self.y_pred_train_proba_hist = None
        self.y_pred_valid_proba_hist = None
        self.y_pred_test_proba_hist = None
        self.y_pred_valid_train_proba_hist = None
        self.y_pred_test_proba_retrained_hist = None

        self.cv = None
        self.cv_splits = None
        self.cv_splits_hist_train = None
        self.cv_splits_hist_valid = None
        self.train_size = None

        self.x_add_valid_use = None
        self.y_add_valid_use = None
        self.x_add_valid = None
        self.y_add_valid = None
        self.y_add_valid_hist = None
        self.y_pred_add_valid_proba_hist = None

        self.cv_add_valid = None
        self.cv_splits_add_valid = None
        self.cv_splits_add_valid_hist_valid = None

        self.study = None
        self.study_name = None
        self.storage = None

    def prepare_data(self) -> None:
        """
        Prepare the data for the optimization.
        """

        if self.data_id == 99999:
            # generate data for testing
            x, y = make_hastie_10_2(
                n_samples=self.train_valid_size
                + self.test_size
                + self.add_valid_size
                + 1000,
                random_state=0,
            )
            x = pd.DataFrame(x)
            x.columns = [f"feature_{i}" for i in range(x.shape[1])]
            y = y.astype(int)
            y = pd.Series(y, dtype="category")
            y.columns = ["target"]
            categorical_indicator = [False] * x.shape[1]
        elif self.data_id == 11111:
            # generate data for testing
            x, y = make_classification(
                n_samples=self.train_valid_size
                + self.test_size
                + self.add_valid_size
                + 1000,
                n_features=20,
                n_informative=10,
                n_redundant=2,
                n_repeated=2,
                n_classes=2,
                n_clusters_per_class=4,
                weights=None,
                flip_y=0.1,
                class_sep=1.0,
                hypercube=True,
                shift=0.0,
                scale=1.0,
                shuffle=True,
                random_state=0,
            )
            x = pd.DataFrame(x)
            x.columns = [f"feature_{i}" for i in range(x.shape[1])]
            y = y.astype(int)
            y = pd.Series(y, dtype="category")
            y.columns = ["target"]
            categorical_indicator = [False] * x.shape[1]
        else:
            # download the OpenML data for the dataset
            dataset = openml.datasets.get_dataset(
                self.data_id,
                download_data=True,
                download_qualities=False,
                download_features_meta_data=False,
            )

            # get the pandas dataframe
            x, y, categorical_indicator, _ = dataset.get_data(
                dataset_format="dataframe",
                target=dataset.default_target_attribute,
            )

        # check if dataset is large enough for train_valid_size, test_size and add_valid_size
        if (
            x.shape[0]
            < self.train_valid_size + self.test_size + self.add_valid_size + 1000
        ):
            raise ValueError(
                "Dataset too small for train_valid_size, test_size and add_valid_size"
            )

        # check if feature names contain "__"
        if any("__" in col for col in x.columns):
            raise ValueError(
                'Feature names contain "__" which is not allowed due to preprocessing'
            )

        # convert categorical columns to object dtype
        for col in range(len(categorical_indicator)):
            if categorical_indicator[col]:
                x[x.columns[col]] = x[x.columns[col]].astype("object")
        self.cat_features = [
            x.columns.get_loc(col) for col in x.columns if x[col].dtype.name == "object"
        ]
        self.num_features = list(set(range(x.shape[1])) - set(self.cat_features))

        # check if missing values are present
        # if yes bring missing values into a standardized format
        if x.isnull().values.any():
            self.has_missings = True
            x = unify_missing_values(x)

        if y.isnull().values.any():
            raise ValueError("Missing values in target column")

        # map the labels to integers
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(y)
        self.labels = np.unique(y).tolist()
        self.n_classes = len(self.label_encoder.classes_)
        if self.n_classes > 2:
            self.multiclass = True

        # split the data into used and unused data
        x_not_use, x_use, y_not_use, y_use = train_test_split(
            x,
            y,
            test_size=self.train_valid_size + self.test_size,
            random_state=self.seed,
            stratify=y,
        )
        # split the used data into valid_train and test
        x_valid_train, x_test, y_valid_train, y_test = train_test_split(
            x_use,
            y_use,
            test_size=self.test_size,
            random_state=self.seed,
            stratify=y_use,
        )
        # from the unused data sample add_valid_size data to use as additional validation data
        _, x_add_valid_use, _, y_add_valid_use = train_test_split(
            x_not_use,
            y_not_use,
            test_size=self.add_valid_size,
            random_state=self.seed,
            stratify=y_not_use,
        )

        self.x_valid_train = x_valid_train
        self.x_add_valid_use = x_add_valid_use
        self.x_test = x_test
        self.y_train_hist = []
        self.y_valid_hist = []
        self.y_add_valid_hist = []
        self.y_pred_train_proba_hist = []
        self.y_pred_valid_proba_hist = []
        self.y_pred_test_proba_hist = []
        self.y_pred_add_valid_proba_hist = []
        self.y_pred_valid_train_proba_hist = []
        self.y_pred_test_proba_retrained_hist = []
        self.y_valid_train = y_valid_train
        self.y_add_valid_use = y_add_valid_use
        self.y_test = y_test

    @abstractmethod
    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        pass

    def prepare_study(self) -> None:
        """
        Prepare the study for the optimization.
        """
        # if results_path does not exist, create it
        if not os.path.exists(self.results_path):
            try:
                os.makedirs(self.results_path)
            except FileExistsError:
                pass
        study_name = os.path.join(self.results_path, self.file_name)
        storage = f"sqlite:///{study_name}.db"
        # raise warning if study already exists
        if os.path.exists(f"{study_name}.db"):
            print(f"WARNING: Study {study_name} already exists. Overwriting it.")
            # remove the study
            os.remove(f"{study_name}.db")
        sampler = SmacSampler(
            seed=self.seed,
            configspace_search_space=self.classifier.get_configspace_search_space(
                n_train_samples=self.train_size
            ),
            internal_optuna_search_space=self.classifier.get_internal_optuna_search_space(
                n_train_samples=self.train_size
            ),
            n_trials=self.n_trials,
            fid=study_name,
        )
        self.study = optuna.create_study(
            storage=storage,
            sampler=sampler,
            study_name=study_name,
            direction="maximize",
        )
        self.study_name = study_name
        self.storage = storage

    @abstractmethod
    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        """
        pass

    @abstractmethod
    def store_results(self) -> None:
        """
        Store additional results.
        """
        pass

    def run(self) -> None:
        """
        Finish all preparations.
        Run the optimization.
        Stores the results in parquet along the study database.
        """
        self.prepare_data()

        self.prepare_resampling()

        self.prepare_study()

        # write a file with the parameters
        params = {
            "classifier": self.classifier.classifier_id,
            "data_id": self.data_id,
            "valid_type": self.valid_type,
            "train_valid_size": self.train_valid_size,
            "reshuffle": self.reshuffle,
            "n_splits": self.n_splits,
            "n_repeats": self.n_repeats,
            "valid_frac": self.valid_frac,
            "test_size": self.test_size,
            "n_trials": self.n_trials,
            "file_name": self.file_name,
            "seed": self.seed,
            "has_missings": self.has_missings,
            "labels": self.labels,
            "n_classes": self.n_classes,
            "multiclass": self.multiclass,
            "metric": self.metric,
            "metrics": self.metrics,
            "metrics_direction": self.metrics_direction,
            "cv_metric_to_metric": self.cv_metric_to_metric,
            "results_path": self.results_path,
            "study_name": self.study_name,
            "storage": self.storage,
        }
        with open(
            os.path.join(self.results_path, f"{self.file_name}_params.json"), "w"
        ) as f:
            json.dump(params, f, indent=4)

        np.random.seed(self.seed)
        random.seed(self.seed)
        self.study.optimize(self.objective, n_trials=self.n_trials)

        self.store_results()

File Path: reshufflebench/learner/learner_smac_cv.py
Content:
import json
import os

import numpy as np
from optuna.trial import Trial
from sklearn.model_selection import StratifiedKFold

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_smac import LearnerSmac
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    NumpyArrayEncoder,
    bootstrap_test_performance,
    check_y_predict_proba,
    construct_x_and_y_add_valid,
    save_list_of_list_of_1d_arrays,
    save_list_of_list_of_pd_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerSmacCV(LearnerSmac):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        n_splits: int,
        n_repeats: int,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        # Note: add_valid size is the size of the total additional validation set, not the size of the additional validation set per fold
        #       this is somewhat in contrast to LearnerRandomHoldout, where add_valid_size is the size of the additional validation set
        #       this is also in contrast to valid_size being the size of the validation set per fold in this class
        if train_valid_size % n_splits != 0:
            raise ValueError("train_valid_size must be divisible by n_splits")
        if add_valid_size % n_splits != 0:
            raise ValueError("add_valid_size must be divisible by n_splits")
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_smac_{classifier.classifier_id}_{data_id}_cv{n_splits}r{n_repeats}_reshuffle_{train_valid_size}_{test_size}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_smac_{classifier.classifier_id}_{data_id}_cv{n_splits}r{n_repeats}_{train_valid_size}_{test_size}",
                )
            )

        cv_metric_to_metric = {
            "accuracies": "accuracy",
            "balanced_accuracies": "balanced_accuracy",
            "loglosses": "logloss",
            "aucs": "auc",
        }

        super().__init__(
            classifier=classifier,
            metric=metric,
            data_id=data_id,
            valid_type="cv",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=None,
            n_splits=n_splits,
            n_repeats=n_repeats,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=cv_metric_to_metric,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        # we use StratifiedKFold for the cross-validation and manually repeat it n_repeats times because this is somewhat easier to handle here than using RepeatedStratifiedKFold
        self.cv = [
            StratifiedKFold(
                n_splits=self.n_splits,
                shuffle=True,
                random_state=self.seed + (i * 1000),
            )
            for i in range(self.n_repeats)
        ]
        self.cv_splits = []
        for cv in self.cv:
            self.cv_splits.append(
                list(cv.split(self.x_valid_train, self.y_valid_train))
            )
        self.train_size = int(
            self.x_valid_train.shape[0] / self.n_splits * (self.n_splits - 1)
        )
        self.valid_size = int(self.x_valid_train.shape[0] / self.n_splits)
        self.cv_splits_hist_train = []
        self.cv_splits_hist_valid = []

        # partition add_valid_use data into n_splits folds and repeat it n_repeats times
        self.cv_add_valid = [
            StratifiedKFold(
                n_splits=self.n_splits,
                shuffle=True,
                random_state=self.seed + (i * 1000),
            )
            for i in range(self.n_repeats)
        ]
        self.cv_splits_add_valid = []
        for cv in self.cv_add_valid:
            self.cv_splits_add_valid.append(
                list(cv.split(self.x_add_valid_use, self.y_add_valid_use))
            )
        self.cv_splits_add_valid_hist_valid = []

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in ["y_valid_train", "y_test"]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
            ]:
                save_list_of_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store cv_splits_hist_train and cv_splits_hist_valid
            for file in [
                # "cv_splits_hist_train",
                "cv_splits_hist_valid"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then cv_splits_hist_train and cv_splits_hist_valid are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

            # store cv_splits_add_valid_hist_valid
            data = self.cv_splits_add_valid_hist_valid
            # if we do not reshuffle, then cv_splits_add_valid_hist_valid is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_cv_splits_add_valid_hist_valid.parquet",
                ),
                data=data,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        Note: Only one metric is used for HEBO.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if trial.study.sampler.fallback_triggered:
            trial.set_user_attr("smac_fallback_triggered", True)
        else:
            trial.set_user_attr("smac_fallback_triggered", False)

        if self.reshuffle:
            self.cv = [
                StratifiedKFold(
                    n_splits=self.n_splits,
                    shuffle=True,
                    random_state=self.seed + (trial.number * 500000) + (i * 1000),
                )
                for i in range(self.n_repeats)
            ]
            self.cv_splits = []
            for cv in self.cv:
                self.cv_splits.append(
                    list(cv.split(self.x_valid_train, self.y_valid_train))
                )

            # partition add_valid_use data into n_splits folds and repeat it n_repeats times
            self.cv_add_valid = [
                StratifiedKFold(
                    n_splits=self.n_splits,
                    shuffle=True,
                    random_state=self.seed + (trial.number * 500000) + (i * 1000),
                )
                for i in range(self.n_repeats)
            ]
            self.cv_splits_add_valid = []
            for cv in self.cv_add_valid:
                self.cv_splits_add_valid.append(
                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))
                )

        cv_splits_hist_train_tmp = []
        cv_splits_hist_valid_tmp = []
        cv_splits_add_valid_hist_valid_tmp = []
        y_train_hist_tmp = []
        y_valid_hist_tmp = []
        y_add_valid_hist_tmp = []

        # for each repeat and each fold fit the classifier and predict on the train, valid, test and add_valid set and compute the metric
        predictions = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        predictions_proba = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        cv_metric_train = []
        cv_metric_valid = []
        cv_metric_add_valid = []
        cv_metric_test = []

        for repeat in range(self.n_repeats):
            for fold in range(self.n_splits):
                train_index, valid_index = self.cv_splits[repeat][fold]
                cv_splits_hist_train_tmp.append(train_index)
                cv_splits_hist_valid_tmp.append(valid_index)
                x_train = self.x_valid_train.iloc[train_index]
                x_valid = self.x_valid_train.iloc[valid_index]
                y_train = self.y_valid_train[train_index]
                y_valid = self.y_valid_train[valid_index]
                y_train_hist_tmp.append(y_train)
                y_valid_hist_tmp.append(y_valid)

                add_valid_index = self.cv_splits_add_valid[repeat][fold][1]
                cv_splits_add_valid_hist_valid_tmp.append(
                    np.concatenate([valid_index, add_valid_index])
                )
                x_add_valid, y_add_valid = construct_x_and_y_add_valid(
                    x_valid=x_valid,
                    y_valid=y_valid,
                    x_add_valid=self.x_add_valid_use.iloc[add_valid_index],
                    y_add_valid=self.y_add_valid_use[add_valid_index],
                )
                y_add_valid_hist_tmp.append(y_add_valid)

                self.classifier.fit(
                    trial=trial,
                    x_train=x_train,
                    y_train=y_train,
                    x_valid=x_valid,
                    y_valid=y_valid,
                    cat_features=self.cat_features,
                )
                for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
                    if data == "x_test":
                        predictions[data].append(
                            self.classifier.predict(getattr(self, data))
                        )
                        predictions_proba[data].append(
                            self.classifier.predict_proba(getattr(self, data))
                        )
                    else:
                        predictions[data].append(self.classifier.predict(eval(data)))
                        predictions_proba[data].append(
                            self.classifier.predict_proba(eval(data))
                        )
                cv_metric_train.append(
                    compute_metric(
                        y_train,
                        y_pred=predictions["x_train"][-1],
                        y_pred_proba=predictions_proba["x_train"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metric_valid.append(
                    compute_metric(
                        y_valid,
                        y_pred=predictions["x_valid"][-1],
                        y_pred_proba=predictions_proba["x_valid"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metric_add_valid.append(
                    compute_metric(
                        y_add_valid,
                        y_pred=predictions["x_add_valid"][-1],
                        y_pred_proba=predictions_proba["x_add_valid"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )
                cv_metric_test.append(
                    compute_metric(
                        self.y_test,
                        y_pred=predictions["x_test"][-1],
                        y_pred_proba=predictions_proba["x_test"][-1],
                        metric=self.metric,
                        labels=self.labels,
                        multiclass=self.multiclass,
                    )
                )

        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)
        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)
        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)
        self.y_train_hist.append(y_train_hist_tmp)
        self.y_valid_hist.append(y_valid_hist_tmp)
        self.y_add_valid_hist.append(y_add_valid_hist_tmp)
        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        cv_metric = dict(
            (value, key) for key, value in self.cv_metric_to_metric.items()
        )[self.metric]
        trial.set_user_attr(
            f"{cv_metric}_train",
            json.dumps(cv_metric_train, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_valid",
            json.dumps(cv_metric_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_add_valid",
            json.dumps(cv_metric_add_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_test",
            json.dumps(cv_metric_test, cls=NumpyArrayEncoder),
        )

        # compute the mean of the metric over the folds and repeats
        metric_train = np.mean(cv_metric_train)
        metric_valid = np.mean(cv_metric_valid)
        metric_add_valid = np.mean(cv_metric_add_valid)
        metric_test = np.mean(cv_metric_test)
        trial.set_user_attr(f"{self.metric}_train", metric_train)
        trial.set_user_attr(f"{self.metric}_valid", metric_valid)
        trial.set_user_attr(f"{self.metric}_add_valid", metric_add_valid)
        trial.set_user_attr(f"{self.metric}_test", metric_test)

        # compute the metric on the test set also in ensemble style
        predictions_proba_test_ensemble = np.mean(predictions_proba["x_test"], axis=0)
        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)
        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums
        check_y_predict_proba(predictions_proba_test_ensemble)
        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)
        metric_test_ensemble = compute_metric(
            self.y_test,
            y_pred=predictions_test_ensemble,
            y_pred_proba=predictions_proba_test_ensemble,
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_ensemble", metric_test_ensemble)

        if self.bootstrap_test:
            # bootstrap the ensemble style test performance
            metric_test_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions_test_ensemble,
                y_pred_proba=predictions_proba_test_ensemble,
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap", metric_test_bootstrap
            )
            average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                metric_test_bootstrap
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap_average",
                average_metric_test_bootstrap,
            )

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metric
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metric_valid_train = compute_metric(
            self.y_valid_train,
            y_pred=predictions["x_valid_train"],
            y_pred_proba=predictions_proba["x_valid_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_valid_train", metric_valid_train)

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metric
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metric_test_retrained = compute_metric(
            self.y_test,
            y_pred=predictions["x_test_retrained"],
            y_pred_proba=predictions_proba["x_test_retrained"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_retrained", metric_test_retrained)

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            metric_test_retrained_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap",
                metric_test_retrained_bootstrap,
            )
            average_metric_test_retrained_bootstrap = sum(
                metric_test_retrained_bootstrap
            ) / len(metric_test_retrained_bootstrap)
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap_average",
                average_metric_test_retrained_bootstrap,
            )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation metric
        # Note: Here, we assume maximization but Sampler assumes minimization and will correct for it
        if self.metrics_direction[self.metric] == "minimize":
            return -metric_valid
        else:
            return metric_valid

File Path: reshufflebench/learner/learner_smac_holdout.py
Content:
import os

from optuna.trial import Trial
from sklearn.model_selection import train_test_split

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_smac import LearnerSmac
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    bootstrap_test_performance,
    construct_x_and_y_add_valid,
    save_list_of_1d_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerSmacHoldout(LearnerSmac):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: float,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_smac_{classifier.classifier_id}_{data_id}_holdout{str(valid_frac).replace('.', '')}_reshuffle_{str(train_valid_size)}_{str(test_size)}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_smac_{classifier.classifier_id}_{data_id}_holdout{str(valid_frac).replace('.', '')}_{str(train_valid_size)}_{str(test_size)}",
                )
            )
        super().__init__(
            classifier=classifier,
            metric=metric,
            data_id=data_id,
            valid_type="holdout",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=valid_frac,
            n_splits=None,
            n_repeats=None,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=None,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(
            self.x_valid_train,
            self.y_valid_train,
            test_size=self.valid_size,
            random_state=self.seed,
            stratify=self.y_valid_train,
        )
        self.train_size = self.x_train.shape[0]

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in [
                # "y_valid_train",
                "y_test"
            ]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        Note: Only one metric is used for HEBO.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if trial.study.sampler.fallback_triggered:
            trial.set_user_attr("smac_fallback_triggered", True)
        else:
            trial.set_user_attr("smac_fallback_triggered", False)

        if self.reshuffle:
            self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(
                self.x_valid_train,
                self.y_valid_train,
                test_size=self.valid_size,
                random_state=self.seed + (trial.number * 500000),
                stratify=self.y_valid_train,
            )

        self.y_train_hist.append(self.y_train)
        self.y_valid_hist.append(self.y_valid)

        self.x_add_valid, self.y_add_valid = construct_x_and_y_add_valid(
            self.x_valid, self.y_valid, self.x_add_valid_use, self.y_add_valid_use
        )
        self.y_add_valid_hist.append(self.y_add_valid)

        # fit the classifier
        self.classifier.fit(
            trial=trial,
            x_train=self.x_train,
            y_train=self.y_train,
            x_valid=self.x_valid,
            y_valid=self.y_valid,
            cat_features=self.cat_features,
        )

        # predict on the train, valid, test and add_valid set and compute the metric
        predictions = {}
        predictions_proba = {}

        for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
            predictions[data] = self.classifier.predict(getattr(self, data))
            predictions_proba[data] = self.classifier.predict_proba(getattr(self, data))
        metric_train = compute_metric(
            self.y_train,
            y_pred=predictions["x_train"],
            y_pred_proba=predictions_proba["x_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        metric_valid = compute_metric(
            self.y_valid,
            y_pred=predictions["x_valid"],
            y_pred_proba=predictions_proba["x_valid"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        metric_add_valid = compute_metric(
            self.y_add_valid,
            y_pred=predictions["x_add_valid"],
            y_pred_proba=predictions_proba["x_add_valid"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        metric_test = compute_metric(
            self.y_test,
            y_pred=predictions["x_test"],
            y_pred_proba=predictions_proba["x_test"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_train", metric_train)
        trial.set_user_attr(f"{self.metric}_valid", metric_valid)
        trial.set_user_attr(f"{self.metric}_add_valid", metric_add_valid)
        trial.set_user_attr(f"{self.metric}_test", metric_test)

        if self.bootstrap_test:
            # bootstrap the test performance
            metric_test_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test"],
                y_pred_proba=predictions_proba["x_test"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(f"{self.metric}_test_bootstrap", metric_test_bootstrap)
            average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                metric_test_bootstrap
            )
            trial.set_user_attr(
                f"{self.metric}_test_bootstrap_average", average_metric_test_bootstrap
            )

        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metric
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metric_valid_train = compute_metric(
            self.y_valid_train,
            y_pred=predictions["x_valid_train"],
            y_pred_proba=predictions_proba["x_valid_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_valid_train", metric_valid_train)

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metric
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metric_test_retrained = compute_metric(
            self.y_test,
            y_pred=predictions["x_test_retrained"],
            y_pred_proba=predictions_proba["x_test_retrained"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_retrained", metric_test_retrained)

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            metric_test_retrained_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap",
                metric_test_retrained_bootstrap,
            )
            average_metric_test_retrained_bootstrap = sum(
                metric_test_retrained_bootstrap
            ) / len(metric_test_retrained_bootstrap)
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap_average",
                average_metric_test_retrained_bootstrap,
            )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation metric
        # Note: Here, we assume maximization but SmacSampler assumes minimization and will correct for it
        if self.metrics_direction[self.metric] == "minimize":
            return -metric_valid
        else:
            return metric_valid

File Path: reshufflebench/learner/learner_smac_repeatedholdout.py
Content:
import json
import os

import numpy as np
from optuna.trial import Trial
from sklearn.model_selection import StratifiedShuffleSplit

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner.learner_smac import LearnerSmac
from reshufflebench.metrics import compute_metric
from reshufflebench.utils import (
    NumpyArrayEncoder,
    bootstrap_test_performance,
    check_y_predict_proba,
    construct_x_and_y_add_valid,
    save_list_of_list_of_1d_arrays,
    save_list_of_list_of_pd_arrays,
    save_list_of_pd_arrays,
    save_single_array,
)


class LearnerSmacRepeatedHoldout(LearnerSmac):
    def __init__(
        self,
        classifier: Classifier,
        metric: str,
        data_id: int,
        train_valid_size: int,
        reshuffle: bool,
        valid_frac: float,
        n_repeats: int,
        test_size: int,
        add_valid_size: int,
        n_trials: int,
        seed: int,
    ):
        # Note: add_valid size is the size of the total additional validation set, not the size of the additional validation set per fold
        #       this is somewhat in contrast to LearnerRandomHoldout, where add_valid_size is the size of the additional validation set
        if reshuffle:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_smac_{classifier.classifier_id}_{data_id}_repeatedholdout{str(valid_frac).replace('.', '')}_{n_repeats}_reshuffle_{train_valid_size}_{test_size}",
                )
            )
        else:
            results_path = os.path.abspath(
                os.path.join(
                    "results",
                    f"results_smac_{classifier.classifier_id}_{data_id}_repeatedholdout{str(valid_frac).replace('.', '')}_{n_repeats}_{train_valid_size}_{test_size}",
                )
            )

        cv_metric_to_metric = {
            "accuracies": "accuracy",
            "balanced_accuracies": "balanced_accuracy",
            "loglosses": "logloss",
            "aucs": "auc",
        }

        super().__init__(
            classifier=classifier,
            metric=metric,
            data_id=data_id,
            valid_type="repeatedholdout",
            train_valid_size=train_valid_size,
            reshuffle=reshuffle,
            valid_frac=valid_frac,
            n_splits=None,
            n_repeats=n_repeats,
            test_size=test_size,
            add_valid_size=add_valid_size,
            n_trials=n_trials,
            seed=seed,
            results_path=results_path,
            cv_metric_to_metric=cv_metric_to_metric,
        )

    def prepare_resampling(self) -> None:
        """
        Prepare the resampling for the optimization.
        """
        self.cv = StratifiedShuffleSplit(
            n_splits=self.n_repeats, test_size=self.valid_size, random_state=self.seed
        )
        self.cv_splits = list(self.cv.split(self.x_valid_train, self.y_valid_train))
        self.train_size = self.y_valid_train.shape[0] - self.valid_size
        self.cv_splits_hist_train = []
        self.cv_splits_hist_valid = []

        # split add_valid_use data and repeat it n_repeats times
        self.cv_add_valid = StratifiedShuffleSplit(
            n_splits=self.n_repeats,
            test_size=int(self.add_valid_size / self.n_repeats),
            random_state=self.seed,
        )
        self.cv_splits_add_valid = list(
            self.cv_add_valid.split(self.x_add_valid_use, self.y_add_valid_use)
        )
        self.cv_splits_add_valid_hist_valid = []

    def store_results(self) -> None:
        """
        Store additional results.
        """
        if self.write_results_to_disk:
            # store y_train_hist, y_valid_hist
            for file in [
                # "y_train_hist",
                "y_valid_hist"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then y_train_hist and y_valid_hist are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_valid_train, y_test
            for file in ["y_valid_train", "y_test"]:
                save_single_array(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_train_proba_hist, y_pred_valid_proba_hist, y_pred_test_proba_hist, y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_train_proba_hist",
                "y_pred_valid_proba_hist",
                "y_pred_test_proba_hist",
            ]:
                save_list_of_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store y_pred_valid_train_proba_hist, y_pred_test_proba_retrained_hist
            for file in [
                # "y_pred_valid_train_proba_hist",
                "y_pred_test_proba_retrained_hist",
            ]:
                save_list_of_pd_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=getattr(self, file),
                )

            # store cv_splits_hist_train and cv_splits_hist_valid
            for file in [
                # "cv_splits_hist_train",
                "cv_splits_hist_valid"
            ]:
                data = getattr(self, file)
                # if we do not reshuffle, then cv_splits_hist_train and cv_splits_hist_valid are the same for all trials, so we only store the first one
                if not self.reshuffle:
                    data = [data[0]]

                save_list_of_list_of_1d_arrays(
                    os.path.join(self.results_path, f"{self.file_name}_{file}.parquet"),
                    data=data,
                )

            # store y_add_valid_use
            save_single_array(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_use.parquet"
                ),
                data=self.y_add_valid_use,
            )

            # store y_add_valid_hist
            data = self.y_add_valid_hist
            # if we do not reshuffle, then y_add_valid_hist is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path, f"{self.file_name}_y_add_valid_hist.parquet"
                ),
                data=data,
            )

            # store y_pred_add_valid_proba_hist
            save_list_of_list_of_pd_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_y_pred_add_valid_proba_hist.parquet",
                ),
                data=self.y_pred_add_valid_proba_hist,
            )

            # store cv_splits_add_valid_hist_valid
            data = self.cv_splits_add_valid_hist_valid
            # if we do not reshuffle, then cv_splits_add_valid_hist_valid is the same for all trials, so we only store the first one
            if not self.reshuffle:
                data = [data[0]]

            save_list_of_list_of_1d_arrays(
                os.path.join(
                    self.results_path,
                    f"{self.file_name}_cv_splits_add_valid_hist_valid.parquet",
                ),
                data=data,
            )

    def objective(self, trial: Trial) -> float:
        """
        Objective function for the optimization.
        Note: Only one metric is used for HEBO.
        """
        # construct classifier pipeline
        self.classifier.construct_pipeline(
            trial,
            refit=False,
            cat_features=self.cat_features,
            num_features=self.num_features,
            n_train_samples=self.train_size,
        )

        if trial.study.sampler.fallback_triggered:
            trial.set_user_attr("smac_fallback_triggered", True)
        else:
            trial.set_user_attr("smac_fallback_triggered", False)

        if self.reshuffle:
            self.cv = StratifiedShuffleSplit(
                n_splits=self.n_repeats,
                test_size=self.valid_size,
                random_state=self.seed + (trial.number * 500000),
            )
            self.cv_splits = list(self.cv.split(self.x_valid_train, self.y_valid_train))

            # split add_valid_use data and repeat it n_repeats times
            self.cv_add_valid = StratifiedShuffleSplit(
                n_splits=self.n_repeats,
                test_size=int(self.add_valid_size / self.n_repeats),
                random_state=self.seed + (trial.number * 500000),
            )
            self.cv_splits_add_valid = list(
                self.cv_add_valid.split(self.x_add_valid_use, self.y_add_valid_use)
            )

        cv_splits_hist_train_tmp = []
        cv_splits_hist_valid_tmp = []
        cv_splits_add_valid_hist_valid_tmp = []
        y_train_hist_tmp = []
        y_valid_hist_tmp = []
        y_add_valid_hist_tmp = []

        # for each repeat fit the classifier and predict on the train, valid, test and add_valid set and compute the metric
        predictions = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        predictions_proba = dict(
            [(data, []) for data in ["x_train", "x_valid", "x_add_valid", "x_test"]]
        )
        cv_metric_train = []
        cv_metric_valid = []
        cv_metric_add_valid = []
        cv_metric_test = []

        for repeat in range(self.n_repeats):
            train_index, valid_index = self.cv_splits[repeat]
            cv_splits_hist_train_tmp.append(train_index)
            cv_splits_hist_valid_tmp.append(valid_index)
            x_train = self.x_valid_train.iloc[train_index]
            x_valid = self.x_valid_train.iloc[valid_index]
            y_train = self.y_valid_train[train_index]
            y_valid = self.y_valid_train[valid_index]
            y_train_hist_tmp.append(y_train)
            y_valid_hist_tmp.append(y_valid)

            add_valid_index = self.cv_splits_add_valid[repeat][1]
            cv_splits_add_valid_hist_valid_tmp.append(
                np.concatenate([valid_index, add_valid_index])
            )
            x_add_valid, y_add_valid = construct_x_and_y_add_valid(
                x_valid=x_valid,
                y_valid=y_valid,
                x_add_valid=self.x_add_valid_use.iloc[add_valid_index],
                y_add_valid=self.y_add_valid_use[add_valid_index],
            )
            y_add_valid_hist_tmp.append(y_add_valid)

            self.classifier.fit(
                trial=trial,
                x_train=x_train,
                y_train=y_train,
                x_valid=x_valid,
                y_valid=y_valid,
                cat_features=self.cat_features,
            )
            for data in ["x_train", "x_valid", "x_add_valid", "x_test"]:
                if data == "x_test":
                    predictions[data].append(
                        self.classifier.predict(getattr(self, data))
                    )
                    predictions_proba[data].append(
                        self.classifier.predict_proba(getattr(self, data))
                    )
                else:
                    predictions[data].append(self.classifier.predict(eval(data)))
                    predictions_proba[data].append(
                        self.classifier.predict_proba(eval(data))
                    )
            cv_metric_train.append(
                compute_metric(
                    y_train,
                    y_pred=predictions["x_train"][-1],
                    y_pred_proba=predictions_proba["x_train"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )
            cv_metric_valid.append(
                compute_metric(
                    y_valid,
                    y_pred=predictions["x_valid"][-1],
                    y_pred_proba=predictions_proba["x_valid"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )
            cv_metric_add_valid.append(
                compute_metric(
                    y_add_valid,
                    y_pred=predictions["x_add_valid"][-1],
                    y_pred_proba=predictions_proba["x_add_valid"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )
            cv_metric_test.append(
                compute_metric(
                    self.y_test,
                    y_pred=predictions["x_test"][-1],
                    y_pred_proba=predictions_proba["x_test"][-1],
                    metric=self.metric,
                    labels=self.labels,
                    multiclass=self.multiclass,
                )
            )

        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)
        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)
        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)
        self.y_train_hist.append(y_train_hist_tmp)
        self.y_valid_hist.append(y_valid_hist_tmp)
        self.y_add_valid_hist.append(y_add_valid_hist_tmp)
        self.y_pred_train_proba_hist.append(predictions_proba["x_train"])
        self.y_pred_valid_proba_hist.append(predictions_proba["x_valid"])
        self.y_pred_add_valid_proba_hist.append(predictions_proba["x_add_valid"])
        self.y_pred_test_proba_hist.append(predictions_proba["x_test"])

        cv_metric = dict(
            (value, key) for key, value in self.cv_metric_to_metric.items()
        )[self.metric]
        trial.set_user_attr(
            f"{cv_metric}_train",
            json.dumps(cv_metric_train, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_valid",
            json.dumps(cv_metric_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_add_valid",
            json.dumps(cv_metric_add_valid, cls=NumpyArrayEncoder),
        )
        trial.set_user_attr(
            f"{cv_metric}_test",
            json.dumps(cv_metric_test, cls=NumpyArrayEncoder),
        )

        # compute the mean of the metric over the folds and repeats
        metric_train = np.mean(cv_metric_train)
        metric_valid = np.mean(cv_metric_valid)
        metric_add_valid = np.mean(cv_metric_add_valid)
        metric_test = np.mean(cv_metric_test)
        trial.set_user_attr(f"{self.metric}_train", metric_train)
        trial.set_user_attr(f"{self.metric}_valid", metric_valid)
        trial.set_user_attr(f"{self.metric}_add_valid", metric_add_valid)
        trial.set_user_attr(f"{self.metric}_test", metric_test)

        # compute the metric on the test set also in ensemble style
        predictions_proba_test_ensemble = np.mean(predictions_proba["x_test"], axis=0)
        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)
        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums
        check_y_predict_proba(predictions_proba_test_ensemble)
        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)
        metric_test_ensemble = compute_metric(
            self.y_test,
            y_pred=predictions_test_ensemble,
            y_pred_proba=predictions_proba_test_ensemble,
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_ensemble", metric_test_ensemble)

        if self.bootstrap_test:
            # bootstrap the ensemble style test performance
            metric_test_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions_test_ensemble,
                y_pred_proba=predictions_proba_test_ensemble,
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap", metric_test_bootstrap
            )
            average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(
                metric_test_bootstrap
            )
            trial.set_user_attr(
                f"{self.metric}_test_ensemble_bootstrap_average",
                average_metric_test_bootstrap,
            )

        # refit on the train_valid set
        self.classifier.construct_pipeline(
            trial,
            refit=True,
            cat_features=self.cat_features,
            num_features=self.num_features,
        )
        self.classifier.fit(
            trial=trial,
            x_train=self.x_valid_train,
            y_train=self.y_valid_train,
            cat_features=self.cat_features,
        )

        # predict on the train_valid set and compute the metric
        predictions["x_valid_train"] = self.classifier.predict(self.x_valid_train)
        predictions_proba["x_valid_train"] = self.classifier.predict_proba(
            self.x_valid_train
        )
        metric_valid_train = compute_metric(
            self.y_valid_train,
            y_pred=predictions["x_valid_train"],
            y_pred_proba=predictions_proba["x_valid_train"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_valid_train", metric_valid_train)

        self.y_pred_valid_train_proba_hist.append(predictions_proba["x_valid_train"])

        # predict on the test set and compute the metric
        predictions["x_test_retrained"] = self.classifier.predict(self.x_test)
        predictions_proba["x_test_retrained"] = self.classifier.predict_proba(
            self.x_test
        )
        metric_test_retrained = compute_metric(
            self.y_test,
            y_pred=predictions["x_test_retrained"],
            y_pred_proba=predictions_proba["x_test_retrained"],
            metric=self.metric,
            labels=self.labels,
            multiclass=self.multiclass,
        )
        trial.set_user_attr(f"{self.metric}_test_retrained", metric_test_retrained)

        if self.bootstrap_test:
            # bootstrap the retrained test performance
            metric_test_retrained_bootstrap = bootstrap_test_performance(
                y_test=self.y_test,
                y_pred=predictions["x_test_retrained"],
                y_pred_proba=predictions_proba["x_test_retrained"],
                metric=self.metric,
                labels=self.labels,
                multiclass=self.multiclass,
                seed=self.seed,
            )
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap",
                metric_test_retrained_bootstrap,
            )
            average_metric_test_retrained_bootstrap = sum(
                metric_test_retrained_bootstrap
            ) / len(metric_test_retrained_bootstrap)
            trial.set_user_attr(
                f"{self.metric}_test_retrained_bootstrap_average",
                average_metric_test_retrained_bootstrap,
            )

        self.y_pred_test_proba_retrained_hist.append(
            predictions_proba["x_test_retrained"]
        )

        self.classifier.reset()

        # return the validation metric
        # Note: Here, we assume maximization but SmacSampler assumes minimization and will correct for it
        if self.metrics_direction[self.metric] == "minimize":
            return -metric_valid
        else:
            return metric_valid

File Path: reshufflebench/metrics/__init__.py
Content:
from .metrics import compute_metric

__all__ = ["compute_metric"]

File Path: reshufflebench/metrics/metrics.py
Content:
from typing import List

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    log_loss,
    roc_auc_score,
)


def compute_accuracy(
    y_true: np.array,
    y_pred: np.array,
    y_pred_proba: np.array,
    labels: List[int],
    multiclass: bool,
) -> float:
    """
    Compute accuracy score.
    """
    return accuracy_score(y_true, y_pred)


def compute_balanced_accuracy(
    y_true: np.array,
    y_pred: np.array,
    y_pred_proba: np.array,
    labels: List[int],
    multiclass: bool,
) -> float:
    """
    Compute balanced accuracy score.
    """
    return balanced_accuracy_score(y_true, y_pred)


def compute_logloss(
    y_true: np.array,
    y_pred: np.array,
    y_pred_proba: np.array,
    labels: List[int],
    multiclass: bool,
) -> float:
    """
    Compute logloss score.
    """
    return log_loss(y_true, y_pred_proba, labels=labels)


def compute_auc(
    y_true: np.array,
    y_pred: np.array,
    y_pred_proba: np.array,
    labels: List[int],
    multiclass: bool,
) -> float:
    """
    Compute AUC score.
    """
    if multiclass:
        return roc_auc_score(
            y_true, y_pred_proba, average="macro", multi_class="ovo", labels=labels
        )
    else:
        return roc_auc_score(y_true, y_pred_proba[:, 1])


def compute_metric(
    y_true: np.array,
    y_pred: np.array,
    y_pred_proba: np.array,
    metric: str,
    labels: List[int],
    multiclass: bool,
) -> float:
    """
    Compute a metric.
    """
    if metric == "accuracy":
        return compute_accuracy(
            y_true,
            y_pred=y_pred,
            y_pred_proba=y_pred_proba,
            labels=labels,
            multiclass=multiclass,
        )
    elif metric == "balanced_accuracy":
        return compute_balanced_accuracy(
            y_true,
            y_pred=y_pred,
            y_pred_proba=y_pred_proba,
            labels=labels,
            multiclass=multiclass,
        )
    elif metric == "logloss":
        return compute_logloss(
            y_true,
            y_pred=y_pred,
            y_pred_proba=y_pred_proba,
            labels=labels,
            multiclass=multiclass,
        )
    elif metric == "auc":
        return compute_auc(
            y_true,
            y_pred=y_pred,
            y_pred_proba=y_pred_proba,
            labels=labels,
            multiclass=multiclass,
        )
    else:
        raise ValueError(f"Unknown metric: {metric}")

File Path: reshufflebench/samplers/__init__.py
Content:
from .hebo import HeboSampler
from .smac_ import SmacSampler

_all__ = ["HeboSampler", "SmacSampler"]

File Path: reshufflebench/samplers/hebo.py
Content:
import random
import warnings
from typing import Optional, Sequence

import numpy as np
import optuna
import pandas as pd
import torch
from hebo.design_space import DesignSpace
from hebo.optimizers.hebo import HEBO
from optuna.study import Study
from optuna.trial import FrozenTrial, TrialState


class HeboSampler(optuna.samplers.BaseSampler):
    """
    Wrapping HEBO as an Optuna sampler.
    """

    def __init__(
        self,
        seed: int,
        hebo_search_space: DesignSpace,
        internal_optuna_search_space: dict,
    ):
        self.seed = seed
        self.hebo_search_space = hebo_search_space
        self.internal_optuna_search_space = internal_optuna_search_space
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)  # GPs
        self.optimizer = HEBO(hebo_search_space, scramble_seed=self.seed)
        self.current_trial = 0
        self.params_history = []
        self.values_history = []
        self.fallback_triggered = False

    def sample_relative(self, study, trial, search_space):
        # Note: HEBO apparently does not have its own fallback mechanism
        try:
            params = self.optimizer.suggest(1)
        except Exception as e:
            warnings.warn(
                f"HEBO failed with error: {e}. Falling back to random sampling."
            )
            self.fallback_triggered = True
            params = {}
            for param_name, param_distribution in search_space.items():
                params[param_name] = self.sample_param_random_fallback(
                    study=study,
                    trial=trial,
                    param_name=param_name,
                    param_distribution=param_distribution,
                )
            params = pd.DataFrame([params])
        params.reset_index(drop=True, inplace=True)
        self.params_history.append(params)
        self.current_trial += 1
        return {
            column: params.at[0, column] for column in params.columns
        }  # workaround to prevent pandas to drop dtype information

    def infer_relative_search_space(self, study, trial):
        # return optuna.search_space.intersection_search_space(study.get_trials(deepcopy=False))
        return self.internal_optuna_search_space

    def sample_independent(self, study, trial, param_name, param_distribution):
        self.fallback_triggered = True
        independent_sampler = optuna.samplers.RandomSampler()
        return independent_sampler.sample_independent(
            study, trial, param_name, param_distribution
        )

    def sample_param_random_fallback(
        self, study, trial, param_name, param_distribution
    ):
        independent_sampler = optuna.samplers.RandomSampler(seed=self.seed)
        return independent_sampler.sample_independent(
            study, trial, param_name, param_distribution
        )

    def before_trial(self, study: Study, trial: FrozenTrial) -> None:
        self.fallback_triggered = False

    def after_trial(
        self,
        study: Study,
        trial: FrozenTrial,
        state: TrialState,
        values: Optional[Sequence[float]],
    ) -> None:
        assert state in [TrialState.COMPLETE, TrialState.FAIL, TrialState.PRUNED]
        # Note: This assumes single-objective optimization
        # Note: HEBO assumes minimization therefore we correct for that here
        if state == TrialState.COMPLETE:
            if study.direction == optuna.study.StudyDirection.MAXIMIZE:
                self.values_history.append(-np.array(values))
            else:
                self.values_history.append(np.array(values))
        else:
            self.values_history.append(np.nan)  # FIXME: check
        self.optimizer.observe(
            self.params_history[-1],
            self.values_history[-1],
        )

File Path: reshufflebench/samplers/smac_.py
Content:
import random
from typing import Optional, Sequence

import numpy as np
import optuna
import pandas as pd
import torch
from ConfigSpace.configuration_space import ConfigurationSpace
from optuna.study import Study
from optuna.trial import FrozenTrial, TrialState
from smac import HyperparameterOptimizationFacade, Scenario
from smac.runhistory.dataclasses import TrialValue


# https://automl.github.io/SMAC3/main/examples/1_basics/3_ask_and_tell.html#ask-and-tell
class SmacSampler(optuna.samplers.BaseSampler):
    """
    Wrapping SMAC as an Optuna sampler.
    """

    def __init__(
        self,
        seed: int,
        configspace_search_space: ConfigurationSpace,
        internal_optuna_search_space: dict,
        n_trials: int,
        fid: str,
    ):
        self.seed = seed
        self.configspace_search_space = configspace_search_space
        self.internal_optuna_search_space = internal_optuna_search_space
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        self.scenario = Scenario(
            self.configspace_search_space,
            deterministic=False,
            n_trials=n_trials,
            seed=self.seed,
        )
        self.intensifier = HyperparameterOptimizationFacade.get_intensifier(
            self.scenario, max_config_calls=1
        )
        self.smac = HyperparameterOptimizationFacade(
            self.scenario,
            target_function=fid,
            intensifier=self.intensifier,
            overwrite=True,
        )
        self.current_trial = 0
        self.info_history = []
        self.params_history = []
        self.values_history = []
        self.fallback_triggered = False

    def sample_relative(self, study, trial, search_space):
        info = self.smac.ask()
        params = pd.DataFrame(info.config.get_dictionary(), index=[0])
        params.reset_index(drop=True, inplace=True)
        self.info_history.append(info)
        self.params_history.append(params)
        self.current_trial += 1
        return {
            column: params.at[0, column] for column in params.columns
        }  # workaround to prevent pandas to drop dtype information

    def infer_relative_search_space(self, study, trial):
        # return optuna.search_space.intersection_search_space(study.get_trials(deepcopy=False))
        return self.internal_optuna_search_space

    def sample_independent(self, study, trial, param_name, param_distribution):
        self.fallback_triggered = True
        independent_sampler = optuna.samplers.RandomSampler()
        return independent_sampler.sample_independent(
            study, trial, param_name, param_distribution
        )

    def before_trial(self, study: Study, trial: FrozenTrial) -> None:
        self.fallback_triggered = False

    def after_trial(
        self,
        study: Study,
        trial: FrozenTrial,
        state: TrialState,
        values: Optional[Sequence[float]],
    ) -> None:
        assert state in [TrialState.COMPLETE, TrialState.FAIL, TrialState.PRUNED]
        # Note: This assumes single-objective optimization
        # Note: SMAC assumes minimization therefore we correct for that here
        if state == TrialState.COMPLETE:
            if study.direction == optuna.study.StudyDirection.MAXIMIZE:
                self.values_history.append(-np.array(values))
            else:
                self.values_history.append(np.array(values))
        else:
            self.values_history.append(np.nan)  # FIXME: check
        value = TrialValue(cost=self.values_history[-1])
        self.smac.tell(self.info_history[-1], value=value)

File Path: reshufflebench/utils/__init__.py
Content:
from .utils import (
    NumpyArrayEncoder,
    bootstrap_test_performance,
    check_y_predict_proba,
    construct_x_and_y_add_valid,
    int_or_none,
    load_list_of_1d_arrays,
    load_list_of_list_of_1d_arrays,
    load_list_of_list_of_pd_arrays,
    load_list_of_pd_arrays,
    load_single_array,
    save_list_of_1d_arrays,
    save_list_of_list_of_1d_arrays,
    save_list_of_list_of_pd_arrays,
    save_list_of_pd_arrays,
    save_single_array,
    str2bool,
    unify_missing_values,
)

__all__ = [
    "NumpyArrayEncoder",
    "bootstrap_test_performance",
    "construct_x_and_y_add_valid",
    "unify_missing_values",
    "load_list_of_1d_arrays",
    "load_list_of_list_of_1d_arrays",
    "load_list_of_list_of_pd_arrays",
    "load_list_of_pd_arrays",
    "load_single_array",
    "save_list_of_1d_arrays",
    "save_list_of_list_of_1d_arrays",
    "save_list_of_list_of_pd_arrays",
    "save_list_of_pd_arrays",
    "save_single_array",
    "str2bool",
    "int_or_none",
    "check_y_predict_proba",
]

File Path: reshufflebench/utils/utils.py
Content:
import argparse
import warnings
from json import JSONEncoder
from typing import List

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from sklearn.utils import resample

from reshufflebench.metrics import compute_metric


class NumpyArrayEncoder(JSONEncoder):
    """
    Encode numpy arrays to lists.
    """

    def default(self, obj: object) -> object:
        """
        Encode numpy arrays to lists.
        """
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return JSONEncoder.default(self, obj)


def str2bool(value: str) -> bool:
    """
    Convert a string to a boolean.
    """
    if isinstance(value, bool):
        return value
    if value.lower() in ("yes", "true", "t", "y", "1"):
        return True
    elif value.lower() in ("no", "false", "f", "n", "0"):
        return False
    else:
        raise argparse.ArgumentTypeError("Boolean value expected.")


def int_or_none(value: str) -> int or None:
    """
    Convert a string to an integer or None.
    """
    try:
        return int(value)
    except ValueError:
        if value.lower() == "none":
            return None
        else:
            raise argparse.ArgumentTypeError(f"{value} is not a valid integer or None")


def save_single_array(file_path: str, data: np.array) -> None:
    """
    Save a single 1-dimensional numpy array to a parquet file.
    This is usually an integer array of the target variable.
    """
    pa_table = pa.Table.from_arrays(
        [pa.array(data.tolist(), type=pa.int32())], names=["data"]
    )
    pq.write_table(pa_table, file_path)


def load_single_array(file_path: str) -> List:
    """
    Load a single 1-dimensional numpy array from a parquet file.
    This is usually an integer array of the target variable.
    """
    table = pq.read_table(file_path, use_threads=False, memory_map=True)
    reconstructed_data = np.array(table.column(0).to_pylist(), dtype=np.int32)
    return reconstructed_data


def save_list_of_1d_arrays(file_path: str, data: List) -> None:
    """
    Save a list of 1-dimensional numpy arrays to a parquet file.
    These are usually integer arrays of the target variable.
    """
    flattened_data = []
    column_names = []
    for i, numpy_array in enumerate(data):
        flattened_data.append(numpy_array.tolist())
        column_names.append(f"data_{i}")
    arrays = [pa.array(column, type=pa.int32()) for column in flattened_data]
    pa_table = pa.Table.from_arrays(arrays, names=column_names)
    pq.write_table(pa_table, file_path)


def load_list_of_1d_arrays(file_path: str) -> List:
    """
    Load a list of 1-dimensional numpy arrays from a parquet file.
    These are usually integer arrays of the target variable.
    """
    df = pd.read_parquet(
        file_path, engine="pyarrow", use_threads=False, memory_map=True
    )
    reconstructed_data = []
    for column in df.columns:
        numpy_array = np.array(df[column].tolist(), dtype=np.int32)
        reconstructed_data.append(numpy_array)
    return reconstructed_data


def save_list_of_pd_arrays(file_path: str, data: List) -> None:
    """
    Save a list of p-dimensional (n times p) numpy arrays to a parquet file.
    These are usually probability arrays.
    """
    flattened_data = []
    column_names = []
    types = []
    for i, numpy_array in enumerate(data):
        for column in range(numpy_array.shape[1]):
            types.append(
                pa.float64() if numpy_array.dtype == np.float64 else pa.float32()
            )
            flattened_data.append(numpy_array[:, column].tolist())
            column_names.append(f"data_{i}_column_{column}")
    if not all(types[0] == t for t in types):
        raise ValueError("All types must be the same")
    arrays = [pa.array(column, type=types[0]) for column in flattened_data]
    pa_table = pa.Table.from_arrays(arrays, names=column_names)
    pq.write_table(pa_table, file_path)


def load_list_of_pd_arrays(file_path: str) -> List:
    """
    Load a list of p-dimensional (n times p) numpy arrays from a parquet file.
    These are usually probability arrays.
    """
    df = pd.read_parquet(
        file_path, engine="pyarrow", use_threads=False, memory_map=True
    )
    indices = set()
    for column in df.columns:
        i, _, _ = column.split("_")[1:4]
        indices.add((int(i)))
    reconstructed_data = []
    for i in sorted(indices):
        cols = [col for col in df.columns if col.startswith(f"data_{i}_")]
        cols.sort(
            key=lambda x: int(x.rsplit("_", 1)[-1])
        )  # robust sorting in the case of p > 10
        array = df[cols].to_numpy()
        reconstructed_data.append(array)
    return reconstructed_data


def save_list_of_list_of_1d_arrays(file_path: str, data: List) -> None:
    """
    Save a list of list of 1-dimensional numpy arrays to a parquet file.
    These are usually integer arrays of the target variable.
    """
    flattened_data = []
    column_names = []
    for i, sublist in enumerate(data):
        for j, numpy_array in enumerate(sublist):
            flattened_data.append(numpy_array.tolist())
            column_names.append(f"data_{i}_{j}")

    pa_table = pa.Table.from_arrays(
        [pa.array(column, type=pa.int32()) for column in flattened_data],
        names=column_names,
    )
    pq.write_table(pa_table, file_path)


def load_list_of_list_of_1d_arrays(file_path: str) -> List:
    """
    Load a list of list of 1-dimensional numpy arrays from a parquet file.
    These are usually integer arrays of the target variable.
    """
    df = pd.read_parquet(
        file_path, engine="pyarrow", use_threads=False, memory_map=True
    )
    max_i = max(int(col.split("_")[1]) for col in df.columns)
    reconstructed_data = [[] for _ in range(max_i + 1)]
    for column in df.columns:
        i, j = map(int, column.split("_")[1:3])
        numpy_array = np.array(df[column].tolist(), dtype=np.int32)
        reconstructed_data[i].append(numpy_array)
    return reconstructed_data


def save_list_of_list_of_pd_arrays(file_path: str, data: List) -> None:
    """
    Save a list of list of p-dimensional (n times p) numpy arrays to a parquet file.
    These are usually probability arrays.
    """
    flattened_data = []
    column_names = []
    types = []
    for i, sublist in enumerate(data):
        for j, numpy_array in enumerate(sublist):
            for column in range(numpy_array.shape[1]):
                types.append(
                    pa.float64() if numpy_array.dtype == np.float64 else pa.float32()
                )
                flattened_data.append(numpy_array[:, column].tolist())
                column_names.append(f"data_{i}_{j}_column_{column}")
    if not all(types[0] == t for t in types):
        raise ValueError("All types must be the same")
    arrays = [pa.array(column, type=types[0]) for column in flattened_data]
    pa_table = pa.Table.from_arrays(arrays, names=column_names)
    pq.write_table(pa_table, file_path)


def load_list_of_list_of_pd_arrays(file_path: str) -> List:
    """
    Load a list of list of p-dimensional (n times p) numpy arrays from a parquet file.
    These are usually probability arrays.
    """
    df = pd.read_parquet(
        file_path, engine="pyarrow", use_threads=False, memory_map=True
    )
    indices = set()
    for column in df.columns:
        i, j, _ = column.split("_")[1:4]
        indices.add((int(i), int(j)))
    reconstructed_data = []
    for i, j in sorted(indices):
        while len(reconstructed_data) <= i:
            reconstructed_data.append([])
        cols = [col for col in df.columns if col.startswith(f"data_{i}_{j}_")]
        cols.sort(
            key=lambda x: int(x.rsplit("_", 1)[-1])
        )  # robust sorting in the case of p > 10
        array = df[cols].to_numpy()
        reconstructed_data[i].append(array)
    return reconstructed_data


def unify_missing_values(df: pd.DataFrame) -> pd.DataFrame:
    """
    Unify missing values in a DataFrame based on the dtype of each column.
    This does not impute missing values but rather brings them into a standardized format.
    """
    for col in df.columns:
        if pd.api.types.is_object_dtype(df[col]):
            df[col] = df[col].fillna("NaN")
        elif pd.api.types.is_numeric_dtype(df[col]):
            df[col] = df[col].fillna(np.nan)
        else:
            raise ValueError(f"Unsupported dtype: {df[col].dtype}")
    return df


def construct_x_and_y_add_valid(
    x_valid: pd.DataFrame,
    y_valid: np.array,
    x_add_valid: pd.DataFrame,
    y_add_valid: np.array,
) -> (pd.DataFrame, np.array):
    """
    Construct the additional validation data by combining the validation data and the additional validation data.
    """
    x_add_valid_final = pd.concat([x_valid, x_add_valid])
    y_add_valid_final = np.concatenate((y_valid, y_add_valid))
    return x_add_valid_final, y_add_valid_final


def check_y_predict_proba(y_predict_proba: np.array) -> np.array:
    """
    Check if a numpy array of probability predictions sums to 1.0.
    """
    row_sums = y_predict_proba.sum(1)
    if any(row_sums != 1.0):
        largest_absolute_difference = np.max(np.abs(row_sums - 1.0))
        warnings.warn(
            f"Probabilities do not sum to 1.0. Largest absolute difference: {largest_absolute_difference}"
        )
        if largest_absolute_difference > 1e-6:
            raise ValueError("Probabilities do not sum to 1.0")
    return y_predict_proba


def bootstrap_test_performance(
    y_test: np.array,
    y_pred: np.array,
    y_pred_proba: np.array,
    metric: str,
    labels: list,
    multiclass: bool,
    seed: int,
    n_samples: int = 10,
) -> List:
    """
    Bootstrap the test performance of a model.
    """
    metric_test_bootstrap = []
    n_test = len(y_test)
    for b in range(n_samples):
        idx = resample(
            list(range(n_test)),
            replace=True,
            n_samples=n_test,
            random_state=seed + b,
            stratify=y_test,
        )
        metric_test_bootstrap.append(
            compute_metric(
                y_test[idx],
                y_pred=y_pred[idx],
                y_pred_proba=y_pred_proba[idx, :],
                metric=metric,
                labels=labels,
                multiclass=multiclass,
            )
        )
    return metric_test_bootstrap

File Path: setup.py
Content:
from setuptools import find_packages, setup

setup(
    name="reshufflebench",
    version="0.0.9",
    author="Lennart Schneider",
    author_email="lennart.sch@web.de",
    packages=find_packages(),
    url="https://github.com/slds-lmu/paper_2024_reshuffling",
    license="MIT License",
    description="Python package to benchmark and analyse the effect of reshuffling during HPO.",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
    install_requires=[
        "numpy==1.25.2",
        "pandas==2.1.0",
        "scipy==1.11.2",
        "optuna==3.3.0",
        "scikit-learn==1.3.0",
        "torch==2.1.1",
        "tqdm==4.66.1",
        "setuptools==65.5.1",
        "pyarrow==13.0.0",
        "openml==0.14.1",
        "tabpfn==0.1.9",
        "xgboost==2.0.2",
        "catboost==1.2.1",
        "numdifftools==0.9.41",
        "ConfigSpace==1.2.0",
    ],
    extras_require={
        "test": ["pytest==7.4.3"],
        "HEBO": ["HEBO==0.3.5"],
        "smac": ["smac==2.2.0"],
    },
    python_requires=">=3.10",
    classifiers=[
        # Trove classifiers
        # Full list at https://pypi.org/classifiers/
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.10",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
)

File Path: simulations/create_simulations.py
Content:
if __name__ == "__main__":
    import os

    file = "run_simulations.sh"

    with open(file, "+w") as f:
        for alpha in [0.5, 1, 5, 10]:
            for lengthscale in [0.1, 0.5, 1, 5]:
                f.write(
                    f"python3 simulate.py --alpha {alpha} --lengthscale {lengthscale}\n"
                )

    os.chmod(file, 0o755)

File Path: simulations/simulate.py
Content:
from functools import partial

import numpy as np
import torch


def simulate_gp(x: torch.Tensor, mu: callable, cov: callable) -> torch.Tensor:
    """
    Simulate a Gaussian process.

    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.
    :param mu: Function to compute the mean of the Gaussian process.
    :param cov: Function to compute the covariance matrix of the Gaussian process.

    :return: A tensor of shape (n, d) representing the simulated Gaussian process.
    """
    sigma_2 = 1e-5
    n = x.shape[0]
    d = x.shape[1]
    mu_x = mu(x)
    K_x = cov(x, x)
    K_x += sigma_2 * torch.eye(n)
    eigenvalues, eigenvectors = torch.linalg.eigh(K_x)
    positive_eigenvalues = torch.clamp(eigenvalues, min=0)
    sqrt_K_x = eigenvectors @ torch.diag(positive_eigenvalues.sqrt()) @ eigenvectors.T
    z = torch.normal(0, 1, (n, d))
    return mu_x + sqrt_K_x @ z


def mu_factory(x: torch.Tensor, alpha: float) -> torch.Tensor:
    """
    The mean function of the Gaussian process.

    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.
    :param alpha: The alpha parameter of the mean function.

    :return: A tensor of shape (n, 1) representing the mean of the Gaussian process at each point.
    """
    values = alpha * (x - 0.5).pow(2)
    return values


def cov_factory(
    x1: torch.Tensor,
    x2: torch.Tensor,
    lengthscale: float,
    tau: float = None,
    shuffled: bool = False,
) -> torch.Tensor:
    """
    Vectorized computation of the covariance matrix of the Gaussian process.

    :param x1: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.
    :param x2: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.
    :param lengthscale: The lengthscale parameter of the covariance function.
    :param shuffled: Whether to assume a shuffled version of the covariance function.
    :param tau: The tau parameter of the shuffled covariance function.

    :return: A tensor of shape (n, n) representing the covariance matrix of the Gaussian process.
    """
    sq_dist = torch.sum((x1[:, None, :] - x2[None, :, :]) ** 2, dim=-1)
    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))
    if shuffled:
        K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K
    return K


def kernel_factory(
    x1: torch.Tensor,
    x2: torch.Tensor,
    lengthscale: float,
    tau: float = None,
    shuffled: bool = False,
) -> torch.Tensor:
    """
    Computation of the covariance between two points.

    :param x1: A tensor of shape (1, d) where d is the dimensionality.
    :param x2: A tensor of shape (1, d) where d is the dimensionality.
    :param lengthscale:  The lengthscale parameter of the covariance function.
    :param shuffled: Whether to assume a shuffled version of the covariance function.
    :param tau: The tau parameter of the shuffled covariance function.

    :return: A scalar tensor representing the covariance between the two points.
    """
    sq_dist = torch.sum((x1 - x2) ** 2, dim=-1)
    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))
    if shuffled:
        if x1 == x2:
            K = (tau**2) * K
    return K


if __name__ == "__main__":
    import argparse
    import os

    import pandas as pd
    from tqdm import tqdm

    parser = argparse.ArgumentParser()
    parser.add_argument("--alpha", type=float, default=1)
    parser.add_argument("--lengthscale", type=float, default=0.5)
    args = parser.parse_args()

    np.random.seed(42)
    torch.manual_seed(42)

    results_path = "results"
    d = 1
    alpha = args.alpha
    lengthscale = args.lengthscale
    n_replicates = 10000

    x = torch.linspace(0, 1, 101).reshape(-1, d)
    mu = partial(mu_factory, alpha=alpha)
    kernel = partial(kernel_factory, lengthscale=lengthscale)
    cov = partial(cov_factory, lengthscale=lengthscale)
    taus = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    taus.sort()
    results = {tau: {"y": [], "y_shuffled": [], "y_diff": []} for tau in taus}
    for tau in taus:
        y_mu_y_list = []
        y_mu_y_shuffled_list = []
        y_diff_list = []
        for _ in tqdm(range(n_replicates)):
            y = simulate_gp(x, mu, cov)
            y_shuffled = simulate_gp(x, mu, partial(cov, shuffled=True, tau=tau))
            y_mu_y = mu(x[y.argmin(dim=0)])
            y_mu_y_shuffled = mu(x[y_shuffled.argmin(dim=0)])
            y_mu_y_list.append(y_mu_y)
            y_mu_y_shuffled_list.append(y_mu_y_shuffled)
            y_diff_list.append(y_mu_y - y_mu_y_shuffled)
        results[tau]["y"] = torch.tensor(y_mu_y_list)
        results[tau]["y_shuffled"] = torch.tensor(y_mu_y_shuffled_list)
        results[tau]["y_diff"] = torch.tensor(y_diff_list)

    results_df = pd.DataFrame(results[taus[0]])
    results_df["replicate"] = range(n_replicates)
    results_df["tau"] = taus[0]
    for tau in taus[1:]:
        results_df_tmp = pd.DataFrame(results[tau])
        results_df_tmp["replicate"] = range(n_replicates)
        results_df_tmp["tau"] = tau
        results_df = pd.concat([results_df, results_df_tmp], axis=0)
    results_df["alpha"] = alpha
    results_df["lengthscale"] = lengthscale
    results_df["scenario"] = str(alpha) + "_" + str(lengthscale)
    results_df = results_df.reset_index(drop=True)
    results_df.to_csv(
        os.path.join(
            results_path,
            f"results_alpha_{alpha}_lengthscale_{lengthscale}.csv",
        ),
        index=False,
    )

File Path: tests/__init__.py
Content:

File Path: tests/test_learner_random_cv.py
Content:
import os

import numpy as np
import optuna
import pytest

from analyze.result_analyzer import ResultAnalyzer
from reshufflebench.algorithms import CatBoost, FunnelMLP, LogReg, XGBoost
from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner import LearnerRandomCV
from reshufflebench.metrics import compute_metric


@pytest.fixture(params=["catboost", "funnel_mlp", "logreg", "xgboost"])
def classifier(request: pytest.FixtureRequest) -> Classifier:
    if request.param == "catboost":
        return CatBoost(seed=2409)
    elif request.param == "funnel_mlp":
        return FunnelMLP(seed=2409)
    elif request.param == "logreg":
        return LogReg(seed=2409)
    elif request.param == "xgboost":
        return XGBoost(seed=2409)


@pytest.fixture(params=[True, False])
def reshuffle(request: pytest.FixtureRequest) -> bool:
    return request.param


def test_learner_random_cv(classifier: Classifier, reshuffle: bool) -> None:
    learner_random = LearnerRandomCV(
        classifier=classifier,
        data_id=1169,
        train_valid_size=100,
        reshuffle=reshuffle,
        n_splits=5,
        n_repeats=2,
        test_size=200,
        add_valid_size=100,
        n_trials=2,
        seed=2409,
    )
    learner_random.run()

    analyzer = ResultAnalyzer(
        learner_random.results_path,
        seed=learner_random.seed,
        n_repeats=learner_random.n_repeats,
    )
    analyzer.load(additional_file_types="all")

    results = analyzer.results
    assert len(results) == 2

    cv_metrics = ["accuracies", "balanced_accuracies", "loglosses", "aucs"]

    columns = [
        f"user_attrs_{metric}_{predict_set}"
        for metric in learner_random.metrics
        for predict_set in [
            "train",
            "valid",
            "add_valid",
            "test",
            "valid_train",
            "test_retrained",
        ]
    ]
    columns_cv = [
        f"user_attrs_{metric}_{predict_set}"
        for metric in cv_metrics
        for predict_set in ["train", "valid", "add_valid", "test"]
    ]
    columns_cv_ensemble = [
        f"user_attrs_{metric}_test_ensemble" for metric in learner_random.metrics
    ]

    cv_metric_to_metric = {
        "accuracies": "accuracy",
        "balanced_accuracies": "balanced_accuracy",
        "loglosses": "logloss",
        "aucs": "auc",
    }
    metric_to_cv_metric = {v: k for k, v in cv_metric_to_metric.items()}

    assert all([column in results.columns for column in columns])
    assert all([column in results.columns for column in columns_cv])
    assert all([column in results.columns for column in columns_cv_ensemble])

    predict_sets = [
        # "y_pred_train_proba_hist",
        "y_pred_valid_proba_hist",
        "y_pred_add_valid_proba_hist",
        "y_pred_test_proba_hist",
        # "y_pred_valid_train_proba_hist",
        "y_pred_test_proba_retrained_hist",
    ]
    map_predict_sets_to_y_sets = {
        # "y_pred_train_proba_hist": "y_train_hist",
        "y_pred_valid_proba_hist": "y_valid_hist",
        "y_pred_add_valid_proba_hist": "y_add_valid_hist",
        "y_pred_test_proba_hist": "y_test",
        # "y_pred_valid_train_proba_hist": "y_valid_train",
        "y_pred_test_proba_retrained_hist": "y_test",
    }
    map_predict_sets_to_orig_metrics = {
        # "y_pred_train_proba_hist": "train",
        "y_pred_valid_proba_hist": "valid",
        "y_pred_add_valid_proba_hist": "add_valid",
        "y_pred_test_proba_hist": "test",
        # "y_pred_valid_train_proba_hist": "valid_train",
        "y_pred_test_proba_retrained_hist": "test_retrained",
    }
    map_predict_sets_to_refit = {
        # "y_pred_train_proba_hist": False,
        "y_pred_valid_proba_hist": False,
        "y_pred_add_valid_proba_hist": False,
        "y_pred_test_proba_hist": False,
        # "y_pred_valid_train_proba_hist": True,
        "y_pred_test_proba_retrained_hist": True,
    }
    map_predict_sets_to_y_vary = {
        # "y_pred_train_proba_hist": True,
        "y_pred_valid_proba_hist": True,
        "y_pred_add_valid_proba_hist": True,
        "y_pred_test_proba_hist": False,
        # "y_pred_valid_train_proba_hist": False,
        "y_pred_test_proba_retrained_hist": False,
    }

    columns_checked = []

    for predict_set in predict_sets:
        y = getattr(analyzer, map_predict_sets_to_y_sets[predict_set])
        y_pred_proba = getattr(analyzer, predict_set)

        if (
            map_predict_sets_to_y_sets[predict_set] == "y_train_hist"
            or map_predict_sets_to_y_sets[predict_set] == "y_valid_hist"
            or map_predict_sets_to_y_sets[predict_set] == "y_add_valid_hist"
        ):
            if reshuffle:
                assert all([any(y[0][i] != y[1][i]) for i in range(len(y[0]))])

        for trial in range(len(results)):
            if map_predict_sets_to_y_vary[predict_set]:
                if reshuffle:
                    y_tmp = y[trial]
                else:
                    y_tmp = y[0]
            else:
                y_tmp = y
            y_pred_proba_tmp = y_pred_proba[trial]
            if map_predict_sets_to_y_vary[predict_set]:
                assert (
                    len(y_pred_proba_tmp)
                    == learner_random.n_repeats * learner_random.n_splits
                )
            if predict_set == "y_pred_train_proba_hist":
                assert y_tmp[0].shape[0] == learner_random.train_size
                assert y_tmp[0].shape[0] == int(0.8 * 100)
                assert y_tmp[0].shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_valid_proba_hist":
                assert y_tmp[0].shape[0] == learner_random.valid_size
                assert y_tmp[0].shape[0] == int(0.2 * 100)
                assert y_tmp[0].shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_add_valid_proba_hist":
                assert y_tmp[0].shape[0] == learner_random.valid_size + int(
                    learner_random.add_valid_size / learner_random.n_splits
                )  # add_valid_size is the size of the total additional validation set, not the size of the validation set for each split
                assert y_tmp[0].shape[0] == int(0.2 * 100) + (100 / 5)
                assert y_tmp[0].shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_test_proba_hist":
                assert y_tmp.shape[0] == learner_random.test_size
                assert y_tmp.shape[0] == 200
                assert y_tmp.shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_valid_train_proba_hist":
                assert (
                    y_tmp.shape[0]
                    == learner_random.train_size + learner_random.valid_size
                )
                assert y_tmp.shape[0] == 100
                assert y_tmp.shape[0] == y_pred_proba_tmp.shape[0]
            elif predict_set == "y_pred_test_proba_retrained_hist":
                assert y_tmp.shape[0] == learner_random.test_size
                assert y_tmp.shape[0] == 200
                assert y_tmp.shape[0] == y_pred_proba_tmp.shape[0]
            for metric in learner_random.metrics:
                if map_predict_sets_to_refit[predict_set]:
                    y_pred_tmp = np.argmax(y_pred_proba_tmp, axis=1)
                    recalculated_metric = compute_metric(
                        y_tmp,
                        y_pred=y_pred_tmp,
                        y_pred_proba=y_pred_proba_tmp,
                        metric=metric,
                        labels=learner_random.labels,
                        multiclass=learner_random.multiclass,
                    )
                    assert (
                        abs(
                            recalculated_metric
                            - results[
                                f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                            ][trial]
                        )
                        < 1e-12
                    )
                    columns_checked.append(
                        f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                    )
                else:
                    recalculated_metric = []
                    for i in range(len(y_pred_proba_tmp)):
                        y_tmp_i = (
                            y_tmp[i]
                            if map_predict_sets_to_y_vary[predict_set]
                            else y_tmp
                        )
                        y_pred_tmp = np.argmax(y_pred_proba_tmp[i], axis=1)
                        recalculated_metric.append(
                            compute_metric(
                                y_tmp_i,
                                y_pred=y_pred_tmp,
                                y_pred_proba=y_pred_proba_tmp[i],
                                metric=metric,
                                labels=learner_random.labels,
                                multiclass=learner_random.multiclass,
                            )
                        )
                    assert all(
                        abs(
                            np.array(recalculated_metric)
                            - np.array(
                                eval(
                                    results[
                                        f"user_attrs_{metric_to_cv_metric[metric]}_{map_predict_sets_to_orig_metrics[predict_set]}"
                                    ][trial]
                                )
                            )
                        )
                        < 1e-12
                    )
                    assert (
                        abs(
                            np.mean(recalculated_metric)
                            - results[
                                f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                            ][trial]
                        )
                        < 1e-12
                    )
                    columns_checked.append(
                        f"user_attrs_{metric_to_cv_metric[metric]}_{map_predict_sets_to_orig_metrics[predict_set]}"
                    )
                    columns_checked.append(
                        f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                    )

    # test_ensemble
    y = getattr(analyzer, "y_test")
    y_pred_proba = getattr(analyzer, "y_pred_test_proba_hist")
    for trial in range(len(results)):
        y_tmp = y
        y_pred_proba_tmp = y_pred_proba[trial]
        assert (
            len(y_pred_proba_tmp) == learner_random.n_repeats * learner_random.n_splits
        )
        assert y_tmp.shape[0] == 200
        assert y_tmp.shape[0] == y_pred_proba_tmp[0].shape[0]
        y_pred_proba_tmp_ensemble = np.mean(y_pred_proba_tmp, axis=0)
        y_pred_tmp_ensemble = np.argmax(y_pred_proba_tmp_ensemble, axis=1)
        for metric in learner_random.metrics:
            recalculated_metric = compute_metric(
                y_tmp,
                y_pred=y_pred_tmp_ensemble,
                y_pred_proba=y_pred_proba_tmp_ensemble,
                metric=metric,
                labels=learner_random.labels,
                multiclass=learner_random.multiclass,
            )
            assert (
                abs(
                    recalculated_metric
                    - results[f"user_attrs_{metric}_test_ensemble"][trial]
                )
                < 1e-12
            )
            columns_checked.append(f"user_attrs_{metric}_test_ensemble")

    if learner_random.classifier.classifier_id in ["catboost", "xgboost"]:
        assert all(
            results["user_attrs_actual_iterations"].apply(lambda x: len(eval(x))).values
            == learner_random.n_repeats * learner_random.n_splits
        )
        columns_checked.append("user_attrs_actual_iterations")

    remaining_columns = [
        column for column in results.columns if column not in columns_checked
    ]
    remaining_columns = [
        column for column in remaining_columns if "_valid_train" not in column
    ]
    remaining_columns = [
        column for column in remaining_columns if "_train" not in column
    ]
    remaining_columns = [
        column
        for column in remaining_columns
        if "user_attrs_error_on_fit" not in column
    ]

    assert all(["user_attrs" not in column for column in remaining_columns])

    # check cv_splits
    y_valid_train = getattr(analyzer, "y_valid_train")

    # cv_splits_hist_train = getattr(analyzer, "cv_splits_hist_train")
    # y_train_hist = getattr(analyzer, "y_train_hist")
    # assert len(cv_splits_hist_train) == 2
    # for i in range(len(cv_splits_hist_train)):
    #    assert (
    #        len(cv_splits_hist_train[i])
    #        == learner_random.n_repeats * learner_random.n_splits
    #    )
    #    for j in range(len(cv_splits_hist_train[i])):
    #        assert cv_splits_hist_train[i][j].shape[0] == learner_random.train_size
    #        train_indices = cv_splits_hist_train[i][j]
    #        assert all(y_valid_train[train_indices] == y_train_hist[i][j])

    cv_splits_hist_valid = getattr(analyzer, "cv_splits_hist_valid")
    y_valid_hist = getattr(analyzer, "y_valid_hist")
    if reshuffle:
        assert len(cv_splits_hist_valid) == 2
    else:
        assert len(cv_splits_hist_valid) == 1
    for i in range(len(cv_splits_hist_valid)):
        assert (
            len(cv_splits_hist_valid[i])
            == learner_random.n_repeats * learner_random.n_splits
        )
        for j in range(len(cv_splits_hist_valid[i])):
            assert cv_splits_hist_valid[i][j].shape[0] == learner_random.valid_size
            valid_indices = cv_splits_hist_valid[i][j]
            assert all(y_valid_train[valid_indices] == y_valid_hist[i][j])

    # Note: y_add_valid is the additional validation set that was then partitioned into n_splits parts and added to the validation set for each split
    #       y_add_valid_hist always contains the concatenation of the original validation set and the additional validation set for each split
    #       similarly cv_splits_add_valid_hist_valid always contains the concatenation of the indices of the original validation set and the additional validation set for each split
    y_add_valid_use = getattr(analyzer, "y_add_valid_use")
    cv_splits_add_valid_hist_valid = getattr(analyzer, "cv_splits_add_valid_hist_valid")
    y_add_valid_hist = getattr(analyzer, "y_add_valid_hist")
    if reshuffle:
        assert len(cv_splits_add_valid_hist_valid) == 2
    else:
        assert len(cv_splits_add_valid_hist_valid) == 1
    for i in range(len(cv_splits_add_valid_hist_valid)):
        assert (
            len(cv_splits_add_valid_hist_valid[i])
            == learner_random.n_repeats * learner_random.n_splits
        )
        for j in range(len(cv_splits_add_valid_hist_valid[i])):
            assert cv_splits_add_valid_hist_valid[i][j].shape[
                0
            ] == learner_random.valid_size + int(
                learner_random.add_valid_size / learner_random.n_splits
            )
            add_valid_indices = cv_splits_add_valid_hist_valid[i][j][
                learner_random.valid_size :
            ]
            assert all(
                y_add_valid_use[add_valid_indices]
                == y_add_valid_hist[i][j][learner_random.valid_size :]
            )

    # remove all files
    for file in os.listdir(learner_random.results_path):
        os.remove(os.path.join(learner_random.results_path, file))

    # remove results_path
    os.rmdir(learner_random.results_path)

    # run again with n_trials=1
    learner_random = LearnerRandomCV(
        classifier=classifier,
        data_id=1169,
        train_valid_size=100,
        reshuffle=True,
        n_splits=5,
        n_repeats=2,
        test_size=200,
        add_valid_size=100,
        n_trials=1,
        seed=2409,
    )
    learner_random.run()

    study = optuna.load_study(
        study_name=learner_random.study_name, storage=learner_random.storage
    )
    results_rerun = study.trials_dataframe()

    assert len(results_rerun) == 1
    assert results["value"].values[0] == results_rerun["value"].values[0]

    # remove all files
    for file in os.listdir(learner_random.results_path):
        os.remove(os.path.join(learner_random.results_path, file))

    # remove results_path
    os.rmdir(learner_random.results_path)

File Path: tests/test_learner_random_holdout.py
Content:
import os

import numpy as np
import optuna
import pytest

from analyze.result_analyzer import ResultAnalyzer
from reshufflebench.algorithms import CatBoost, FunnelMLP, LogReg, XGBoost
from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner import LearnerRandomHoldout
from reshufflebench.metrics import compute_metric


@pytest.fixture(params=["catboost", "funnel_mlp", "logreg", "xgboost"])
def classifier(request: pytest.FixtureRequest) -> Classifier:
    if request.param == "catboost":
        return CatBoost(seed=2906)
    elif request.param == "funnel_mlp":
        return FunnelMLP(seed=2906)
    elif request.param == "logreg":
        return LogReg(seed=2906)
    elif request.param == "xgboost":
        return XGBoost(seed=2906)


@pytest.fixture(params=[True, False])
def reshuffle(request: pytest.FixtureRequest) -> bool:
    return request.param


def test_learner_random_holdout(classifier: Classifier, reshuffle: bool) -> None:
    learner_random = LearnerRandomHoldout(
        classifier=classifier,
        data_id=1169,
        train_valid_size=100,
        reshuffle=reshuffle,
        valid_frac=0.2,
        test_size=200,
        add_valid_size=100,
        n_trials=2,
        seed=2906,
    )
    learner_random.run()

    analyzer = ResultAnalyzer(learner_random.results_path, seed=learner_random.seed)
    analyzer.load(additional_file_types="all")

    results = analyzer.results
    assert len(results) == 2

    columns = [
        f"user_attrs_{metric}_{predict_set}"
        for metric in learner_random.metrics
        for predict_set in [
            "train",
            "valid",
            "add_valid",
            "test",
            "valid_train",
            "test_retrained",
        ]
    ]
    assert all([column in results.columns for column in columns])

    predict_sets = [
        # "y_pred_train_proba_hist",
        "y_pred_valid_proba_hist",
        "y_pred_add_valid_proba_hist",
        "y_pred_test_proba_hist",
        # "y_pred_valid_train_proba_hist",
        "y_pred_test_proba_retrained_hist",
    ]
    map_predict_sets_to_y_sets = {
        # "y_pred_train_proba_hist": "y_train_hist",
        "y_pred_valid_proba_hist": "y_valid_hist",
        "y_pred_add_valid_proba_hist": "y_add_valid_hist",
        "y_pred_test_proba_hist": "y_test",
        # "y_pred_valid_train_proba_hist": "y_valid_train",
        "y_pred_test_proba_retrained_hist": "y_test",
    }
    map_predict_sets_to_orig_metrics = {
        # "y_pred_train_proba_hist": "train",
        "y_pred_valid_proba_hist": "valid",
        "y_pred_add_valid_proba_hist": "add_valid",
        "y_pred_test_proba_hist": "test",
        # "y_pred_valid_train_proba_hist": "valid_train",
        "y_pred_test_proba_retrained_hist": "test_retrained",
    }
    map_predict_sets_to_hist = {
        # "y_pred_train_proba_hist": True,
        "y_pred_valid_proba_hist": True,
        "y_pred_add_valid_proba_hist": True,
        "y_pred_test_proba_hist": False,
        # "y_pred_valid_train_proba_hist": False,
        "y_pred_test_proba_retrained_hist": False,
    }

    columns_checked = []

    for predict_set in predict_sets:
        y = getattr(analyzer, map_predict_sets_to_y_sets[predict_set])
        y_pred_proba = getattr(analyzer, predict_set)

        if (
            map_predict_sets_to_y_sets[predict_set] == "y_train_hist"
            or map_predict_sets_to_y_sets[predict_set] == "y_valid_hist"
            or map_predict_sets_to_y_sets[predict_set] == "y_add_valid_hist"
        ):
            if reshuffle:
                assert any(y[0] != y[1])

        for trial in range(len(results)):
            if map_predict_sets_to_hist[predict_set]:
                if reshuffle:
                    y_tmp = y[trial]
                else:
                    y_tmp = y[0]
            else:
                y_tmp = y
            y_pred_proba_tmp = y_pred_proba[trial]
            y_pred_tmp = np.argmax(y_pred_proba_tmp, axis=1)
            if predict_set == "y_pred_train_proba_hist":
                assert y_tmp.shape[0] == learner_random.train_size
                assert y_tmp.shape[0] == int(0.8 * 100)
            elif predict_set == "y_pred_valid_proba_hist":
                assert y_tmp.shape[0] == learner_random.valid_size
                assert y_tmp.shape[0] == int(0.2 * 100)
            elif predict_set == "y_pred_add_valid_proba_hist":
                assert (
                    y_tmp.shape[0]
                    == learner_random.valid_size + learner_random.add_valid_size
                )
                assert y_tmp.shape[0] == int(0.2 * 100) + 100
            elif predict_set == "y_pred_test_proba_hist":
                assert y_tmp.shape[0] == learner_random.test_size
                assert y_tmp.shape[0] == 200
            elif predict_set == "y_pred_valid_train_proba_hist":
                assert (
                    y_tmp.shape[0]
                    == learner_random.train_size + learner_random.valid_size
                )
                assert y_tmp.shape[0] == 100
            elif predict_set == "y_pred_test_proba_retrained_hist":
                assert y_tmp.shape[0] == learner_random.test_size
                assert y_tmp.shape[0] == 200
            assert y_tmp.shape[0] == y_pred_proba_tmp.shape[0]
            assert y_tmp.shape[0] == y_pred_tmp.shape[0]
            for metric in learner_random.metrics:
                recalculated_metric = compute_metric(
                    y_tmp,
                    y_pred=y_pred_tmp,
                    y_pred_proba=y_pred_proba_tmp,
                    metric=metric,
                    labels=learner_random.labels,
                    multiclass=learner_random.multiclass,
                )
                assert (
                    abs(
                        recalculated_metric
                        - results[
                            f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                        ][trial]
                    )
                    < 1e-12
                )
                columns_checked.append(
                    f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                )

    if learner_random.classifier.classifier_id in ["catboost", "xgboost"]:
        assert all(
            results["user_attrs_actual_iterations"].apply(lambda x: len(eval(x))).values
            == 1
        )
        columns_checked.append("user_attrs_actual_iterations")

    remaining_columns = [
        column for column in results.columns if column not in columns_checked
    ]
    remaining_columns = [
        column for column in remaining_columns if "_valid_train" not in column
    ]
    remaining_columns = [
        column for column in remaining_columns if "_train" not in column
    ]
    remaining_columns = [
        column
        for column in remaining_columns
        if "user_attrs_error_on_fit" not in column
    ]

    assert all(["user_attrs" not in column for column in remaining_columns])

    # remove all files
    for file in os.listdir(learner_random.results_path):
        os.remove(os.path.join(learner_random.results_path, file))

    # remove results_path
    os.rmdir(learner_random.results_path)

    # run again with n_trials = 1
    learner_random = LearnerRandomHoldout(
        classifier=classifier,
        data_id=1169,
        train_valid_size=100,
        reshuffle=True,
        valid_frac=0.2,
        test_size=200,
        add_valid_size=100,
        n_trials=1,
        seed=2906,
    )
    learner_random.run()

    study = optuna.load_study(
        study_name=learner_random.study_name, storage=learner_random.storage
    )
    results_rerun = study.trials_dataframe()

    assert len(results_rerun) == 1
    assert results["value"].values[0] == results_rerun["value"].values[0]

    # remove all files
    for file in os.listdir(learner_random.results_path):
        os.remove(os.path.join(learner_random.results_path, file))

    # remove results_path
    os.rmdir(learner_random.results_path)

File Path: tests/test_learner_random_repeatedholdout.py
Content:
import os

import numpy as np
import optuna
import pytest

from analyze.result_analyzer import ResultAnalyzer
from reshufflebench.algorithms import CatBoost, FunnelMLP, LogReg, XGBoost
from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.learner import LearnerRandomRepeatedHoldout
from reshufflebench.metrics import compute_metric


@pytest.fixture(params=["catboost", "funnel_mlp", "logreg", "xgboost"])
def classifier(request: pytest.FixtureRequest) -> Classifier:
    if request.param == "catboost":
        return CatBoost(seed=2409)
    elif request.param == "funnel_mlp":
        return FunnelMLP(seed=2409)
    elif request.param == "logreg":
        return LogReg(seed=2409)
    elif request.param == "xgboost":
        return XGBoost(seed=2409)


@pytest.fixture(params=[True, False])
def reshuffle(request: pytest.FixtureRequest) -> bool:
    return request.param


def test_learner_random_repeatedholdout(
    classifier: Classifier, reshuffle: bool
) -> None:
    learner_random = LearnerRandomRepeatedHoldout(
        classifier=classifier,
        data_id=1169,
        train_valid_size=100,
        reshuffle=reshuffle,
        valid_frac=0.2,
        n_repeats=2,
        test_size=200,
        add_valid_size=100,
        n_trials=2,
        seed=2409,
    )
    learner_random.run()

    analyzer = ResultAnalyzer(
        learner_random.results_path,
        seed=learner_random.seed,
        n_repeats=learner_random.n_repeats,
    )
    analyzer.load(additional_file_types="all")

    results = analyzer.results
    assert len(results) == 2

    cv_metrics = ["accuracies", "balanced_accuracies", "loglosses", "aucs"]

    columns = [
        f"user_attrs_{metric}_{predict_set}"
        for metric in learner_random.metrics
        for predict_set in [
            "train",
            "valid",
            "add_valid",
            "test",
            "valid_train",
            "test_retrained",
        ]
    ]
    columns_cv = [
        f"user_attrs_{metric}_{predict_set}"
        for metric in cv_metrics
        for predict_set in ["train", "valid", "add_valid", "test"]
    ]
    columns_cv_ensemble = [
        f"user_attrs_{metric}_test_ensemble" for metric in learner_random.metrics
    ]

    cv_metric_to_metric = {
        "accuracies": "accuracy",
        "balanced_accuracies": "balanced_accuracy",
        "loglosses": "logloss",
        "aucs": "auc",
    }
    metric_to_cv_metric = {v: k for k, v in cv_metric_to_metric.items()}

    assert all([column in results.columns for column in columns])
    assert all([column in results.columns for column in columns_cv])
    assert all([column in results.columns for column in columns_cv_ensemble])

    predict_sets = [
        # "y_pred_train_proba_hist",
        "y_pred_valid_proba_hist",
        "y_pred_add_valid_proba_hist",
        "y_pred_test_proba_hist",
        # "y_pred_valid_train_proba_hist",
        "y_pred_test_proba_retrained_hist",
    ]
    map_predict_sets_to_y_sets = {
        # "y_pred_train_proba_hist": "y_train_hist",
        "y_pred_valid_proba_hist": "y_valid_hist",
        "y_pred_add_valid_proba_hist": "y_add_valid_hist",
        "y_pred_test_proba_hist": "y_test",
        # "y_pred_valid_train_proba_hist": "y_valid_train",
        "y_pred_test_proba_retrained_hist": "y_test",
    }
    map_predict_sets_to_orig_metrics = {
        # "y_pred_train_proba_hist": "train",
        "y_pred_valid_proba_hist": "valid",
        "y_pred_add_valid_proba_hist": "add_valid",
        "y_pred_test_proba_hist": "test",
        # "y_pred_valid_train_proba_hist": "valid_train",
        "y_pred_test_proba_retrained_hist": "test_retrained",
    }
    map_predict_sets_to_refit = {
        # "y_pred_train_proba_hist": False,
        "y_pred_valid_proba_hist": False,
        "y_pred_add_valid_proba_hist": False,
        "y_pred_test_proba_hist": False,
        # "y_pred_valid_train_proba_hist": True,
        "y_pred_test_proba_retrained_hist": True,
    }
    map_predict_sets_to_y_vary = {
        # "y_pred_train_proba_hist": True,
        "y_pred_valid_proba_hist": True,
        "y_pred_add_valid_proba_hist": True,
        "y_pred_test_proba_hist": False,
        # "y_pred_valid_train_proba_hist": False,
        "y_pred_test_proba_retrained_hist": False,
    }

    columns_checked = []

    for predict_set in predict_sets:
        y = getattr(analyzer, map_predict_sets_to_y_sets[predict_set])
        y_pred_proba = getattr(analyzer, predict_set)

        if (
            map_predict_sets_to_y_sets[predict_set] == "y_train_hist"
            or map_predict_sets_to_y_sets[predict_set] == "y_valid_hist"
            or map_predict_sets_to_y_sets[predict_set] == "y_add_valid_hist"
        ):
            if reshuffle:
                assert all([any(y[0][i] != y[1][i]) for i in range(len(y[0]))])

        for trial in range(len(results)):
            if map_predict_sets_to_y_vary[predict_set]:
                if reshuffle:
                    y_tmp = y[trial]
                else:
                    y_tmp = y[0]
            else:
                y_tmp = y
            y_pred_proba_tmp = y_pred_proba[trial]
            if map_predict_sets_to_y_vary[predict_set]:
                assert len(y_pred_proba_tmp) == learner_random.n_repeats
            if predict_set == "y_pred_train_proba_hist":
                assert y_tmp[0].shape[0] == learner_random.train_size
                assert y_tmp[0].shape[0] == int(0.8 * 100)
                assert y_tmp[0].shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_valid_proba_hist":
                assert y_tmp[0].shape[0] == learner_random.valid_size
                assert y_tmp[0].shape[0] == int(0.2 * 100)
                assert y_tmp[0].shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_add_valid_proba_hist":
                assert y_tmp[0].shape[0] == learner_random.valid_size + int(
                    learner_random.add_valid_size / learner_random.n_repeats
                )  # add_valid_size is the size of the total additional validation set, not the size of the validation set for each repeat
                assert y_tmp[0].shape[0] == int(0.2 * 100) + (100 / 2)
                assert y_tmp[0].shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_test_proba_hist":
                assert y_tmp.shape[0] == learner_random.test_size
                assert y_tmp.shape[0] == 200
                assert y_tmp.shape[0] == y_pred_proba_tmp[0].shape[0]
            elif predict_set == "y_pred_valid_train_proba_hist":
                assert (
                    y_tmp.shape[0]
                    == learner_random.train_size + learner_random.valid_size
                )
                assert y_tmp.shape[0] == 100
                assert y_tmp.shape[0] == y_pred_proba_tmp.shape[0]
            elif predict_set == "y_pred_test_proba_retrained_hist":
                assert y_tmp.shape[0] == learner_random.test_size
                assert y_tmp.shape[0] == 200
                assert y_tmp.shape[0] == y_pred_proba_tmp.shape[0]
            for metric in learner_random.metrics:
                if map_predict_sets_to_refit[predict_set]:
                    y_pred_tmp = np.argmax(y_pred_proba_tmp, axis=1)
                    recalculated_metric = compute_metric(
                        y_tmp,
                        y_pred=y_pred_tmp,
                        y_pred_proba=y_pred_proba_tmp,
                        metric=metric,
                        labels=learner_random.labels,
                        multiclass=learner_random.multiclass,
                    )
                    assert (
                        abs(
                            recalculated_metric
                            - results[
                                f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                            ][trial]
                        )
                        < 1e-12
                    )
                    columns_checked.append(
                        f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                    )
                else:
                    recalculated_metric = []
                    for i in range(len(y_pred_proba_tmp)):
                        y_tmp_i = (
                            y_tmp[i]
                            if map_predict_sets_to_y_vary[predict_set]
                            else y_tmp
                        )
                        y_pred_tmp = np.argmax(y_pred_proba_tmp[i], axis=1)
                        recalculated_metric.append(
                            compute_metric(
                                y_tmp_i,
                                y_pred=y_pred_tmp,
                                y_pred_proba=y_pred_proba_tmp[i],
                                metric=metric,
                                labels=learner_random.labels,
                                multiclass=learner_random.multiclass,
                            )
                        )
                    assert all(
                        abs(
                            np.array(recalculated_metric)
                            - np.array(
                                eval(
                                    results[
                                        f"user_attrs_{metric_to_cv_metric[metric]}_{map_predict_sets_to_orig_metrics[predict_set]}"
                                    ][trial]
                                )
                            )
                        )
                        < 1e-12
                    )
                    assert (
                        abs(
                            np.mean(recalculated_metric)
                            - results[
                                f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                            ][trial]
                        )
                        < 1e-12
                    )
                    columns_checked.append(
                        f"user_attrs_{metric_to_cv_metric[metric]}_{map_predict_sets_to_orig_metrics[predict_set]}"
                    )
                    columns_checked.append(
                        f"user_attrs_{metric}_{map_predict_sets_to_orig_metrics[predict_set]}"
                    )

    # test_ensemble
    y = getattr(analyzer, "y_test")
    y_pred_proba = getattr(analyzer, "y_pred_test_proba_hist")
    for trial in range(len(results)):
        y_tmp = y
        y_pred_proba_tmp = y_pred_proba[trial]
        assert len(y_pred_proba_tmp) == learner_random.n_repeats
        assert y_tmp.shape[0] == 200
        assert y_tmp.shape[0] == y_pred_proba_tmp[0].shape[0]
        y_pred_proba_tmp_ensemble = np.mean(y_pred_proba_tmp, axis=0)
        y_pred_tmp_ensemble = np.argmax(y_pred_proba_tmp_ensemble, axis=1)
        for metric in learner_random.metrics:
            recalculated_metric = compute_metric(
                y_tmp,
                y_pred=y_pred_tmp_ensemble,
                y_pred_proba=y_pred_proba_tmp_ensemble,
                metric=metric,
                labels=learner_random.labels,
                multiclass=learner_random.multiclass,
            )
            assert (
                abs(
                    recalculated_metric
                    - results[f"user_attrs_{metric}_test_ensemble"][trial]
                )
                < 1e-12
            )
            columns_checked.append(f"user_attrs_{metric}_test_ensemble")

    if learner_random.classifier.classifier_id in ["catboost", "xgboost"]:
        assert all(
            results["user_attrs_actual_iterations"].apply(lambda x: len(eval(x))).values
            == learner_random.n_repeats
        )
        columns_checked.append("user_attrs_actual_iterations")

    remaining_columns = [
        column for column in results.columns if column not in columns_checked
    ]
    remaining_columns = [
        column for column in remaining_columns if "_valid_train" not in column
    ]
    remaining_columns = [
        column for column in remaining_columns if "_train" not in column
    ]
    remaining_columns = [
        column
        for column in remaining_columns
        if "user_attrs_error_on_fit" not in column
    ]

    assert all(["user_attrs" not in column for column in remaining_columns])

    # check cv_splits
    y_valid_train = getattr(analyzer, "y_valid_train")

    # cv_splits_hist_train = getattr(analyzer, "cv_splits_hist_train")
    # y_train_hist = getattr(analyzer, "y_train_hist")
    # assert len(cv_splits_hist_train) == 2
    # for i in range(len(cv_splits_hist_train)):
    #    assert (
    #        len(cv_splits_hist_train[i])
    #        == learner_random.n_repeats
    #    )
    #    for j in range(len(cv_splits_hist_train[i])):
    #        assert cv_splits_hist_train[i][j].shape[0] == learner_random.train_size
    #        train_indices = cv_splits_hist_train[i][j]
    #        assert all(y_valid_train[train_indices] == y_train_hist[i][j])

    cv_splits_hist_valid = getattr(analyzer, "cv_splits_hist_valid")
    y_valid_hist = getattr(analyzer, "y_valid_hist")
    if reshuffle:
        assert len(cv_splits_hist_valid) == 2
    else:
        assert len(cv_splits_hist_valid) == 1
    for i in range(len(cv_splits_hist_valid)):
        assert len(cv_splits_hist_valid[i]) == learner_random.n_repeats
        for j in range(len(cv_splits_hist_valid[i])):
            assert cv_splits_hist_valid[i][j].shape[0] == learner_random.valid_size
            valid_indices = cv_splits_hist_valid[i][j]
            assert all(y_valid_train[valid_indices] == y_valid_hist[i][j])

    # Note: add_valid size is the size of the total additional validation set, not the size of the additional validation set per repeat
    #       this is somewhat in contrast to LearnerRandomHoldout, where add_valid_size is the size of the additional validation set
    y_add_valid_use = getattr(analyzer, "y_add_valid_use")
    cv_splits_add_valid_hist_valid = getattr(analyzer, "cv_splits_add_valid_hist_valid")
    y_add_valid_hist = getattr(analyzer, "y_add_valid_hist")
    if reshuffle:
        assert len(cv_splits_add_valid_hist_valid) == 2
    else:
        assert len(cv_splits_add_valid_hist_valid) == 1
    for i in range(len(cv_splits_add_valid_hist_valid)):
        assert len(cv_splits_add_valid_hist_valid[i]) == learner_random.n_repeats
        for j in range(len(cv_splits_add_valid_hist_valid[i])):
            assert cv_splits_add_valid_hist_valid[i][j].shape[
                0
            ] == learner_random.valid_size + int(
                learner_random.add_valid_size / learner_random.n_repeats
            )
            add_valid_indices = cv_splits_add_valid_hist_valid[i][j][
                learner_random.valid_size :
            ]
            assert all(
                y_add_valid_use[add_valid_indices]
                == y_add_valid_hist[i][j][learner_random.valid_size :]
            )

    # remove all files
    for file in os.listdir(learner_random.results_path):
        os.remove(os.path.join(learner_random.results_path, file))

    # remove results_path
    os.rmdir(learner_random.results_path)

    # run again with n_trials=1
    learner_random = LearnerRandomRepeatedHoldout(
        classifier=classifier,
        data_id=1169,
        train_valid_size=100,
        reshuffle=True,
        valid_frac=0.2,
        n_repeats=2,
        test_size=200,
        add_valid_size=100,
        n_trials=1,
        seed=2409,
    )
    learner_random.run()

    study = optuna.load_study(
        study_name=learner_random.study_name, storage=learner_random.storage
    )
    results_rerun = study.trials_dataframe()

    assert len(results_rerun) == 1
    assert results["value"].values[0] == results_rerun["value"].values[0]

    # remove all files
    for file in os.listdir(learner_random.results_path):
        os.remove(os.path.join(learner_random.results_path, file))

    # remove results_path
    os.rmdir(learner_random.results_path)

File Path: tests/test_preprocessor.py
Content:
from typing import List, Optional

import numpy as np
import pandas as pd
import pytest
from optuna.trial import Trial

from reshufflebench.algorithms.classifier import Classifier
from reshufflebench.utils import unify_missing_values


class TestClassifier(Classifier):
    def __init__(
        self,
        impute_x_cat: bool,
        impute_x_num: bool,
        encode_x: bool,
        scale_x: bool,
        seed: int,
    ):
        super().__init__(
            classifier_id="test",
            impute_x_cat=impute_x_cat,
            impute_x_num=impute_x_num,
            encode_x=encode_x,
            scale_x=scale_x,
            seed=seed,
        )
        self.x_train = None
        self.y_train = None
        self.x_valid = None
        self.y_valid = None
        self.cat_features = None

    def get_hebo_search_space(self, **kwargs):
        return None

    def get_configspace_search_space(self, **kwargs):
        return None

    def get_internal_optuna_search_space(self, **kwargs):
        return None

    def construct_classifier(self, trial: Trial, **kwargs) -> None:
        pass

    def construct_classifier_refit(self, trial: Trial, **kwargs) -> None:
        pass

    def _fit(
        self,
        trial: Trial,
        x_train: np.array,
        y_train: np.array,
        x_valid: Optional[np.array] = None,
        y_valid: Optional[np.array] = None,
        cat_features: Optional[List[int]] = None,
    ) -> None:
        self.x_train = x_train
        self.y_train = y_train
        self.x_valid = x_valid
        self.y_valid = y_valid
        self.cat_features = cat_features


@pytest.fixture(params=[True, False])
def impute_x_cat(request: pytest.FixtureRequest) -> bool:
    return request.param


@pytest.fixture(params=[True, False])
def impute_x_num(request: pytest.FixtureRequest) -> bool:
    return request.param


@pytest.fixture(params=[True, False])
def encode_x(request: pytest.FixtureRequest) -> bool:
    return request.param


@pytest.fixture(params=[True, False])
def scale_x(request: pytest.FixtureRequest) -> bool:
    return request.param


def test_preprocessor(
    impute_x_cat: bool, impute_x_num: bool, encode_x: bool, scale_x: bool
):
    classifier = TestClassifier(
        impute_x_cat=impute_x_cat,
        impute_x_num=impute_x_num,
        encode_x=encode_x,
        scale_x=scale_x,
        seed=0,
    )

    assert classifier.classifier_id == "test"
    assert classifier.impute_x_cat == impute_x_cat
    assert classifier.impute_x_num == impute_x_num
    assert classifier.encode_x == encode_x
    assert classifier.scale_x == scale_x
    assert classifier.seed == 0
    assert classifier.classifier is None

    n = 1000

    feature_1 = np.repeat(1.0, n)

    categories_2 = ["2A", "2B", "2C"]
    feature_2 = np.random.choice(categories_2, size=n).astype(object)

    feature_3 = np.repeat(3, n)

    categories = ["4A", "4B", "4C"]
    feature_4 = np.random.choice(categories, size=n).astype(object)

    data = pd.DataFrame(
        {
            "NumericFeature1": feature_1,
            "CategoricalFeature1": feature_2,
            "IntegerFeature1": feature_3,
            "CategoricalFeature2": feature_4,
        }
    )

    cat_features = [1, 3]
    num_features = [0, 2]

    data.iloc[0:200, 0:4] = None

    data = unify_missing_values(data)

    y = np.random.choice([0, 1], size=n)

    x_train = data.iloc[:800, :]
    x_valid = data.iloc[800:, :]
    y_train = y[:800]
    y_valid = y[800:]

    classifier.construct_pipeline(
        trial=None, refit=False, cat_features=cat_features, num_features=num_features
    )
    classifier.fit(
        trial=None,
        x_train=x_train,
        y_train=y_train,
        x_valid=x_valid,
        y_valid=y_valid,
        cat_features=cat_features,
    )

    assert classifier.x_train.shape == (800, 4)
    assert classifier.y_train.shape == (800,)
    assert classifier.x_valid.shape == (200, 4)
    assert classifier.y_valid.shape == (200,)

    # catboost
    if not impute_x_cat and not impute_x_num and not encode_x and not scale_x:
        assert all(
            classifier.preprocessor.feature_names_in_
            == [
                "NumericFeature1",
                "CategoricalFeature1",
                "IntegerFeature1",
                "CategoricalFeature2",
            ]
        )
        assert all(
            classifier.preprocessor.get_feature_names_out()
            == [
                "passthrough__passthrough__passthrough__passthrough__NumericFeature1",
                "passthrough__passthrough__passthrough__passthrough__CategoricalFeature1",
                "passthrough__passthrough__passthrough__passthrough__IntegerFeature1",
                "passthrough__passthrough__passthrough__passthrough__CategoricalFeature2",
            ]
        )
        assert classifier.x_valid[0, 0] == 1.0
        assert classifier.x_valid[0, 1] in ["2A", "2B", "2C"]
        assert classifier.x_valid[0, 2] == 3.0
        assert classifier.x_valid[0, 3] in ["4A", "4B", "4C"]

    # logreg, funnel_mlp
    if impute_x_cat and impute_x_num and encode_x and scale_x:
        assert all(
            classifier.preprocessor.feature_names_in_
            == [
                "NumericFeature1",
                "CategoricalFeature1",
                "IntegerFeature1",
                "CategoricalFeature2",
            ]
        )
        assert all(
            classifier.preprocessor.get_feature_names_out()
            == [
                "scaler__encoder__remainder__imputer_cat__CategoricalFeature1",
                "scaler__encoder__remainder__imputer_cat__CategoricalFeature2",
                "scaler__remainder__imputer_num__remainder__NumericFeature1",
                "scaler__remainder__imputer_num__remainder__IntegerFeature1",
            ]
        )
        for i in range(4):
            assert all(classifier.x_valid[:, i] < 7) and all(
                classifier.x_valid[:, i] > -7
            )
            assert (
                len(np.unique(classifier.x_valid[:, i])) == 3
                if i in [0, 1]
                else len(np.unique(classifier.x_valid[:, i])) == 1
            )
        assert all(pd.isnull(classifier.x_train).sum(axis=0) == np.array([0, 0, 0, 0]))

    # xgboost, tabpfn
    if impute_x_cat and not impute_x_num and encode_x and not scale_x:
        assert all(
            classifier.preprocessor.feature_names_in_
            == [
                "NumericFeature1",
                "CategoricalFeature1",
                "IntegerFeature1",
                "CategoricalFeature2",
            ]
        )
        assert all(
            classifier.preprocessor.get_feature_names_out()
            == [
                "passthrough__encoder__passthrough__imputer_cat__CategoricalFeature1",
                "passthrough__encoder__passthrough__imputer_cat__CategoricalFeature2",
                "passthrough__remainder__passthrough__remainder__NumericFeature1",
                "passthrough__remainder__passthrough__remainder__IntegerFeature1",
            ]
        )
        for i in range(2):
            assert all(classifier.x_valid[:, i] < 7) and all(
                classifier.x_valid[:, i] > -7
            )
            assert len(np.unique(classifier.x_valid[:, i])) == 3
        assert all(
            pd.isnull(classifier.x_train).sum(axis=0) == np.array([0, 0, 200, 200])
        )

Output:
{
    "experimental_code": "File Path: simulations/simulate.py\nContent:\nfrom functools import partial\n\nimport numpy as np\nimport torch\n\ndef simulate_gp(x: torch.Tensor, mu: callable, cov: callable) -> torch.Tensor:\n    \"\"\"\n    Simulate a Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param mu: Function to compute the mean of the Gaussian process.\n    :param cov: Function to compute the covariance matrix of the Gaussian process.\n\n    :return: A tensor of shape (n, d) representing the simulated Gaussian process.\n    \"\"\"\n    sigma_2 = 1e-5\n    n = x.shape[0]\n    d = x.shape[1]\n    mu_x = mu(x)\n    K_x = cov(x, x)\n    K_x += sigma_2 * torch.eye(n)\n    eigenvalues, eigenvectors = torch.linalg.eigh(K_x)\n    positive_eigenvalues = torch.clamp(eigenvalues, min=0)\n    sqrt_K_x = eigenvectors @ torch.diag(positive_eigenvalues.sqrt()) @ eigenvectors.T\n    z = torch.normal(0, 1, (n, d))\n    return mu_x + sqrt_K_x @ z\n\n\ndef mu_factory(x: torch.Tensor, alpha: float) -> torch.Tensor:\n    \"\"\"\n    The mean function of the Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param alpha: The alpha parameter of the mean function.\n\n    :return: A tensor of shape (n, 1) representing the mean of the Gaussian process at each point.\n    \"\"\"\n    values = alpha * (x - 0.5).pow(2)\n    return values\n\n\ndef cov_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Vectorized computation of the covariance matrix of the Gaussian process.\n\n    :param x1: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param x2: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param lengthscale: The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A tensor of shape (n, n) representing the covariance matrix of the Gaussian process.\n    \"\"\"\n    sq_dist = torch.sum((x1[:, None, :] - x2[None, :, :]) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K\n    return K\n\n\ndef kernel_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Computation of the covariance between two points.\n\n    :param x1: A tensor of shape (1, d) where d is the dimensionality.\n    :param x2: A tensor of shape (1, d) where d is the dimensionality.\n    :param lengthscale:  The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A scalar tensor representing the covariance between the two points.\n    \"\"\"\n    sq_dist = torch.sum((x1 - x2) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        if x1 == x2:\n            K = (tau**2) * K\n    return K\n\n\nif __name__ == \"__main__\":\n    import argparse\n    import os\n\n    import pandas as pd\n    from tqdm import tqdm\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--alpha\", type=float, default=1)\n    parser.add_argument(\"--lengthscale\", type=float, default=0.5)\n    args = parser.parse_args()\n\n    np.random.seed(42)\n    torch.manual_seed(42)\n\n    results_path = \"results\"\n    d = 1\n    alpha = args.alpha\n    lengthscale = args.lengthscale\n    n_replicates = 10000\n\n    x = torch.linspace(0, 1, 101).reshape(-1, d)\n    mu = partial(mu_factory, alpha=alpha)\n    kernel = partial(kernel_factory, lengthscale=lengthscale)\n    cov = partial(cov_factory, lengthscale=lengthscale)\n    taus = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n    taus.sort()\n    results = {tau: {\"y\": [], \"y_shuffled\": [], \"y_diff\": []} for tau in taus}\n    for tau in taus:\n        y_mu_y_list = []\n        y_mu_y_shuffled_list = []\n        y_diff_list = []\n        for _ in tqdm(range(n_replicates)):\n            y = simulate_gp(x, mu, cov)\n            y_shuffled = simulate_gp(x, mu, partial(cov, shuffled=True, tau=tau))\n            y_mu_y = mu(x[y.argmin(dim=0)])\n            y_mu_y_shuffled = mu(x[y_shuffled.argmin(dim=0)])\n            y_mu_y_list.append(y_mu_y)\n            y_mu_y_shuffled_list.append(y_mu_y_shuffled)\n            y_diff_list.append(y_mu_y - y_mu_y_shuffled)\n        results[tau][\"y\"] = torch.tensor(y_mu_y_list)\n        results[tau][\"y_shuffled\"] = torch.tensor(y_mu_y_shuffled_list)\n        results[tau][\"y_diff\"] = torch.tensor(y_diff_list)\n\n    results_df = pd.DataFrame(results[taus[0]])\n    results_df[\"replicate\"] = range(n_replicates)\n    results_df[\"tau\"] = taus[0]\n    for tau in taus[1:]:\n        results_df_tmp = pd.DataFrame(results[tau])\n        results_df_tmp[\"replicate\"] = range(n_replicates)\n        results_df_tmp[\"tau\"] = tau\n        results_df = pd.concat([results_df, results_df_tmp], axis=0)\n    results_df[\"alpha\"] = alpha\n    results_df[\"lengthscale\"] = lengthscale\n    results_df[\"scenario\"] = str(alpha) + \"_\" + str(lengthscale)\n    results_df = results_df.reset_index(drop=True)\n    results_df.to_csv(\n        os.path.join(\n            results_path,\n            f\"results_alpha_{alpha}_lengthscale_{lengthscale}.csv\",\n        ),\n        index=False,\n    )\nFile Path: analyze/result_analyzer.py\nContent:\n    def calculate_curvature(self) -> None:\n        \"\"\"\n        Fit a GP on observed values and calculate some curvature metrics at the empirical optimum.\n        \"\"\"\n        for metric in self.params[\"metrics\"]:\n            dat = self.results_raw[metric]\n            relevant_columns_valid = [\n                column for column in dat.columns if \"params_\" in column\n            ] + [\"valid\"]\n            dat_valid = dat.loc[:, relevant_columns_valid]\n            dat_valid.rename(columns={\"valid\": \"y\"}, inplace=True)\n            X = dat_valid.drop(columns=[\"y\"])\n            y = dat_valid[\"y\"].values.reshape(-1, 1)\n            X.rename(\n                columns={column: column.replace(\"params_\", \"\") for column in X.columns},\n                inplace=True,\n            )\n            if self.params[\"classifier\"] == \"logreg\":\n                classifier = LogReg(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            elif self.params[\"classifier\"] == \"funnel_mlp\":\n                classifier = FunnelMLP(self.params[\"seed\"])\n                n_train_samples = int(\n                    0.8 * self.params[\"train_valid_size\"]\n                )  # Holdout 80/20 or 5-fold CV variants\n                space = classifier.get_hebo_search_space(\n                    n_train_samples=n_train_samples\n                )\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub)\n                    for name in space.paras\n                    if not space.paras[name].is_categorical\n                ]\n            elif self.params[\"classifier\"] == \"xgboost\":\n                classifier = XGBoost(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            elif self.params[\"classifier\"] == \"catboost\":\n                classifier = CatBoost(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            # https://github.com/huawei-noah/HEBO/blob/c1c7d72b996a7d11eb2b86e25f21a174b0cc7bd4/HEBO/hebo/optimizers/hebo.py#L117\n            X, Xe = space.transform(X)\n            model_config = {\n                \"lr\": 0.01,\n                \"num_epochs\": 100,\n                \"verbose\": False,\n                \"noise_lb\": 8e-4,\n                \"pred_likeli\": False,\n            }\n            if space.num_categorical > 0:\n                model_config[\"num_uniqs\"] = [\n                    len(space.paras[name].categories) for name in space.enum_names\n                ]\n            # try:\n            #    if y.min() <= 0:\n            #        y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))\n            #    else:\n            #        y = torch.FloatTensor(power_transform(y / y.std(), method='box-cox'))\n            #        if y.std() < 0.5:\n            #            y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))\n            #    if y.std() < 0.5:\n            #        raise RuntimeError('Power transformation failed')\n            #    model = get_model(\"gp\", space.num_numeric, space.num_categorical, 1, **model_config)\n            #    model.fit(X, Xe, y)\n            # except:\n            y = torch.FloatTensor(y).clone()\n            model = get_model(\n                \"gp\", space.num_numeric, space.num_categorical, 1, **model_config\n            )\n            model.fit(X, Xe, y)\n\n            empirical_argmin = model.predict(X, Xe)[0].argmin()\n            X_argmin = X[empirical_argmin, :].unsqueeze(0)\n            Xe_argmin = Xe[empirical_argmin, :].unsqueeze(0)\n\n            def posterior_mean_wrapper(x, model, Xe_argmin):\n                x_tensor = torch.FloatTensor(x).unsqueeze(0).requires_grad_(True)\n                return model.predict(x_tensor, Xe_argmin)[0][0, 0].detach().numpy()\n\n            x0 = X[empirical_argmin, :].numpy()\n            result = opt.minimize(\n                posterior_mean_wrapper,\n                x0,\n                args=(model, Xe_argmin),\n                bounds=bounds,\n                method=\"Nelder-Mead\",\n            )\n\n            x_optimal = result.x\n            hessian_function = numdifftools.Hessian(posterior_mean_wrapper)\n            hessian_optimal = hessian_function(x_optimal, model, Xe_argmin)\n\n            def make_psd(matrix):\n                eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n                already_is_psd = np.all(eigenvalues >= 0)\n                eigenvalues[eigenvalues < 0] = 0\n                return (\n                    already_is_psd,\n                    eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T,\n                )\n\n            already_is_psd, hessian_optimal = make_psd(hessian_optimal)\n\n            det_hessian = np.linalg.det(hessian_optimal)\n            trace_hessian = np.trace(hessian_optimal)\n            eigenvalues_hessian = np.linalg.eigvals(hessian_optimal)\n            smallest_eigenvalue_hessian = np.min(eigenvalues_hessian)\n            biggest_eigenvalue_hessian = np.max(eigenvalues_hessian)\n\n            curvature_pd = pd.DataFrame(\n                {\n                    \"det_hessian\": [det_hessian],\n                    \"trace_hessian\": [trace_hessian],\n                    \"smallest_eigenvalue_hessian\": [smallest_eigenvalue_hessian],\n                    \"biggest_eigenvalue_hessian\": [biggest_eigenvalue_hessian],\n                    \"already_is_psd\": [already_is_psd],\n                    \"gp_noise\": [\n                        model.noise.item()\n                    ],  # homoscedastic observation variance (noise) from the GP\n                    \"seed\": [self.seed],\n                    \"classifier\": [self.params[\"classifier\"]],\n                    \"data_id\": [self.params[\"data_id\"]],\n                    \"train_valid_size\": [self.params[\"train_valid_size\"]],\n                    \"resampling\": [self.params[\"resampling\"]],\n                    \"metric\": [metric],\n                }\n            )\n            self.curvature.update({metric: curvature_pd})\nFile Path: reshufflebench/learner/learner_random_cv.py\nContent:\nclass LearnerRandomCV(LearnerRandom):\n    # ... (init and prepare_resampling methods)\n\n    def objective(self, trial: Trial) -> float:\n        \"\"\"\n        Objective function for the optimization.\n        \"\"\"\n        # construct classifier pipeline\n        self.classifier.construct_pipeline(\n            trial,\n            refit=False,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n            n_train_samples=self.train_size,\n        )\n\n        if self.reshuffle:\n            self.cv = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits = []\n            for cv in self.cv:\n                self.cv_splits.append(\n                    list(cv.split(self.x_valid_train, self.y_valid_train))\n                )\n\n            # partition add_valid_use data into n_splits folds and repeat it n_repeats times\n            self.cv_add_valid = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits_add_valid = []\n            for cv in self.cv_add_valid:\n                self.cv_splits_add_valid.append(\n                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))\n                )\n\n        cv_splits_hist_train_tmp = []\n        cv_splits_hist_valid_tmp = []\n        cv_splits_add_valid_hist_valid_tmp = []\n        y_train_hist_tmp = []\n        y_valid_hist_tmp = []\n        y_add_valid_hist_tmp = []\n\n        # for each repeat and each fold fit the classifier and predict on the train, valid, test and add_valid set and compute the metrics\n        predictions = dict(\n            [(data, []) for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]]\n        )\n        predictions_proba = dict(\n            [(data, []) for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]]\n        )\n        cv_metrics_train = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_valid = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_add_valid = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_test = dict([(metric, []) for metric in self.metrics])\n\n        for repeat in range(self.n_repeats):\n            for fold in range(self.n_splits):\n                train_index, valid_index = self.cv_splits[repeat][fold]\n                cv_splits_hist_train_tmp.append(train_index)\n                cv_splits_hist_valid_tmp.append(valid_index)\n                x_train = self.x_valid_train.iloc[train_index]\n                x_valid = self.x_valid_train.iloc[valid_index]\n                y_train = self.y_valid_train[train_index]\n                y_valid = self.y_valid_train[valid_index]\n                y_train_hist_tmp.append(y_train)\n                y_valid_hist_tmp.append(y_valid)\n\n                add_valid_index = self.cv_splits_add_valid[repeat][fold][1]\n                cv_splits_add_valid_hist_valid_tmp.append(\n                    np.concatenate([valid_index, add_valid_index])\n                )\n                x_add_valid, y_add_valid = construct_x_and_y_add_valid(\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    x_add_valid=self.x_add_valid_use.iloc[add_valid_index],\n                    y_add_valid=self.y_add_valid_use[add_valid_index],\n                )\n                y_add_valid_hist_tmp.append(y_add_valid)\n\n                self.classifier.fit(\n                    trial=trial,\n                    x_train=x_train,\n                    y_train=y_train,\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    cat_features=self.cat_features,\n                )\n                for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]:\n                    if data == \"x_test\":\n                        predictions[data].append(\n                            self.classifier.predict(getattr(self, data))\n                        )\n                        predictions_proba[data].append(\n                            self.classifier.predict_proba(getattr(self, data))\n                        )\n                    else:\n                        predictions[data].append(self.classifier.predict(eval(data)))\n                        predictions_proba[data].append(\n                            self.classifier.predict_proba(eval(data))\n                        )\n                for metric in self.metrics:\n                    cv_metrics_train[metric].append(\n                        compute_metric(\n                            y_train,\n                            y_pred=predictions[\"x_train\"][-1],\n                            y_pred_proba=predictions_proba[\"x_train\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_valid[metric].append(\n                        compute_metric(\n                            y_valid,\n                            y_pred=predictions[\"x_valid\"][-1],\n                            y_pred_proba=predictions_proba[\"x_valid\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_add_valid[metric].append(\n                        compute_metric(\n                            y_add_valid,\n                            y_pred=predictions[\"x_add_valid\"][-1],\n                            y_pred_proba=predictions_proba[\"x_add_valid\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_test[metric].append(\n                        compute_metric(\n                            self.y_test,\n                            y_pred=predictions[\"x_test\"][-1],\n                            y_pred_proba=predictions_proba[\"x_test\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n\n        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)\n        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)\n        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)\n        self.y_train_hist.append(y_train_hist_tmp)\n        self.y_valid_hist.append(y_valid_hist_tmp)\n        self.y_add_valid_hist.append(y_add_valid_hist_tmp)\n        self.y_pred_train_proba_hist.append(predictions_proba[\"x_train\"])\n        self.y_pred_valid_proba_hist.append(predictions_proba[\"x_valid\"])\n        self.y_pred_add_valid_proba_hist.append(predictions_proba[\"x_add_valid\"])\n        self.y_pred_test_proba_hist.append(predictions_proba[\"x_test\"])\n\n        for cv_metric in self.cv_metric_to_metric.keys():\n            metric = self.cv_metric_to_metric[cv_metric]\n            trial.set_user_attr(\n                f\"{cv_metric}_train\",\n                json.dumps(cv_metrics_train[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_valid\",\n                json.dumps(cv_metrics_valid[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_add_valid\",\n                json.dumps(cv_metrics_add_valid[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_test\",\n                json.dumps(cv_metrics_test[metric], cls=NumpyArrayEncoder),\n            )\n\n        # compute the mean of the metrics over the folds and repeats\n        metrics_train = {}\n        metrics_valid = {}\n        metrics_add_valid = {}\n        metrics_test = {}\n\n        for metric in self.metrics:\n            metrics_train[metric] = np.mean(cv_metrics_train[metric])\n            metrics_valid[metric] = np.mean(cv_metrics_valid[metric])\n            metrics_add_valid[metric] = np.mean(cv_metrics_add_valid[metric])\n            metrics_test[metric] = np.mean(cv_metrics_test[metric])\n            trial.set_user_attr(f\"{metric}_train\", metrics_train[metric])\n            trial.set_user_attr(f\"{metric}_valid\", metrics_valid[metric])\n            trial.set_user_attr(f\"{metric}_add_valid\", metrics_add_valid[metric])\n            trial.set_user_attr(f\"{metric}_test\", metrics_test[metric])\n\n        # compute the metrics on the test set also in ensemble style\n        metrics_test_ensemble = {}\n        predictions_proba_test_ensemble = np.mean(predictions_proba[\"x_test\"], axis=0)\n        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)\n        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums\n        check_y_predict_proba(predictions_proba_test_ensemble)\n        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)\n        for metric in self.metrics:\n            metrics_test_ensemble[metric] = compute_metric(\n                self.y_test,\n                y_pred=predictions_test_ensemble,\n                y_pred_proba=predictions_proba_test_ensemble,\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(\n                f\"{metric}_test_ensemble\", metrics_test_ensemble[metric]\n            )\n\n        if self.bootstrap_test:\n            # bootstrap the ensemble style test performance\n            for metric in self.metrics:\n                metric_test_bootstrap = bootstrap_test_performance(\n                    y_test=self.y_test,\n                    y_pred=predictions_test_ensemble,\n                    y_pred_proba=predictions_proba_test_ensemble,\n                    metric=metric,\n                    labels=self.labels,\n                    multiclass=self.multiclass,\n                    seed=self.seed,\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_ensemble_bootstrap\", metric_test_bootstrap\n                )\n                average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(\n                    metric_test_bootstrap\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_ensemble_bootstrap_average\",\n                    average_metric_test_bootstrap,\n                )\n\n        # refit on the train_valid set\n        self.classifier.construct_pipeline(\n            trial,\n            refit=True,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n        )\n        self.classifier.fit(\n            trial=trial,\n            x_train=self.x_valid_train,\n            y_train=self.y_valid_train,\n            cat_features=self.cat_features,\n        )\n\n        # predict on the train_valid set and compute the metrics\n        predictions[\"x_valid_train\"] = self.classifier.predict(self.x_valid_train)\n        predictions_proba[\"x_valid_train\"] = self.classifier.predict_proba(\n            self.x_valid_train\n        )\n        metrics_valid_train = {}\n        for metric in self.metrics:\n            metrics_valid_train[metric] = compute_metric(\n                self.y_valid_train,\n                y_pred=predictions[\"x_valid_train\"],\n                y_pred_proba=predictions_proba[\"x_valid_train\"],\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(f\"{metric}_valid_train\", metrics_valid_train[metric])\n\n        self.y_pred_valid_train_proba_hist.append(predictions_proba[\"x_valid_train\"])\n\n        # predict on the test set and compute the metrics\n        predictions[\"x_test_retrained\"] = self.classifier.predict(self.x_test)\n        predictions_proba[\"x_test_retrained\"] = self.classifier.predict_proba(\n            self.x_test\n        )\n        metrics_test_retrained = {}\n        for metric in self.metrics:\n            metrics_test_retrained[metric] = compute_metric(\n                self.y_test,\n                y_pred=predictions[\"x_test_retrained\"],\n                y_pred_proba=predictions_proba[\"x_test_retrained\"],\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(\n                f\"{metric}_test_retrained\", metrics_test_retrained[metric]\n            )\n\n        if self.bootstrap_test:\n            # bootstrap the retrained test performance\n            for metric in self.metrics:\n                metric_test_retrained_bootstrap = bootstrap_test_performance(\n                    y_test=self.y_test,\n                    y_pred=predictions[\"x_test_retrained\"],\n                    y_pred_proba=predictions_proba[\"x_test_retrained\"],\n                    metric=metric,\n                    labels=self.labels,\n                    multiclass=self.multiclass,\n                    seed=self.seed,\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_retrained_bootstrap\",\n                    metric_test_retrained_bootstrap,\n                )\n                average_metric_test_retrained_bootstrap = sum(\n                    metric_test_retrained_bootstrap\n                ) / len(metric_test_retrained_bootstrap)\n                trial.set_user_attr(\n                    f\"{metric}_test_retrained_bootstrap_average\",\n                    average_metric_test_retrained_bootstrap,\n                )\n\n        self.y_pred_test_proba_retrained_hist.append(\n            predictions_proba[\"x_test_retrained\"]\n        )\n\n        self.classifier.reset()\n\n        # return the validation accuracy (mean over folds and repeats)\n        return metrics_valid[\"accuracy\"]\nFile Path: main.py\nContent:\nif __name__ == \"__main__\":\n    import argparse\n\n    from reshufflebench.algorithms import (\n        CatBoost,\n        Featureless,\n        FunnelMLP,\n        LogReg,\n        TabPFN,\n        XGBoost,\n        XGBoostLarge,\n    )\n    from reshufflebench.learner import (\n        LearnerHeboCV,\n        LearnerHeboHoldout,\n        LearnerHeboRepeatedHoldout,\n        LearnerRandomCV,\n        LearnerRandomHoldout,\n        LearnerRandomRepeatedHoldout,\n        LearnerSmacCV,\n        LearnerSmacHoldout,\n        LearnerSmacRepeatedHoldout,\n    )\n    from reshufflebench.utils import str2bool\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--classifier\",\n        type=str,\n        default=\"catboost\",\n        choices=[\n            \"catboost\",\n            \"funnel_mlp\",\n            \"logreg\",\n            \"tabpfn\",\n            \"xgboost\",\n            \"xgboost_large\",\n            \"featureless\",\n        ],\n    )\n    parser.add_argument(\"--default\", type=str2bool, default=False)\n    parser.add_argument(\n        \"--optimizer\",\n        type=str,\n        default=\"random\",\n        choices=[\"random\", \"hebo\", \"smac\"],\n    )\n    parser.add_argument(\n        \"--data_id\",\n        type=int,\n        default=11111,\n        choices=[\n            23517,\n            1169,\n            41147,\n            4135,\n            1461,\n            1590,\n            41150,\n            41162,\n            42733,\n            42742,\n            99999,\n            11111,\n        ],\n    )\n    parser.add_argument(\n        \"--valid_type\",\n        type=str,\n        default=\"holdout\",\n        choices=[\"cv\", \"holdout\", \"repeatedholdout\"],\n    )\n    parser.add_argument(\n        \"--train_valid_size\",\n        type=int,\n        default=500,\n        choices=[500, 1000, 5000],\n    )\n    parser.add_argument(\"--reshuffle\", type=str2bool, default=True)\n    parser.add_argument(\"--n_splits\", type=int, default=5, choices=[5])\n    # n_repeats = 1 or 5 for cv, 1 for holdout and 5 for repeatedholdout\n    parser.add_argument(\"--n_repeats\", type=int, default=1, choices=[1, 5])\n    parser.add_argument(\"--valid_frac\", type=float, default=0.2, choices=[0.2])\n    parser.add_argument(\"--test_size\", type=int, default=5000)\n    parser.add_argument(\"--add_valid_size\", type=int, default=5000)\n    parser.add_argument(\"--n_trials\", type=int, default=10)\n    parser.add_argument(\"--seed\", type=int, default=42)\n\n    args = parser.parse_args()\n\n    classifiers = {\n        \"catboost\": CatBoost(seed=args.seed, default=args.default),\n        \"funnel_mlp\": FunnelMLP(seed=args.seed, default=args.default),\n        \"logreg\": LogReg(seed=args.seed, default=args.default),\n        \"tabpfn\": TabPFN(seed=args.seed),\n        \"xgboost\": XGBoost(seed=args.seed, default=args.default),\n        \"xgboost_large\": XGBoostLarge(seed=args.seed, default=args.default),\n        \"featureless\": Featureless(seed=args.seed),\n    }\n    classifier = classifiers[args.classifier]\n\n    if args.n_trials > 500:\n        raise ValueError(\n            \"n_trials must be <= 500 - or you must adjust seeds in codebase\"\n        )\n\n    if args.optimizer == \"random\":\n        if args.valid_type == \"cv\":\n            learner = LearnerRandomCV(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerRandomHoldout(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerRandomRepeatedHoldout(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    elif args.optimizer == \"hebo\":\n        if args.valid_type == \"cv\":\n            learner = LearnerHeboCV(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerHeboHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerHeboRepeatedHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    elif args.optimizer == \"smac\":\n        if args.valid_type == \"cv\":\n            learner = LearnerSmacCV(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerSmacHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerSmacRepeatedHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    else:\n        raise ValueError(\"Invalid optimizer\")\n    learner.run()\n",
    "experimental_info": "Simulation Study:\n- `d` (dimensionality of each point): 1\n- `alpha` (mean function parameter for quadratic loss surface): [0.5, 1, 5, 10]\n- `lengthscale` (covariance function parameter for squared exponential kernel): [0.1, 0.5, 1, 5]\n- `n_replicates`: 10000\n- `taus` (parameter for shuffled covariance function, representing reshuffling extent/noise correlation): [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\nBenchmark Experiments:\n- Optimizers: Random Search, HEBO (Bayesian Optimization), SMAC3 (Bayesian Optimization)\n- Classifiers (methods evaluated): CatBoost, FunnelMLP, LogReg (Logistic Regression), TabPFN, XGBoost, XGBoostLarge, Featureless (baseline)\n- Resampling Strategies (`valid_type`):\n    - Holdout: `valid_type=\"holdout\"`, `valid_frac=0.2`\n    - M-fold CV: `valid_type=\"cv\"`, `n_splits=5`, `n_repeats=1`\n    - M-fold Holdout (repeated holdout / Monte Carlo CV): `valid_type=\"repeatedholdout\"`, `valid_frac=0.2`, `n_repeats=5`\n    - 5x 5-fold CV: `valid_type=\"cv\"`, `n_splits=5`, `n_repeats=5`\n- Reshuffling (`reshuffle`): True, False\n- Number of trials (`n_trials`):\n    - 250 for HEBO and SMAC\n    - 500 for Random Search (unless `classifier` is 'tabpfn' or 'default', then 1 trial)\n- Seeds: 42 to 51 (range(42, 52))\n- Data IDs (OpenML datasets): [23517, 1169, 41147, 4135, 1461, 1590, 41150, 41162, 42733, 42742] (plus synthetic 99999, 11111 for testing)\n- Training/Validation set sizes (`train_valid_size`): [500, 1000, 5000]\n- Test set size (`test_size`): 5000\n- Additional validation set size (`add_valid_size`): 5000\n- Metrics:\n    - For Random Search: Accuracy, Balanced Accuracy, Logloss, AUC (Area Under the Curve)\n    - For HEBO and SMAC: AUC\n"
}
