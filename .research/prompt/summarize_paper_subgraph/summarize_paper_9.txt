
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Published as a conference paper at ICLR 2023 PASHA: E FFICIENT HPO AND NAS WITH PROGRESSIVE RESOURCE ALLOCATION Ondrej Bohdal1∗, Lukas Balles2, Martin Wistuba2, Beyza Ermis3, C´edric Archambeau2, Giovanni Zappella2 1The University of Edinburgh 2AWS, Berlin 3Cohere for AI 1ondrej.bohdal@ed.ac.uk 3beyza@cohere.com 2{balleslb,marwistu,cedrica,zappella}@amazon.com ABSTRACT Hyperparameter optimization (HPO) and neural architecture search (NAS) are methods of choice to obtain the best-in-class machine learning models, but in practice they can be costly to run. When models are trained on large datasets, tuning them with HPO or NAS rapidly becomes prohibitively expensive for practitioners, even when efﬁcient multi-ﬁdelity methods are employed. We propose an approach to tackle the challenge of tuning machine learning models trained on large datasets with limited computational resources. Our approach, named PASHA, extends ASHA and is able to dynamically allocate maximum resources for the tuning procedure depending on the need. The experimental comparison shows that PASHA identiﬁes well-performing hyperparameter conﬁgurations and architectures while consuming signiﬁcantly fewer computational resources than ASHA. 1 I NTRODUCTION Hyperparameter optimization (HPO) and neural architecture search (NAS) yield state-of-the-art models, but often are a very costly endeavor, especially when working with large datasets and models. For example, using the results of (Sharir et al., 2020) we can estimate that evaluating 50 conﬁgurations for a 340-million-parameter BERT model (Devlin et al., 2019) on the 15GB Wikipedia and Book corpora would cost around $500,000. To make HPO and NAS more efﬁcient, researchers explored how we can learn from cheaper evaluations (e.g. on a subset of the data) to later allocate more resources only to promising conﬁgurations. This created a family of methods often described as multi- ﬁdelity methods. Two well-known algorithms in this family are Successive Halving (SH) (Jamieson & Talwalkar, 2016; Karnin et al., 2013) and Hyperband (HB) (Li et al., 2018). Multi-ﬁdelity methods signiﬁcantly lower the cost of the tuning. Li et al. (2018) reported speedups up to 30x compared to standard Bayesian Optimization (BO) and up to 70x compared to random search. Unfortunately, the cost of current multi-ﬁdelity methods is still too high for many practitioners, also because of the large datasets used for training the models. As a workaround, they need to design heuristics which can select a set of hyperparameters or an architecture with a cost comparable to training a single conﬁguration, for example, by training the model with multiple conﬁgurations for a single epoch and then selecting the best-performing candidate. On one hand, such heuristics lack robustness and need to be adapted to the speciﬁc use-cases in order to provide good results. On the other hand, they build on an extensive amount of practical experience suggesting that multi-ﬁdelity methods are often not sufﬁciently aggressive in leveraging early performance measurements and that identifying the best performing set of hyperparameters (or the best architecture) does not require training a model until convergence. For example, Bornschein et al. (2020) show that it is possible to ﬁnd the best hyperparameter – number of channels in ResNet- 101 architecture (He et al., 2015) for ImageNet (Deng et al., 2009) – using only one tenth of the data. However, it is not known beforehand that one tenth of data is sufﬁcient for the task. Our aim is to design a method that consumes fewer resources than standard multi-ﬁdelity algorithms such as Hyperband (Li et al., 2018) or ASHA (Li et al., 2020), and yet is able to identify conﬁgurations ∗Work done during an internship at AWS, Berlin. 1 arXiv:2207.06940v2  [cs.LG]  8 Mar 2023Published as a conference paper at ICLR 2023 that produce models with a similar predictive performance after full retraining from scratch. Models are commonly retrained on a combination of training and validation sets to obtain the best performance after optimizing the hyperparameters. To achieve the speedup, we propose a variant of ASHA, called Progressive ASHA (PASHA), that starts with a small amount of initial maximum resources and gradually increases them as needed. ASHA in contrast has a ﬁxed amount of maximum resources, which is a hyperparameter deﬁned by the user and is difﬁcult to select. Our empirical evaluation shows PASHA can save a signiﬁcant amount of resources while ﬁnding similarly well-performing conﬁgurations as conventional ASHA, reducing the entry barrier to do HPO and NAS. To summarize, our contributions are as follows: 1) We introduce a new approach called PASHA that dynamically selects the amount of maximum resources to allocate for HPO or NAS (up to a certain budget), 2) Our empirical evaluation shows the approach signiﬁcantly speeds up HPO and NAS without sacriﬁcing the performance, and 3) We show the approach can be successfully combined with sample-efﬁcient strategies based on Bayesian Optimization, highlighting the generality of our approach. Our implementation is based on the Syne Tune library (Salinas et al., 2022). 2 R ELATED WORK Real-world machine learning systems often rely on a large number of hyperparameters and require testing many combinations to identify suitable values. This makes data-inefﬁcient techniques such as Grid Search or Random Search (Bergstra & Bengio, 2012) very expensive in most practical scenarios. Various approaches have been proposed to ﬁnd good parameters more quickly, and they can be classiﬁed into two main families: 1) Bayesian Optimization: evaluates the most promising conﬁgurations by modelling their performance. The methods are sample-efﬁcient but often designed for environments with limited amount of parallelism; 2) Multi-ﬁdelity: sequentially allocates more resources to conﬁgurations with better performance and allows high level of parallelism during the tuning. Multi-ﬁdelity methods have typically been faster when run at scale and will be the focus of this work. Ideas from these two families can also be combined together, for example as done in BOHB by Falkner et al. (2018), and we will test a similar method in our experiments. Successive Halving (SH) (Karnin et al., 2013; Jamieson & Talwalkar, 2016) is conceptually the simplest multi-ﬁdelity method. Its key idea is to run all conﬁgurations using a small amount of resources and then successively promote only a fraction of the most promising conﬁgurations to be trained using more resources. Another popular multi-ﬁdelity method, called Hyperband (Li et al., 2018), performs SH with different early schedules and number of candidate conﬁgurations. ASHA (Li et al., 2020) extends the simple and very efﬁcient idea of successive halving by introducing asynchronous evaluation of different conﬁgurations, which leads to further practical speedups thanks to better utilisation of workers in a parallel setting. Related to the problem of efﬁciency in HPO, cost-aware HPO explicitly accounts for the cost of the evaluations of different conﬁgurations. Previous work on cost-aware HPO for multi-ﬁdelity algorithms such as CAHB (Ivkin et al., 2021) keeps a tight control on the budget spent during the HPO process. This is different from our work, as we reduce the budget spent by terminating the HPO procedure early instead of allocating the compute budget in its entirety. Moreover, PASHA could be combined with CAHB to leverage the cost-based resources allocation. Recently, researchers considered dataset subsampling to speedup HPO and NAS. Shim et al. (2021) have combined coresets with PC-DARTS (Xu et al., 2020) and showed that they can ﬁnd well- performing architectures using only 10% of the data and 8.8x less search time. Similarly, Visalpara et al. (2021) have combined subset selection methods with the Tree-structured Parzen Estimator (TPE) for HPO (Bergstra et al., 2011). With a 5% subset they obtained between an 8x to 10x speedup compared to standard TPE. However, in both cases it is difﬁcult to say in advance what subsampling ratio to use. For example, the 10% ratio in (Shim et al., 2021) incurs no decrease in accuracy, while reducing further to 2% leads to a substantial (2.6%) drop in accuracy. In practice, it is difﬁcult to ﬁnd a trade-off between the time required for tuning (proportional to the subset size) and the loss of performance for the ﬁnal model because these change, sometimes wildly, between datasets. Further, Zhou et al. (2020) have observed that for a ﬁxed number of iterations, rank consistency is better if we use more training samples and fewer epochs rather than fewer training samples and more epochs. This observation gives further motivation for using the whole dataset for HPO/NAS and design new approaches, like PASHA, to save computational resources. 2Published as a conference paper at ICLR 2023 3 P ROBLEM SETUP The problem of selecting the best conﬁguration of a machine learning algorithm to be trained is formalized in (Jamieson & Talwalkar, 2016) as a non-stochastic bandit problem. In this setting the learner (the hyperparameter optimizer) receives N hyperparameter conﬁgurations and it has to identify the best performing one with the constraint of not spending more than a ﬁxed amount of resources R(e.g. total number of training epochs) on a speciﬁc conﬁguration. Ris considered given, but in practice users do not have a good way for selecting it, which can have undesirable consequences: if the value is too small, the model performance will be sub-optimal, while if the budget is too large, the user will incur a signiﬁcant cost without any practical return. This leads users to overestimate R, setting it to a large amount of resources in order to guarantee the convergence of the model. We maintain the concept of maximum amount of resources in our algorithm but we prefer to interpret Ras a “safety net”, a cost not to be surpassed (e.g. in case an error prevents a normal behaviour of the algorithm), instead of the exact amount of resources spent for the optimization. This setting could be extended with additional assumptions, based on empirical observation, removing some extreme cases and leading to a more practical setup. In particular, when working with large datasets we observe that the curve of the loss for conﬁgurations (called arms in the bandit literature) continuously decreases (in expectation). Moreover, “crossing points” between the curves are rare (excluding noise), and they are almost always in the very initial part of the training procedure. Viering & Loog (2021); Mohr & van Rijn (2022) provide an analysis of learning curves and note that in practice most learning curves are well-behaved, with Bornschein et al. (2020); Domhan et al. (2015) reporting similar ﬁndings. More formally, let us deﬁne Ras the maximum number of resources needed to train an ML algorithm to convergence. Given πm(i) the ranking of conﬁguration iafter using mresources for training, there exists minimum R∗much smaller than Rsuch that for all amounts of resources rlarger than R∗the rankings of conﬁgurations trained with r resources remain the same: ∃R∗ ≪R : ∀i ∈ {conﬁgurations},∀r > R∗,πR∗(i) =πr(i). The existence of such a quantity, limited to the best performing conﬁguration, is also assumed by Jamieson & Talwalkar (2016), and it is leveraged to quantify the budget required to identify the best performing conﬁguration. If we knew R∗, it would be sufﬁcient to run all conﬁgurations with exactly that amount of resources to identify the best one and then just train the model from scratch with all the data using that conﬁguration. Unfortunately that quantity is unknown and can only be estimated during the optimization procedure. Note that in practice there is noise involved in training of neural networks, so similarly performing conﬁgurations will repeatedly swap their ranks. 4 M ETHOD PASHA is an extension of ASHA (Li et al., 2020) inspired by the “doubling trick” (Auer et al., 1995). PASHA targets improvements for hyperparameter tuning on large datasets by hinging on the assumptions made about the crossing points of the learning curves in Section 3. The algorithm starts with a small initial amount of resources and progressively increases them if the ranking of the conﬁgurations in the top two rungs (rounds of promotion) has not stabilized. The ability of our approach to stop early automatically is the key beneﬁt. We illustrate the approach in Figure 1, showing how we stop evaluating conﬁgurations for additional rungs if rankings are stable. We describe the details of our proposed approach in Algorithm 1. Given η, a hyperparameter used both in ASHA and PASHA to control the fraction of conﬁgurations to prune, PASHA sets the current maximum resources Rt to be used for evaluating a conﬁguration using the reduction factor ηand the minimum amount of resources rto be used (Kt is the current maximum rung). The approach increases the maximum number of resources allocated to promising conﬁgurations each time the ranking of conﬁgurations in the top two rungs becomes inconsistent. For example, if we can currently train conﬁgurations up to rung 2 and the ranking of conﬁgurations in rung 1 and rung 2 is not consistent, then we allow training part of the conﬁgurations up to rung 3, i.e. one additional rung. The minimum amount of resources ris a hyperparameter to be set by the user. It is signiﬁcantly easier to set compared to Ras ris the minimum amount of resources required to see a meaningful difference in the performance of the models, and it can be easily estimated empirically by running a few small-scale experiments. 3Published as a conference paper at ICLR 2023 Figure 1: Illustration of how PASHA stops early if the ranking of conﬁgurations has stabilized. Left: the ranking of the conﬁgurations (displayed inside the circles) has stabilized, so we can select the best conﬁguration and stop the search. Right: the ranking has not stabilized, so we continue. Algorithm 1Progressive Asynchronous Successive Halving (PASHA) 1: input minimum resource r, reduction factor η 2: function PASHA() 3: t= 0,R0 = ηr, K0 = ⌊logη(R0/r)⌋ 4: while desired do 5: for each free worker do 6: (θ,k) =get job() 7: run then return val loss(θ,rηk) 8: end for 9: for completed job (θ, k) with loss ldo 10: Update conﬁguration θin rung kwith loss l 11: if k≥Kt −1 then 12: πk = configuration ranking(k) 13: end if 14: if k= Kt and πk ̸≡πk−1 then 15: t= t+ 1 16: Rt = ηtR0 17: Kt = ⌊logη(Rt/r)⌋ 18: end if 19: end for 20: end while 21: end function 22: function get job() 23: // Check if there is a promotable config 24: for k= Kt −1,..., 1,0 do 25: candidates = top k(rung k,|rung k|/η) 26: promotable = {c∈candidates : cnot promoted} 27: if |promotable|>0 then 28: return promotable[0],k + 1 29: end if 30: // If not, grow bottom rung 31: Draw random conﬁguration θ 32: return θ,0 33: end for 34: end function We also set a maximum amount of resources Rso that PASHA can default to ASHA if needed and avoid increasing the resources indeﬁnitely. While it is not generally reached, it provides a safety net. 4.1 S OFT RANKING Due to the noise present in the training process, negligible differences in the measured predictive performance of different conﬁgurations can lead to signiﬁcantly different rankings. For these reasons we adopt what we call “soft ranking”. In soft ranking, conﬁgurations are still sorted by predictive performance but are considered equivalent if the performance difference is smaller 4Published as a conference paper at ICLR 2023 than a value ϵ (or equal to it). Instead of producing a sorted list of conﬁguration, this provides a list of lists where for every position of the ranking there is a list of equivalent conﬁgurations. The concept is explained graphically in Figure 2, and we also provide a formal deﬁnition. For a set of n conﬁgurations c1,c2,··· ,ci,··· ,cn and performance metric f (e.g. accuracy) with f(c1) ≤f(c2) ≤···≤ f(ci) ≤···≤ f(cn), soft rank at position iis deﬁned as soft ranki = {cj ∈conﬁgurations : |f(ci) −f(cj)|≤ ϵ}. When deciding on if to increase the resources, we go through the ranked list of conﬁgurations in the top rung and check if the current conﬁguration at the given rank was in the list of conﬁgurations for that rank in the previous rung. If there is a conﬁguration which does not satisfy the condition, we increase resources. Figure 2: Illustration of soft ranking. There are three lists with the ﬁrst two containing two items because the scores of the two conﬁgurations are closer to each other than ϵ. 4.2 A UTOMATIC ESTIMATION OF ϵBY MEASURING NOISE IN RANKINGS Every operation involving randomization gives slightly different results when repeated, the training process and the measurement of performance on the validation set are no exception. In an ideal world, we could repeat the process multiple times to compute empirical mean and variance to make a better decision. Unfortunately this is not possible in our case since the repeating portions of the training process will defeat the purpose of our work: speeding up the tuning process. Understanding when the differences between the performance measured for different conﬁgurations are “signiﬁcant” is crucial for ranking them correctly. We devise a method to estimate a threshold below which differences are meaningless. Our intuition is that conﬁgurations with different performance maintain their relative ranking over time. On the other hand, conﬁgurations that repeatedly swap their rankings perform similarly well and the performance difference in the current epoch or rung is simply due to noise. We want to measure this noise and use it to automatically estimate the threshold value ϵto be used in the soft-ranking described above. Formally we can deﬁne a set of pairs of conﬁgurations that perform similarly well by the following: S : {(c,c′) : ( πrj (c) >πrj (c′) ∧πrk (c) <πrk (c′) ∧πrl (c) >πrl (c′) ) ∨ ( πrj (c) <πrj (c′) ∧πrk (c) >πrk (c′) ∧πrl (c) <πrl (c′) ) }, (1) for resource levels (e.g. epochs – not rungs) rj > rk > rl, using the same notation as earlier to refer to resources. In practice we have per-epoch validation performance statistics and use these to ﬁnd resource levels rj,rk,rl that have conﬁgurations with the criss-crossing behaviour (there can be several epochs between such resource levels). We only consider conﬁgurations (c,c′) that made it to the latest rung, so rηKt−1 ≥rj >rη Kt−2. However, we allow for the criss-crossing to happen across epochs from any rungs. The value of ϵcan then be calculated as the N-th percentile of distances between the performances of conﬁgurations in S: ϵ= PN,(c,c′)∈S|frj (c) −frj (c′)|. The exact value of rj depends on the considered pair of conﬁgurations (c,c′). To uniquely deﬁne frj , we take the maximum resources rj currently available for both conﬁgurations in the consid- ered pair (c,c′). Let us consider the following example setup: the top rung has 8 epochs and 5Published as a conference paper at ICLR 2023 the next one has 4 epochs, there are three conﬁgurations ca,cb,cc that made it to the top rung and were trained for 8, 8 and 6 epochs so far respectively. Assuming there was criss-crossing within each pair (ca,cb), (ca,cc) and (cb,cc), the set of distances between conﬁgurations in S is {|f8(ca) −f8(cb)|,|f6(ca) −f6(cc)|,|f6(cb) −f6(cc)|}. The value of ϵis recalculated every time we receive new information about the performances of conﬁgurations. Initially the value of ϵis set to 0, which means that we check for exact ranking if we cannot yet calculate the value of ϵ. 5 E XPERIMENTS In this section we empirically evaluate the performance of PASHA. Its goal is not to provide a model with a higher accuracy, but to identify the best conﬁguration in a shorter amount of time so that we can then re-train the model from scratch. Overall, we target a signiﬁcantly faster tuning time and on-par predictive performance when comparing with the models identiﬁed by state-of-the-art optimizers like ASHA. Re-training after HPO or NAS is important because HPO and NAS in general require to reserve a signiﬁcant part of the data (often around 20 or 30%) to be used as a validation set. Training with fewer data is not desirable because in practice it is observed that training a model on the union of training and validation sets provides better results. We tested our method on two different sets of experiments. The ﬁrst set evaluates the algorithm on NAS problems and uses NASBench201 (Dong & Yang, 2020), while the second set focuses on HPO and was run on two large-scale tasks from PD1 benchmark (Wang et al., 2021). 5.1 S ETUP Our experimental setup consists of two phases: 1) run the hyperparameter optimizer until N = 256 candidate conﬁgurations are evaluated; and 2) use the best conﬁguration identiﬁed in the ﬁrst phase to re-train the model from scratch. For the purpose of these experiments we re-train all the models using only the training set. This avoids introducing an arbitrary choice on the validation set size and allows us to leverage standard benchmarks such as NASBench201. In real-world applications the model can be trained on both training and validation sets. All our results report only the time invested in identifying the best conﬁguration since the re-training time is comparable for all optimizers. All results are averaged over multiple repetitions, with the details speciﬁed for each set of experiments separately. We use N = 90-th percentile when calculating the value of ϵ. We use 4 workers to perform parallel and asynchronous evaluations. The choice of Ris sensitive for ASHA as it can make the optimizer consume too many resources and penalize the performance. For a fair comparison, we make Rdataset-dependent taking the maximum amount of resources in the considered benchmarks. ris also dataset-dependent and η, the halving factor, is set to 3 unless otherwise speciﬁed. The same values are used for both ASHA and PASHA. Runtime reported is the time spent on HPO (without retraining), including the time for computing validation set performance. We compare PASHA with ASHA (Li et al., 2020), a widely-adopted approach for multi-ﬁdelity HPO, and other relevant baselines. In particular, we consider “one-epoch baseline” that trains all conﬁgurations for one epoch (the minimum available resources) and then selects the most promising conﬁguration, and “random baseline” that randomly selects the conﬁguration without any training. For both one-epoch and random baselines we sample N = 256conﬁgurations, using the same scheduler and seeds as for PASHA and ASHA. All reported accuracies are after retraining for R= 200epochs. In addition, two, three and ﬁve-epoch baselines are evaluated in Appendix A. 5.2 NAS EXPERIMENTS For our NAS experiments we leverage the well-known NASBench201 (Dong & Yang, 2020) bench- mark. The task is to identify the network structure providing the best accuracy on three different datasets (CIFAR-10, CIFAR-100 and ImageNet16-120) independently. We use r = 1epoch and R= 200epochs. We repeat the experiments using 5 random seeds for the scheduler and 3 random seeds for NASBench201 (all that are available), resulting in 15 repetitions. Some conﬁgurations in NASBench201 do not have all seeds available, so we impute them by averaging over the available seeds. To measure the predictive performance we report the best accuracy on the combined validation and test set provided by the creators of the benchmark. 6Published as a conference paper at ICLR 2023 Table 1: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h±0.6h 1.0x 200.0 ±0.0 PASHA 93.57 ±0.75 1.3h±0.6h 2.3x 36.1 ±50.0 One-epoch baseline 93.30±0.61 0.3h±0.0h 8.5x 1.0 ±0.0 Random baseline 72.88±19.20 0.0h±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 One-epoch baseline 65.57±5.53 0.3h±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83±18.20 0.0h±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h±2.2h 1.0x 200.0 ±0.0 PASHA 45.13 ±1.51 2.9h±1.7h 3.1x 21.3 ±48.1 One-epoch baseline 41.42±4.98 1.0h±0.0h 8.8x 1.0 ±0.0 Random baseline 20.75±9.97 0.0h±0.0h N/A 0.0 ±0.0 The results in Table 1 suggest PASHA consistently leads to strong improvements in runtime, while achieving similar accuracy values as ASHA. The one-epoch baseline has noticeably worse accuracies than ASHA or PASHA, suggesting that PASHA does a good job of deciding when to continue increasing the resources – it does not stop too early. Random baseline is a lot worse than the one- epoch baseline, so there is value in performing NAS. We also report the maximum resources used to ﬁnd how early the ranking becomes stable in PASHA. The large variances are caused by stopping HPO at different rung levels for different seeds (e.g. 27 and 81 epochs). Note that the time required to train a model is about 1.3h for CIFAR-10 and CIFAR-100, and about 4.1h for ImageNet16-120, making the total tuning time of PASHA comparable or faster than the training time. We also ran additional experiments testing PASHA with a reduction factor ofη= 2and η= 4instead of η = 3, the usage of PASHA as a scheduler in MOBSTER (Klein et al., 2020) and alternative ranking functions. These experiments provided similar ﬁndings as the above and are described next. 5.2.1 R EDUCTION FACTOR An important parameter for the performance of multi-ﬁdelity algorithms like ASHA is the reduction factor. This hyperparameter controls the fraction of pruned candidates at every rung. The optimal theoretical value is eand it is typically set to 2 or 3. In Table 2 we report the results of the different algorithms ran with η= 2and η= 4on CIFAR-100 (the full set of results is in Appendix B). The gains are consistent also for η= 2and η= 4, with a larger speedup when using η= 2as that allows PASHA to make more decisions and identify earlier that it can stop the search. Table 2: NASBench201 – CIFAR-100 results with various reduction factorsη. The speedup is large for both η= 2and η= 4, and accuracy similar to ASHA is retained. Dataset Reduction factor Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-100 η= 2 ASHA 71.67 ±0.84 3.8h ±1.0h 1.0x 200.0 ±0.0 PASHA 71.65±1.42 0.9h ±0.1h 4.2x 5.9 ±2.0 η= 4 ASHA 71.43 ±1.13 2.7h ±0.9h 1.0x 200.0 ±0.0 PASHA 72.09±1.22 1.0h ±0.4h 2.8x 25.1 ±49.0 5.2.2 B AYESIAN OPTIMIZATION Bayesian Optimization combined with multi-ﬁdelity methods such as Successive Halving can improve the predictive performance of the ﬁnal model (Klein et al., 2020). In this set of experiments, we verify PASHA can speedup also these kinds of methods. Our results are reported in Table 3, where we can clearly see PASHA obtains a similar accuracy result as ASHA with signiﬁcant speedup. 7Published as a conference paper at ICLR 2023 Table 3: NASBench201 results for ASHA with Bayesian Optimization searcher – MOBSTER (Klein et al., 2020) and similarly extended version of PASHA. The results show PASHA can be successfully combined with a smarter conﬁguration selection strategy. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 MOBSTER 94.21±0.28 5.0h ±1.1h 1.0x 200.0 ±0.0 PASHA BO 94.00±0.20 2.6h ±1.8h 2.0x 70.7 ±81.6 CIFAR-100 MOBSTER 72.79±0.68 5.7h ±1.4h 1.0x 200.0 ±0.0 PASHA BO 72.16±1.07 1.6h ±0.5h 3.7x 13.0 ±8.7 ImageNet16-120MOBSTER 46.21±0.70 15.1h±4.0h 1.0x 200.0 ±0.0 PASHA BO 45.36±1.06 3.9h ±1.2h 3.9x 11.8 ±7.9 5.2.3 A LTERNATIVE RANKING FUNCTIONS We have considered a variety of alternative ranking functions in addition to the soft ranking function that automatically estimates the value of ϵby measuring noise in rankings. These include simple ranking (equivalent to soft ranking withϵ= 0.0), soft ranking with ﬁxed values of ϵor obtained using various heuristics (for example based on the standard deviation of objective values in the previous rung), Rank Biased Overlap (RBO) (Webber et al., 2010), and our own reciprocal rank regret metric (RRR) that considers the objective values of conﬁgurations. Details of the ranking functions and additional results are in Appendix C. Table 4 shows a selection of the results on CIFAR-100 with full results in the appendix. We can see there are also other ranking functions that work well and that simple ranking is not sufﬁciently robust – some benevolence is needed. However, the ranking function that estimates the value of ϵ by measuring noise in rankings (to which we refer simply as PASHA) remains the easiest to use, is well-motivated and offers both excellent performance and large speedup. Table 4: NASBench201 – CIFAR-100 results for a variety of ranking functions, showing there are also other well-performing options, even though those are harder to use and are less interpretable. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 PASHA direct ranking 71.69 ±1.05 2.8h±0.7h 1.1x 200.0 ±0.0 PASHA soft rankingϵ= 2.5% 71.41±1.15 1.5h±0.7h 2.1x 88.3 ±74.4 PASHA soft rankingϵ= 2σ 71.14±0.97 1.9h±0.7h 1.7x 136.4 ±75.8 PASHA RBO 71.51 ±0.93 2.4h±0.7h 1.3x 180.5 ±50.6 PASHA RRR 71.42 ±1.51 1.2h±0.5h 2.6x 39.3 ±51.4 One-epoch baseline 65.57 ±5.53 0.3h±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83 ±18.20 0.0h±0.0h N/A 0.0 ±0.0 5.3 HPO EXPERIMENTS We further utilize the PD1 HPO benchmark (Wang et al., 2021) to show the usefulness of PASHA in large-scale settings. In particular, we take WMT15 German-English (Bojar et al., 2015) and ImageNet (Deng et al., 2009) datasets that use xformer (Lefaudeux et al., 2021) and ResNet50 (He et al., 2015) models. Both of them are datasets with a large amount of training examples, in particular WMT15 German-English has about 4.5M examples, while ImageNet has about 1.3M examples. In PD1 we optimize four hyperparameters: base learning rate η∈ [ 10−5,10.0 ] (log scale), momen- tum 1 −β ∈ [ 10−3,1.0 ] (log scale), polynomial learning rate decay schedule power p∈[0.1,2.0] (linear scale) and decay steps fraction λ∈[0.01,0.99] (linear scale). The minibatch size used for WMT experiments is 64, while the minibatch size for ImageNet experiments is 512. There are 1414 epochs available for WMT and 251 for ImageNet. There are also other datasets in PD1, but these only have a small number of epochs with 1 epoch being the minimum amount of resources. As a result there would not be enough rungs to see beneﬁts of the early stopping provided by PASHA. 8Published as a conference paper at ICLR 2023 If resources could be deﬁned in terms of fractions of epochs, PASHA could be beneﬁcial there too. Most public benchmarks have resources deﬁned in terms of epochs, but in practice it is possible to deﬁne resources also in alternative ways. We use 1-NN as a surrogate model for the PD1 benchmark. We repeat our experiments using 5 random seeds and there is only one dataset seed available. The results in Table 5 show that PASHA leads to large speedups on both WMT and ImageNet datasets. The speedup is particularly impressive for the signiﬁcantly larger WMT dataset where it is about 15.5x, highlighting how PASHA can signiﬁcantly accelerate the HPO search on datasets with millions of training examples (WMT has about 4.5M training examples). The one-epoch baseline obtains similar accuracy as ASHA and PASHA for WMT, but performs signiﬁcantly worse on ImageNet dataset. This result suggests that simple approaches such as the one-epoch baseline are not robust and solutions such as PASHA are needed (which we also saw on NASBench201). Selecting the hyperparameters randomly leads to signiﬁcantly worse performance than any of the other approaches. Table 5: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ±1.41 43.7h±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ±2.05 2.8h ±0.6h 15.5x 37.8 ±21.6 One-epoch baseline 62.36±1.40 0.6h ±0.0h 67.3x 1.0 ±0.0 Random baseline 33.93±21.96 0.0h ±0.0h N/A 0.0 ±0.0 ImageNet ASHA 75.10 ±2.03 7.3h ±1.2h 1.0x 251.0 ±0.0 PASHA 73.37 ±2.71 3.8h ±1.0h 1.9x 45.0 ±30.1 One-epoch baseline 63.40±9.91 1.1h ±0.0h 6.7x 1.0 ±0.0 Random baseline 36.94±31.05 0.0h ±0.0h N/A 0.0 ±0.0 6 L IMITATIONS PASHA is designed to speed up ﬁnding the best conﬁguration, making HPO and NAS more accessible. To do so, PASHA interrupts the tuning process when it considers the ranking of conﬁgurations to be sufﬁciently stable, not spending resources on evaluating conﬁgurations in later rungs. However, the beneﬁts of such mechanism will be small in some circumstances. When the number of rungs is small, there will be few opportunities for PASHA to interrupt the tuning and provide large speedups. This phenomenon is demonstrated in Appendix D on the LCBench benchmark (Zimmer et al., 2021). Public benchmarks usually ﬁx the minimum resources to one epoch, while the maximum is benchmark-dependent (e.g. 200 epochs for NASBench201 and 50 for LCBench), leaving little control for algorithms like PASHA in some cases. Appendix E analyses the impact of these choices. For practical usage, we recommend having a maximum amount of resources at least 100 times larger than the minimum amount of resources when using η= 3(default). This can be achieved by measuring resources with higher granularity (e.g. in terms of gradient updates) if needed. 7 C ONCLUSIONS In this work we have introduced a new variant of Successive Halving called PASHA. Despite its simplicity, PASHA leads to strong improvements in the tuning time. For example, in many cases it reduces the time needed to about one third compared to ASHA without a noticeable impact on the quality of the found conﬁguration. For benchmarks with a small number of rungs (LCBench), PASHA provides more modest speedups but this limitation can be mitigated in practice by adopting a more granular unit of measure for resources. Further work could investigate the deﬁnition of rungs and resource levels, with the aim of understanding how they impact the decisions of the algorithm. More broadly this applies not only to PASHA but also to multi-ﬁdelity algorithms in general. PASHA can also be successfully combined with more advanced search strategies based on Bayesian Optimization to obtain improvements in accuracy at a fraction of the time. In the future, we would like to test combinations of PASHA with transfer-learning techniques for multi-ﬁdelity such as RUSH (Zappella et al., 2021) to further decrease the tuning time. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT We include the code for our approach at https://github.com/ondrejbohdal/pasha, including details for how to run the experiments. We use pre-computed benchmarks that make it possible to run the NAS and HPO experiments even without large computational resources. In addition, PASHA is available as part of the Syne Tune library (Salinas et al., 2022). ACKNOWLEDGEMENTS We would like to thank the Syne Tune developers for providing us with a library to easily extend and use in our experiments. We would like to thank Aaron Klein, Matthias Seeger and David Salinas for their support on questions regarding Syne Tune and hyperparameter optimization more broadly. We would also like to thank Valerio Perrone, Sanyam Kapoor and Aditya Rawal for insightful discussions when working on the project. Further, we are thankful to the anonymous reviewers for helping us improve our paper. REFERENCES Peter Auer, Nicol´o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Annual Foundations of Computer Science, 1995. James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. JMLR, 2012. James Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter optimization. In NIPS, 2011. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, 2015. Jorg Bornschein, Francesco Visin Visin, and Simon Osindero. Small data, big decisions: Model selection in the small-data regime. In ICML, 2020. Jia Deng, Wei Dong, Richard Socher, Li-jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical image database, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In IJCAI, 2015. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In ICLR, 2020. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efﬁcient hyperparameter opti- mization at scale. In ICML, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2015. Nikita Ivkin, Zohar Karnin, Valerio Perrone, and Giovanni Zappella. Cost-aware adversarial best arm identiﬁcation. In ICLR NAS Workshop, 2021. Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter optimization. In AISTATS, 2016. Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In ICML, 2013. 10Published as a conference paper at ICLR 2023 Aaron Klein, Louis C. Tiao, Thibaut Lienart, Cedric Archambeau, and Matthias Seeger. Model-based asynchronous hyperparameter and neural architecture search. In arXiv, 2020. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, and Susan Zhang. xformers: A modular and hackable transformer modelling library, 2021. Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In MLSys, 2020. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. JMLR, 2018. Felix Mohr and Jan N. van Rijn. Learning curves for decision making in supervised machine learning - a survey. In arXiv, 2022. David Salinas, Matthias Seeger, Aaron Klein, Valerio Perrone, Martin Wistuba, and Cedric Archam- beau. Syne tune: A library for large scale hyperparameter tuning and reproducible research. In First Conference on Automated Machine Learning (Main Track), 2022. Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training nlp models: A concise overview. In arXiv, 2020. Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. Core-set sampling for efﬁcient neural architecture search. In ICML Workshop on Subset Selection in ML, 2021. Tom Viering and Marco Loog. The shape of learning curves: a review. In arXiv, 2021. Savan Visalpara, Krishnateja Killamsetty, and Rishabh Iyer. A data subset selection framework for efﬁcient hyper-parameter tuning and automatic machine learning. In ICML Workshop on Subset Selection in ML, 2021. Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained Gaussian processes for Bayesian Optimization. In arXiv, 2021. William Webber, Alistair Moffat, and Justin Zobel. A similarity measure for indeﬁnite rankings. In ACM Transactions on Information Systems, 2010. Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. PC-DARTS: Partial channel connections for memory-efﬁcient architecture search. In ICLR, 2020. Giovanni Zappella, David Salinas, and C´edric Archambeau. A resource-efﬁcient method for repeated HPO and NAS problems. In ICML AutoML Workshop, 2021. Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang, and Wanli Ouyang. EcoNAS: Finding proxies for economical neural architecture search. In CVPR, 2020. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-PyTorch Tabular: Multi-ﬁdelity metalearn- ing for efﬁcient and robust AutoDL. TPAMI, 2021. 11Published as a conference paper at ICLR 2023 A A DDITIONAL BASELINES We consider additional baselines that evaluate how good two, three and ﬁve-epoch baselines are compared to PASHA. From Table 6 and 7 we see that while these usually get closer to the performance of ASHA and PASHA than the one-epoch baseline, they are still relatively far compared to PASHA. Moreover, it is crucial to observe that such baselines cannot dynamically allocate resources and decide when to stop, and as a result PASHA can outperform them both in terms of speedup and the quality of the found conﬁguration. Table 6: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h ±0.6h 1.0x 200.0 ±0.0 PASHA 93.57 ±0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 One-epoch baseline 93.30±0.61 0.3h ±0.0h 8.5x 1.0 ±0.0 Two epoch baseline 92.75±0.91 0.7h ±0.0h 4.2x 2.0 ±0.0 Three epoch baseline 93.47±0.71 1.0h ±0.0h 2.8x 3.0 ±0.0 Five epoch baseline 93.38±0.90 1.7h ±0.0h 1.7x 5.0 ±0.0 Random baseline 72.88 ±19.20 0.0h±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h ±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h ±0.4h 3.4x 20.5 ±48.3 One-epoch baseline 65.57±5.53 0.3h ±0.0h 9.2x 1.0 ±0.0 Two-epoch baseline 68.28±4.25 0.7h ±0.0h 4.6x 2.0 ±0.0 Three-epoch baseline 70.47±1.60 1.0h ±0.0h 3.1x 3.0 ±0.0 Five-epoch baseline 70.95±0.95 1.7h ±0.0h 1.8x 5.0 ±0.0 Random baseline 42.83 ±18.20 0.0h±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h ±2.2h 1.0x 200.0 ±0.0 PASHA 45.13 ±1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 One-epoch baseline 41.42±4.98 1.0h ±0.0h 8.8x 1.0 ±0.0 Two-epoch baseline 42.99±1.89 2.0h ±0.0h 4.4x 2.0 ±0.0 Three-epoch baseline 44.65±0.95 3.0h ±0.0h 2.9x 3.0 ±0.0 Five-epoch baseline 44.75±1.03 5.0h ±0.1h 1.8x 5.0 ±0.0 Random baseline 20.75 ±9.97 0.0h ±0.0h N/A 0.0 ±0.0 Table 7: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ± 1.41 43.7h ±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ± 2.05 2.8h ± 0.6h 15.5x 37.8 ±21.6 One-epoch baseline 62.36 ± 1.40 0.6h ± 0.0h 67.3x 1.0 ± 0.0 Two-epoch baseline 60.16± 3.58 1.1h ± 0.0h 39.1x 2.0 ± 0.0 Three-epoch baseline 61.12± 3.47 1.6h ± 0.0h 27.6x 3.0 ± 0.0 Five-epoch baseline 57.89 ± 5.33 2.5h ± 0.0h 17.3x 5.0 ± 0.0 Random baseline 33.93 ±21.96 0.0h ± 0.0h N/A 0.0 ± 0.0 ImageNet ASHA 75.10 ± 2.03 7.3h ± 1.2h 1.0x 251.0 ± 0.0 PASHA 73.37 ± 2.71 3.8h ± 1.0h 1.9x 45.0 ±30.1 One-epoch baseline 63.40 ± 9.91 1.1h ± 0.0h 6.7x 1.0 ± 0.0 Two-epoch baseline 64.61±10.81 1.7h ± 0.0h 4.2x 2.0 ± 0.0 Three-epoch baseline 68.74± 3.79 2.3h ± 0.1h 3.2x 3.0 ± 0.0 Five-epoch baseline 65.91 ± 3.99 3.7h ± 0.1h 2.0x 5.0 ± 0.0 Random baseline 36.94 ±31.05 0.0h ± 0.0h N/A 0.0 ± 0.0 B R EDUCTION FACTOR Table 8 shows the full set of results for our experiments with different reduction factors. The behaviour is the same across all cases. 12Published as a conference paper at ICLR 2023 Table 8: NASBench201 results with various reduction factors η. Dataset Reduction factor Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 η= 2 ASHA 93.88 ±0.27 3.6h ±1.1h 1.0x 200.0 ±0.0 PASHA 93.53 ±0.76 1.0h ±0.3h 3.5x 9.1 ±8.1 η= 4 ASHA 93.75 ±0.28 2.4h ±0.6h 1.0x 200.0 ±0.0 PASHA 93.65 ±0.65 1.1h ±0.5h 2.3x 32.3 ±50.2 CIFAR-100 η= 2 ASHA 71.67 ±0.84 3.8h ±1.0h 1.0x 200.0 ±0.0 PASHA 71.65 ±1.42 0.9h ±0.1h 4.2x 5.9 ±2.0 η= 4 ASHA 71.43 ±1.13 2.7h ±0.9h 1.0x 200.0 ±0.0 PASHA 72.09 ±1.22 1.0h ±0.4h 2.8x 25.1 ±49.0 ImageNet16-120 η= 2 ASHA 46.09 ±0.68 11.9h ±4.0h 1.0x 200.0 ±0.0 PASHA 45.35 ±1.52 2.8h ±0.6h 4.2x 9.3 ±7.1 η= 4 ASHA 45.43 ±0.98 7.9h ±3.0h 1.0x 200.0 ±0.0 PASHA 45.52 ±1.30 2.9h ±1.1h 2.8x 18.4 ±18.7 C E XPERIMENTS WITH ALTERNATIVE RANKING FUNCTIONS C.1 D ESCRIPTION PASHA employs a ranking function whose choice is completely arbitrary. In our main set of experiments we used soft ranking that automatically estimates the value of ϵby measuring noise in rankings. In this set of experiments we would like to evaluate different criteria to deﬁne the ranking of the candidates. We describe the functions considered next. C.1.1 D IRECT RANKING As a baseline, we study if we can use the simple ranking of conﬁgurations by predictive performance (e.g., sorting from the ones with the highest accuracy to the ones with the lowest). If any of the conﬁgurations change their order, we consider the ranking unstable and increase the resources. C.1.2 S OFT RANKING VARIATIONS We consider several variations of soft ranking. The ﬁrst variation is to ﬁx the value of the ϵparameter. We have considered values 0.01, 0.02, 0.025, 0.03, 0.05. The second set of variations aim to estimate the value of ϵautomatically, using various heuristics. The heuristics we have evaluated include: • Standard deviation: calculate the standard deviation of the considered performance measure (e.g. accuracy) of the conﬁgurations in the previous rung and set a multiple of it as the value of ϵ– we tried multiples of 1, 2 and 3. • Mean distance: value of ϵis set as the mean distance between the score of the conﬁgurations in the previous rung. • Median distance: similar to the mean distance, but using the median distance. There are various beneﬁts for estimating the value of ϵby measuring noise in rankings, as presented in our paper: • There is no need to set the value of ϵmanually. • Estimation of ϵhas an intuitive motivation that makes sense. • The value of ϵcan dynamically adapt to the different stages of hyperparameter optimization. • The approach works well in practice. C.1.3 R ANK BIASED OVERLAP (RBO) (W EBBER ET AL ., 2010) A score that can be broadly interpreted as a weighted correlation between rankings. We can specify how much we want to prioritize the top of the ranking using parameter pthat is between 0.0 and 1.0, 13Published as a conference paper at ICLR 2023 with a smaller value giving more priority to the top of the ranking. The best value is 1.0, while it gives value of 0.0 for rankings that are completely the opposite. We compute the RBO value and then compare it to the selected threshold t, increasing the resources if the value is less than the threshold. C.1.4 R ECIPROCAL RANK REGRET (RRR) A key insight is that conﬁgurations can be very similar to each other and differences in their rankings will not affect the quality of the found solution signiﬁcantly. To account for this we look at the objective values of the conﬁgurations (e.g. accuracy) and compute the relative regret that we would pay at the current rung if we would have assumed the ranking at the previous rung correct. We deﬁne reciprocal rank regret (RRR) as: RRR = n−1∑ i=0 (fi −f′ i) fi wi, where f represents the ordered scores in the top rung, f′represents the reordered scores from the top rung according to the second rung, nis the number of conﬁgurations in the top rung and pis the parameter that says how much attention to give to the top of the ranking. The weights wi sum to 1 and can be selected in different ways to e.g. give more priority to the top of the ranking. For example, we could use the following weights: wi = pi ∑n−1 i=0 pi The metric has an intuitive interpretation: it is the average relative regret with priority on top of the ranking. The best value of RRR is 0.0, while the worst possible value is 1.0. We also consider a version of RRR which considers the absolute values of the differences in the objectives - Absolute RRR (ARRR). We have evaluated these additional ranking functions using NASBench201 benchmark. C.2 R ESULTS We report the results in Table 9, 10 and 11. We see there are also several other variations that achieve strong results across a variety of datasets within NASBench201, most notably soft ranking 2σand variations based on RRR. In these cases we obtain similar performance as ASHA, but at a signiﬁcantly shorter time. We additionally also give a similar analysis in Table 12 (analogous to Table 4), where we analyse a selection of the most interesting ranking functions for the PD1 benchmark. 14Published as a conference paper at ICLR 2023 Table 9: NASBench201 – CIFAR-10 results for a variety of ranking functions. Approach Accuracy (%) Runtime Speedup factor Max resources ASHA 93.85 ± 0.25 3.0h ±0.6h 1.0x 200.0 ± 0.0 PASHA 93.57 ± 0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 PASHA direct ranking 93.79 ± 0.26 2.7h ±0.6h 1.1x 198.4 ± 6.0 PASHA soft rankingϵ= 0.01 93.79± 0.26 2.6h ±0.5h 1.1x 194.3 ±21.2 PASHA soft rankingϵ= 0.02 93.78± 0.31 2.4h ±0.5h 1.2x 152.4 ±58.3 PASHA soft rankingϵ= 0.025 93.78± 0.31 2.3h ±0.5h 1.3x 144.5 ±59.4 PASHA soft rankingϵ= 0.03 93.78± 0.32 2.2h ±0.6h 1.3x 128.6 ±58.3 PASHA soft rankingϵ= 0.05 93.79± 0.49 1.8h ±0.7h 1.6x 76.0 ±66.0 PASHA soft ranking1σ 93.75± 0.32 2.4h ±0.5h 1.2x 186.4 ±35.2 PASHA soft ranking2σ 93.88± 0.28 1.9h ±0.5h 1.5x 132.7 ±68.7 PASHA soft ranking3σ 93.56± 0.69 0.9h ±0.3h 3.1x 16.2 ±19.9 PASHA soft ranking mean distance 93.73± 0.52 2.3h ±0.4h 1.3x 184.1 ±40.5 PASHA soft ranking median distance 93.82± 0.26 2.3h ±0.5h 1.3x 169.2 ±51.2 PASHA RBO p=1.0, t=0.5 93.49 ± 0.78 0.7h ±0.1h 4.2x 4.6 ± 6.0 PASHA RBO p=0.5, t=0.5 93.77 ± 0.35 2.2h ±0.6h 1.3x 144.0 ±71.2 PASHA RRR p=1.0, t=0.05 93.49 ± 0.78 0.7h ±0.0h 4.4x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 93.76 ± 0.31 2.1h ±0.6h 1.4x 140.9 ±69.7 PASHA ARRR p=1.0, t=0.05 93.71 ± 0.35 2.4h ±0.4h 1.2x 179.0 ±42.9 PASHA ARRR p=0.5, t=0.05 93.81 ± 0.30 2.5h ±0.4h 1.2x 181.0 ±40.9 One-epoch baseline 93.30 ± 0.61 0.3h ±0.0h 8.5x 1.0 ± 0.0 Random baseline 72.88 ±19.20 0.0h ±0.0h N/A 0.0 ± 0.0 Table 10: NASBench201 – CIFAR-100 results for a variety of ranking functions. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 71.69 ± 1.05 3.2h ±0.9h 1.0x 200.0 ± 0.0 PASHA 71.84 ± 1.41 0.9h ±0.4h 3.4x 20.5 ±48.3 PASHA direct ranking 71.69 ± 1.05 2.8h ±0.7h 1.1x 200.0 ± 0.0 PASHA soft rankingϵ= 0.01 71.55± 1.04 2.5h ±0.7h 1.3x 198.3 ± 6.5 PASHA soft rankingϵ= 0.02 70.94± 0.85 2.0h ±0.5h 1.6x 160.5 ±62.9 PASHA soft rankingϵ= 0.025 71.41± 1.15 1.5h ±0.7h 2.1x 88.3 ±74.4 PASHA soft rankingϵ= 0.03 71.00± 1.38 1.0h ±0.5h 3.2x 39.4 ±63.4 PASHA soft rankingϵ= 0.05 70.71± 1.66 0.7h ±0.0h 4.9x 3.0 ± 0.0 PASHA soft ranking1σ 71.56± 1.03 2.5h ±0.6h 1.3x 184.1 ±40.5 PASHA soft ranking2σ 71.14± 0.97 1.9h ±0.7h 1.7x 136.4 ±75.8 PASHA soft ranking3σ 71.63± 1.60 1.0h ±0.3h 3.3x 20.2 ±25.3 PASHA soft ranking mean distance 71.51± 0.99 2.4h ±0.5h 1.4x 189.8 ±30.3 PASHA soft ranking median distance 71.52± 0.98 2.4h ±0.6h 1.3x 189.5 ±30.6 PASHA RBO p=1.0, t=0.5 70.69 ± 1.67 0.7h ±0.1h 4.6x 3.8 ± 2.0 PASHA RBO p=0.5, t=0.5 71.51 ± 0.93 2.4h ±0.7h 1.3x 180.5 ±50.6 PASHA RRR p=1.0, t=0.05 70.71 ± 1.66 0.7h ±0.0h 4.9x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 71.42 ± 1.51 1.2h ±0.5h 2.6x 39.3 ±51.4 PASHA ARRR p=1.0, t=0.05 70.80 ± 1.70 0.8h ±0.4h 3.8x 22.9 ±51.3 PASHA ARRR p=0.5, t=0.05 71.41 ± 1.05 1.8h ±0.6h 1.7x 110.0 ±68.7 One-epoch baseline 65.57 ± 5.53 0.3h ±0.0h 9.2x 1.0 ± 0.0 Random baseline 42.83 ±18.20 0.0h ±0.0h N/A 0.0 ± 0.0 15Published as a conference paper at ICLR 2023 Table 11: NASBench201 – ImageNet16-120 results for a variety of ranking functions. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 45.63 ± 0.81 8.8h ±2.2h 1.0x 200.0 ± 0.0 PASHA 45.13 ± 1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 PASHA direct ranking 45.63 ± 0.81 8.3h ±2.5h 1.1x 200.0 ± 0.0 PASHA soft rankingϵ= 0.01 45.52± 0.89 7.0h ±1.5h 1.3x 185.7 ±36.1 PASHA soft rankingϵ= 0.02 45.79± 1.16 4.4h ±1.4h 2.0x 71.4 ±50.8 PASHA soft rankingϵ= 0.025 46.01± 1.00 3.2h ±1.0h 2.8x 28.6 ±27.7 PASHA soft rankingϵ= 0.03 45.62± 1.48 2.4h ±0.7h 3.6x 11.0 ±10.0 PASHA soft rankingϵ= 0.05 44.90± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA soft ranking1σ 45.63± 0.89 6.5h ±1.3h 1.4x 177.1 ±44.2 PASHA soft ranking2σ 45.39± 1.22 4.5h ±1.4h 1.9x 91.2 ±58.0 PASHA soft ranking3σ 44.90± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA soft ranking mean distance 45.50± 1.12 6.2h ±1.5h 1.4x 157.7 ±54.7 PASHA soft ranking median distance 45.67± 0.95 6.3h ±1.6h 1.4x 156.3 ±52.2 PASHA RBO p=1.0, t=0.5 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA RBO p=0.5, t=0.5 45.24 ± 1.13 6.4h ±1.3h 1.4x 148.3 ±56.9 PASHA RRR p=1.0, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA ARRR p=1.0, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA ARRR p=0.5, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 One-epoch baseline 41.42 ± 4.98 1.0h ±0.0h 8.8x 1.0 ± 0.0 Random baseline 20.75 ± 9.97 0.0h ±0.0h N/A 0.0 ± 0.0 Table 12: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark, using a selection of the most interesting candidates for ranking functions. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ±1.41 43.7h±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ±2.05 2.8h ±0.6h 15.5x 37.8 ±21.6 PASHA direct ranking 62.16 ±1.75 39.3h±38.3h 1.1x 1024.0 ±466.6 PASHA soft rankingϵ= 2.5% 62.09±2.04 1.3h ±0.4h 33.4x 4.2 ± 2.4 PASHA soft ranking2σ 62.52±2.18 1.1h ±0.1h 38.8x 3.0 ± 0.0 PASHA RBO p=0.5, t=0.5 61.44±1.23 6.7h ±7.8h 6.5x 147.6 ±113.2 PASHA RRR p=0.5, t=0.05 62.52±2.18 1.1h ±0.1h 38.8x 3.0 ± 0.0 One-epoch baseline 62.36 ±1.40 0.6h ±0.0h 67.3x 1.0 ± 0.0 Random baseline 33.93 ±21.96 0.0h ±0.0h N/A 0.0 ± 0.0 ImageNet ASHA 75.10 ±2.03 7.3h ±1.2h 1.0x 251.0 ± 0.0 PASHA 73.37 ±2.71 3.8h ±1.0h 1.9x 45.0 ±30.1 PASHA direct ranking 75.10 ±2.03 6.8h ±0.7h 1.1x 247.8 ± 3.9 PASHA soft rankingϵ= 2.5% 74.73±1.99 4.3h ±2.5h 1.7x 140.4 ±112.8 PASHA soft ranking2σ 75.82±0.82 5.0h ±1.6h 1.5x 133.0 ±96.8 PASHA RBO p=0.5, t=0.5 74.80±2.19 4.4h ±2.1h 1.6x 117.4 ±109.4 PASHA RRR p=0.5, t=0.05 74.98±2.12 1.6h ±0.0h 4.7x 3.0 ± 0.0 One-epoch baseline 63.40 ±9.91 1.1h ±0.0h 6.7x 1.0 ± 0.0 Random baseline 36.94 ±31.05 0.0h ±0.0h N/A 0.0 ± 0.0 16Published as a conference paper at ICLR 2023 D A DDITIONAL RESULTS ON LCB ENCH We additionally evaluate PASHA on the LCBench benchmark (Zimmer et al., 2021) where only modest speedups can be expected due to a small number of epochs (and hence rungs) available. LCBench limits the maximum amount of resources per conﬁguration to 50 epochs, so when using and setting the minimum resource level to 1 epoch, it is a challenging testbed for an algorithm like PASHA. The hyperparameters optimized include number of layers ∈[1,5], max. number of units ∈[64,1024] (log scale), batch size ∈[16,512] (log scale), learning rate ∈ [ 10−4,10−1] (log scale), weight decay ∈ [ 10−5,10−1] , momentum ∈[0.1,0.99] and max. value of dropout ∈[0.0,1.0]. Similarly as in our other experiments, we use η= 3and stop after sampling 256 candidates. Overall, the results in Table 13 conﬁrm an accuracy level on-par with ASHA. While, as expected, the speedup is reduced compared to the experiments on NASBench, in several cases PASHA achieves a 20+% speedup with peaks around 40%. If only a small number of epochs is sufﬁcient for training the model on the given dataset, then HPO can be performed on a sub-epoch basis, e.g. deﬁning the rung levels in terms of iterations instead of epochs. PASHA would then be able to give a large speedup even in cases with smaller numbers of epochs – an example of which is LCBench. Table 13: Results of the HPO experiments on the LCBench benchmark. Mean and std of the test accuracy across ﬁve random seeds. PASHA achieves similar accuracies as ASHA, but gives only modest speedups because of the limited number of rung levels and opportunities to stop the HPO early. To enable large speedup from PASHA, we could redeﬁne the rung levels in terms of neural network weights updates rather than epochs. Dataset ASHA accuracy (%) PASHA accuracy (%) PASHA speedup APSFailure 97.52 ± 0.92 97.01 ±0.75 1.3x Amazonemployeeaccess 94.01 ± 0.40 94.21 ±0.00 1.1x Australian 83.35 ± 0.33 83.35 ±0.51 1.1x Fashion-MNIST 86.70 ± 1.87 86.34 ±1.32 1.1x KDDCup09appetency 98.22 ± 0.00 98.22 ±0.00 1.1x MiniBooNE 86.13 ± 1.57 86.24 ±1.62 1.4x Adult 79.14 ± 0.85 79.14 ±0.85 1.2x Airlines 59.57 ± 1.32 59.22 ±0.76 1.4x Albert 64.31 ± 0.99 64.23 ±0.61 1.2x Bank-marketing 88.34 ± 0.07 88.38 ±0.00 1.2x Blood-transfusion-service-center 79.92 ± 0.20 76.99 ±6.00 1.1x Car 86.60 ± 6.41 86.60 ±6.41 1.1x Christine 71.05 ± 1.17 70.15 ±1.85 1.2x Cnae-9 94.10 ± 0.31 94.44 ±0.11 1.0x Connect-4 62.28 ± 6.87 65.69 ±0.04 1.2x Covertype 59.76 ± 3.24 61.64 ±1.64 1.2x Credit-g 70.30 ± 0.84 70.79 ±0.68 1.1x Dionis 64.58 ± 9.89 64.58 ±9.89 1.1x Fabert 56.11 ±10.89 53.47 ±9.75 1.1x Helena 19.16 ± 3.20 19.16 ±3.20 1.1x Higgs 66.48 ± 3.16 66.48 ±3.16 1.1x Jannis 58.92 ± 2.38 59.63 ±2.81 1.4x Jasmine 75.85 ± 0.36 75.55 ±0.68 1.0x Junglechess2pcsrawendgamecomplete 72.86 ± 4.69 74.94 ±7.84 1.3x Kc1 80.32 ± 4.37 80.86 ±3.37 1.2x Kr-vs-kp 92.50 ± 3.93 90.95 ±4.70 1.0x Mfeat-factors 98.21 ± 0.15 98.15 ±0.15 1.1x Nomao 94.12 ± 0.60 94.25 ±0.64 1.1x Numerai28.6 52.03 ± 0.55 52.30 ±0.24 1.3x Phoneme 76.65 ± 2.65 75.42 ±2.87 1.1x Segment 83.15 ± 2.54 83.15 ±2.54 1.0x Sylvine 90.57 ± 1.87 90.89 ±2.04 1.0x Vehicle 71.76 ± 2.57 71.76 ±2.57 1.1x V olkert 50.72 ± 1.91 50.72 ±1.91 1.1x 17Published as a conference paper at ICLR 2023 E I NVESTIGATION WITH VARIABLE MAXIMUM RESOURCES We analyse the impact of variable maximum resources (number of epochs) on how large speedup PASHA provides over ASHA. More speciﬁcally, we change the maximum resources available for ASHA and also the upper boundary on maximum resources for PASHA. We utilize NASBench201 benchmark for these experiments and set the number of epochs to 200 (default) or 50 (other details are the same as earlier). The results in Table 14 conﬁrm that PASHA leads to larger speedups when there are more epochs (and rung levels) available. This analysis also explains the modest speedups on LCBench analysed earlier. If the model is trained for a small number of epochs, it is worth redesigning the HPO so that there are more rung levels available, enabling PASHA to give larger speedups. This can be achieved by using sub-epoch resource levels – specifying the rung levels and the minimum resources in terms of the number of iterations (neural network weights updates). Based on the results observed across various benchmarks, we would recommend having at least 5 rung levels in ASHA, with more rung levels leading to larger speedups from PASHA over ASHA. Table 14: NASBench201 results. PASHA leads to larger speedups if the models are trained with more epochs. Dataset Number of epochs Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 200 ASHA 93.85 ±0.25 3.0h±0.6h 1.0x 200.0 ±0.0 PASHA 93.57±0.75 1.3h±0.6h 2.3x 36.1 ±50.0 50 ASHA 93.78 ±0.39 1.8h±0.2h 1.0x 50.0 ±0.0 PASHA 93.58±0.75 1.2h±0.4h 1.5x 22.0 ±16.8 CIFAR-100 200 ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 50 ASHA 72.24 ±0.87 1.8h±0.3h 1.0x 50.0 ±0.0 PASHA 71.91±1.32 0.9h±0.3h 2.0x 10.5 ±12.1 ImageNet16-120 200 ASHA 45.63 ±0.81 8.8h±2.2h 1.0x 200.0 ±0.0 PASHA 45.13±1.51 2.9h±1.7h 3.1x 21.3 ±48.1 50 ASHA 45.97 ±0.99 5.2h±0.7h 1.0x 50.0 ±0.0 PASHA 45.09±1.52 2.7h±1.0h 1.9x 11.3 ±11.7 F A NALYSIS OF LEARNING CURVES We analyse the NASBench201 learning curves in Figure 3 and 4. To make the analysis realistic and easier to grasp, we ﬁrst sample a random subset of 256 conﬁgurations, similarly as we do for our NAS experiments. Figure 3 shows the learning curves of the top three conﬁgurations (selected in terms of their ﬁnal performance). We see that these learning curves are very close to each other and frequently cross due to noise in the training, allowing us to estimate a meaningful value of ϵ parameter (conﬁgurations that repeatedly swap their order are very likely to be similarly good, so we can select any of them because the goal is to ﬁnd a strong conﬁguration quickly rather than the very best one). Figure 4 shows all learning curves from the same random sample of 256 conﬁgurations. In this case we can see that the learning curves are relatively well-behaved (especially the ones at the top), and any exceptions are rare. 18Published as a conference paper at ICLR 2023 0 50 100 150 200 Epoch 40 50 60 70 80 90Validation accuracy (%) CIFAR-10 0 50 100 150 200 Epoch 20 40 60Validation accuracy (%) CIFAR-100 0 50 100 150 200 Epoch 10 20 30 40Validation accuracy (%) ImageNet16-120 Figure 3: Analysis of how the learning curves of the top three conﬁgurations (in terms of ﬁnal validation accuracy; from a random sample of 256 conﬁgurations) evolve across epochs. We see that such similar conﬁgurations frequently change their ranks, enabling us to calculate a meaningful value of ϵparameter. 0 50 100 150 200 Epoch 20 40 60 80Validation accuracy (%) CIFAR-10 0 50 100 150 200 Epoch 0 20 40 60Validation accuracy (%) CIFAR-100 0 50 100 150 200 Epoch 0 10 20 30 40Validation accuracy (%) ImageNet16-120 Figure 4: Analysis of what the learning curves look like for a random sample of 256 conﬁgurations. We see that the learning curves are relatively well-behaved (especially the ones at the top), and any exceptions are rare. G I NVESTIGATION OF HOW VALUE ϵEVOLVES We analyse how the value of ϵthat is used for calculating soft ranking develops during the HPO process. We show the results in Figure 5 for the three different datasets available in NASBench201 (taking one seed). The results show the obtained values of ϵare relatively small. 0 500 1000 1500 2000 Number of updates received 0.00 0.05 0.10 0.15 0.20 0.25 0.30Value of  CIFAR-10 0 200 400 600 Number of updates received 0.00 0.05 0.10 0.15Value of  CIFAR-100 0 200 400 600 800 Number of updates received 0.00 0.02 0.04 0.06Value of  ImageNet16-120 Figure 5: Analysis of how the value of ϵevolves as we receive additional updates about the perfor- mances of candidate conﬁgurations. Note that most of the updates are obtained in the top rung due to how multi-ﬁdelity methods work. 19Published as a conference paper at ICLR 2023 H I NVESTIGATION OF PERCENTILE VALUE N We investigate the impact of using various percentile valuesN used for estimating the value of ϵin Table 15. The intuition is that we want to take some value on the top end rather than the maximum distance in case there are some outliers. We see that the results are relatively stable, even though larger value of N can lead to further speedups. However, from the point of view of a practitioner we would still take N = 90in case there are any outliers in the speciﬁc new use-case. Table 15: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Investigation of various percentile values ( N) to use for calculating parameter ϵ. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h ±0.6h 1.0x 200.0 ±0.0 PASHAN= 100%93.70±0.61 1.0h ±0.4h 3.0x 13.8 ±19.5 PASHAN= 95% 93.64±0.59 1.0h ±0.4h 2.8x 15.4 ±19.5 PASHAN= 90% 93.57±0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 PASHAN= 80% 93.86±0.53 1.5h ±0.6h 1.9x 60.9 ±60.7 One-epoch baseline 93.30±0.61 0.3h ±0.0h 8.5x 1.0 ±0.0 Random baseline 72.88±19.20 0.0h ±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h ±0.9h 1.0x 200.0 ±0.0 PASHAN= 100%71.84±1.41 0.8h ±0.1h 3.9x 6.6 ±2.9 PASHAN= 95% 71.84±1.41 0.8h ±0.1h 3.9x 6.6 ±2.9 PASHAN= 90% 71.91±1.32 0.9h ±0.3h 3.5x 12.6 ±19.2 PASHAN= 80% 71.78±1.31 1.2h ±0.6h 2.6x 56.0 ±76.2 One-epoch baseline 65.57±5.53 0.3h ±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83±18.20 0.0h ±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h ±2.2h 1.0x 200.0 ±0.0 PASHAN= 100%45.09±1.61 2.3h ±0.4h 3.7x 7.0 ±2.8 PASHAN= 95% 45.26±1.58 2.4h ±0.4h 3.7x 7.4 ±2.7 PASHAN= 90% 45.13±1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 PASHAN= 80% 45.36±1.38 3.6h ±1.2h 2.5x 40.5 ±47.7 One-epoch baseline 41.42±4.98 1.0h ±0.0h 8.8x 1.0 ±0.0 Random baseline 20.75±9.97 0.0h ±0.0h N/A 0.0 ±0.0 20
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper addresses the high computational cost of Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS), especially with large datasets, even when using multi-fidelity methods. It proposes PASHA (Progressive ASHA), an extension of ASHA that dynamically allocates maximum resources for the tuning procedure based on the need. PASHA significantly reduces computational resources and tuning time compared to ASHA while maintaining similar predictive performance. Key contributions include: 1) Introducing PASHA for dynamic resource allocation in HPO/NAS, 2) Demonstrating significant speedups without sacrificing performance through empirical evaluation, and 3) Showing PASHA's compatibility with sample-efficient strategies like Bayesian Optimization.",
    "methodology": "PASHA extends ASHA and is inspired by the 'doubling trick' concept. It starts with a small initial amount of resources and progressively increases them only if the ranking of configurations in the top two rungs (rounds of promotion) has not stabilized. This allows for early stopping when the relative performance of promising configurations becomes consistent. To handle noise in the training process, PASHA employs a 'soft ranking' approach where configurations are considered equivalent if their performance difference is below a threshold \\u03b5. This \\u03b5 value is automatically estimated by identifying pairs of configurations that repeatedly swap ranks across different resource levels (epochs/iterations), calculating it as the N-th percentile (default 90th) of the performance differences among these 'criss-crossing' configurations. The algorithm maintains a maximum resource 'safety net' to prevent indefinite resource increases.",
    "experimental_setup": "The method was empirically evaluated on two main sets of experiments. For Neural Architecture Search (NAS), it used the NASBench201 benchmark across three datasets: CIFAR-10, CIFAR-100, and ImageNet16-120. For Hyperparameter Optimization (HPO), it used two large-scale tasks from the PD1 benchmark: WMT15 German-English (with xformer) and ImageNet (with ResNet50). All experiments involved two phases: 1) running the optimizer until 256 candidate configurations were evaluated, and 2) retraining the best identified configuration from scratch (using only the training set for experiments). Performance was measured by runtime (HPO phase only) and accuracy after retraining. Default parameters included a minimum resource 'r' of 1 epoch, a maximum resource 'R' (for ASHA or as a safety net for PASHA) of 200 epochs for NASBench201 and 1414/251 epochs for PD1 tasks, and a reduction factor \\u03b7=3. Experiments were averaged over multiple random seeds (e.g., 5 scheduler seeds, 3 NASBench201 seeds, totaling 15 repetitions). Baselines included ASHA, 'one-epoch', 'two-epoch', 'three-epoch', 'five-epoch' and 'random' baselines, and a combination with Bayesian Optimization (MOBSTER). Various alternative ranking functions were also tested, including direct ranking, soft ranking with fixed or heuristic-estimated \\u03b5, Rank Biased Overlap (RBO), and Reciprocal Rank Regret (RRR).",
    "limitations": "PASHA's benefits in speedup are limited when the number of available rungs (resource levels) is small. This is observed in benchmarks like LCBench, which has a maximum of 50 epochs and a minimum of 1 epoch, offering few opportunities for early stopping. Public benchmarks often fix resource granularities (e.g., in epochs), which can constrain PASHA's ability to demonstrate large speedups. The authors recommend having a maximum amount of resources at least 100 times larger than the minimum amount (for \\u03b7=3) to leverage PASHA effectively. If the number of epochs is small, achieving this might require defining resources with higher granularity, such as in terms of gradient updates or iterations, rather than epochs.",
    "future_research_directions": "Future work could involve investigating the optimal definition of rungs and resource levels, not just for PASHA but for multi-fidelity algorithms in general, to understand their impact on algorithmic decisions. Another promising direction is to test combinations of PASHA with transfer-learning techniques specifically designed for multi-fidelity problems, such as RUSH (Zappella et al., 2021), with the goal of further decreasing the overall tuning time."
}
