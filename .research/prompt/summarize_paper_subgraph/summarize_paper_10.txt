
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
arXiv:2007.03117v4  [cs.LG]  10 Dec 2020 Multi-Fidelity Bayesian Optimization via Deep Neural Networks Shibo Li School of Computing University of Utah Salt Lake City, UT 84112 shibo@cs.utah.edu W ei Xing Scientiﬁc Computing and Imaging Institute University of Utah Salt Lake City, UT 84112 wxing@sci.utah.edu Robert M. Kirby School of Computing University of Utah Salt Lake City, UT 84112 kirby@cs.utah.edu Shandian Zhe School of Computing University of Utah Salt Lake City, UT 84112 zhe@cs.utah.edu Abstract Bayesian optimization (BO) is a popular framework for optim izing black-box functions. In many applications, the objective function ca n be evaluated at mul- tiple ﬁdelities to enable a trade-off between the cost and ac curacy. T o reduce the optimization cost, many multi-ﬁdelity BO methods have b een proposed. De- spite their success, these methods either ignore or over-si mplify the strong, com- plex correlations across the ﬁdelities. While the acquisit ion function is therefore easy and convenient to calculate, these methods can be inefﬁ cient in estimating the objective function. T o address this issue, we propose De ep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can ﬂexibly capture all kinds of complicated relationships between the ﬁdelities t o improve the objective function estimation and hence the optimization performanc e. W e use sequential, ﬁdelity-wise Gauss-Hermite quadrature and moment-matchi ng to compute a mu- tual information based acquisition function in a tractable and highly efﬁcient way. W e show the advantages of our method in both synthetic benchm ark datasets and real-world applications in engineering design. 1 Introduction Bayesian optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a general and powerful ap- proach for optimizing black-box functions. It uses a probab ilistic surrogate model (typically Gaus- sian process (GP) (Rasmussen and Williams, 2006)) to estima te the objective function. By repeat- edly maximizing an acquisition function computed with the i nformation of the surrogate model, BO ﬁnds and queries at new input locations that are closer and cl oser to the optimum; meanwhile the new training examples are incorporated into the surrogate m odel to improve the objective estimation. In practice, many applications allow us to query the objecti ve function at different ﬁdelities, where low ﬁdelity queries are cheap yet inaccurate, and high ﬁdeli ty queries more accurate but costly. For example, in physical simulation (Peherstorfer et al., 2018 ), the computation of an objective ( e.g., the elasticity of a part or energy of a system) often involves solving partial differential equations. Running a numerical solver with coarse meshes gives a quick y et rough result; using dense meshes substantially improves the accuracy but dramatically incr eases the computational cost. The multi- ﬁdelity queries enable us to choose a trade-off between the c ost and accuracy. 34th Conference on Neural Information Processing Systems ( NeurIPS 2020), V ancouver, Canada.Accordingly, to reduce the optimization cost, many multi-ﬁ delity BO methods (Huang et al., 2006; Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; T akeno et al., 2019) have been pro- posed to jointly select the input locations and ﬁdelities to best balance the optimization progress and query cost, i.e., the beneﬁt-cost ratio. Despite their success, these method s often ignore the strong, complex correlations between the function outputs at diffe rent ﬁdelities, and learn an independent GP for each ﬁdelity (Lam et al., 2015; Kandasamy et al., 2016) . Recent works use multi-output GPs to capture the ﬁdelity correlations. However, to avoid intr actable computation of the acquisition function, they have to impose simpliﬁed correlation struct ures. For example, T akeno et al. (2019) assume a linear correlation between the ﬁdelities; Zhang et al. (2017) use kernel convolution to con- struct the cross-covariance function, and have to choose si mple, smooth kernels ( e.g., Gaussian) to ensure a tractable convolution. Therefore, the existing me thods can be inefﬁcient and inaccurate in estimating the objective function, which further lowers th e optimization efﬁciency and increases the cost. T o address these issues, we propose DNN-MFBO, a deep neural n etwork based multi-ﬁdelity Bayesian optimization that is ﬂexible enough to capture all kinds of complex (possibly highly nonlin- ear and nonstationary) relationships between the ﬁdelitie s, and exploit these relationships to jointly estimate the objective function in all the ﬁdelities to impr ove the optimization performance. Speciﬁ- cally, we stack a set of neural networks (NNs) where each NN mo dels one ﬁdelity. In each ﬁdelity, we feed both the original input (to the objective) and output from the previous ﬁdelity into the NN to propagate information throughout and to estimate the com plex relationships across the ﬁdelities. Then, the most challenging part is the calculation of the acq uisition function. For efﬁcient inference and tractable computation, we consider the NN weights in the output layer as random variables and all the other weights as hyper-parameters. W e develop a stoc hastic variational learning algorithm to jointly estimate the posterior of the random weights and h yper-parameters. Next, we sequen- tially perform Gauss-Hermite quadrature and moment matchi ng to approximate the posterior and conditional posterior of the output in each ﬁdelity, based o n which we calculate and optimize an information based acquisition function, which is not only c omputationally tractable and efﬁcient, but also conducts maximum entropy search (W ang and Jegelka, 2017), the state-of-the-art criterion in BO. For evaluation, we examined DNN-MFBO in three benchmark functions and two real-world applica- tions in engineering design that requires physical simulat ions. The results consistently demonstrate that DNN-MFBO can optimize the objective function (in the hi ghest ﬁdelity) more effectively, mean- while with smaller query cost, as compared with state-of-th e-art multi-ﬁdelity and single ﬁdelity BO algorithms. 2 Background Bayesian optimization. T o optimize a black-box objective function f : X → R, BO learns a probabilistic surrogate model to predict the function valu es across the input domain X and quantiﬁes the uncertainty of the predictions. This information is use d to calculate an acquisition function that measures the utility of querying at different input locatio ns, which usually encodes a exploration- exploitation trade-off. By maximizing the acquisition fun ction, BO ﬁnds new input locations at which to query, which are supposed to be closer to the optimum ; meanwhile the new examples are added into the training set to improve the accuracy of the surrogate model. The most commonly used surrogate model is Gaussian process (GP) (Rasmussen an d Williams, 2006). Given the training inputs X = [ x1, . . . , xN ]⊤ and (noisy) outputs y = [ y1, . . . , y N ]⊤, GP assumes the outputs follow a multivariate Gaussian distribution, p(y|X) = N (y|m, K + σ2I) where m are the values of the mean function at the inputs X, K is a kernel matrix on X, [K]ij = k(xi, xj ) (k(·, ·) is the kernel function), and σ2 is the noise variance. The mean function is usually set to the constant function 0 and so m = 0. Due to the multi-variate Gaussian form, given a new input x∗, the posterior distribution of the function output, p ( f(x∗)|x∗, X, y ) is a closed-form conditional Gaussian, and hence is convenient to quantify the uncertainty and calcula te the acquisition function. There are a variety of commonly used acquisition functions, such as expected improvement (EI) (Jones et al., 1998), upper conﬁdent bound (UCB) (Srini vas et al., 2010), entropy search (ES) (Hennig and Schuler, 2012), and predictive entropy sea rch (PES) (Hernández-Lobato et al., 2014). A particularly successful recent addition is the max -value entropy search (MES) (W ang and Jegelka, 2017), which not only enjoys a global util ity measure (like ES and PES), but also is computationally efﬁcient (because it calculates th e entropy of the function output rather than input like in ES/PES). Speciﬁcally, MES maximizes the mutua l information between the function 2value and its maximum f∗ to ﬁnd the next input at which to query, a(x) = I ( f(x), f ∗|D ) = H ( f(x)|D ) − Ep(f∗|D)[H ( f(x)|f∗, D ) ], (1) where I(·, ·) is the mutual information, H(·) the entropy, and D the training examples collected so far. Note that the function values and extremes are consider ed as generated from the posterior in the surrogate model, which includes all the knowledge we have fo r the black-box objective function. Multi-ﬁdelity Bayesian optimization . Many applications allow multi-ﬁdelity queries of the obje c- tive function, {f1(x), . . . , f M (x)}, where the higher (larger) the ﬁdelity m, the more accurate yet costly the query of fm(·). Many studies have extended BO for multi-ﬁdelity settings. For exam- ple, MF-GP-UCB (Kandasamy et al., 2016) starts from the lowe st ﬁdelity ( m = 1 ), and queries the objective at each ﬁdelity until the conﬁdence band excee ds a particular threshold. Despite its effectiveness and theoretical guarantees, MF-GP-UCB lear ns an independent GP surrogate for each ﬁdelity and ignores the strong correlations between the ﬁde lities. Recent works use a multi-output GP to model the ﬁdelity correlations. For example, MF-PES (Z hang et al., 2017) introduces a shared latent function, and uses kernel convolution to derive the c ross-covariance between the ﬁdelities. The most recent work, MF-MES (T akeno et al., 2019) introduces C kernel functions {κc(·, ·)} and, for each ﬁdelity m, C latent features {ωcm}. The covariance function is deﬁned as k ( fm(x), fm′ (x′) ) = ∑ C c=1 (ωcmωcm′ + τcmδmm′ )κc(x, x′), (2) where τcm > 0, δmm′ = 1 if and only if m = m′, and each kernel κc(·, ·) is usually assumed to be stationary, e.g., Gaussian kernel. 3 Multi-Fidelity Modeling with Deep Neural Networks Despite the success of existing multi-ﬁdelity BO methods, they either overlook the strong, complex correlations between different ﬁdelities ( e.g., MF-GP-UCB) or model these correlations with an over-simpliﬁed structure. For example, the convolved GP in MF-PES has to employ simple/smooth kernels (typically Gaussian) for both the latent function a nd convolution operation to obtain an ana- lytical cross-covariance function, which has limited expr essiveness. MF-MES essentially adopts a linear correlation assumption between the ﬁdelities. Acc ording to (2), if we choose each κc as a Gaussian kernel (with amplitude one), we have k ( fm(x), fm′ (x) ) = ω⊤ mωm′ + δmm′ τm where ωm = [ ω1m, . . . , ω Cm ]⊤ and τm = ∑ C c=1 τcm. These correlation structures might be over-simpliﬁed and insufﬁcient to estimate the complicate d relationships between the ﬁdelities ( e.g., highly nonlinear and nonstationary). Hence, they can limit the accuracy of the surrogate model and lower the optimization efﬁciency while increasing the quer y cost. T o address this issue, we use deep neural networks to build a m ulti-ﬁdelity model that is ﬂexible enough to capture all kinds of complicated relationships be tween the ﬁdelities, taking advantage of the relationships to promote the accuracy of the surrogat e model. Speciﬁcally, for each ﬁdelity m > 1, we introduce a neural network (NN) parameterized by {wm, θm}, where wm are the weights in the output layer and θm the weights in all the other layers. Denote the NN input by xm, the output by fm(x) and the noisy observation by ym(x). The model is deﬁned as xm = [ x; fm−1(x)], f m(x) = w⊤ mφθ m (xm), y m(x) = fm(x) + ǫm, (3) where x is the original input to the objective function, φθ m (xm) is the output vector of the second last layer (hence parameterized by θm) which can be viewed as a set of nonlinear basis functions, an d ǫm ∼ N (ǫm|0, σ2 m) is a Gaussian noise. The input xm is obtained by appending the output from the previous ﬁdelity to the original input. Through a series of l inear and nonlinear transformations inside the NN, we obtain the output fm(x). In this way, we digest the information from the lower ﬁdelit ies, and capture the complex relationships between the current a nd previous ﬁdelities by learning a nonlinear mapping fm(x) = h(x, fm−1(x)), where h(·) is fulﬁlled by the NN. When m = 1 , we set xm = x. A graphical representation of our model is given in Fig. 1 of the supplementary material. W e assign a standard normal prior over each wm. Following (Snoek et al., 2015), we con- sider all the remaining NN parameters as hyper-parameters. Given the training set D = {{(xnm, ynm)}Nm n=1}M m=1, the joint probability of our model is p(W, Y|X , Θ , s) = ∏ M m=1 N (wm|0, I) ∏ Nm n=1 N ( ynm|fm(xnm), σ2 m ) , (4) 3where W = {wm}, Θ = {θm}, s = [ σ2 1, . . . , σ 2 M ]⊤, and X , Y are the inputs and outputs in D. In order to obtain the posterior distribution of our model (w hich is in turn used to compute the ac- quisition function), we develop a stochastic variational l earning algorithm. Speciﬁcally, for each wm, we introduce a multivariate Gaussian posterior, q(wm) = N (wm|µm, Σ m). W e further pa- rameterize Σ m with its Cholesky decomposition to ensure the positive deﬁn iteness, Σ m = LmL⊤ m where Lm is a lower triangular matrix. W e assume q(W) = ∏ M m=1 q(wm), and construct a varia- tional model evidence lower bound (ELBO), L ( q(W), Θ , s ) = Eq[log(p(W, Y|X , Θ , s)/q(W))]. W e then maximize the ELBO to jointly estimate the variationa l posterior q(W) and all the other hyper-parameters. The ELBO is analytically intracta ble, and we use the reparameterization trick (Kingma and W elling, 2013) to conduct efﬁcient stocha stic optimization. The details are given in the supplementary material (Sec. 3). 4 Multi-Fidelity Optimization with Max-V alue Entropy Search W e now consider an acquisition function to select both the ﬁd elities and input locations at which we query during optimization. Following (T akeno et al., 2019) , we deﬁne the acquisition function as a(x, m) = 1 λm I (f∗, fm(x)|D) = 1 λm ( H ( fm(x)|D ) − Ep(f∗|D) [ H ( fm(x)|f∗, D )]) (5) where λm > 0 is the cost of querying with ﬁdelity m. In each step, we maximize the acquisition function to ﬁnd a pair of input location and ﬁdelity that prov ides the largest beneﬁt-cost ratio. However, given the model inference result, i.e., p(W|D) ≈ q(W), a critical challenge is to compute the posterior distribution of the output in each ﬁdelity, p(fm(x)|D), and use them to compute the acquisition function. Due to the nonlinear coupling of the o utputs in different ﬁdelities (see (3)), the computation is analytically intractable. T o address th is issue, we conduct ﬁdelity-wise moment matching and Gauss-Hermite quadrature to approximate each p(fm(x)|D) as a Gaussian distribu- tion. 4.1 Computing Output Posteriors Speciﬁcally, we ﬁrst assume that we have obtained the posterior of the output for ﬁdelity m − 1, p ( fm−1(x)|D ) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . For convenience, we slightly abuse the notation and use fm−1 and fm to denote fm−1(x) and fm(x), respectively. Now we consider calculat- ing p(fm|D). According to (3), we have fm = w⊤ mφθ m ([x; fm−1]). Based on our variational posterior q(wm) = N (wm|µm, LmL⊤ m), we can immediately derive the conditional posterior p(fm|fm−1, D) = N ( fm|u(fm−1, x), γ(fm−1, x) ) where u(fm−1, x) = µ⊤ mφθ m ([x; fm−1]) and γ(fm−1, x) = ∥L⊤ mφθ m ([x; fm−1])∥2. Here ∥ · ∥ 2 is the square norm. W e can thereby read out the ﬁrst and second conditional moments, E[fm|fm−1, D] = u(fm−1, x), E[f2 m|fm−1, D] = γ(fm−1, x) + u(fm−1, x)2. (6) T o obtain the moments, we need to take the expectation of the c onditional moments w .r.t p(fm−1|D) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . While the conditional moments are nonlinear to fm−1 and their expectation is not analytical, we can use Gauss-He rmite quadrature to give an accu- rate, closed-form approximation, E[fm|D] = Ep(fm−1|D)E[fm|fm−1, D] ≈ ∑ k gk · u(tk, x), E[f2 m|D] = Ep(fm−1|D)E[f2 m|fm−1, D] ≈ ∑ k gk · [γ(tk, x) + u(tk, x)2], (7) where {gk} and {tk} are quadrature weights and nodes, respectively. Note that e ach node tk is determined by αm−1(x) and ηm−1(x). W e then use these moments to construct a Gaus- sian posterior approximation, p(fm|D) ≈ N ( fm|αm(x), ηm(x) ) where αm(x) = E[fm|D] and ηm(x) = E[f2 m|D] − E[fm|D]2. This is called moment matching, which is widely used and ver y successful in approximate Bayesian inference, such as expe ctation-propagation (Minka, 2001). One may concern if the quadrature will give a positive variance. This is guaranteed by the follow lemma. Lemma 4.1. As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7), is positive. 4The proof is given in the supplementary material. Following the same procedure, we can compute the posterior of the output in ﬁdelity m + 1. Note that when m = 1 , we do not need quadrature because the input of the NN is the same as the original input, not inclu ding other NN outputs. Hence, we can derive the Gaussian posterior outright from q(w1) — p(f1(x)|D) = N ( f1(x)|α1(x), η1(x) ) , where α1(x) = µ⊤ 1φθ 1 (x) and η1(x) = ∥L⊤ 1φθ 1 (x)∥2. 4.2 Computing Acquisition Function Given the posterior of the NN output in each ﬁdelity,p ( fm(x)|D) ≈ N (fm(x)|αm(x), ηm(x) ) (1 ≤ m ≤ M), we consider how to compute the acquisition function (5). Du e to the Gaussian posterior, the ﬁrst entropy term is straightforward, H ( fm(x)|D ) = 1 2 log ( 2πeηm(x) ) . The second term — a conditional entropy, however, is intractable. Hence, we f ollow (W ang and Jegelka, 2017) to use a Monte-Carlo approximation, Ep(f∗|D)[H ( fm(x)|f∗, D ) ] ≈ 1 |F| ∑ f∗∈F∗ H ( fm(x)|f∗, D ) , where F∗ are a collection of independent samples of the function maxi mums based on the posterior distribution of our model. T o obtain a sample of the function maximum, we ﬁrst generate a posterior sample for each wm, according to q(wm) = N (wm|µm, LmL⊤ m). W e replace each wm by their sample in calculating fM (x) so as to obtain a posterior sample of the objective function. W e then maximize this sample function to obtain one instance of f∗. W e use L-BFGS (Liu and Nocedal, 1989) for optimization. Given f∗, the computation of H ( fm(x)|f∗, D ) = H ( fm(x)| max fM (x) = f∗, D ) is still in- tractable. W e then follow (W ang and Jegelka, 2017) to calcul ate H ( fm(x)|fM (x) ≤ f∗, D ) instead as a reasonable approximation. For m = M, the entropy is based on a truncated Gaussian distribu- tion, p(fM (x)|fM (x) ≤ f∗, D) ∝ N ( fM (x)|αM (x), ηM (x) ) 1(fM (x) ≤ f∗) where 1(·) is the indicator function, and is given by H ( fm(x)|fM (x) ≤ f∗, D ) = log ( √ 2πeηM (x)Φ( β) ) − β · N (β|0, 1)/ ( 2Φ( β) ) , (8) where Φ( ·) is the cumulative density function (CDF) of the standard nor mal distribution, and β =( f∗ − αM (x) ) / √ ηM (x). When m < M , the entropy is based on the conditional distribution p(fm(x)|fM (x) ≤ f∗, D) = 1 Z · p ( fm(x)|D ) p(fM (x) ≤ f∗|fm(x), D) ≈ 1 Z · N ( fm(x)|αm(x), ηm(x) ) p(fM (x) ≤ f∗|fm(x), D). (9) where Z is the normalizer. T o obtain p(fM (x) ≤ f∗|fm(x), D), we ﬁrst consider how to compute p(fM (x)|fm(x), D). According to (3), it is trivial to derive that p(fm+1(x)|fm(x), D) = N ( fm+1|ˆαm+1(x, fm), ˆηm+1(x, fm) ) , where ˆαm+1(x, fm) = µ⊤ m+1φθ m+1 ([x; fm]) and ˆηm+1(x, fm) = ∥L⊤ m+1φθ m+1 ([x; fm])∥2. Note that we again use fm+1 and fm to denote fm+1(x) and fm(x) for convenience. Next, we follow the same method as in Section 4.1 to sequentially obtain the c onditional posterior for each higher ﬁdelity, p(fm+k|fm, D)(1 < k ≤ M − m). In more detail, we ﬁrst base on q(wm+k) to derive the conditional moments E(fm+k|fm+k−1, fm, D) and E(f2 m+k|fm+k−1, fm, D). They are calculated in the same way as in (6), because fm+k are independent to fm conditioned on fm+k−1. Then we take the expectation of the conditional moments w .r.t p(fm+k−1|fm, D) (that is Gaussian) to obtain E(fm+k|fm, D) and E(f2 m+k|fm, D). This again can be done by Gauss-Hermite quadrature. Finally, we use these moments to construct a Gaussian approx imation to the conditional posterior, p(fm+k|fm, D) ≈ N ( fm+k|ˆαm+k(x, fm), ˆηm+k(x, fm) ) , (10) where ˆαm+k(x, fm) = E(fm+k|fm, D) and ˆηm+k(x, fm) = E(f2 m+k|fm, D) − E(fm+k|fm, D)2. According to Lemma 4.1, we guarantee ˆηm+k(x, fm) > 0. Now we can obtain p(fm(x)|fM (x) ≤ f∗, D) ≈ 1 Z · N ( fm|αm(x), ηm(x) ) Φ ( f∗ − ˆαM (x, fm)√ ˆηM (x, fm) ) . (11) 5In order to compute the entropy analytically, we use moment m atching again to approximate this distribution as a Gaussian distribution. T o this end, w e use Gauss-Hermite quadrature to compute three integrals, Z = ∫ R(fm) · N ( fm|αm(x), ηm(x) ) dfm, Z1 = ∫ fmR(fm) · N ( fm|αm(x), ηm(x) ) dfm, and Z2 = ∫ f2 mR(fm) · N ( fm|αm(x), ηm(x) ) dfm, where R(fm) = Φ ( (f∗ − ˆαM (x, fm))/ √ ˆηM (x, fm) ) . Then we can obtain E[fm|fM ≤ f∗, D] = Z1/Z and E[f2 m|fM ≤ f∗, D] = Z2/Z, based on which we approximate p(fm(x)|fM (x) ≤ f∗, D) ≈ N ( fm|Z1/Z, Z2/Z − Z2 1 /Z2) . (12) Following the same idea to prove Lemma 4.1, we can show that th e variance is non-negative. See the details in the supplementary material (Sec. 5). With the Gaussian form, we can analytically compute the entropy, H(fm(x)|fM (x) ≤ f∗, D) = 1 2 log ( 2πe(Z2/Z − Z2 1 /Z2) ) . Although our calculation of the acquisition function is qui te complex, due to the analytical form, we can use automatic differentiation libraries (Baydin et al. , 2017), to compute the gradient efﬁciently and robustly for optimization. In our experiments, we used T ensorFlow (Abadi et al., 2016) and L-BFGS to maximize the acquisition function to ﬁnd the ﬁdeli ty and input location we query at in the next step. Our multi-ﬁdelity Bayesian optimization alg orithm is summarized in Algorithm 1. Algorithm 1 DNN-MFBO ( D, M, T , {λm}M m=1 ) 1: Learn the DNN-based multi-ﬁdelity model (4) on Dwith stochastic variational learning. 2: for t = 1, . . . , T do 3: Generate F∗ from the variational posterior q(W) and the NN output at ﬁdelity M, i.e., fM (x) 4: (xt, m t) = argmaxx∈X ,1≤ m≤ M MutualInfo(x, m, λ m, F∗ , D, M ) 5: D←D∪{ (xt, m t)} 6: Re-train the DNN-based multi-ﬁdelity model on D 7: end for Algorithm 2 MutualInfo(x, m, λm, F∗, D, M) 1: Compute each p(fm(x)|D) ≈N ( fm|α m(x), η m(x) ) (Sec. 4.1) 2: H0 ← 1 2 log(2πeη m(x)), H1 ← 0 3: for f∗ ∈F ∗ do 4: if m = M then 5: Use (8) to compute H(fm|fM ≤ f∗ , D) and add it to H1 6: else 7: Compute p(fm(x)|fM (x), D) following (10) and p(fm(x)|fM (x) ≤ f∗ , D) with (12) 8: H1 ← H1 + 1 2 log ( 2πe (Z2/Z − Z2 1 /Z 2) ) 9: end if 10: end for 11: return (H0 − H1/ |F∗ |)/λ m 5 Related W ork Most surrogate models used in Bayesian optimization (BO) (Mockus, 2012; Snoek et al., 2012) are based on Gaussian processes (GPs) (Rasmussen and Williams, 2006), partly because their closed- form posteriors (Gaussian) are convenient to quantify the u ncertainty and calculate the acquisition functions. However, GPs are known to be costly for training, and the exact inference takes O(N3) time complexity ( N is the number of samples). Recently, Snoek et al. (2015) show ed deep neural networks (NNs) can also be used in BO and performs very well. T he training of NNs are much more efﬁcient ( O(N)). T o conveniently quantify the uncertainty, Snoek et al. (2 015) consider the NN weights in the output layer as random variables and all the ot her weights as hyper-parameters (like the kernel parameters in GPs). They ﬁrst obtain a point estim ation of the hyper-parameters (typically through stochastic training). Then they ﬁx the hyper-param eters and compute the posterior distribu- tion of the random weights (in the last layer) and NN output — t his can be viewed as the inference for Bayesian linear regression. In our multi-ﬁdelity model , we also only consider the NN weights in the output layer of each ﬁdelity as random variables. Howe ver, we jointly estimate the hyper- parameters and posterior distribution of the random weight s. Since the NN outputs in successive ﬁdelities are coupled non-linearly, we use the variational estimation framework (W ainwright et al., 2008). 6Many multi-ﬁdelity BO algorithms have been proposed. For ex ample, Huang et al. (2006); Lam et al. (2015); Picheny et al. (2013) augmented the standa rd EI for the multi-ﬁdelity settings. Kandasamy et al. (2016, 2017) extended GP upper conﬁdence bo und (GP-UCB) (Srinivas et al., 2010). Poloczek et al. (2017); Wu and Frazier (2017) develop ed multi-ﬁdelity BO with knowledge gradients (Frazier et al., 2008). EI is a local measure of the utility and UCB requires us to explicitly tune the exploit-exploration trade-off. The recent works a lso extend the information-based acqui- sition functions to enjoy a global utility for multi-ﬁdelit y optimization, e.g., (Swersky et al., 2013; Klein et al., 2017) using entropy search (ES), (Zhang et al., 2017; McLeod et al., 2017) (PES) using predictive entropy search (PES), and (Song et al., 2019; T ak eno et al., 2019) using max-value en- tropy search (MES). Note that ES and PES are computationally more expensive than MES because the former calculate the entropy of the input (vector) and la tter the output scalar. Despite the great success of the existing methods, they either ignore or overs implify the complex correlations across the ﬁdelities, and hence might hurt the accuracy of the surro gate model and further the optimiza- tion performance. For example, Picheny et al. (2013); Lam et al. (2015); Kandasamy et al. (2016); Poloczek et al. (2017) train an independent GP for each ﬁdeli ty; Song et al. (2019) combined all the examples indiscriminately to train a single GP; Huang et al. (2006); T akeno et al. (2019) assume a linear correlation structure between ﬁdelities, and Zhang et al. (2017) used the convolution opera- tion to construct the covariance and so the involved kernels have to be simple and smooth enough (yet less expressive) to obtain an analytical form. T o overc ome these limitations, we propose an NN-based multi-ﬁdelity model, which is ﬂexible enough to ca pture arbitrarily complex relation- ships between the ﬁdelities and to promote the performance o f the surrogate model. Recently, a NN-based multi-task model (Perrone et al., 2018) was also de veloped for BO and hyper-parameter transfer learning. The model uses an NN to construct a shared feature map ( i.e., bases) across the tasks, and generates the output of each task by a linear combi nation of the latent features. While this model can also be used for multi-ﬁdelity BO (each task co rresponds to one ﬁdelity), it views each ﬁdelity as symmetric and does not reﬂect the monotonici ty of function accuracy/importance along with the ﬁdelities. More important, the model does not capture the correlation between ﬁdeli- ties — given the shared bases, different ﬁdelities are assum ed to be independent. Finally, while a few algorithms deal with continuous ﬁdelities, e.g., (Kandasamy et al., 2017; McLeod et al., 2017; Wu and Frazier, 2017), we focus on discrete ﬁdelities in this work. 6 Experiment 6.1 Synthetic Benchmarks W e ﬁrst evaluated DNN-MFBO in three popular synthetic bench mark tasks. (1) Branin func- tion (Forrester et al., 2008; Perdikaris et al., 2017) with t hree ﬁdelities. The input is two dimensional and ranges from [−5, 10] × [0, 15]. (2) P ark1function (Park, 1991) with two ﬁdelities. The input is four dimensional and each dimension is in [0, 1]. (3) Levy function (Laguna and Martí, 2005), having three ﬁdelities and two dimensional inputs. The doma in is [−10, 10] × [−10, 10]. For each objective function, between ﬁdelities can be nonlinear and /or nonstationary transformations. The detailed deﬁnitions are given in the supplementary materia l (Sec. 1). Competing Methods. W e compared with the following popular and state-of-the-ar t multi- ﬁdelity BO algorithms: (1) Multi-Fidelity Sequential Krig ing (MF-SKO) (Huang et al., 2006) that models the function of the current ﬁdelity as the functi on of the previous ﬁdelity plus a GP , (2) MF-GP-UCB (Kandasamy et al., 2016), (3) Multi- Fidelity Predictive Entropy Search (MF-PES) (Zhang et al., 2017) and (4) Multi-Fidelity Maximum Entropy Search (MF- MES) (T akeno et al., 2019). These algorithms extend the stan dard BO with EI, UCB, PES and MES principles respectively. W e also compared with (5) mult i-task NN based BO (MTNN-BO) by Perrone et al. (2018), where a set of latent bases (generat ed by an NN) are shared across the tasks, and the output of each task ( i.e., ﬁdelity) is predicted by a linear combination of the bases. W e tested the single ﬁdelity BO with MES, named as (6) SF-MES (W a ng and Jegelka, 2017). SF-MES only queries the objective at the highest ﬁdelity. Settings and Results.W e implemented our method and MTNN-BO with T ensorFlow . W e used the original Matlab implementatio n for MF-GP-UCB ( https://github.com/kirthevasank/mf-gp-ucb), MF-PES ( https://github.com/YehongZ/MixedTypeBO) and SF- MES ( https://github.com/zi-w/Max-value-Entropy-Search/ ), and 7500 1000 1500 2000 2500 Total Cos  10−1 100 101 102 103 Simple Regre  DNN -MFBO MF-MES MF-PES MF-SKO MF-GP-UCB SF-MES MTNN-BO (a) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−4 10−2 100 (b) P ark1 500 1000 1500 2000 2500 Total Cost 10 0 10 1 10 2 (c) Levy 100 200 300 400 500 Total Cost 170 180 190190 200 210 220Queried Maximum (d) V ibration Plate 500 1000 1500 2000 2500 Total Cost 10 −2 10 −1 10 0 10 1 10 2 10 3 Inference Regret (e) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−3 10−1 100 101 (f) P ark1 500 1000 1500 2000 2500 Total Cost 10−2 10−1 100 101 102 (g) Levy 100 200 300 400 500 Total cost 1.05 1.10 1.15 1.20 1.25 1.3 × 100 1.35 × 100 1.4 × 100 Queried Minimum (h) Thermal Conductor Figure 1: Simple and Inference regrets on three synthetic benchmark t asks (a-c, e-g) and the optimum queried function values (d, h) along with the query cost. Python/Numpy implementation for MF-MES. MF-SKO was implemented with Python as well. W e used the default settings in their implementations . SF-MES and MF-GP-UCB used the Squared Exponential (SE) kernel. MF-PES used the Automa tic Relevance Determination (ARD) kernel. MF-MES and MF-SKO used the Radial Basis (RBF) k ernel (within each ﬁdelity). For DNN-MFBO and MTNN-BO, we used ReLU activation. T o identi fy the architecture of the neural network in each ﬁdelity and learning rate, we ﬁrst ran the AutoML tool SMAC3 (https://github.com/automl/SMAC3) on the initial training dataset (we randomly split the data into half for training and the other half for test, an d repeated multiple times to obtain a cross-validation accuracy to guide the search) and then man ually tuned these hyper-parameters. The depth and width of each network were chosen from [2, 12] and [32, 512], and the learning rate [10−5, 10−1]. W e used ADAM (Kingma and Ba, 2014) for stochastic training. The number of epochs was set to 5, 000, which is enough for convergence. T o optimize the acquisiti on function, MF-MES and MF-PES ﬁrst run a global optimization algorithm D IRECT (Jones et al., 1993; Gablonsky et al., 2001) and then use the results as the initia lization to run L-BFGS. SF-MES uses a grid search ﬁrst and then runs L-BFGS. DNN-MFBO and MTNN-BO d irectly use L-BFGS with a random initialization. T o obtain the initial training poin ts, we randomly query in each ﬁdelity. For Branin and Levy, we generated 20, 20 and 2 training samples for the ﬁrst, second and third ﬁdelity, respectively. For P ark1, we generated 5 and 2 examples for the ﬁrst and second ﬁdelity. The query costs is (λ1, λ2, λ3) = (1 , 10, 100). W e examined the simple regret (SR) and inference regret (IR ). SR is deﬁned as the difference between the global optimum and the best queried function value so far: maxx∈X fM (x) − maxi∈{i|i∈[t],mi=M} fM (xi); IR is the difference between the global optimum and the optimum estimated by the surrogate model: maxx∈X fM (x) − maxx∈X ˆfM (x) where ˆfM (·) is the estimated objective. W e repeated the experiment for ﬁ ve times, and report on average how the simple and inference regrets vary along with the query cost in Fig. 1 (a-c, e-g). W e also show the standard error bars. As we can see, in all the t hree tasks, DNN-MFBO achieves the best regrets with much smaller or comparable querying co sts. The best regrets obtained by our method are much smaller (often orders of magnitude) than the baselines. In particular, DNN-MFBO almost achieved the global optimum after querying one point (IR < 10−6) (Fig. 1f). These results demonstrate our DNN based surrogate model is more ac curate in estimating the objective. Furthermore, our method spends less or comparable cost to ac hieve the best regrets, showing a much better beneﬁt/cost ratio. 6.2 Real-W orld Applications in Engineering Design Mechanical Plate Vibration Design.W e aim to optimize three material properties, Y oung’s modu- lus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]), to maximize the fourth vibration mode frequency of a 3-D simp ly supported, square, elastic plate, of size 10 × 10 × 1. T o evaluate the frequency, we need to run a numerical solver on the discretized 8DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (a) Branin DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200 250Time (seconds) (b) P ark1 DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (c) Levy DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (d) V ibration Plate DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (e) Heat Conductor Figure 2: The average query time on three synthetic tasks (a- c) and two real-world applications (d-e). plate. W e considered two ﬁdelities, one with a coarse mesh an d the other a dense mesh. The details about the settings of the solvers are provided the supplemen tary document. Thermal Conductor Design. Given the property of a particular thermal conductor, our go al is to optimize the shape of the central hole where we install/ﬁx th e conductor to make the heat conduction (from left to right) to be as as fast as possible. The shape of t he hole (an ellipse) is described by three parameters: x-radius, y-radius and angle. W e used the time to reach 70 degrees as the objective function value and we want to minimize the objective. W e need to run numerical solvers to calculate the objective. W e considered two ﬁdelities. The details are given in the supplementary material. For both problems, we randomly queried at 20 and 5 inputs in th e low and high ﬁdelities respectively, at the beginning. The query cost is (λ1, λ2) = (1 , 10). W e then ran each algorithm until convergence. W e repeated the experiments for ﬁve times. Since we do not kno w the ground-truth of the global optimum, we report how the average of the best function value s queried improves along with the cost. The results are shown in Fig. 1d and h. As we can see, in bo th applications, DNN-MFBO reaches the maximum/minimum function values with a smaller cost than all the competing methods, which is consistent with results in the synthetic benchmark tasks. Finally, we examined the average query time of each multi-ﬁd elity BO method, which is spent in calculating and optimizing the acquisition function to ﬁnd new inputs and ﬁdelities to query at in each step. For a fair comparison, we ran all the methods on a Li nux workstation with a 16-core Intel(R) Xeon(R) CPU E5-2670 and 16GB RAM. As shown in Fig. 2, DNN-MFBO spends much less time than MF-MES and MF-PES that are based on multi-outp ut GPs, and the speed of DNN- MFBO is close or comparable to MF-GP-UCB and MF-SKO, which us e independent and additive GPs for each ﬁdelity, respectively. On average, DNN-MFBO ac hieves 25x and 60x speedup over MF-MES and MF-PES. One reason might be that DNN-MFBO simply a dopts a random initializa- tion for L-BFGS rather than runs an expensive global optimiz ation (so does MTNN-BO). However, as we can see from Fig. 1, DNN-MFBO still obtains new input and ﬁdelities that achieve much better beneﬁt/cost ratio. On the other hand, the close speed to MF-GP-UCB and MF-SKO also demonstrate that our method is efﬁcient in acquisition func tion calculation, despite its seemingly complex approximations. 7 Conclusion W e have presented DNN-MFBO, a deep neural network based mult i-ﬁdelity Bayesian optimization algorithm. Our DNN surrogate model is ﬂexible enough to capt ure the strong and complicated relationships between ﬁdelities and promote objective est imation. Our information based acquisition function not only enjoys a global utility measure, but also i s computationally tractable and efﬁcient. Acknowledgments This work has been supported by DARP A TRADES A ward HR0011-17 -2-0016 and NSF IIS- 1910983. Broader Impact This work can be used in a variety of engineering design probl ems that involve intensive computa- tion, e.g., ﬁnite elements or differences. Hence, the work has potentia l positive impacts in the society if it is used to design passenger aircrafts, biomedical devi ces, automobiles, and all the other devices 9or machines that can beneﬁt human lives. At the same time, thi s work may have some negative consequences if it is used to design weapons or weapon parts. References Abadi, M., Barham, P ., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. (2016). T ensorﬂow: A system for large-s cale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) , pages 265–283. Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595–5637. Forrester, A., Sobester, A., and Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons. Frazier, P . I., Powell, W . B., and Dayanik, S. (2008). A knowl edge-gradient policy for sequential information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439. Gablonsky, J. M. et al. (2001). Modiﬁcations of the DIRECT Algorithm. PhD thesis. Hennig, P . and Schuler, C. J. (2012). Entropy search for info rmation-efﬁcient global optimization. Journal of Machine Learning Research, 13(Jun):1809–1837. Hernández-Lobato, J. M., Hoffman, M. W ., and Ghahramani, Z. (2014). Predictive entropy search for efﬁcient global optimization of black-box functions. I n Advances in neural information processing systems, pages 918–926. Huang, D., Allen, T . T ., Notz, W . I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382. Incropera, F . P ., Lavine, A. S., Bergman, T . L., and DeWitt, D . P . (2007). Fundamentals of heat and mass transfer. Wiley. Jones, D. R., Perttunen, C. D., and Stuckman, B. E. (1993). Li pschitzian optimization without the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157–181. Jones, D. R., Schonlau, M., and W elch, W . J. (1998). Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492. Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., a nd Póczos, B. (2016). Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems, pages 992–1000. Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. (2017). Multi-ﬁdelity bayesian optimi- sation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1799–1808. JMLR. org. Kingma, D. P . and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kingma, D. P . and W elling, M. (2013). Auto-encoding variati onal bayes. arXiv preprint arXiv:1312.6114. Klein, A., Falkner, S., Bartels, S., Hennig, P ., and Hutter, F . (2017). Fast bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536. Laguna, M. and Martí, R. (2005). Experimental testing of adv anced scatter search designs for global optimization of multimodal functions. Journal of Global Optimization, 33(2):235–255. Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdel ity optimization using statistical surrogate modeling for non-hierarchical information sour ces. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, page 0143. 10Liu, D. C. and Nocedal, J. (1989). On the limited memory bfgs m ethod for large scale optimization. Mathematical programming, 45(1-3):503–528. McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practi cal bayesian optimization for variable cost objectives. arXiv preprint arXiv:1703.04335. Minka, T . P . (2001). Expectation propagation for approxima te bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence, pages 362–369. Mockus, J. (2012). Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media. Mockus, J., Tiesis, V ., and Zilinskas, A. (1978). The applic ation of Bayesian methods for seeking the extremum. T owards global optimization, 2(117-129):2. Park, J. S. (1991). Tuning complex computer codes to data and optimal designs. Peherstorfer, B., Willcox, K., and Gunzburger, M. (2018). S urvey of multiﬁdelity methods in uncer- tainty propagation, inference, and optimization. Siam Review, 60(3):550–591. Perdikaris, P ., Raissi, M., Damianou, A., Lawrence, N., and Karniadakis, G. E. (2017). Nonlinear in- formation fusion algorithms for data-efﬁcient multi-ﬁdel ity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751. Perrone, V ., Jenatton, R., Seeger, M. W ., and Archambeau, C. (2018). Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855. Picheny, V ., Ginsbourger, D., Richet, Y ., and Caplin, G. (20 13). Quantile-based optimization of noisy computer experiments with tunable precision. T echnometrics, 55(1):2–13. Poloczek, M., W ang, J., and Frazier, P . (2017). Multi-infor mation source optimization. In Advances in Neural Information Processing Systems, pages 4288–4298. Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. Snoek, J., Larochelle, H., and Adams, R. P . (2012). Practica l bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Su ndaram, N., Patwary, M., Prabhat, M., and Adams, R. (2015). Scalable bayesian optimization us ing deep neural networks. In International conference on machine learning, pages 2171–2180. Song, J., Chen, Y ., and Y ue, Y . (2019). A general framework fo r multi-ﬁdelity bayesian optimization with gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010). Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1015–1022. Swersky, K., Snoek, J., and Adams, R. P . (2013). Multi-task b ayesian optimization. In Advances in neural information processing systems, pages 2004–2012. T akeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T ., Shiga, M., T akeuchi, I., and Karasuyama, M. (2019). Multi-ﬁdelity bayesian optimization with max-val ue entropy search. arXiv preprint arXiv:1901.08275. W ainwright, M. J., Jordan, M. I., et al. (2008). Graphical mo dels, exponential families, and varia- tional inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305. W ang, Z. and Jegelka, S. (2017). Max-value entropy search fo r efﬁcient bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning-V olume 70 , pages 3627– 3635. JMLR. org. 11Wu, J. and Frazier, P . I. (2017). Continuous-ﬁdelity bayesi an optimization with knowledge gradient. In NIPS W orkshop on Bayesian Optimization. Zhang, Y ., Hoang, T . N., Low , B. K. H., and Kankanhalli, M. (20 17). Information-based multi- ﬁdelity bayesian optimization. In NIPS W orkshop on Bayesian Optimization. Zienkiewicz, O. C., T aylor, R. L., Zienkiewicz, O. C., and T a ylor, R. L. (1977). The ﬁnite element method, volume 36. McGraw-hill London. 12Supplementary Material . . .x x x f1(x) f2(x) fM (x) Figure 3: Graphical representation of the DNN based multi-ﬁdelity su rrogate model. The output in each ﬁdelity fm(x) (1 ≤ m ≤ M) is fulﬁlled by a (deep) neural network. 1 Deﬁnitions of Synthetic Benchmark Functions In the experiments, we used three synthetic benchmark tasksto evaluate our method. The deﬁnitions of the objective functions are given as follows. 1.1 Branin Function The input is two dimensional, x = [ x1, x2] ∈ [−5, 10] × [0, 15]. W e have three ﬁdelities to query the function, which, from high to low , are given by f3(x) = − ( −1.275x2 1 π2 + 5x1 π + x2 − 6 ) 2 − ( 10 − 5 4π ) cos(x1) − 10, f2(x) = −10 √ −f3(x − 2) − 2(x1 − 0.5) + 3(3 x2 − 1) + 1 , f1(x) = −f2 ( 1.2(x + 2) ) + 3x2 − 1. (13) W e can see that between ﬁdelities are nonlinear transformat ions and non-uniform scaling and shifts. The global maximum is -0.3979 at (−π, 12.275), (π, 2.275) and (9.425, 2.475). 1.2 Park1 Function The input is four dimensional, x = [ x1, x2, x3, x4] ∈ [0, 1]4. W e have two ﬁdelities, f2(x) = x1 2 [ √ 1 + ( x2 + x2 3)x4 x2 1 − 1 ] + (x1 + 3x4) exp[1 + sin( x3)], f1(x) = [ 1 + sin(x1) 10 ] f2(x) − 2x1 + x2 2+ x2 3+ 0.5. (14) The global maximum is at 25.5893 at (1.0, 1.0, 1.0, 1.0). 1.3 Levy Function The input is two dimensional, x = [ x1, x2] ∈ [−10, 10]2. The query has three ﬁdelities, f3(x) = − sin2(3πx1) − (x1 − 1)2[1 + sin 2(3πx2)] − (x2 − 1)2[1 + sin 2(2πx2)], f2(x) = − exp(0.1 · √ −f3(x)) − 0.1 · √ 1 + f2 3 (x), f1(x) = − √ 1 + f2 3 (x). (15) The global maximum is 0.0 at (1.0, 1.0). 2 Details of Real-W orld Applications 2.1 Mechanical Plate Vibration Design In this application, we want to make a 3-D simply supported, square, elastic plate, of size 10×10×1, as shown in Fig. 4. The goal is to ﬁnd materials that can maximi ze the fourth vibration mode 13frequency (so as to avoid resonance with other parts which ca uses damages). The materials are parameterized by three properties, Y oung’s modulus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]). T o compute the frequency, we discretize the plate with quadr atic tetrahedral elements (see Fig. 4). W e consider two ﬁdelities. The low-ﬁdelity solution is o btained from setting a maximum mesh edge length to 1.2, while the high-ﬁdelity 0.6. W e then use the ﬁnite ﬁnite element method (Zienkiewicz et al., 1977) to solve for the ﬁrst 4th vibratio n mode and compute the frequency as our objective. Figure 4: The plate discretized with quadratic tetrahedral elements (the maximum mesh edge length is 1. 2). 2.2 Thermal Conductor Design In the second application, we consider the design of a thermal conductor, shown in Fig. 5a. The heat source is on the left, where the temperature is zero at th e beginning and ramps to 100 degrees in 0.5 seconds. The heat runs through the conductor to the right end . The size and properties of the conductor are ﬁxed: the thermal conductivity and mass densi ty are both 1. W e need to bore a hole in the centre to install the conductor. The edges on the top, bot tom and inside the hole are all insulated, i.e., no heat is transferred across these edges. Note that the size and the angle of the hole determine the speed of the heat transfusion. The hole in general is an el lipse, described by three parameters, x-radius, y-radius and angle. The goal is to make the heat con duction (from left to right) as fast as possible. Hence, we use the time to reach 70 degrees on the r ight end as the objective function value. T o compute the time, we discretize the conductor with quadratic tetrahedral elements, and apply the ﬁnite element methods to solve a transient heat tra nsfer problem (Incropera et al., 2007) to obtain a response heat curve on the right edge. An example is g iven in Fig. 5b. The response curve is a function of time, from which we can calculate when the tem perature reaches 70 degrees. W e consider queries of two ﬁdelities. The low ﬁdelity queries a re computed with the maximum mesh edge length being 0.8 in solving the heat transfer problem; t he high ﬁdelity queries are computed with the maximum mesh edge length being 0.2. 3 Details of Stochastic V ariational Learning W e develop a stochastic variational learning algorithm to j ointly estimate the posterior of W = {wm} — the NN weights in the output layer in each ﬁdelity, and the hy perparameters, including all the other NN weights Θ = {θm} and noise variance s = [ σ2 1 , . . . , σ 2 M ]⊤. T o this end, we assume q(W) = ∏ M m=1 q(wm) where each q(wm) = N (wm|µm, Σ m). W e parameterize Σ m with its Cholesky decomposition to ensure the positive deﬁniteness , Σ m = LmL⊤ mwhere Lm is a lower triangular matrix. W e then construct a variational model ev idence lower bound (ELBO) from the 14-0.5 0 0.5 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 88 90 92 94 96 98 100 (a) Conductor 0 1 2 3 4 5 Time (seconds) -20 0 20 40 60 80 100Temperature (degrees-Celsius) (b) Heat Response Curve Figure 5: The thermal conductor with one transient heat solution (a), and the heat responsive curve on the right edge (b). The white triangles in (a) are the ﬁnite eleme nts used to discretize the conductor to compute the solution. joint probability of our model (see (4) of the main paper), L ( q(W), Θ , s ) = Eq [ log(p(W, Y|X , Θ , s) q(W) ] = − M∑ m=1 KL ( q(wm)∥p(wm) ) + M∑ m=1 Nm∑ n=1 Eq [ log ( N (ynm|fm(xnm), σ2 m) )] , (16) where p(wm) = N (wm|0, I) and KL (·∥·) is the Kullback Leibler divergence. W e maximize L to estimate q(W), Θ and s jointly. However, since the NN outputs fm(·) in each ﬁdelity are coupled in a highly nonlinear way (see (3) of the main paper), the expect ation terms in L is analytical intractable. T o address this issue, we apply stochastic optimization. Sp eciﬁcally, we use the reparameterization trick (Kingma and W elling, 2013) and for each wm generate parameterized samples from their variational posterior, ˆwm = µm + Lmǫ where ǫ ∼ N (·|0, I). W e then substitute each sample ˆwm for wm in computing all log ( N (ynm|fm(xnm), σ2 m) ) in (16) and remove the expectation in front of them. W e therefore obtain ˆL, an unbiased estimate of ELBO, which is analytically tracta ble. Next, we compute ∇ ˆL, which is an unbiased estimate of the ∇L and hence can be used to maximize L. W e can use any stochastic optimization algorithm. 4 Proof of Lemma 4.1 Lemma 4.1.As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7) of the main paper , ispositive. Proof. First, for brevity, we denote u(tk, x) and γ(tk, x) in (7) of the main paper by uk and γk, respectively. Then from the quadrature results, we compute the variance V ar(fm|D) = ∑ k gkγk + ∑ k gku2 k− ( ∑ k gkuk)2. 15Since γk > 0, the ﬁrst summation ∑ k gkγk > 0. Note that the quadrature weights have all gk > 0 and ∑ k gk = 1 . W e deﬁne ¯u = ∑ k gkuk. Next, we derive that ∑ k gku2 k− ( ∑ k gkuk)2 = ∑ k gku2 k− ¯u2 = ∑ k gku2 k+ ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2 ∑ k gkuk ¯u = ∑ k gk(u2 k+ ¯u2 − 2uk ¯u) = ∑ k gk(uk − ¯u)2 ≥ 0. (17) Therefore, V ar(fm|D) > 0. 5 Proof of Nonnegative V ariance in (12) of the Main Paper W e show the variance in (12) of the main paper, computed by qua drature, is non-negative. The proof is very similar to that of Lemma 4.1 (Section 4). W e denote the quadrature weights and nodes by {gk} and {tk}. Then we have Z = ∑ k gkR(tk), Z 1 = ∑ k gktkR(tk), Z 2 = ∑ k gkt2 kR(tk). (18) Therefore, Z1 Z = ∑ k tk gkR(tk)∑ j gjR(tj ) = ∑ k tkνk, Z2 Z = ∑ k t2 k gkR(tk) ∑ j gjR(tj ) = ∑ k t2 kνk (19) where νk = gk R(tk) ∑ j gj R(tj ) > 0 and ∑ k νk = 1 . Following the same derivation as in (17), we can immediately show that the variance Z2/Z − Z2 1 /Z2 = ∑ k νk(tk − ¯t)2 ≥ 0 where ¯t = Z1/Z = ∑ k tkνk. 16
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper introduces Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) to address the limitations of existing multi-fidelity BO methods that either ignore or oversimplify the strong, complex correlations across different fidelities. DNN-MFBO employs deep neural networks to flexibly capture all kinds of complicated (potentially nonlinear and nonstationary) relationships between fidelities, thereby improving the estimation of the objective function and overall optimization performance. A tractable and efficient mutual information based acquisition function is computed using sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching. The method demonstrates superior optimization effectiveness and lower query costs compared to state-of-the-art multi-fidelity and single-fidelity BO algorithms on both synthetic benchmarks and real-world engineering design applications.",
    "methodology": "DNN-MFBO constructs a multi-fidelity model by stacking a set of deep neural networks, where each NN models one fidelity. For fidelities m > 1, the input to the NN is the concatenation of the original input and the output from the previous fidelity, allowing information propagation and capturing complex inter-fidelity relationships. The model assumes output layer weights (wm) as random variables with Gaussian priors and other NN weights (θm) as hyper-parameters. A stochastic variational learning algorithm is developed to jointly estimate the posterior distribution of the random weights and the hyper-parameters by maximizing an analytically intractable Evidence Lower Bound (ELBO) using the reparameterization trick. For optimization, a Max-Value Entropy Search (MES) acquisition function is used, which is defined as the mutual information between the objective function's maximum and the function output at a given fidelity, normalized by query cost. The output posterior distributions for each fidelity are approximated as Gaussian via fidelity-wise moment matching and Gauss-Hermite quadrature. The conditional entropy term in the acquisition function is approximated using Monte-Carlo sampling for function maxima (f*) and further approximated by considering the entropy given fM(x) <= f*, where fM(x) is the highest fidelity output. This conditional entropy is also computed using Gauss-Hermite quadrature and moment matching to achieve a Gaussian approximation. The acquisition function is optimized using L-BFGS with random initialization.",
    "experimental_setup": "DNN-MFBO was evaluated against several popular and state-of-the-art multi-fidelity BO algorithms, including MF-SKO, MF-GP-UCB, MF-PES, MF-MES, and MTNN-BO, as well as single-fidelity SF-MES. Experiments were conducted on three synthetic benchmark functions: Branin (3 fidelities, 2D), Park1 (2 fidelities, 4D), and Levy (3 fidelities, 2D), and two real-world engineering design problems: Mechanical Plate Vibration Design (3 material properties, 2 fidelities) and Thermal Conductor Design (3 shape parameters, 2 fidelities). DNN-MFBO and MTNN-BO were implemented in TensorFlow, while other methods used their original implementations. Neural network architectures (depths from 2 to 12, widths from 32 to 512) and learning rates (10^-5 to 10^-1) were tuned using SMAC3 and manual adjustments. ADAM optimizer was used for 5,000 epochs of stochastic training. Initial training points were randomly queried (e.g., 20, 20, 2 for Branin/Levy; 5, 2 for Park1; 20, 5 for real-world problems). Query costs were set as (1, 10, 100) for three-fidelity tasks and (1, 10) for two-fidelity tasks. Performance was measured by simple regret (SR), inference regret (IR) for synthetic tasks, and best queried function values for real-world tasks, along with average query time. All query time comparisons were performed on a Linux workstation with a 16-core Intel(R) Xeon(R) CPU E5-2670 and 16GB RAM. Experiments were repeated five times, and average results with standard error bars were reported.",
    "limitations": "The calculation of the acquisition function, while analytically tractable due to its form, is noted to be quite complex, involving multiple layers of approximation through Gauss-Hermite quadrature, moment matching, and Monte-Carlo sampling. Specifically, the method approximates the conditional entropy H(fm(x)|f*, D) using H(fm(x)|fM(x) <= f*, D), which is a reasonable but not exact substitution. The current work explicitly focuses on discrete fidelities and does not address continuous fidelity settings. The initial tuning of neural network hyper-parameters relied on AutoML (SMAC3) followed by manual adjustments, which could be resource-intensive or require expert domain knowledge for different applications. Although DNN-MFBO showed efficiency in query time, part of this efficiency might stem from using random initialization for L-BFGS for acquisition function optimization, in contrast to some competitors that employ more expensive global optimization strategies.",
    "future_research_directions": "Not mentioned"
}
