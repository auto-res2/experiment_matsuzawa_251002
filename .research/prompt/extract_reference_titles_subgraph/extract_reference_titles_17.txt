
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
AUTOMATA : G RADIENT BASED DATA SUBSET SELECTION FOR COMPUTE -EFFICIENT HYPER -PARAMETER TUNING Krishnateja Killamsetty1, Guttu Sai Abhishek 2, Aakriti 2, Alexandre V . Evﬁmievski 3 Lucian Popa3, Ganesh Ramakrishnan 2, Rishabh Iyer 1 1 The University of Texas at Dallas 2Indian Institute of Technology Bombay, India 3 IBM Research {krishnateja.killamsetty, rishabh.iyer}@utdallas.edu {gsaiabhishek, aakriti, ganesh}@cse.iitb.ac.in {evfimi, lpopa}@us.ibm.com March 17, 2022 ABSTRACT Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, ﬁnding the optimal hyper-parameter conﬁguration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset for different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to ﬁnd the optimal hyper-parameter conﬁguration signiﬁcantly faster. In this work, we propose AUTOMATA , a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves signiﬁcantly faster turnaround times and speedups of 3×-30×while achieving comparable performance to the hyper-parameters found using the entire dataset. 1 Introduction In recent years, deep learning systems have found great success in a wide range of tasks, such as object recognition [14], speech recognition [16], and machine translation [1], making people’s lives easier on a daily basis. However, in the quest for near-human performance, more complex and deeper machine learning models trained on increasingly large datasets are being used at the expense of substantial computational costs. Furthermore, deep learning is associated with a signiﬁcantly large number of hyper-parameters such as the learning algorithm, batch size, learning rate, and model conﬁguration parameters (e.g., depth, number of hidden layers, etc.) that need to be tuned. Hence, running extensive hyper-parameter tuning and auto-ml pipelines is becoming increasingly necessary to achieve state-of-the-art models. However, tuning the hyper-parameters requires multiple training runs over the entire datasets (which are signiﬁcantly large nowadays), resulting in staggering compute costs, running times, and, more importantly, CO2 emissions. To give an idea of staggering compute costs, we consider an image classiﬁcation task on a relatively simple CIFAR-10 dataset where a single training run using a relatively simple model class of Residual Networks [15] on a V100 GPU takes around 6 hours. If we perform 1000 training runs (which is not uncommon today) naively using grid search for hyper-parameter tuning, it will take 6000 GPU hours. The resulting CO2 emissions would be between 640 to 1400 arXiv:2203.08212v1  [cs.LG]  15 Mar 2022A PREPRINT - MARCH 17, 2022 kg of CO2 emitted1, which is equivalent to 1600 to 3500 miles of car travel in the US. Similarly, the costs of training state-of-the-art NLP models and vision models on larger datasets like ImageNet are even more staggering [47]2. Figure 1: AUTOMATA ’s performance summary showing speedups, relative test errors, and tuning times on SST2, glue-SST2, CIFAR10, CIFAR100, and CONNECT-4 datasets. We observe that AUTOMATA achieves speedups(and similar energy savings) of 10x - 30x with around 2% performance loss using Hyperband as a sched- uler. Similarly, even when using a more efﬁcient ASHA sched- uler, AUTOMATA achieves a speedup of around 2x-3x with a per- formance loss of 0%-2%. Naive hyper-parameter tuning methods like grid search [ 4] often fail to scale up with the dimensionality of the search space and are computationally expensive. Hence, more efﬁcient and sophisticated Bayesian optimization methods [ 2, 18, 5, 45, 26] have dominated the ﬁeld of hyper-parameter optimization in recent years. Bayesian optimization methods aim to identify good hyper-parameter conﬁg- urations quickly by building a posterior distribution over the search space and by adaptively selecting conﬁgurations based on the proba- bility distribution. More recent methods [48, 49, 10, 26] try to speed up conﬁguration evaluations for efﬁcient hyper-parameter search; these approaches speed up the conﬁguration evaluation by adaptively allocating more resources to promising hyper-parameter conﬁgura- tions while eliminating poor ones quickly. Existing SOTA methods like SHA [19], Hyperband [ 31], ASHA [ 32] use aggressive early- stopping strategies to stop not-so-promising conﬁgurations quickly while allocating more resources to the promising ones. Generally, these resources can be the size of the training set, number of gra- dient descent iterations, training time, etc. Other approaches like [39, 28] try to quickly evaluate a conﬁguration’s performance on a large dataset by evaluating the training runs on small, random subsets; they empirically show that small data subsets could sufﬁce to estimate a conﬁguration’s quality. The past works [39, 28] show that very small data subsets can be effectively used to ﬁnd the best hyper-parameters quickly. However, all these approaches have naively used random training data subsets and did not place much focus on selecting informative subsets instead. Our central insight is that using small informative data subsets allows us to ﬁnd good hyper-parameter conﬁgurations more effectively than random data subsets. In this work, we study the application of the gradient-based subset selection approach for the task of hyper-parameter tuning and automatic machine learning. On that note, the use of gradient-based data subset selection approach in supervised learning setting was explored earlier in GRAD -MATCH [22] where the authors showed that training on gradient based data subsets allows the models to achieve comparable accuracy to full data training while being signiﬁcantly faster. In this work, we empirically study the advantage of using informative gradient-based subset selection algorithms for the hyper-parameter tuning task and study its accuracy when compared to using random subsets and a full dataset. So essentially, we use subsets of data to tune the hyper-parameters. Once we obtain the tuned hyper-parameters, we then train the model (with the obtained hyper-parameters) on the full dataset. The smaller the data subset we use, the more the speed up and energy savings (and hence the decrease in CO2 emissions). In light of all these insights, we propose AUTOMATA , an efﬁcient hyper-parameter tuning framework that combines existing hyper-parameter search and scheduling algorithms with intelligent subset selection. We further empirically show the effectiveness and efﬁciency of AUTOMATA for hyper-parameter tuning, when used with existing hyper-parameter search approaches (more speciﬁcally, TPE [2], Random search [42]), and hyper-parameter tuning schedulers (more speciﬁcally, Hyperband [31], ASHA [32]) on datasets spanning text, image, and tabular domains. 1.1 Related Work Hyper-parameter tuning and auto-ml approaches: A number of algorithms have been proposed for hyper-parameter tuning including grid search3, bayesian algorithms [3], random search [42], etc. Furthermore, a number of scalable toolkits and platforms for hyper-parameter tuning exist like Ray-tune [35]4, H2O automl [30], etc. See [44, 53] for a survey of current approaches and also tricks for hyper-parameter tuning for deep models. The biggest challenges of existing hyper-parameter tuning approaches are a) the large search space and high dimensionality of hyper-parameters and b) the increased training times of training models. Recent work [ 33] has proposed an efﬁcient approach for parallelizing hyper-parameter tuning using Asynchronous Successive Halving Algorithm (ASHA). AUTOMATA is complementary to such approaches and we show that our work can be be combined effectively with them. 1https://mlco2.github.io/impact/#compute 2https://tinyurl.com/a66fexc7 3https://tinyurl.com/3hb2hans 4https://docs.ray.io/en/master/tune/index.html 2A PREPRINT - MARCH 17, 2022 Configurations  comparison Choice of    Hyper-parameters Repeat until all possible sets of hyper- parameters are exhausted DSS based model training loop using the selected configuration DSS based model training loop using the selected configuration DSS based model training loop using the selected configuration Component-1  Hyper-parameter search algorithm Model's Validation set performance  Model's Validation set performance  Model's Validation set performance  Component-3  Hyper-parameter scheduler Initial  parameters     Mini-batch SGD on  for  epochs  Gradient based    DSS Mini-batch SGD on   for  epochs Gradient based    DSS Repeat   times    DSS based model training loop Promoted Configurations Component-2 Figure 2: Diagram of AUTOMATA , including hyper-parameter search, subset based conﬁguration evaluation (where models are trained on subsets of data), and hyper-parameter scheduler. Data Subset Selection: Several recent papers have used submodular functions 5 for data subset selection towards various applications like speech recognition [52, 51], machine translation [25] and computer vision [20]. Other common approaches for subset selection include the usage of coresets. Coresets are weighted subsets of the data, which approximate certain desirable characteristics of the full data (, e.g., the loss function) [11]. Coreset algorithms have been used for several problems including k-means and k-median clustering [13], SVMs [8] and Bayesian inference [6]. Recent coreset selection-based methods [37, 23, 22, 24] have shown great promise for efﬁcient and robust training of deep models. CRAIG [37] tries to select a coreset summary of the training data that estimate the full training gradient closely. Whereas GLISTER [ 23] poses the coreset selection problem as a discrete-continuous bilevel optimization problem that minimizes the validation set loss. Similarly, RETRIEVE [24] also uses a discrete bilevel coreset selection problem to select unlabeled data subsets for efﬁcient semi-supervised learning. Another approach GRAD -MATCH [22] selects coreset summary that approximately matches the full training loss gradient using orthogonal matching pursuit. 1.2 Contributions of the Work The contributions of our work can be summarized as follows: AUTOMATA Framework: We propose AUTOMATA a framework that combines intelligent subset selection with hyper-parameter search and scheduling algorithms to enable faster hyper-parameter tuning. To our knowledge, ours is the ﬁrst work that studies the role of intelligent data subset selection for hyper-parameter tuning. In particular, we seek to answer the following question: Is it possible to use small informative data subsets between 1% to 30% for faster conﬁguration evaluations in hyper-parameter tuning, thereby enabling faster tuning times while maintaining comparable accuracy to tuning hyper-parameters on the full dataset? Effectiveness of A UTOMATA : We empirically demonstrate the effectiveness of AUTOMATA framework used in conjunction with existing hyper-parameter search algorithms like TPE, Random Search, and hyper-parameter scheduling algorithms like Hyperband, ASHA through a set of extensive experiments on multiple real-world datasets. We give a summary of the speedup vs. relative performance achieved by AUTOMATA compared to full data training in Figure 1. More speciﬁcally, AUTOMATA achieves a speedup of 3x - 30x with minimal performance loss for hyper-parameter tuning. Further, in Section 3, we show that the gradient-based subset selection approach of AUTOMATA outperforms the previously considered random subset selection for hyper-parameter tuning. 5Let V = {1, 2, ··· , n}denote a ground set of items. A set function f : 2V →R is a submodular [ 12] if it satisﬁes the diminishing returns property: for subsets S ⊆T ⊆V and j ∈V \T, f(j|S) ≜ f(S ∪j) −f(S) ≥f(j|T). 3A PREPRINT - MARCH 17, 2022 2 A UTOMATA Framework In this section, we present AUTOMATA , a hyper-parameter tuning framework, and discuss its different components shown in Figure 2. The AUTOMATA framework consists of three components: a hyper-parameter search algorithm that identiﬁes which conﬁguration sets need to be evaluated, a gradient-based subset selection algorithm for training and evaluating each conﬁguration efﬁciently, and a hyper-parameter scheduling algorithm that provides early stopping by eliminating the poor conﬁgurations quickly. With AUTOMATA framework, one can use any of the existing hyper- parameter search and hyper-parameter scheduling algorithms and still achieve signiﬁcant speedups with minimal performance degradation due to faster conﬁguration evaluation using gradient-based subset training. 2.1 Notation Denote by H, the set of conﬁgurations selected by the hyper-parameter search algorithm. Let D= {(xi,yi)}N i=1, denote the set of training examples, and V= {(xj,yj)}M j=1 the validation set. Let θi denote the classiﬁer model parameters trained using the conﬁguration i∈H. Let Si be the subset used for training the ith conﬁguration model θi and wi be its associated weight vector i.e., each data sample in the subset has an associated weight that is used for computing the weighted loss. We superscript the changing variables like model parameters θ, subset Swith the timestep tto denote their speciﬁc values at that timestep. Next, denote by Lj T(θi) =LT(xj,yj,θi), the training loss of the jth data sample in the dataset for ith classiﬁer model, and let LT(θi) =∑ k∈DLk T(θi) be the loss over the entire training set for ith conﬁguration model. Let, Lj T(S,θi) =∑ k∈XLT(xk,yk,θi) be the loss on a subset S⊆V of the training examples at timestep j. Let the validation loss be denoted by LV. 2.2 Component-1: Hyper-parameter Search Algorithm Given a hyper-parameter search space, hyper-parameter search algorithms provide a set of conﬁgurations that need to be evaluated. A naive way of performing the hyper-parameter search is Grid-Search, which deﬁnes the search space as a grid and exhaustively evaluates each grid conﬁguration. However, Grid-Search is a time-consuming process, meaning that thousands to millions of conﬁgurations would need to be evaluated if the hyper-parameter space is large. In order to ﬁnd optimal hyper-parameter settings quickly, Bayesian optimization-based hyper-parameter search algorithms have been developed. To investigate the effectiveness ofAUTOMATA across the spectrum of search algorithms, we used the Random Search method and the Bayesian optimization-based TPE method as representative hyper-parameter search algorithms. We provide more details on Random Search and TPE in Appendix D. 2.3 Component-2: Subset based Conﬁguration Evaluation Earlier, we discussed how a hyper-parameter search algorithm presents a set of potential hyper-parameter conﬁgurations that need to be evaluated when tuning hyper-parameters. Every time a conﬁguration needs to be evaluated, prior work trained the model on the entire dataset until the resource allocated by the hyper-parameter scheduler is exhausted. Rather than using the entire dataset for training, we propose using subsets of informative data selected based on gradients instead. As a result, given any hyper-parameter search algorithm, we can use the data subset selection to speed up each training epoch by a signiﬁcant factor (say 10x), thus improving the overall turnaround time of the hyper-parameter tuning. However, the critical advantage of AUTOMATA is that we can achieve speedups while still retaining the hyper-parameter tuning algorithm’s performance in ﬁnding the best hyper-parameters. The fundamental feature of AUTOMATA is that the subset selected by AUTOMATA changes adaptively over time, based on the classiﬁer model training. Thus, instead of selecting a common subset among all conﬁgurations,AUTOMATA selects the subset that best suits each conﬁguration. We give a detailed overview of the gradient-based subset selection process of AUTOMATA below. 2.3.1 Gradient Based Subset Selection (GSS) The key idea of gradient-based subset selection of AUTOMATA is to select a subset Sand its associated weight vector w such that the weighted subset loss gradient best approximates the entire training loss gradient. The subset selection of AUTOMATA for ith conﬁguration at time step tis as follows: wt i,St i = argmin wt i,St i:|St i|≤k,wt i≥0 ∥ ∑ l∈St i wt il∇θLl T(θt i) −∇θLT(θt i)∥+ λ wt i 2 (1) 4A PREPRINT - MARCH 17, 2022 The additional regularization term prevents assignment of very large weight values to data samples, thereby reducing the possibility of overﬁtting on a few data samples. A similar formulation for subset selection in the context of efﬁcient supervised learning was employed in a recent work called GRAD -MATCH [22]. The authors of the work [22] proved that the optimization problem given in Equation (1) is approximately submodular. Therefore, the above optimization problem can be solved using greedy algorithms with approximation guarantees [9, 36]. Similar to GRAD -MATCH, we use a greedy algorithm called orthogonal matching pursuit (OMP) to solve the above optimization problem. The goal of AUTOMATA is to accelerate the hyper-parameter tuning algorithm while preserving its original performance. Efﬁciency is, therefore, an essential factor that AUTOMATA considers even when selecting subsets. Due to this, we employ a faster per-batch subset selection introduced in the work [22] in our experiments, which is described in the following section. Per-Batch Subset Selection: Instead of selecting a subset of data points, one selects a subset of mini-batches by matching the weighted sum of mini-batch training gradients to the full training loss gradients. Therefore, one will have a subset of slected mini-batches and the associated mini-batch weights. One trains the model on the selected mini-batches by performing mini-batch gradient descent using the weighted mini-batch loss. Let us denote the batch size as B, and the total number of mini-batches as bN = N B, and the training set of mini-batches as DB. Let us denote the number of mini-batches that needs to be selected as bk = k B. Let us denote the subset of mini-batches that needs to be selected as SBi and denote the weights associated with mini-batches as wBi = {wBi1,wBi2 ···wBik}for the ith model conﬁguration. Let us denote the mini-batch gradients as ∇θLBi T (θi),··· ,∇θL BbN T (θi) be the mini-batch gradients for the ith model conﬁguration. Let us denote LB T(θi) =∑ i∈[1,bN] LBk T (θi) be the loss over the entire training set. The subset selection problem of mini-batches at time step tcan be written as follows: wt Bi,St Bi = argmin wt Bi,St Bi:|St Bi|≤bk,wt Bi≥0 ∥ ∑ l∈St Bi wt Bil∇θLBl T (θt i) −∇θLB T(θt i)∥+ λ wt Bi 2 (2) In the per-batch version, because the number of samples required for selection is bk is less than k, the number of greedy iterations required for data subset selection in OMP is reduced, resulting in a speedup of B×. A critical trade-off in using larger batch sizes is that in order to get better speedups, we must also sacriﬁce data subset selection performance. Therefore, it is recommended to use smaller batch sizes for subset selection to get a optimal trade-off between speedups and performance. In our experiments on Image datasets, we use a batch size of B = 20, and on text datasets, we use the batch size as a hyper-parameter with B ∈[16,32,64]. Apart from per-batch selection, we use model warm-starting to get more informative data subsets. Further, in our experiments, we use a regularization coefﬁcient of λ= 0. We give more details on warm-starting below. Warm-starting data selection: We warm-start each conﬁguration model by training on the entire training dataset for a few epochs similar to [22]. The warm-starting process enables the model to have informative loss gradients used for subset selection. To be more speciﬁc, the classiﬁer model is trained on the entire training data for Tw = κTk N epochs, where kis the coreset size, T is the total number of epochs, κis the fraction of warm start, and N is the size of the training dataset. We use a κvalue of 0 (i.e., no warm start) for experiments using Hyperband as scheduling algorithm, and a κvalue of 0.35 for experiments using ASHA. 2.4 Component-3: Hyper-parameter Scheduling Algorithm Hyper-parameter scheduling algorithms improve the overall efﬁciency of the hyper-parameter tuning by terminating some of the poor conﬁgurations runs early. In our experiments, we consider Hyperband [31], and ASHA [32], which are extensions of the Sequential Halving algorithm (SHA) [19] that uses aggressive early stopping to terminate poor conﬁguration runs and allocates an increasingly exponential amount of resources to the better performing conﬁgurations. SHA starts with nnumber of initial conﬁgurations, each assigned with a minimum resource amount r. The SHA algorithm uses a reduction factor η to reduce the number of conﬁgurations in each round by selecting the top 1 η th fraction of conﬁgurations while also increasing the resources allocated to these conﬁgurations by ηtimes each round. We discuss Hyperband and ASHA and the issues within SHA that each of them addresses in more detail in Appendix E. Detailed pseudocode of the AUTOMATA algorithm is provided in Appendix C due to space constraints in the main paper. We use the popular deep learning framework [40] for implementation of AUTOMATA framework, Ray-tune[35] for hyper-parameter search and scheduling algorithms, and CORDS [21] for subset selection strategies. 5A PREPRINT - MARCH 17, 2022 Full Random Automata Craig a) SST5(Random,HB)  b) SST5(TPE,HB)  c) SST5(Random,ASHA)  d) SST5(TPE,ASHA) e) TREC6(Random,HB)  f) TREC6(TPE,HB)  g) TREC6(Random,ASHA)  h) TREC6(TPE,ASHA) i) CIFAR10(Random,HB)  j) CIFAR10(TPE,HB)  k) CIFAR10(Random,ASHA)  l) CIFAR10(TPE,ASHA) m) CIFAR100(Random,HB)  n) CIFAR100(TPE,HB)  o) CIFAR100(Random,ASHA)  p) CIFAR100(TPE,ASHA) q) CONNECT-4(Random,HB)  r) CONNECT-4(TPE,HB)  s) CONNECT-4(Rand,ASHA)  t) CONNECT-4(TPE,ASHA) Figure 3: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-t), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) SST5, (e-h) TREC6, (i-l) CIFAR10, (m-p) CIFAR100, and (q-t) CONNECT-4 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 6A PREPRINT - MARCH 17, 2022 3 Experiments In this section, we present the effectiveness and the efﬁciency of AUTOMATA framework for hyper-parameter tuning by evaluating AUTOMATA on datasets spanning text, image, and tabular domains. Further, to assess AUTOMATA ’s effectiveness across the spectrum of existing hyper-parameter search and scheduling algorithms, we conduct experiments using combinations of different search and scheduling algorithms. As discussed earlier, we employ Random Search [42], TPE [2] as representative hyper-parameter search algorithms, and Hyperband [ 31], ASHA [ 32] as representative hyper-parameter scheduling algorithms. However, we believe the takeaways would remain the same even with other approaches. We repeat each experiment ﬁve times on the text and tabular datasets, thrice on the image datasets, and report the mean accuracy and speedups in the plots. Below, we provide further details on datasets, baselines, models, and the hyper-parameter search space used for experiments. 3.1 Baselines Our experiments aim to demonstrate the consistency and efﬁciency of AUTOMATA more speciﬁcally, the effectiveness of AUTOMATA ’s gradient-based subset selection (GSS) for hyper-parameter tuning. As baselines, we replace the GSS subset selection strategy in AUTOMATA with different subset selection strategies, namely RANDOM (randomly sample a same sized subset as AUTOMATA from the training data), CRAIG [37] (a gradient-based subset selection proposed for efﬁcient supervised learning), and FULL (using the entire training data for model training during conﬁguration evaluation). For ease of notation, we refer to baselines by the names of corresponding subset selection strategies. Note that by CRAIG baseline, we mean the faster per-batch version of CRAIG [37] for subset selection shown [22] to be more efﬁcient than the original. In addition, for all methods, we do not use any warm-start for experiments with Hyperband and use a warm start of κ= 0.35 for experiments with ASHA. We give more details on the reason for using warm-start with ASHA and no warm-start with Hyperband in Appendix F.3. We perform experiments with different subset size fractions of 1%, 5%, 10%, and 30%. In our experiments, we compare our approach’s accuracy and efﬁciency (time/energy) with Full training, Per Batch CRAIG selection, and Random selection. 3.2 Datasets, Model Architecture, and Experimental Setup To demonstrate the effectiveness ofAUTOMATA for hyper-parameter tuning, we performed experiments on datasets spanning text, image and tabular domains. Text datasets include SST2 [46], SST5 [46], glue-SST2 [50], and TREC6 [34, 17]. Image datasets include CIFAR10 [27], CIFAR100 [27], and Street View House Numbers (SVHN) [38]. Tabular datasets include DNA, SATIMAGE, LETTER, and CONNECT-4 fromLIBSVM (a library for Support Vector Machines (SVMs)) [7]. We give more details on dataset sizes and splits in Appendix F.1. For the Text datasets, we use the LSTM model (from PyTorch) with trainable GloVe [41] embeddings of 300 dimension as input. For Image datasets, we use the ResNet18 [15] and ResNet50 [15] models. For Tabular datasets, we use a multi-layer perceptron with 2 hidden layers. Once the best hyper-parameter conﬁguration is found, we perform one more ﬁnal training of the model using the best conﬁguration on the entire dataset and report the achieved test accuracy. We use ﬁnal training for all methods except FULL since the models trained on small data subsets (especially with small subset fractions of 1%, 5%) during tuning do not achieve high test accuracies. We also include the ﬁnal training times while calculating the tuning times for a more fair comparison6 For text datasets, we train the LSTM model for 20 epochs while choosing subsets (except for FULL ) every 5 epochs. The hyper-parameter space includes learning rate, hidden size & number of layers of LSTM, batch size of training. Some experiments (with TPE as the search algorithm) use 27 conﬁgurations in the hyper-parameter space, while others use 54. More details on hyper-parameter search space for text datasets are given in Appendix F.2.1. For image datasets, we train the ResNet [15] model for 300 epochs while choosing subsets (except for FULL ) every 20 epochs. We use a Stochastic Gradient Descent (SGD) optimizer with momentum set to 0.9 and weight decay factor set to 0.0005. The hyper-parameter search space consists of a choice between the Momentum method and Nesterov Accelerated Gradient method, choice of learning rate scheduler and their corresponding parameters, and four different group-wise learning rates. We use 27 conﬁgurations in the hyper-parameter space for Image datasets. More details on hyper-parameter search space for image datasets are given in Appendix F.2.2. For tabular datasets, we train a multi-layer perceptron with 2 hidden layers for 200 epochs while choosing subsets every 10 epochs. The hyper-parameter search space consists of a choice between the SGD optimizer or Adam optimizer, choice of learning rate, choice of learning rate scheduler, the sizes of the two hidden layers and batch size for training. We use 27 conﬁgurations in the hyper-parameter space for Tabular datasets. More details on hyper-parameter search space for tabular datasets are provided in Appendix F.2.3. 6Note that with a 30% subset, ﬁnal training is not required as the models trained with 30% subsets achieve similar accuracy to full data training. However, for the sake of consistency, we useﬁnal training with 30% subsets as well. 7A PREPRINT - MARCH 17, 2022 3.3 Hyper-parameter Tuning Results Results comparing the accuracy vs. efﬁciency tradeoff of different subset selection strategies for hyper-parameter tuning are shown in Figure 3. Performance is compared for different sizes of subsets of training data: 1%, 5%, 10%, and 30% along with four possible combinations of search algorithm (Random or TPE) and scheduling algorithm (ASHA or Hyperband). Text datasets results: Sub-ﬁgures(3a, 3b, 3c, 3d) show the plots of relative test error vs. speed ups, both w.r.tfull data tuning for SST5 dataset with different combinations of search and scheduling methods. Similarly, in sub-ﬁgures(3e, 3f, 3g, 3h) we present the plots of relative test error vs. speed ups for TREC6 dataset. From the results, we observe that AUTOMATA achieves best speed up vs. accuracy tradeoff and consistently gives better performance even with small subset sizes unlike other baselines like RANDOM , CRAIG . In particular, AUTOMATA achieves a speedup of 9.8×and 7.35×with a performance loss of 2.8% and a performance gain of 0.9% respectively on the SST5 dataset with TPE and Hyperband. Additionally, AUTOMATA achieves a speedup of around 3.15×, 2.68×with a performance gain of 3.4%, 4.6% respectively for the TREC6 dataset with TPE and ASHA. Image datasets results: Sub-ﬁgures(3i, 3j, 3k, 3l) show the plots of relative test error vs. speed ups, both w.r.tfull data tuning for CIFAR10 dataset with different combinations of search and scheduling methods. Similarly, sub-ﬁgures (3m, 3n, 3o, 3p) show the plots of relative test error vs. speed ups on CIFAR100. The results show that AUTOMATA achieves the best speed up vs. accuracy tradeoff consistently compared to other baselines. More speciﬁcally, AUTOMATA achieves a speedup of around 15×, 8.7× with a performance loss of 0.65%, 0.14% respectively on the CIFAR10 dataset with Random and Hyperband. Further, AUTOMATA achieves a speedup of around 3.7×, 2.3×with a performance gain of 1%, 2% for CIFAR100 dataset with TPE and ASHA. Tabular datasets results: Sub-ﬁgures(3q, 3r, 3s, 3t) show the plots of relative test errorvs. speed ups for the CONNECT-4 dataset. AUTOMATA consistently achieved better speedup vs. accuracy tradeoff compared to other baselines on CONNECT-4 as well. Owing to space constraints, we provide additional results showing the accuracy vs. efﬁciency tradeoff on additional text, image, and tabular datasets in the Appendix F.4. It is important to note that AUTOMATA obtains better speedups when used for hyper-parameter tuning on larger datasets and larger models (in terms of parameters). Apart from the speedups achieved by AUTOMATA , we show in Appendix F.5 that it also achieves similar reductions of energy consumption and CO2 emissions, thereby making it more environmentally friendly. 4 Conclusion, Limitations, and Broader Impact We introduce AUTOMATA , an efﬁcient hyper-parameter tuning framework that uses intelligent subset selection for model training for faster conﬁguration evaluations. Further, we perform extensive experiments showing the effectiveness of AUTOMATA for Hyper-parameter tuning. In particular, it achieves speedups of around 10×- 15×using Hyperband as scheduler and speedups of around 3×even with a more efﬁcient ASHA scheduler. AUTOMATA signiﬁcantly decreases CO2 emissions by making hyper-parameter tuning fast and energy-efﬁcient, in turn reducing environmental impact of such hyper-parameter tuning on society at large. We hope that the AUTOMATA framework will popularize the trend of using subset selection for hyper-parameter tuning and encourage further research on efﬁcient subset selection approaches for faster hyper-parameter tuning, helping us move closer to the goal of Green AI [43]. One of the limitations of AUTOMATA is that in scenarios in which no performance loss is desired, we do not know the minimum subset size to improve speed and, therefore, rely on larger subset sizes such as 10%, 30%. In the future, we consider adaptively changing subset sizes based on model performance for each conﬁguration to remove the dependency on subset size. References [1] L. Barrault, O. Bojar, M. R. Costa-jussà, C. Federmann, M. Fishel, Y . Graham, B. Haddow, M. Huck, P. Koehn, S. Malmasi, C. Monz, M. Müller, S. Pal, M. Post, and M. Zampieri. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1–61, Florence, Italy, Aug. 2019. Association for Computational Linguistics. [2] J. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. [3] J. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In25th annual conference on neural information processing systems (NIPS 2011), volume 24. Neural Information Processing Systems Foundation, 2011. [4] J. Bergstra and Y . Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(10):281–305, 2012. [5] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th 8A PREPRINT - MARCH 17, 2022 International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 115–123, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. [6] T. Campbell and T. Broderick. Bayesian coreset construction via greedy iterative geodesic ascent. InInternational Conference on Machine Learning, pages 698–706, 2018. [7] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/ libsvm. [8] K. L. Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):1–30, 2010. [9] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection. arXiv preprint arXiv:1102.3975, 2011. [10] T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the 24th International Conference on Artiﬁcial Intelligence, IJCAI’15, page 3460–3468. AAAI Press, 2015. [11] D. Feldman. Core-sets: Updated survey. In Sampling Techniques for Supervised or Unsupervised Tasks, pages 23–44. Springer, 2020. [12] S. Fujishige. Submodular functions and optimization. Elsevier, 2005. [13] S. Har-Peled and S. Mazumdar. On coresets for k-means and k-median clustering. InProceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291–300, 2004. [14] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026–1034, 2015. [15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. [16] J. R. Hershey, S. J. Rennie, P. A. Olsen, and T. T. Kristjansson. Super-human multi-talker speech recognition: A graphical modeling approach. Comput. Speech Lang., 24(1):45–66, Jan. 2010. [17] E. Hovy, L. Gerber, U. Hermjakob, C.-Y . Lin, and D. Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. [18] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In C. A. C. Coello, editor,Learning and Intelligent Optimization, pages 507–523, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. [19] K. Jamieson and A. Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter optimization. In A. Gretton and C. C. Robert, editors, Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 240–248, Cadiz, Spain, 09–11 May 2016. PMLR. [20] V . Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor, and G. Ramakrishnan. Learning from less data: A uniﬁed data subset selection and active learning framework for computer vision. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1289–1299. IEEE, 2019. [21] K. Killamsetty, D. Bhat, G. Ramakrishnan, and R. Iyer. CORDS: COResets and Data Subset selection for Efﬁcient Learning, March 2021. [22] K. Killamsetty, D. Sivasubramanian, G. Ramakrishnan, A. De, and R. Iyer. Grad-match: Gradient matching based data subset selection for efﬁcient deep model training. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5464–5474. PMLR, 18–24 Jul 2021. [23] K. Killamsetty, D. Sivasubramanian, G. Ramakrishnan, and R. Iyer. Glister: Generalization based data subset selection for efﬁcient and robust learning. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 35(9):8110–8118, May 2021. [24] K. Killamsetty, X. Zhao, F. Chen, and R. K. Iyer. RETRIEVE: Coreset selection for efﬁcient and robust semi- supervised learning. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. W. Vaughan, editors,Advances in Neural Information Processing Systems, 2021. [25] K. Kirchhoff and J. Bilmes. Submodularity for data selection in machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131–141, 2014. 9A PREPRINT - MARCH 17, 2022 [26] A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter. Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets. In A. Singh and J. Zhu, editors, Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 528–536. PMLR, 20–22 Apr 2017. [27] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. [28] T. Krueger, D. Panknin, and M. Braun. Fast cross-validation via sequential testing. J. Mach. Learn. Res. , 16(1):1103–1155, jan 2015. [29] A. Lacoste, A. Luccioni, V . Schmidt, and T. Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. [30] E. LeDell and S. Poirier. H2o automl: Scalable automatic machine learning. In Proceedings of the AutoML Workshop at ICML, volume 2020, 2020. [31] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18(1):6765–6816, jan 2017. [32] L. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, J. Ben-tzur, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. In I. Dhillon, D. Papailiopoulos, and V . Sze, editors,Proceedings of Machine Learning and Systems, volume 2, pages 230–246, 2020. [33] L. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 2018. [34] X. Li and D. Roth. Learning question classiﬁers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [35] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, and I. Stoica. Tune: A research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118, 2018. [36] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization techniques, pages 234–243. Springer, 1978. [37] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efﬁcient training of machine learning models, 2020. [38] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng. Reading digits in natural images with unsupervised feature learning. 2011. [39] T. Nickson, M. A. Osborne, S. Reece, and S. J. Roberts. Automated machine learning on big data using stochastic algorithm tuning, 2014. [40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. [41] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [42] N. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Computational Biology, 5, 2009. [43] R. Schwartz, J. Dodge, N. Smith, and O. Etzioni. Green ai. Communications of the ACM, 63:54 – 63, 2020. [44] L. N. Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [45] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. M. A. Patwary, P. Prabhat, and R. P. Adams. Scalable bayesian optimization using deep neural networks. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 2171–2180. JMLR.org, 2015. [46] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, Oct. 2013. Association for Computational Linguistics. [47] E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019. 10A PREPRINT - MARCH 17, 2022 [48] K. Swersky, J. Snoek, and R. P. Adams. Multi-task bayesian optimization. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. [49] K. Swersky, J. Snoek, and R. P. Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. [50] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR. [51] K. Wei, Y . Liu, K. Kirchhoff, C. Bartels, and J. Bilmes. Submodular subset selection for large-scale speech training data. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3311–3315. IEEE, 2014. [52] K. Wei, Y . Liu, K. Kirchhoff, and J. Bilmes. Unsupervised submodular subset selection for speech data. In2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4107–4111. IEEE, 2014. [53] T. Yu and H. Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. 11A PREPRINT - MARCH 17, 2022 Supplementary Material A Code The code of AUTOMATA is available at the following link: https://github.com/decile-team/cords. B Licenses We release the code repository ofAUTOMATA with MIT license, and it is available for everybody to use freely. We use the popular deep learning framework [40] for implementation of AUTOMATA framework, Ray-tune[35] for hyper-parameter search and scheduling algorithms, and CORDS [21] for subset selection strategies. As far as the datasets are considered, we use SST2 [46], SST5 [46], glue-SST2 [50], TREC6 [34, 17], CIFAR10 [27], SVHN [38], CIFAR100 [27], and DNA, SATIMAGE, LETTER, CONNECT-4 fromLIBSVM (a library for Support Vector Machines (SVMs)) [7] datasets. CIFAR10, CIFAR100 datasets are released with an MIT license. SVHN dataset is released with a CC0:Public Domain license. Furthermore, all the datasets used in this work are publicly available. In addition, the datasets used do not contain any personally identiﬁable information. 12A PREPRINT - MARCH 17, 2022 C A UTOMATA Algorithm Pseudocode We give the pseudo code of AUTOMATA algorithm in Algorithm 1. Algorithm 1: AUTOMATA Algorithm Input: Hyper-parameter scheduler Algorithm: scheduler , Hyper-parameter search Algorithm: search , No. of conﬁguration evaluations: n, Hyper-parameter search space: H, Training dataset: D, Validation dataset: V, Total no of epochs: T, Epoch interval for subset selection: R, Size of the coreset: k, Reg. Coefﬁcient: λ, Learning rates: {αt}t=T−1 t=0 , Tolerance: ϵ Generate nconﬁgurations by calling the search algorithm H = {h1,h2,··· ,hn}= search (H,n) Randomly initialize each conﬁguration model parameters h1.θ= h2.θ= ··· = hn.θ= θ Set h1.t= h2.t= ··· = hn.t= 0; Set h1.eval= h2.eval= ··· = hn.eval= 0; Assign the initial resources(i.e., in our case training epochs) using the scheduler for all initialized conﬁgurations {hi.r}i=n i=1 = scheduler (H,T) repeat ***Evaluate all remaining conﬁgurations*** for each conﬁguration numbered iin Hdo ***Train conﬁguration hiusing informative data subsets for hi.repochs and evaluate on validation set*** hi.eval,hi.theta= subset-conﬁg-evaluation (D,V,hi.theta,hi.r,R,k,λ, {αt} t=hi.r t=0 ,ϵ) hi.t= hi.t+ hi.r ***Assign resources again based on evaluation performance*** {hi.r}i=n i=1 = scheduler (H,T) until until h1.r== 0 & h2.r== 0 & ··· hn.r== 0 ***Get the best performing hyper-parameters based on ﬁnal conﬁguration evaluations*** finalconfig = argmax hi.config [hi.eval]n i=1 ***Perform ﬁnal training using the best hyper-parameter conﬁgurations*** θfinal = ﬁnaltrain (θ,D,finalconfig,T ) return θfinal Algorithm 2: subset-conﬁg-evaluation Input: Training dataset: D, Validation dataset: V, Initial model parameters: θ0, Total no of epochs: T, Epoch interval for subset selection: R, Size of the coreset: k, Reg. Coefﬁcient: λ, Learning rates: {αt}t=T−1 t=0 , Tolerance: ϵ Set t= 0; Randomly initialize coreset S0 ⊆D : |S0|= k; repeat if (t%R== 0) ∧(t> 0) then St= OMP(D,θt,λ,α t,k,ϵ ) else St = St−1 Compute batches Db = ((xb,yb); b∈(1 ··· B)) from D Compute batches Stb = ((xb); b∈(1 ··· B)) from S *** Mini-batch SGD *** Set θt0 = θt for b= 1 to Bdo Compute mask mton Stbfrom current model parameters θt(b−1) θtb = θt(b−1) −αt∇θLS(Db,θt) −αtλt ∑ j∈Stb mjt∇θlu(xj,θt(b−1)) Set θt+1 = θtB t= t+ 1 until until t≥T *** Evaluate trained model on validation set *** eval= evaluate (θT,V) return eval,θT Algorithm 3: OMP Input: Training loss LT, current parameters: θ, regularization coefﬁcient: λ, subset size: k, tolerance: ϵ Initialize S= ∅ r←∇w(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 |)w=0 repeat e= argmaxj|rj| S←S∪{ e} w ←argminw(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2) r←∇w(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 |) until until |S|≤ kand ∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 ≥ϵ return S,w D More details on Hyper-parameter Search Algorithms We give a brief overview of few representative hyper-parameter search algorithms, such as TPE [ 2] and Random Search [42] which we used in our experiments. As discussed earlier, given a hyper-parameter search space, hyper- parameter search algorithms provide a set of conﬁgurations that need to be evaluated. A naive way of performing the 13A PREPRINT - MARCH 17, 2022 hyper-parameter search is Grid Search, which deﬁnes the search space as a grid and exhaustively evaluates each grid conﬁguration. However, Grid Search is a time-consuming process, meaning that thousands to millions of conﬁgurations would need to be evaluated if the hyper-parameter space is large. In order to ﬁnd optimal hyper-parameter settings quickly, Bayesian optimization-based hyper-parameter search algorithms have been developed. To investigate the effectiveness of AUTOMATA across the spectrum of search algorithms, we used the Random Search method and the Bayesian optimization-based TPE method (described below) as representative hyper-parameter search algorithms. D.1 Random Search In random search [42], hyper-parameter conﬁgurations are selected at random and evaluated to discover the optimal conﬁguration among those chosen. As well as being more efﬁcient than a grid search since it does not evaluate all possible conﬁgurations exhaustively, random search also reduces overﬁtting [4]. D.2 Tree Parzen Structured Estimator (TPE) TPE [2] is a sequential model-based optimization (SMBO) approach that sequentially constructs a probability model to approximate the performance of hyper-parameters based on historical conﬁguration evaluations and then subsequently uses the model to select new conﬁgurations. TPE models the likelihood function P(D|f) and the prior over the function space P(f) using the kernel density estimation. TPE algorithm sorts the collected observations by the function evaluation value, typically validation set performance, and divides them into two groups based on some quantile. The ﬁrst group x1 contains best-performing observations, and the second group x2 contains all other observations. Then TPE models two different densities i(x1) and g(x2) based on the observations from the respective groups using kernel density estimation. Finally, TPE selects the subset observations that need to be evaluated by sampling from the distribution that models the maximum expected improvement, i.e., E[i(x)/g(x)]. E More details on Hyper-parameter Scheduling Algorithms We give a brief overview of some representative hyper-parameter scheduling algorithms, such as HyperBand [31] and ASHA [32] which we used in our experiments. As discussed earlier, hyper-parameter scheduling algorithms improve the overall efﬁciency of the hyper-parameter tuning by terminating some of the poor conﬁgurations runs early. In our experiments, we consider Hyperband, and ASHA, which are extensions of the Sequential Halving algorithm (SHA) [19] that uses aggressive early stopping to terminate poor conﬁguration runs and allocates an increasingly exponential amount of resources to the better performing conﬁgurations. SHA starts with nnumber of initial conﬁgurations, each assigned with a minimum resource amount r. The SHA algorithm uses a reduction factor η to reduce the number of conﬁgurations each round by selecting the top 1 η th fraction of conﬁgurations while also increasing the resources allocated to these conﬁgurations by ηtimes each round. Following, we will discuss Hyperband and ASHA and the issues within SHA that each of them addresses. E.1 HyperBand One of the issues with SHA is that its performance largely depends on the initial number nof conﬁgurations. Hyper- band [31] addresses this issue by performing a grid search over various feasible values of n. Further, each value of nis associated with a minimum resource rallocated to all conﬁgurations before some are terminated; larger values of nare assigned smaller rand hence more aggressive early-stopping. On the whole, in Hyperband [31] for different values of nand r, the SHA algorithm is run until completion. E.2 ASHA: One of the other issues with SHA is that the algorithm is sequential and has to wait for all the processes (assigned with an equal amount of resources) at a particular bracket to be completed before choosing the conﬁgurations to be selected for subsequent runs. Hence, due to the sequential nature of SHA, some GPU/CPU resources (with no processes running) cannot be effectively utilized in the distributed training setting, thereby taking more time for tuning. By contrast, ASHA [32] is an asynchronous variant of SHA and addresses the sequential issue of SHA by promoting a conﬁguration to the next rung as long as there are GPU or CPU resources available. If no resources appear to be promotable, it randomly adds a new conﬁguration to the base rung. 14A PREPRINT - MARCH 17, 2022 F More Experimental Details and Additional Results We performed experiments on a mix of RTX 1080, RTX 2080, and V100 GPU servers containing 2-8 GPUs. To be fair in timing computation, we ran AUTOMATA and all other baselines for a particular setting on the same GPU server. F.1 Additional Datasets Details F.1.1 Text Datasets We performed experiments on SST2 [46], SST5 [46], glue-SST2 [50], and TREC6 [34, 17] text datasets. SST2 [46] and glue-SST2 [50] dataset classify the sentiment of the sentence (movie reviews) as negative or positive. Whereas SST5 [46] classify sentiment of sentence as negative, somewhat negative, neutral, somewhat positive or positive. TREC6 [34, 17] is a dataset for question classiﬁcation consisting of open-domain, fact-based questions divided into broad semantic categories(ABBR - Abbreviation, DESC - Description and abstract concepts, ENTY - Entities, HUM - Human beings, LOC - Locations, NYM - Numeric values). The train, text and validation splits for SST2 [ 46] and SST5 [46] are used from the source itself while the validation data for TREC6 [34, 17] is obtained using 10% of the train data. The train and validation data for glue-SST2 [50] is used from source itself. In Table 1, we summarize the number classes, and number of instances in each split in the text datasets. Dataset #Classes #Train #Validation #Test SST2 2 8544 1101 2210 SST5 5 8544 1101 2210 glue-SST2 2 63982 872 3367 TREC6 6 4907 545 500 Table 1: Number of classes, Number of instances in Train, Validation and Test split in Text datasets F.1.2 Vision Datasets We performed experiments on CIFAR10 [27], CIFAR100 [27], and SVHN [38] vision datasets. The CIFAR-10 [27] dataset contains 60,000 colored images of size 32×32 divided into ten classes, each with 6000 images. CIFAR100 [27] is also similar but that it has 600 images per class and 100 classes. Both CIFAR10 [ 27] and CIFAR100 [27] have 50,000 training samples and 10,000 test samples distributed equally across all classes. SVHN [38] is obtained from house numbers in Google Street View images and has 10 classes, one for each digit. The colored images of size 32×32 are centered around a single digit with some distracting characters on the side. SVHN [38] has 73,257 training digits, 26,032 testing digits. For all 3 datasets, 10% of the training data is used for validation. In Table 2, we summarize the number classes, and number of instances in each split in the image datasets. Dataset #Classes #Train #Validation #Test CIFAR10 10 45000 5000 10000 CIFAR100 100 45000 5000 10000 SVHN 10 65932 7325 26032 Table 2: Number of classes, Number of instances in Train, Validation and Test split in Image datasets F.1.3 Tabular Datasets We performed experiments on the following tabular datasetsdna, letter, connect-4, and satimage from LIBSVM (a library for Support Vector Machines (SVMs)) [7]. Name #Classes #Train #Validation #Test #Features dna 3 1,400 600 1,186 180 satimage 6 3,104 1,331 2,000 36 letter 26 10,500 4,500 5,000 16 connect_4 3 67,557 - - 126 Table 3: Number of classes, Number of instances in Train, Validation and Test split in Tabular datasets 15A PREPRINT - MARCH 17, 2022 A brief description of the tabular datasets can be found in Table 3. For datasets without explicit validation and test datasets, 10% and 20% samples from the training set are used as validation and test datasets, respectively. F.2 Additional Experimental Details For tuning with FULL datasets, the entire dataset is used to train the model during hyper-parameter tuning. But when the AUTOMATA (or CRAIG ) is used, only a fraction of the dataset is used to train various models during tuning. Similar is the case with Random subset selection approach but the subsets are chosen at RANDOM . Note that subset selection techniques used are adaptive in nature, which mean that they chose subset every few epochs for the model to train on for coming few epochs. F.2.1 Details of Text Experiments The hyper-parameter space for experiments on text datasets include learning rate, hidden size & number of layers of LSTM and batch size of training. Some experiments (with TPE search algorithm) where the best conﬁguration among 27 conﬁgurations are found, the hyper-parameter space is learning rate: [0.001,0.1], LSTM hidden size: {64,128,256}, batch size: {16,32,64}. While the rest of the experiments where the best conﬁguration among 54 conﬁgurations are found, the hyper-parameter space is learning rate: [0.001,0.1], LSTM hidden size: {64,128,256}, number of layers in LSTM: {1, 2}, batch size: {16,32,64}. F.2.2 Details of Image Experiments The hyper-parameter search space for tuning experiments on image datasets include a choice between Momentum method and Nesterov Accelerated Gradient method, choice of learning rate scheduler and their corresponding parameters, and four different group-wise learning rates, lr1 for layers of the ﬁrst group, lr2 for layers of intermediate groups, lr3 for layers of the last group of ResNet model, and lr4 for the ﬁnal fully connected layer. For learning rate scheduler, we change the learning rates during training using either a cosine annealing schedule or decay it linearly by γafter every 20 epochs. Best conﬁguration for most experiments is selected from 27 conﬁgurations where the hyper-parameter space is lr1: [0.001, 0.01], lr2: [0.001, 0.01], lr3: [0.001, 0.01], lr4: [0.001, 0.01], Nesterov: {True, False}, learning rate scheduler: {Cosine Annealing, Linear Decay}, γ: [0.05, 0.5]. F.2.3 Details of Tabular Experiments The hyper-parameter search space consists of a choice between the Stochastic Gradient Descent(SGD) optimizer or Adam optimizer, choice of learning rate lr, choice of learning rate scheduler, the sizes of the two hidden layers h1 and h2 and batch size for training. For learning rate scheduler, we either don’t use a learning rate scheduler or change the learning rates during training using a cosine annealing schedule or decay it linearly by 0.05 after every 20 epochs. Best conﬁguration for most experiments is selected from 27 conﬁgurations where the hyper-parameter space is lr: [0.001, 0.01], Optimizer: {Adam, SGD}, learning rate scheduler: {None, Cosine Annealing, Linear Decay}, h1: {150, 200, 250, 300}, h2: {150, 200, 250, 300} and batch size: {16,32,64}. F.3 Use of Warm-start for subset selection We use warm-starting with ASHA as a scheduler because the initial bracket occurs early (i.e., att= 1) with ASHA; this implies that some of the initial conﬁguration evaluations are discarded made just after training for one epoch. The training of such conﬁgurations on small data subsets may not be sufﬁcient to make a sound decision about better- performing conﬁgurations in these scenarios. As a solution, we use warm-starting with ASHA so that all conﬁgurations are trained on the entire data for an initial few epochs. With Hyperband, the brackets do not occur very early during training, so no warm-up is necessary. F.4 More Hyper-parameter Tuning Results We present more hyper-parameter tuning results of AUTOMATA on additional text, image, and tabular datasets in Figures 4,5,6. From the results, it is evident that AUTOMATA achieves best speedup vs. accuracy tradeoff in almost all of the cases. 16A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) SST2(Random,HB)  (b) SST2(TPE,HB)  (c) SST2(Random,ASHA)  (d) SST2(TPE,ASHA) (e) glue-SST2(Random,HB)  (f) glue-SST2(TPE,HB)  (g) glue-SST2(Random,ASHA)  (h) glue-SST2(TPE,ASHA) (i) SST5(Random,HB)  (j) SST5(TPE,HB)  (k) SST5(Random,ASHA)  (l) SST5(TPE,ASHA) (m) TREC6(Random,HB)  (n) TREC6(TPE,HB)  (o) TREC6(Random,ASHA)  (p) TREC6(TPE,ASHA) Figure 4: Tuning Results on Text Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-p), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) SST2, (e-h) glue-SST2, (i-l) SST5, (m-p) TREC6 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). F.5 CO2 Emissions and Energy Consumption Results Sub-ﬁgures 7a,7b,7c,7d shows the energy efﬁciency plot of AUTOMATA on CIFAR100 dataset for 1%, 5%, 10%, 30% subset fractions. For calculating the energy consumed by the GPU/CPU cores, we use pyJoules 7. From the plot, it is evident that AUTOMATA is more energy efﬁcient compared to the other baselines and full data tuning. Sub- ﬁgures 7e,7f,7g,7h shows the plot of relative error vs CO2 emissions efﬁciency, both w.r.t full training. CO2 emissions were estimated based on the total compute time using the Machine Learning Impact calculator presented in [29]. From the results, it is evident that AUTOMATA achieved the best energy vs. accuracy tradeoff and is environmentally friendly based on CO2 emissions compared to other baselines (including CRAIG and RANDOM ). 7https://pypi.org/project/pyJoules/. 17A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) CIFAR10(Random,HB)  (b) CIFAR10(TPE,HB)  (c) CIFAR10(Random,ASHA)  (d) CIFAR10(TPE,ASHA) (e) CIFAR100(Random,HB)  (f) CIFAR100(TPE,HB)  (g) CIFAR100(Random,ASHA)  (h) CIFAR100(TPE,ASHA) (i) SVHN(Random,HB)  (j) SVHN(TPE,HB)  (k) SVHN(Random,ASHA)  (l) SVHN(TPE,ASHA) Figure 5: Tuning Results on Image Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-l), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) CIFAR10, (e-h) CIFAR100, (i-l) SVHN datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 18A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) DNA(Random,HB)  (b) DNA(TPE,HB)  (c) DNA(Random,ASHA)  (d) DNA(TPE,ASHA) (e) SATIMAGE(Random,HB)  (f) SATIMAGE(TPE,HB)  (g) SATIMAGE(Random,ASHA)  (h) SATIMAGE(TPE,ASHA) (i) LETTER(Random,HB)  (j) LETTER(TPE,HB)  (k) LETTER(Random,ASHA)  (l) LETTER(TPE,ASHA) (m) CONNECT-4(Random,HB)  (n) CONNECT-4(TPE,HB)  (o) CONNECT-4(Random,ASHA)  (p) CONNECT-4(TPE,ASHA) Figure 6: Tuning Results on Tabular Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-p), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) DNA, (e-h) SATIMAGE, (i-l) LETTER, (m-p) CONNECT-4 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 19A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) CIFAR100(Random,HB)  (b) CIFAR100(TPE,HB)  (c) CIFAR100(Random,ASHA)  (d) CIFAR100(TPE,ASHA) (e) SVHN(Random,HB)  (f) SVHN(TPE,HB)  (g) SVHN(Random,ASHA)  (h) SVHN(TPE,ASHA) Figure 7: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-d), we present energy ratio vs. relative test error (in %), compared to Full data tuning for different methods on CIFAR100 dataset. In sub-ﬁgures (e-h), we present co2 emissions ratio vs. relative test error (in %), compared to Full data tuning for different methods on SVHN dataset. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. The scatter plots show that AUTOMATA achieves the best energy savings and CO2 reductions, thereby achieving the best efﬁciency vs. performance tradeoff in almost every case. (Bottom-right corner of each plot indicates the best efﬁciency vs. performance tradeoff region). 20
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Findings of the 2019 conference on machine translation (WMT19)",
        "Algorithms for hyper-parameter optimization",
        "Algorithms for hyper-parameter optimization",
        "Random search for hyper-parameter optimization",
        "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures",
        "Bayesian coreset construction via greedy iterative geodesic ascent",
        "LIBSVM: A library for support vector machines",
        "Coresets, sparse greedy approximation, and the frank- Wolfe algorithm",
        "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection",
        "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves",
        "Core-sets: Updated survey",
        "Submodular functions and optimization",
        "On coresets for k-means and k-median clustering",
        "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
        "Deep residual learning for image recognition",
        "Super-human multi-talker speech recognition: A graphical modeling approach",
        "Toward semantics-based answer pinpointing",
        "Sequential model-based optimization for general algorithm configuration",
        "Non-stochastic best arm identification and hyperparameter optimization",
        "Learning from less data: A unified data subset selection and active learning framework for computer vision",
        "CORDS: COResets and Data Subset selection for Efﬁcient Learning",
        "Grad-match: Gradient matching based data subset selection for efﬁcient deep model training",
        "Glister: Generalization based data subset selection for efﬁcient and robust learning",
        "RETRIEVE: Coreset selection for efﬁcient and robust semi-supervised learning",
        "Submodularity for data selection in machine translation",
        "Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets",
        "Learning multiple layers of features from tiny images",
        "Fast cross-validation via sequential testing",
        "Quantifying the carbon emissions of machine learning",
        "H2o automl: Scalable automatic machine learning",
        "Hyperband: A novel bandit-based approach to hyperparameter optimization",
        "A system for massively parallel hyperparameter tuning",
        "A system for massively parallel hyperparameter tuning",
        "Learning question classifiers",
        "Tune: A research platform for distributed model selection and training",
        "Accelerated greedy algorithms for maximizing submodular set functions",
        "Coresets for data-efﬁcient training of machine learning models",
        "Reading digits in natural images with unsupervised feature learning",
        "Automated machine learning on big data using stochastic algorithm tuning",
        "Pytorch: An imperative style, high-performance deep learning library",
        "GloVe: Global vectors for word representation",
        "A high-throughput screening approach to discovering good forms of biologically inspired visual representation",
        "Green ai",
        "A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay",
        "Scalable bayesian optimization using deep neural networks",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Energy and policy considerations for deep learning in nlp",
        "Multi-task bayesian optimization",
        "Freeze-thaw bayesian optimization",
        "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
        "Submodular subset selection for large-scale speech training data",
        "Unsupervised submodular subset selection for speech data",
        "Hyper-parameter optimization: A review of algorithms and applications"
    ]
}
