
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wangâˆ— Sheng Gaoâ€  Huanyu Zhangâ€¡ Weijie J. SuÂ§ Milan ShenÂ¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize â€œadaptiveâ€ hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for â€œadaptiveâ€ private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. âˆ—Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. â€ Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. â€¡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. Â§Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. Â¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedâ„“2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions â€¢ We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. â€¢ We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the RÃ©nyi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the RÃ©nyi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. â€¢ Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions â€¢ We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. â€¢ We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the RÃ©nyi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the RÃ©nyi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. â€¢ Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use â€“ except for a single individual â€“ would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forÎµ >0, and0 â‰¤ Î´ <1, we consider a (randomized) algorithmM : Zn â†’ Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (Îµ, Î´)-DP if for any neighboring dataset D, Dâ€² âˆˆ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) âˆˆ E] â©½ eÎµ Â· P  M   Dâ€² âˆˆ E  + Î´. Here, Îµ and Î´ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms MÎ» : Zn â†’ Y, whereÎ» âˆˆ Î› represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setÎ› may be any infinite set, e.g., the cross product of the learning rateÎ· and clipping normR in DP-SGD. We require that the set Î› is a measure space with an associated measureÂµ. Common choices forÂµ include the counting measure or Lebesgue measure. We make a mild assumption thatÂµ(Î›) < âˆ. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their â€œperformance measureâ€, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterÎ», the return value ofMÎ» is (x, q) âˆ¼ MÎ»(D), wherex represents the combination of the model parameters and the hyperparameter Î», andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3Îµ, 0)-DP as long as each training run of a hyperparameter was (Îµ, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise RÃ©nyi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (Îµ, Î´)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned â€œadaptivity.â€ However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainÎ›. In practice, this setting is unrealistic since we want to â€œadaptâ€ the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionÏ€ at each iteration that reflects accumulated information. Let Q(D, Ï€) be the procedure that randomly draws a hyperparameterÎ» from the distribution1 Ï€ âˆˆ D(Î›) , and then returns the output fromMÎ»(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asÏ€(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset Dâ€² is denoted asÏ€â€²(j). We now present the DP-HyPO framework, denoted asA(D, Ï€(0), T , C, c), in Framework 1. The algorithm takes a prior distributionÏ€(0) âˆˆ D(Î›) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, Ï€(0), T , C, c) Initialize Ï€(0), a prior distribution overÎ›. Initialize the result setA = {} Draw T âˆ¼ T for j = 0 to T âˆ’ 1 do (x, q) âˆ¼ Q(D, Ï€(j)) A = A âˆª {(x, q)} Update Ï€(j+1) based onA according to any adaptive algorithm such that for allÎ» âˆˆ Î›, c â‰¤ Ï€(j+1)(Î») Ï€(0)(Î») â‰¤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forÏ€(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c â‰¤ Ï€(j+1)(Î») Ï€(0)(Î») â‰¤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Î›) represents the space of probability densities onÎ›. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of RÃ©nyi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(Îµ, Î´)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(Î¸, Î³) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(Î¸, Î³) for simplicity. We also assume for simplicity that the prior distributionÏ€(0) is a uniform distribution overÎ›. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT âˆ¼ NegBin(Î¸, Î³). Let Î¸ âˆˆ (âˆ’1, âˆ), Î³ âˆˆ (0, 1), and 0 < câ‰¤ C. Suppose for allMÎ» : Zn â†’ Yover Î» âˆˆ Î›, the base algorithms satisfy(Î±, Îµ)-RDP and(Ë†Î±, Ë†Îµ)-RDP for someÎµ, Ë†Îµ â‰¥ 0, Î±âˆˆ (1, âˆ), and Ë†Î± âˆˆ [1, âˆ). Then the DP-HyPO algorithmA(D, Ï€(0), NegBin(Î¸, Î³), C, c) satisfies (Î±, Îµâ€²)-RDP where Îµâ€² = Îµ + (1 +Î¸) Â·  1 âˆ’ 1 Ë†Î±  Ë†Îµ +  Î± Î± âˆ’ 1 + 1 +Î¸  log C c + (1 + Î¸) Â· log(1/Î³) Ë†Î± + log E[T] Î± âˆ’ 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the RÃ©nyi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the RÃ©nyi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(Îµ, 0)-DP is equivalent to (âˆ, Îµ)-RDP [29]. When bothÎ± and Ë†Î± tend towards infinity, we easily obtain the following theorem in terms of(Îµ, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT âˆ¼ NegBin(Î¸, Î³). Let Î¸ âˆˆ (âˆ’1, âˆ) and Î³ âˆˆ (0, 1). If all the base algorithmsMÎ» satisfies (Îµ, 0)-DP, then the DP-HyPO algorithm A(D, Ï€(0), NegBin(Î¸, Î³), C, c) satisfies   (2 + Î¸)   Îµ + log C c  , 0  -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerÎµ improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionÏ€. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, Î¸, Î³,Î›) Let Ï€ = Unif({1, ...,|Î›|}), andA = {} Draw T âˆ¼ NegBin(Î¸, Î³) for j = 0 to T âˆ’ 1 do (x, q) âˆ¼ Q(D, Ï€) A = A âˆª {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Î› is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setÎ¸ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityÏ€(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingÏ€(j) based on the history. It is crucial to note that this distributionÏ€(j) can be significantly different from the distribution Ï€â€²(j) if we were given a different input datasetDâ€². Therefore, we require the probability mass/density function to satisfy c Âµ(Î›) â‰¤ Ï€(j)(Î») â‰¤ C Âµ(Î›) for allÎ» âˆˆ Î› to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityÏ€(j). When sampling from the spaceÎ›, we perform a projection from Ï€(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f âˆˆ Î›R+ : ess supf â‰¤ C Âµ(Î›), ess inff â‰¥ c Âµ(Î›), R Î±âˆˆÎ› f(Î±)dÎ± = 1}. For such a space to be non-empty, we require thatc â‰¤ 1 â‰¤ C, where Âµ is the measure onÎ›. This condition is well-defined as we assumeÂµ(Î›) < âˆ. To privatizeÏ€(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f âˆ¥f âˆ’ Ï€(j)âˆ¥2, s.t. f âˆˆ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(Ï€(j)). Theoretically, problem(3.1) allows the hyperparameter space Î› to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeÎ› to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, Î¸, Î³, Ï„, Î²,Î›, C, c) Initialize Ï€(0) = Unif(Î›), andA = {} Draw T âˆ¼ NegBin(Î¸, Î³) for t = 0 to T âˆ’ 1 do Truncate the density of currentÏ€(t) to be bounded into the range of[c, C] by projecting to SC,c. ËœÏ€(t) = PSC,c(Ï€(t)). Sample (x, q) âˆ¼ Q(D, ËœÏ€(j)), and updateA = A âˆª {(x, q)} Update mean estimation and variance estimation of the Gaussian processÂµÎ», Ïƒ2 Î», and get the score assÎ» = ÂµÎ» + Ï„ÏƒÎ». Update true (untruncated) posteriorÏ€(t+1) with softmax, byÏ€(t+1)(Î») = exp(Î²Â·sÎ»)R Î»â€²âˆˆÎ› exp(Î²Â·sâ€² Î»). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a â€œscoreâ€ to each hyperparameter Î», known as the estimated upper confidence bound (UCB). The weight factorÏ„ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterÎ² as the inverse temperature. A higher value ofÎ² signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as â€œGPâ€) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9â€œUniformâ€, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateÎ· and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the â€œactual accuracyâ€ of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the â€œactual accuracyâ€ corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetÎµ = 153 and Î´ = 1e âˆ’ 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The Îµ values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anÎµ as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with Îµ = 15, Î´ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with Îµ = 12, Î´ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateÎ· and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (ÂµÎ») and standard error (ÏƒÎ») of accuracy for each hyperparameter combinationÎ». When the algorithm (GP or Uniform) visits a specificÎ», our oracle returns a noisy scoreq(Î») drawn from a normal distribution of N(ÂµÎ», ÏƒÎ»). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofÎµ = 12 and Î´ = 1e âˆ’ 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(Î³) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofÎ³ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(Î³) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofÎ³ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308â€“318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524â€“21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464â€“473. IEEE, 2014. [5] James Bergstra, RÃ©mi Bardenet, Yoshua Bengio, and BalÃ¡zs KÃ©gl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, TamÃ¡s SarlÃ³s, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre CsiszÃ¡r and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474â€“1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3â€“37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265â€“284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381â€“390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3â€“33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785â€“1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507â€“523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376â€“1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358â€“1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367â€“377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765â€“6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298â€“309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCSâ€™07), pages 94â€“103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58â€“65. PMLR, 2016. [29] Ilya Mironov. RÃ©nyi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263â€“275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806â€“7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739â€“753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, TÃ¼bingen, Germany, August 4-16, 2003, Revised Lectures, pages 63â€“71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148â€“175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245â€“248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347â€“450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208â€“12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define RÃ©nyi divergence as follows. Definition A.1(RÃ©nyi Divergences). Let P and Q be probability distributions on a common space â„¦. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE âŠ‚ â„¦, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(Pâˆ¥Q) := E Xâ†P  log P(X) Q(X)  = Z â„¦ P(x) log P(x) Q(x)  dx. The max divergence fromP to Q is defined as Dâˆ(Pâˆ¥Q) := sup  log P(E) Q(E)  : P(E) > 0  . For Î± âˆˆ (1, âˆ), the RÃ©nyi divergence fromP to Q of orderÎ± is defined as DÎ±(Pâˆ¥Q) := 1 Î± âˆ’ 1 log   E Xâ†P "P(X) Q(X) Î±âˆ’1#! = 1 Î± âˆ’ 1 log  E Xâ†Q P(X) Q(X) Î± = 1 Î± âˆ’ 1 log Z Q P(x)Î±Q(x)1âˆ’Î±dx  . We now present the definition of RÃ©nyi DP (RDP) in [29]. Definition A.2(RÃ©nyi Differential Privacy). A randomized algorithmM : Xn â†’ Yis (Î±, Îµ)-RÃ©nyi differentially private if, for all neighbouring pairs of inputsD, Dâ€² âˆˆ Xn, DÎ± (M(x)âˆ¥M (xâ€²)) â‰¤ Îµ. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 â‰¤ j â‰¤ T, and neighboring datasetD and Dâ€², we define the following notations for anyy = (x, q) âˆˆ Y, the totally ordered range set. Pj(y) = PËœyâˆ¼Q(D,Ï€(j))(Ëœy = y) and Pâ€² j(y) = PËœyâˆ¼Q(Dâ€²,Ï€â€²(j))(Ëœy = y) Pj(â‰¤ y) = PËœyâˆ¼Q(D,Ï€(j))(Ëœy â‰¤ y) and Pâ€² j(â‰¤ y) = PËœyâˆ¼Q(Dâ€²,Ï€â€²(j))(Ëœy â‰¤ y) Pj(< y) = PËœyâˆ¼Q(D,Ï€(j))(Ëœy < y) and Pâ€² j(< y) = PËœyâˆ¼Q(Dâ€²,Ï€â€²(j))(Ëœy < y). By these definitions, we havePj(â‰¤ y) = Pj(< y) + Pj(y), andPâ€² j(â‰¤ y) = Pâ€² j(< y) + Pâ€² j(y). And additionally, we have Pj(y) Pâ€² j(y) = R Î»âˆˆÎ› P(MÎ»(D) = y)Ï€(j)(Î»)dÎ»R Î»âˆˆÎ› P(MÎ»(Dâ€²) = y)Ï€â€²(j)(Î»)dÎ» â‰¤ sup Î»âˆˆÎ› P(MÎ»(D) = y)Ï€(j)(Î») P(MÎ»(Dâ€²) = y)Ï€â€²(j)(Î») â‰¤ C c Â· sup Î»âˆˆÎ› P(MÎ»(D) = y) P(MÎ»(Dâ€²) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatÏ€(j) has bounded density betweenc and C. Similarly, we have Pj(â‰¤ y) Pâ€² j(â‰¤ y) â‰¤ C c Â· sup Î»âˆˆÎ› P(MÎ»(D) â‰¤ y) P(MÎ»(Dâ€²) â‰¤ y), (A.2) and Pj(< y) Pâ€² j(< y) â‰¤ C c Â· sup Î»âˆˆÎ› P(MÎ»(D) < y) P(MÎ»(Dâ€²) < y). (A.3) Note thatD and Dâ€² are neighboring datasets, andMÎ» satisfies some DP guarantees. So the ratio P(MÎ»(D)âˆˆE) P(MÎ»(Dâ€²)âˆˆE) for any eventE can be bounded. For simplicity, we define the inner product of a distribution Ï€ with the vector M(D) = (P(MÎ»(D) = y) : Î» âˆˆ Î›) as Ï€ Â· M(D) := Z Î»âˆˆÎ› P(MÎ»(D) = y)Ï€(Î»)dÎ». (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f âˆˆ Î›R+ : ess supf â‰¤ C, ess inff â‰¥ c, R Î±âˆˆÎ› f(Î±)dÎ± = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(D) = y)Ï€(j)(Î»)dÎ» = Ï€+ Â· M(D), (A.5) where Ï€+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define Pâ€²âˆ’(y) for Dâ€² as given by Pâ€²âˆ’(y) := inf Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(Dâ€²) = y) Â· Ï€â€²(j)(Î»)dÎ» = Ï€â€²âˆ’ Â· M. (A.6) Similarly, we can definePâ€²+(y) and Pâˆ’(y) accordingly. From the definition, we know that Pâˆ’(y) â‰¤ Pj(y) â‰¤ P+(y) and Pâ€²âˆ’(y) â‰¤ Pâ€² j(y) â‰¤ Pâ€²+(y). (A.7) We also have P+(y) Pâ€²âˆ’(y) = Ï€âˆ— Â· M(D) Ï€â€²âˆ’ Â· M(Dâ€²) â‰¤ sup Î» P(MÎ»(D) = y) P(MÎ»(Dâ€²) = y) Â· C c . (A.8) It is similar to define P+(â‰¤ y) := sup Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(D) â‰¤ y) and Pâ€²+(â‰¤ y) := sup Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(Dâ€²) â‰¤ y) Pâˆ’(â‰¤ y) := inf Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(D) â‰¤ y) and Pâ€²âˆ’(â‰¤ y) := inf Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(Dâ€²) â‰¤ y) P+(< y) := sup Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(D) < y) and Pâ€²+(< y) := sup Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(Dâ€²) < y) 18Pâˆ’(< y) := inf Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(D) < y) and Pâ€²âˆ’(< y) := inf Ï€âˆˆSC,c Z Î»âˆˆÎ› P(MÎ»(Dâ€²) < y). Following the exact same proof, we have Pâˆ’(â‰¤ y) â‰¤ Pj(â‰¤ y) â‰¤ P+(â‰¤ y) and Pâ€²âˆ’(â‰¤ y) â‰¤ Pâ€² j(â‰¤ y) â‰¤ Pâ€²+(â‰¤ y) (A.9) Pâˆ’(< y) â‰¤ Pj(< y) â‰¤ P+(< y) and Pâ€²âˆ’(< y) â‰¤ Pâ€² j(< y) â‰¤ Pâ€²+(< y) (A.10) P+(â‰¤ y) Pâ€²âˆ’(â‰¤ y) â‰¤ sup Î» P(MÎ»(D) â‰¤ y) P(MÎ»(Dâ€²) â‰¤ y) Â· C c and P+(< y) Pâ€²âˆ’(< y) â‰¤ sup Î» P(MÎ»(D) < y) P(MÎ»(Dâ€²) < y) Â· C c . (A.11) It is also straightforward to verify from the definition that P+(â‰¤ y) = P+(< y) + P+(y) and Pâ€²+(â‰¤ y) = Pâ€²+(< y) + Pâ€²+(y) (A.12) P+ âˆ’ (â‰¤ y) = Pâˆ’(< y) + Pâˆ’(y) and Pâ€²âˆ’(â‰¤ y) = Pâ€²âˆ’(< y) + Pâ€²âˆ’(y). (A.13) Lemma A.3.Suppose ifaÎ», bÎ» are non-negative andcÎ», câ€² Î» are positive for allÎ». Then we have P Î» aÎ»cÎ»P Î» bÎ»câ€² Î» â‰¤ P Î» aÎ»P Î» bÎ» Â· sup Î»,Î»â€²  cÎ» câ€² Î» . Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X Î» aÎ»cÎ» X Î»â€² bâ€² Î» â‰¤ X Î» aÎ» X Î»â€² bâ€² Î»câ€² Î» Â· sup Î»,Î»â€²  cÎ» câ€² Î» . (A.14) For each termaÎ»bâ€² Î», its coefficient on the left hand side of(A.14) is cÎ», but its coefficient on the right hand side of(A.14) is câ€² Î» Â· supÎ»,Î»â€² cÎ» câ€² Î» . Since we always havecâ€² Î» Â· supÎ»,Î»â€² cÎ» câ€² Î»  â‰¥ cÎ», andaÎ»bâ€² Î» â‰¥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The RÃ©nyi divergence betweenP+ and Pâˆ’ is be bounded as follows: DÎ±(P+âˆ¥Pâ€²âˆ’) â‰¤ Î± Î± âˆ’ 1 log C c + sup Î»âˆˆÎ› DÎ±   MÎ»(D)âˆ¥MÎ»(Dâ€²)  Proof of Lemma A.4.We write that e(Î±âˆ’1)DÎ±(P+âˆ¥Pâ€²âˆ’) = X yâˆˆY P+(y)Î± Â· Pâ€²âˆ’(y)1âˆ’Î± = X yâˆˆY (P Î» Ï€+(Î»)P(MÎ»(D) = y))Î± (P Î» Ï€â€²âˆ’(Î»)P(MÎ»(Dâ€²) = y))Î±âˆ’1 (A.15) Here, Ï€+ and Ï€â€²âˆ’ are defined in(A.5) and (A.6), so they are essentiallyÏ€+ y and Ï€â€²âˆ’ y as they depend on the value ofy. Therefore, we need to â€œremoveâ€ this dependence ony to leverage the RDP guarantees for each base algorithmMÎ». We accomplish this task by bridging viaÏ€, the uniform 19density onÎ› (that isÏ€(Î») = Ï€(Î»â€²) for anyÎ», Î»â€² âˆˆ Î›). Specifically, we defineaÎ» = Ï€(Î»)P(MÎ»(D) = y), bÎ» = Ï€(Î»)P(MÎ»(Dâ€²) = y), cÎ» = Ï€+ y (Î») Ï€(Î») , andcâ€² Î» = Ï€â€²âˆ’ y (Î») Ï€(Î») . We see that sup Î»,Î»â€²  cÎ» câ€² Î»  = sup Î»,Î»â€²  Ï€+ y (Î»)/Ï€(Î») Ï€â€²âˆ’y (Î»â€²)/Ï€(Î»â€²)  = sup Î»,Î»â€²  Ï€+ y (Î»)) Ï€â€²âˆ’y (Î»â€²)  â‰¤ C/c, (A.16) since Ï€ is the uniform, andÏ€+ y and Ï€â€²âˆ’ y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X yâˆˆY (P Î» Ï€+(Î»)P(MÎ»(D) = y))Î± (P Î» Ï€â€²âˆ’(Î»)P(MÎ»(Dâ€²) = y))Î±âˆ’1 = X yâˆˆY P Î» Ï€(Î»)P(MÎ»(D) = y) Â· Ï€+(Î») Ï€(Î») Î±âˆ’1 P Î» Ï€(Î»)P(MÎ»(D) = y) Â· Ï€+(Î») Ï€(Î»)  P Î» Ï€(Î»)P(MÎ»(Dâ€²) = y) Â· Ï€â€²âˆ’(Î») Ï€(Î») Î±âˆ’1 = X yâˆˆY (P Î» aÎ» Â· cÎ»)Î±âˆ’1 P Î» Ï€(Î»)P(MÎ»(D) = y) Â· Ï€+(Î») Ï€(Î»)  (P Î» bÎ» Â· câ€² Î»)Î±âˆ’1 â‰¤ X yâˆˆY sup Î»,Î»â€²  cÎ» câ€² Î»  Î±âˆ’1 (P Î» aÎ»)Î±âˆ’1 P Î» Ï€(Î»)P(MÎ»(D) = y) Â· Ï€+(Î») Ï€(Î»)  (P Î» bÎ»)Î±âˆ’1 = X yâˆˆY sup Î»,Î»â€²  cÎ» câ€² Î»  Î±âˆ’1 (P Î» aÎ»)Î±âˆ’1 (P Î» aÎ» Â· cÎ») (P Î» bÎ»)Î±âˆ’1 â‰¤ X yâˆˆY sup Î»,Î»â€²  cÎ» câ€² Î»  Î±âˆ’1 (P Î» aÎ»)Î±âˆ’1 (P Î» aÎ») Â· supÎ» cÎ» (P Î» bÎ»)Î±âˆ’1 â‰¤ X yâˆˆY C c Î±âˆ’1 (P Î» aÎ»)Î±âˆ’1 (P Î» aÎ») Â·  C c  (P Î» bÎ»)Î±âˆ’1 = X yâˆˆY C c Î± Â· (P Î» Ï€(Î»)P(MÎ»(D) = y))Î± (P Î» Ï€(Î»)P(MÎ»(Dâ€²) = y))Î±âˆ’1 The first inequality is due to Lemma A.3, the second inequality is becauseaÎ» are non-negative, and the last inequality is because of(A.16) and the fact that bothÏ€+(Î») and Ï€(Î») are defined inSC,c, and thus their ratio is upper bounded byC c for anyÎ». Now we only need to prove that for any fixed distributionÏ€ that doesnâ€™t depend on valuey, we have X yâˆˆY (P Î» Ï€(Î»)P(MÎ»(D) = y))Î± (P Î» Ï€(Î»)P(MÎ»(Dâ€²) = y))Î±âˆ’1 â‰¤ sup Î»âˆˆÎ› e(Î±âˆ’1)DÎ±(MÎ»(D)âˆ¥MÎ»(Dâ€²)). (A.17) With this result, we immediately know the result holds for uniform distributionÏ€ as a special case. To prove this result, we first observe that the functionf(u, v) = uÎ±v1âˆ’Î± is a convex function. This 20is because the Hessian off is Î±(Î± âˆ’ 1)uÎ±âˆ’2v1âˆ’Î± âˆ’Î±(Î± âˆ’ 1)uÎ±âˆ’1vâˆ’Î± âˆ’Î±(Î± âˆ’ 1)uÎ±âˆ’1vâˆ’Î± Î±(Î± âˆ’ 1)uÎ±vâˆ’Î±âˆ’1  , which is easy to see to be positive semi-definite. And now, consider any distributionÏ€, denote u(Î») = P(MÎ»(D) = y) and v(Î») = P(MÎ»(Dâ€²) = y) by Jensenâ€™s inequality, we have f( X Î» Ï€(Î»)u(Î»), X Î» Ï€(Î»)v(Î»)) â‰¤ X Î» Ï€(Î»)f(u(Î»), v(Î»)). By adding the summation overy on both side of the above inequality, we have X yâˆˆY (P Î» Ï€(Î»)P(MÎ»(D) = y))Î± (P Î» Ï€(Î»)P(MÎ»(Dâ€²) = y))Î±âˆ’1 â‰¤ X yâˆˆY X Î» Ï€(Î») P(MÎ»(D) = y)Î± P(MÎ»(Dâ€²) = y)Î±âˆ’1 = X Î» X yâˆˆY Ï€(Î») P(MÎ»(D) = y)Î± P(MÎ»(Dâ€²) = y)Î±âˆ’1 â‰¤ sup Î» X yâˆˆY P(MÎ»(D) = y)Î± P(MÎ»(Dâ€²) = y)Î±âˆ’1 . The first equality is due to Fubiniâ€™s theorem. And the second inequality is straight forward as one observe Ï€(Î») only depends onÎ». This concludes the proof as we know that e(Î±âˆ’1)DÎ±(P+âˆ¥Pâ€²âˆ’) â‰¤ C c Î± sup Î» X yâˆˆY P(MÎ»(D) = y)Î± P(MÎ»(Dâ€²) = y)Î±âˆ’1 = C c Î± sup Î» e(Î±âˆ’1)DÎ±(MÎ»(D)âˆ¥MÎ»(Dâ€²) or equivalently, DÎ±(P+âˆ¥Pâ€²âˆ’) â‰¤ Î± Î± âˆ’ 1 log C c + sup Î»âˆˆÎ› DÎ±   MÎ»(D)âˆ¥MÎ»(Dâ€²)  . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix Î± >1. LetT be a random variable supported onNâ‰¥0. Letf : [0, 1] â†’ R be the probability generating function ofK, that is,f(x) = Pâˆ k=0 P[T = k]xk. Let MÎ» and Mâ€² Î» be the base algorithm forÎ» âˆˆ Î› on Y on D and Dâ€² respectively. Define A1 := A(D, Ï€(0), T , C, c), andA2 := A(Dâ€², Ï€(0), T , C, c). Then DÎ± (A1âˆ¥A2) â‰¤ sup Î» DÎ±   MÎ»âˆ¥Mâ€² Î»  + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log  fâ€²(q)Î± Â· fâ€²   qâ€²1âˆ’Î± , where applying the same postprocessing to the bounding probabilitiesP+ and Pâ€²âˆ’ gives probabilitiesq and qâ€² respectively. This means that, there exist a function setg : Y â†’[0, 1] such thatq = E Xâ†P+ [g(X)] and qâ€² = E Xâ€²â†Pâ€²âˆ’ [g (Xâ€²)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = âˆX k=1 P(T = k)[ kY j=1 Pj(â‰¤ y) âˆ’ kY i=1 Pj(< y)] = âˆX k=1 P(T = k)[ kX i=1 Pi(y) iâˆ’1Y j=1 Pj(< y) Â· kY j=i+1 Pj(â‰¤ y)] â‰¤ âˆX k=1 P(T = k)[ kX i=1 P+(y) iâˆ’1Y j=1 P+(< y) Â· kY j=i+1 P+(â‰¤ y)] = âˆX k=1 P(T = k)[ kX i=1 P+(y) Â· P+(< y)iâˆ’1 Â· P+(â‰¤ y)kâˆ’i] = âˆX k=1 P(T = k)[P+(â‰¤ y)k âˆ’ P+(< y)k] = f(P+(â‰¤ y)) âˆ’ f(P+(< y)) = P+(y) Â· E Xâ†Uniform([P+(<y),P+(â‰¤y)]) [fâ€²(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) â‰¥ âˆX k=1 P(T = k)[Pâ€²âˆ’(â‰¤ y)k âˆ’ Pâ€²âˆ’(< y)k] = Pâ€²âˆ’(y) Â· E Xâ†Uniform([Pâ€²âˆ’(<y),Pâ€²âˆ’(â‰¤y)]) [fâ€²(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(Î±âˆ’1)DÎ±(A1âˆ¥A2) = X yâˆˆY A1(y)Î± Â· A2(y)1âˆ’Î± â‰¤ X yâˆˆY P+(y)Î± Â· Pâ€²âˆ’(y)1âˆ’Î± Â· E Xâ†[P+(<y),P+(â‰¤y)]  fâ€²(X) Î± Â· E Xâ€²â†[Pâ€²âˆ’(<y),Pâ€²âˆ’(â‰¤y)]  fâ€²   Xâ€²1âˆ’Î± â‰¤ X yâˆˆY P+(y)Î± Â· Pâ€²âˆ’(y)1âˆ’Î± Â· E Xâ†[P+(<y),P+(â‰¤y)] Xâ€²â†[Pâ€²âˆ’(<y),Pâ€²âˆ’(â‰¤y)] h fâ€²(X)Î± Â· fâ€²   Xâ€²1âˆ’Î±i â‰¤ C c Î± sup Î» e(Î±âˆ’1)DÎ±(MÎ»(D)âˆ¥MÎ»(Dâ€²)) Â· max yâˆˆY E Xâ†[P+(<y),P+(â‰¤y)] Xâ€²â†[Pâ€²âˆ’(<y),Pâ€²âˆ’(â‰¤y)] h fâ€²(X)Î± Â· fâ€²   Xâ€²1âˆ’Î±i . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any Î± âˆˆ R, the functionh : (0, âˆ)2 â†’ (0, âˆ) given byh(u, v) = uÎ± Â· v1âˆ’Î± is convex. Therefore, E[U]Î±E[V ]1âˆ’Î± = h(E[(U, V)]) â‰¤ E[h(U, V)] = E  UÎ± Â· V 1âˆ’Î± all positive random variables(U, V). Note that X and Xâ€² are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatXâˆ’P+(<y) P+(y) = Xâ€²âˆ’Pâ€²âˆ’(<y) Pâ€²âˆ’(y) . In particular, this implies that, for eachy âˆˆ Y, there exists somet âˆˆ [0, 1] such that E Xâ†[P+(<y),P+(â‰¤y)] Xâ€²â†[Pâ€²âˆ’(<y),Pâ€²âˆ’(â‰¤y)] h fâ€²(X)Î± Â· fâ€²   Xâ€²1âˆ’Î±i â‰¤ fâ€²(P+(< y)+tÂ·P+(y))Î± Â·fâ€²   Pâ€²âˆ’(< y) + t Â· Pâ€²âˆ’(y) 1âˆ’Î± Therefore, we have DÎ± (A1âˆ¥A2) â‰¤sup Î» DÎ±   MÎ»âˆ¥Mâ€² Î»  + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log ï£« ï£­max yâˆˆY tâˆˆ[0,1] fâ€²(P+(< y) + t Â· P+(y))Î± Â· fâ€²   Pâ€²âˆ’(< y) + t Â· Pâ€²âˆ’(y) 1âˆ’Î± ï£¶ ï£¸. To prove the result, we simply fixyâˆ— âˆˆ Yand tâˆ— âˆˆ [0, 1] achieving the maximum above and define g(y) := ï£± ï£² ï£³ 1 if y < yâˆ— tâˆ— if y = yâˆ— 0 if y > yâˆ— The result directly follows by settingq = E Xâ†P+ [g(X)] and qâ€² = E Xâ€²â†Pâ€²âˆ’ [g (Xâ€²)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E Tâˆ¼NegBin(Î¸,Î³)  xT  = ((1âˆ’(1âˆ’Î³)x)âˆ’Î¸âˆ’1 Î³âˆ’Î¸âˆ’1 if Î¸ Ì¸= 0 log(1âˆ’1âˆ’Î³)x) log(Î³) if Î¸ = 0 Therefore, fâ€²(x) = (1 âˆ’ (1 âˆ’ Î³)x)âˆ’Î¸âˆ’1 Â· (Î¸Â·(1âˆ’Î³) Î³âˆ’Î¸âˆ’1 if Î¸ Ì¸= 0 1âˆ’Î³ log(1/Î³) if Î¸ = 0 = (1 âˆ’ (1 âˆ’ Î³)x)âˆ’Î¸âˆ’1 Â· Î³Î¸+1 Â· E[T]. By Lemma A.5, for appropriate valuesq, qâ€² âˆˆ [0, 1] and for allÎ± >1 and all Ë†Î± >1, we have DÎ± (A1âˆ¥A2) â‰¤ sup Î» DÎ±   MÎ»âˆ¥Mâ€² Î»  + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log  fâ€²(q)Î± Â· fâ€²   qâ€²1âˆ’Î± â‰¤ Îµ + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log  Î³Î¸+1 Â· E[T] Â· (1 âˆ’ (1 âˆ’ Î³)q)âˆ’Î±(Î¸+1) Â·   1 âˆ’ (1 âˆ’ Î³)qâ€²âˆ’(1âˆ’Î±)(Î¸+1) = Îµ + Î± Î± âˆ’ 1 log C c 23+ 1 Î± âˆ’ 1 log  Î³Î¸+1 Â· E[T] Â·  (Î³ + (1 âˆ’ Î³)(1 âˆ’ q))1âˆ’Ë†Î± Â·   Î³ + (1 âˆ’ Î³)   1 âˆ’ qâ€²Ë†Î±Î½ Â· (Î³ + (1 âˆ’ Î³)(1 âˆ’ q))u  (Here, we letË†Î±Î½ = (Î± âˆ’ 1)(1 + Î¸) and (1 âˆ’ Ë†Î±)Î½ + u = âˆ’Î±(Î¸ + 1)) â‰¤ Îµ + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log  Î³Î¸+1 Â· E[T] Â·  Î³ + (1 âˆ’ Î³) Â· e(Ë†Î±âˆ’1)DË†Î±(P+âˆ¥Pâˆ’)Î½ Â· (Î³ + (1 âˆ’ Î³)(1 âˆ’ q))u  (Here,1 âˆ’ q and 1 âˆ’ qâ€² are postprocessings of someP+ and Pâ€²âˆ’ respectively ande(Ë†Î±âˆ’1)DË†Î±(Â·âˆ¥Â·) is convex) â‰¤ Îµ + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log  Î³Î¸+1 Â· E[T] Â·  Î³ + (1 âˆ’ Î³) Â· e(Ë†Î±âˆ’1) supÎ» DË†Î±(MÎ»âˆ¥Mâ€² Î»)+Ë†Î± log C c Î½ Â· (Î³ + (1 âˆ’ Î³)(1 âˆ’ q))u  (By Lemma A.4) â‰¤ Îµ + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log  Î³Î¸+1 Â· E[T] Â·  Î³ + (1 âˆ’ Î³) Â· e(Ë†Î±âˆ’1) supÎ» DË†Î±(MÎ»âˆ¥Mâ€² Î»)+Ë†Î± log C c Î½ Â· (Î³ + (1 âˆ’ Î³)(1 âˆ’ q))u  â‰¤ Îµ + Î± Î± âˆ’ 1 log C c + 1 Î± âˆ’ 1 log  Î³Î¸+1 Â· E[T] Â·  Î³ + (1 âˆ’ Î³) Â· e(Ë†Î±âˆ’1) supÎ» DË†Î±(MÎ»âˆ¥Mâ€² Î»)+Ë†Î± log C c Î½ Â· Î³u  (Here Î³ â‰¤ Î³ + (1 âˆ’ Î³)(1 âˆ’ q) and u â‰¤ 0) = Îµ + Î± Î± âˆ’ 1 log C c + Î½ Î± âˆ’ 1 log  Î³ + (1 âˆ’ Î³) Â· e(Ë†Î±âˆ’1) supÎ» DË†Î±(MÎ»âˆ¥Mâ€² Î»)+Ë†Î± log C c  + 1 Î± âˆ’ 1 log  Î³Î¸+1 Â· E[T] Â· Î³u  = Îµ + Î± Î± âˆ’ 1 log C c + Î½ Î± âˆ’ 1  (Ë†Î± âˆ’ 1) sup Î» DË†Î±   MÎ»âˆ¥Mâ€² Î»  + Ë†Î± log C c + log  1 âˆ’ Î³ Â·  1 âˆ’ eâˆ’(Ë†Î±âˆ’1) supÎ» DË†Î±(MÎ»âˆ¥Mâ€² Î»)+Ë†Î± log C c  + 1 Î± âˆ’ 1 log  Î³u+Î¸+1 Â· E[T]  = Îµ + Î± Î± âˆ’ 1 log C c + (1 +Î¸)  1 âˆ’ 1 Ë†Î±  sup Î» DË†Î±   MÎ»âˆ¥Mâ€² Î»  + (1 +Î¸) log C c + 1 + Î¸ Ë†Î± log  1 âˆ’ Î³ Â·  1 âˆ’ eâˆ’(Ë†Î±âˆ’1) supÎ» DË†Î±(MÎ»âˆ¥Mâ€² Î»)+Ë†Î± log C c  + log(E[T]) Î± âˆ’ 1 + 1 + Î¸ Ë†Î± log(1/Î³) (Here we haveÎ½ = (Î± âˆ’ 1)(1 + Î¸) Ë†Î± and u = âˆ’(1 + Î¸) Î± âˆ’ 1 Ë†Î± + 1  ) = Îµ + Î± Î± âˆ’ 1 log C c + (1 +Î¸)  1 âˆ’ 1 Ë†Î±  sup Î» DË†Î±   MÎ»âˆ¥Mâ€² Î»  + (1 +Î¸) log C c + 1 + Î¸ Ë†Î± log 1 Î³ âˆ’ 1 + eâˆ’(Ë†Î±âˆ’1) supÎ» DË†Î±(MÎ»âˆ¥Mâ€² Î»)âˆ’Ë†Î± log C c  + log(E[T]) Î± âˆ’ 1 â‰¤ Îµ + Î± Î± âˆ’ 1 log C c + (1 +Î¸)  1 âˆ’ 1 Ë†Î±  Ë†Îµ + (1 +Î¸) log C c + 1 + Î¸ Ë†Î± log 1 Î³  + log(E[T]) Î± âˆ’ 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let Î³ âˆˆ (0, 1) and Î¸ âˆˆ (âˆ’1, âˆ). Define a distribution NegBin(Î¸, Î³) on N+ as follows: 24â€¢ If Î¸ Ì¸= 0 and T is drawn from NegBin(Î¸, Î³), then âˆ€k âˆˆ N P [T = k] = (1 âˆ’ Î³)k Î³âˆ’Î¸ âˆ’ 1 Â· kâˆ’1Y â„“=0 â„“ + Î¸ â„“ + 1  and E[T] = Î¸Â·(1âˆ’Î³) Î³Â·(1âˆ’Î³Î¸). Note that whenÎ¸ = 1, it reduces to the geometric distribution with parameter Î³. â€¢ If Î¸ = 0 and T is drawn from NegBin(0, Î³), then P[T = k] = (1 âˆ’ Î³)k k Â· log(1/Î³) and E[T] = 1/Î³âˆ’1 log(1/Î³). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofâ„“2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantÎ½: min f âˆ¥f âˆ’ Ï€(j)âˆ¥2 + Î½KL(Ï€(j), f) (C.1) s.t. f âˆˆ SC,c. When Î½ = 0, we recover the originalâ„“2 projection. Moreover, itâ€™s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterÎ½ approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onÎ›. However, whenÎ› represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Î› and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeÏ€(0) follows a uniform distribution over the parameter spaceÎ› for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toÏ€(0) as SC,c(Ï€(0)) = {f âˆˆ Î›R+ : ess supf/Ï€(0) â‰¤ C, ess inff/Ï€(0) â‰¥ c, Z Î±âˆˆÎ› f(Î±)dÎ± = 1, fâ‰ª Ï€(0)}. When Ï€(0) = 1 Âµ(Î»), we recover the original definition ofSC,c. Note that heref â‰ª Ï€(0) means thatf is absolute continuous with respect to the prior distributionÏ€(0) and this ensures thatSC,c(Ï€(0)) is non-empty. Note that such condition is automatically satisfied whenÏ€(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, Ï€(j), into the spaceSC,c(Ï€(0)), we consider the following functional programming problem: min f âˆ¥f âˆ’ Ï€(j)âˆ¥2 s.t. f âˆˆ SC,c(Ï€(0)), which is a direct generalization of Equation (3.1). As before,SC,c(Ï€(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onÎ›. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateÎ· and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofÏƒ is determined based on the allocatedÎµ budget for each base algorithm. Specifically,Ïƒ = 0.71 for GP andÏƒ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setÏ„ to 0.1 andÎ² to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onÎ· in the range[0.0001, 10] with a multiplicative factor of 3âˆš 10, resulting in16 observations forÎ·. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the â€œactual accuracyâ€ of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the â€œactual accuracyâ€ corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofÏƒ is determined based on the allocatedÎµ budget for each base algorithm. Specifically,Ïƒ = 0.65 for GP andÏƒ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, Ï„ = 0.1, andÎ² = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onÎ· in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forÎ·. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forÏƒ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for Ïƒ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forÏƒ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterÎ», our oracle returns a noisy scoreq(Î») drawn from a normal distributionN(ÂµÎ», ÏƒÎ»). Figure 3 displays a heatmap that presents the mean (ÂµÎ») and standard error (ÏƒÎ») structure of the loss over these two hyperparameters, providing insights into the landscapeâ€™s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Î› can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use â€“ except for a single individual â€“ would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the â€œhonest HPOâ€ setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(Îµ, Î´)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of RÃ©nyi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3Îµ, 0)-DP as long as each training run of a hyperparameter was(Îµ, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise RÃ©nyi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (Îµ, Î´)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned â€œadaptivity.â€ However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Deep learning with differential privacy",
        "Cvxopt: A python package for convex optimization",
        "Botorch: A framework for efficient monte-carlo bayesian optimization",
        "Private empirical risk minimization: Efficient algorithms and tight error bounds",
        "Algorithms for hyperparameter optimization",
        "Private hypothesis selection",
        "The secret sharer: Evaluating and testing unintended memorization in neural networks",
        "A stability-based validation procedure for differentially private machine learning",
        "Generalized private selection and testing with high confidence",
        "Information projections revisited",
        "Unlocking high-accuracy differentially private image classification through scale",
        "Gaussian differential privacy",
        "On the complexity of differentially private data release: efficient algorithms and hardness results",
        "Hyperparameter optimization",
        "Deep active learning with a neural architecture search",
        "Locally private hypothesis selection",
        "Automl: A survey of the state-of-the-art",
        "Sharpdarts: Faster and more accurate differentiable architecture search",
        "Sequential model-based optimization for general algorithm configuration",
        "The composition theorem for differential privacy",
        "Neural architecture search with bayesian optimisation and optimal transport",
        "Information projection and approximate inference for structured sparse variables",
        "Random search and reproducibility for neural architecture search",
        "Hyper-band: A novel bandit-based approach to hyperparameter optimization",
        "Private selection from private candidates",
        "Mechanism design via differential privacy",
        "Towards automatically-tuned neural networks",
        "RÃ©nyi differential privacy",
        "The role of adaptive optimizers for honest private hyperparameter selection",
        "Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning",
        "Towards modular and programmable architecture search",
        "Dp-raft: A differentially private recipe for accelerated fine-tuning",
        "Hyperparameter tuning with renyi differential privacy",
        "Gaussian processes in machine learning",
        "Taking the human out of the loop: A review of bayesian optimization",
        "Stochastic gradient descent with differentially private updates",
        "The complexity of differential privacy",
        "Analytical composition of differential privacy via the edgeworth accountant",
        "Gaussian processes for machine learning",
        "Large scale private learning via low-rank reparametrization",
        "Hyper-parameter optimization: A review of algorithms and applications",
        "Towards automated deep learning: Efficient joint neural architecture and hyperparameter search",
        "Wide network learning with differential privacy"
    ]
}
