
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
FastBO: Fast HPO and NAS with Adaptive Fidelity Identification Jiantong Jiang and Ajmal Mian The University of Western Australia, Perth WA 6009, Australia {jiantong.jiang@research.,ajmal.mian@}uwa.edu.au Abstract. Hyperparameter optimization (HPO) and neural architec- ture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts ofefficient pointand saturation point for each configuration. We also show that our adaptive fidelity iden- tification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability. Keywords: HPO · NAS · Multi-fidelity 1 Introduction HPO [9] and NAS [7] aim to find the hyperparameter configuration or archi- tecture λ∗ that minimizesf(λ), the performance obtained by configurationλ. BO [2,10,30] is an effective model-based method for HPO and NAS. It maintains asurrogate modelof the performancebasedon past evaluations ofconfigurations, which guides the choice of promising configurations to evaluate. Recent studies onBOhaveexploredexpertpriors[11,20,26,29],derivativeinformation[1,27,35], and enhancing the interpretability [5,36–39] of HPO and NAS [3,24,25]. However, standard BO requires full evaluations of configurations, which in- curssignificantcosts,especiallyconsideringtheescalatingmodelevaluationover- head. Despite efforts to accelerate model evaluation [13,15–17], smart strategies are required to widely adopt HPO and NAS. Thus, multi-fidelity methods have been proposed [4,12,21,22], where the fidelities mean the performance levels obtained under various resource levels. They follow the idea of successive halv- ing (SHA) [12]: initially, they evaluate many random configurations using few resources; then, based on the low-fidelity performances, only the well-performing ones successively continue to be evaluated with increasing resources. Follow-up studies [8,19,23,28,33] propose model-based multi-fidelity meth- ods, replacing random sampling with more informed models to improve samplearXiv:2409.00584v1  [cs.LG]  1 Sep 20242 J. Jiang and A. Mian efficiency. However, they are based on SHA, which assumes that learning curves of different configurations rarely intersect - a condition that often fails in prac- tice [32], i.e., early performance observations cannot always indicate the final fidelity performance. This raises a key challenge:What is the appropriate fidelity for each configuration to fit the surrogate model?In other words, which fidelity can offer observations that reliably indicate the final fidelity performance? Cur- rent methods struggle with this. Hyper-Tune [23] and BOHB [8] fit separate models for different fidelities, missing inter-fidelity correlations. FTBO [31] and A-BOHB [19] fit joint models but require strong assumptions. Salinas et al. [28] use the last observed fidelity performance, which may get inaccurate surrogate models as it widens the gap between poorly- and well-performing configurations. This paper is an extended abstract of our conference paper [14], highlighting key ideas and the main experimental results, while omitting finer details. 2 Key Idea of FastBO We propose a multi-fidelity extension of BO, namely FastBO, which tackles the challenge of deciding the appropriate fidelity for each configuration to fit the surrogate model. Here, we first propose the key concepts ofefficient point and saturation point, which are crucial in the optimization process. Then, we briefly describe the process of FastBO and highlight its generality. 2.1 Efficient Point and Saturation Point We first formally define the efficient point as follows. Definition 1 (Efficient point). For a given learning curveCi(r) of hyper- parameter configuration or architectureλi, wherer represents the resource level (also referred to as fidelity), the efficient pointei of λi is defined as:ei = min{r | Ci(r) − Ci(2r) < δ1}, whereδ1 is a predefined small threshold. The semantic of Definition 1 is that starting from the efficient point onwards, whentheresourcesaredoubled,theperformanceimprovementfallsbelowasmall threshold. Consequently, this point signifies a fidelity of performance achieved with comparably efficient resource usage. Thus, we make the following remark. Remark 1. The efficient points of the configurations can serve as their ap- propriate fidelities used for fitting the surrogate model. This is due to their (i) optimal resource-to-performance balance, (ii) ability to capture valuable learning curve trends, and (iii) customization for different hyperparameter configurations. We elaborate on the reasons as follows. Firstly, efficient points balance the trade- off between computational cost and result quality. Beyond the efficient point, allocating additional resources becomes less efficient. Secondly, efficient points capture valuable behaviors within the learning curves, enabling more informed decision-making. Thirdly, the ability to customize the fidelity for each specific configuration is an advantage. This adaptive approach is more reasonable than previous studies that use a fixed fidelity for all the configurations.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 3 1 FastBO Methodology•Process overviewWarm-up Learning curve modelingEfficient & saturation pointsConfig !! •Evaluate !! to the warm-up point to get early observation set.•Stop some bad configs. •Estimate !!’s learning curve from its observation set. Post-processing……•Adaptively extract the points from the learning curve. •Stop evaluating at efficient point.•Fit surrogate model. •Resume some best-performing configs to saturation point.Optimal config Fig. 1:Main process of FastBO. FastBO involves estimating efficient and saturation points, modeling learning curves, and auxiliary stages of warm-up and post-processing. Besides efficient points, we identify saturation points for all configurations as well. We provide the definition of the saturation point as follows. Definition 2 (Saturation point).For a given learning curveCi(r) of config- uration λi, wherer represents the resource level (also referred to as fidelity), the saturation pointsi of λi is defined as:si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}, where δ2 is a predefined small threshold. The semantic of Definition 2 is that beyond the saturation point, the observed performance no longer exhibits notable variations with more resources. Thus, this point characterizes the fidelity at which the performance of a configuration stabilizes. Building on the above definition, we make the following remark. Remark 2. The saturation points of the configurations can serve as their ap- proximate final fidelities, as they provide performance results that meet predefined quality thresholds while reducing resource wastage. 2.2 FastBO Process and Generalization With the two crucial points, we show the main process of FastBO in Fig. 1. Each configurationλi first enters a warm-up stage to get its early observation set. Some configurations are terminated here if they are detected consecutive performance deterioration. Then, FastBO estimates the learning curve ofλi from its observation set. Thus, the efficient point and saturation points are adaptively extracted. After that, λi continues to be evaluated to its efficient point; the result is used to update the surrogate model. Finally, the post-processing stage let a small set of promising configurations resume evaluating to their saturation points, and the optimal configurations can be obtained. Generalizing FastBO to single-fidelity methods.The inefficiency of single- fidelity methods like BO stems from their reliance on expensive final fidelity evaluations. Notably, low-fidelity evaluations provide informative insights but are computationally cheaper. Therefore, we can extend single-fidelity methods to the multi-fidelity setting by acquiring the low-fidelity performance for each configuration to fit the surrogate model. To do this, we need to determine the fidelity used to fit the surrogate model. FastBO adaptively determines this fi- delity for each configuration by identifying its efficient point. While this adaptive identification strategy is described in the context of model-based methods, it can4 J. Jiang and A. Mian Fashion-MNIST Airlines Albert CovertypeChristine Fig. 2:Anytime performance on the LCBench benchmark. Slice CIFAR-10 CIFAR-100 ProteinImageNet16-120 (a) NAS-Bench-201 benchmark (b) FCNet benchmark Fig. 3:Anytime performance on(a) NAS-Bench-201 and(b) FCNet. be generalized to various single-fidelity methods. For example, when evaluating configurations within the population for an evolutionary algorithm-based HPO method, we can similarly evaluate the efficient point performances instead of the final performances and integrate them in the subsequent processes, such as selection and variation. To conclude, the proposed strategy provides a simple but effective way to bridge the gap between single- and multi-fidelity methods. 3 Experimental Evaluation We compare the performance of FastBO with random search (RS), standard BO [30], ASHA [21], Hyperband [22], PASHA [4], A-BOHB [19], A-CQR [28], BOHB [8], DyHPO [34], and Hyper-Tune [23]. The results on the LCBench [40], NAS-Bench-201 [6], and FCNet [18] benchmarks are shown in Figs. 2 and 3. Overall, FastBO can handle various performance metrics and shows strong any- time performance. We can observe that FastBO gains an advantage earlier than other methods, rapidly converging to the global optimum after the initial phase. 4 Conclusion and Discussion We propose FastBO, a model-based multi-fidelity HPO method, which excels in adaptively identifying the fidelity for each configuration to fit the surrogate modelandefficientlyprovidinghigh-qualityperformance.Theproposed adaptive fidelity identification strategy also provides a simple way to extend any single- fidelity method to the multi-fidelity setting. While this paper provides a strong foundation on HPO and NAS, we see challenges that demand future improve- ments. Future work could refine and expand Fast-BO to larger search spaces and distributed computing systems to improve its applicability and scalablity.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 5 References 1. Ament, S.E., Gomes, C.P.: Scalable first-order Bayesian Optimization via struc- tured automatic differentiation. In: International Conference on Machine Learning. pp. 500–516. PMLR (2022) 1 2. Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter optimization. Advances in Neural Information Processing Systems24 (2011) 1 3. Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A.L., et al.: Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisci- plinary Reviews: Data Mining and Knowledge Discovery13(2), e1484 (2023) 1 4. Bohdal, O., Balles, L., Wistuba, M., Ermis, B., Archambeau, C., Zappella, G.: PASHA: efficient HPO and NAS with progressive resource allocation. In: Interna- tional Conference on Learning Representations. OpenReview.net (2023) 1, 4 5. Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.: This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems32 (2019) 1 6. Dong, X., Yang, Y.: NAS-Bench-201: Extending the scope of reproducible neu- ral architecture search. In: International Conference on Learning Representations (2020) 4 7. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. The Journal of Machine Learning Research20(1), 1997–2017 (2019) 1 8. Falkner, S., Klein, A., Hutter, F.: BOHB: Robust and efficient hyperparameter optimization at scale. In: International Conference on Machine Learning. pp. 1437– 1446. PMLR (2018) 1, 2, 4 9. Feurer, M., Hutter, F.: Hyperparameter optimization. Automated Machine Learn- ing: Methods, Systems, Challenges pp. 3–33 (2019) 1 10. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. In: Learning and Intelligent Optimization. pp. 507–523. Springer (2011) 1 11. Hvarfner,C.,Stoll,D.,Souza,A.L.F.,Lindauer,M.,Hutter,F.,Nardi,L.:$\pi$BO: Augmenting acquisition functions with user beliefs for bayesian optimization. In: International Conference on Learning Representations. OpenReview.net (2022) 1 12. Jamieson, K., Talwalkar, A.: Non-stochastic best arm identification and hyperpa- rameter optimization. In: Artificial Intelligence and Statistics. pp. 240–248. PMLR (2016) 1 13. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast parallel exact inference on Bayesian networks. In: ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. pp. 425–426 (2023) 1 14. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Efficient hyperparameter optimization with adaptive fidelity identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 26181–26190 (2024) 2 15. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast inference for probabilistic graphical models. In: 2024 USENIX Annual Technical Conference (USENIX ATC 24) (2024) 1 16. Jiang, J., Wen, Z., Mian, A.: Fast parallel bayesian network structure learning. In: IEEE International Parallel and Distributed Processing Symposium. pp. 617–627. IEEE (2022) 1 17. Jiang, J., Wen, Z., Yang, P., Mansoor, A., Mian, A.: Fast-pgm: Fast probabilistic graphical model learning and inference. arXiv preprint arXiv:2405.15605 (2024) 16 J. Jiang and A. Mian 18. Klein, A., Hutter, F.: Tabular benchmarks for joint architecture and hyperparam- eter optimization. arXiv preprint arXiv:1905.04970 (2019) 4 19. Klein, A., Tiao, L.C., Lienart, T., Archambeau, C., Seeger, M.: Model-based asynchronous hyperparameter and neural architecture search. arXiv preprint arXiv:2003.10865 (2020) 1, 2, 4 20. Li, C., Rana, S., Gupta, S., Nguyen, V., Venkatesh, S., Sutti, A., de Celis Leal, D.R.,Slezak,T.,Height,M.,Mohammed,M.,Gibson,I.:Acceleratingexperimental design by incorporating experimenter hunches. In: International Conference on Data Mining. pp. 257–266. IEEE Computer Society (2018) 1 21. Li,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,E.,Ben-Tzur,J.,Hardt,M.,Recht, B., Talwalkar, A.: A system for massively parallel hyperparameter tuning. Proceed- ings of Machine Learning and Systems2, 230–246 (2020) 1, 4 22. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., Talwalkar, A.: Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research18(1), 6765–6816 (2017) 1, 4 23. Li, Y., Shen, Y., Jiang, H., Zhang, W., Li, J., Liu, J., Zhang, C., Cui, B.: Hyper- tune: Towards efficient hyper-parameter tuning at scale. Proceedings of the VLDB Endowment 15(6), 1256–1265 (2022) 1, 2, 4 24. Moosbauer, J., Casalicchio, G., Lindauer, M., Bischl, B.: Improving accuracy of interpretability measures in hyperparameter optimization via Bayesian algorithm execution. arXiv preprint arXiv:2206.05447 (2022) 1 25. Moosbauer, J., Herbinger, J., Casalicchio, G., Lindauer, M., Bischl, B.: Explaining hyperparameter optimization via partial dependence plots. Advances in Neural Information Processing Systems34, 2280–2291 (2021) 1 26. Oh, C., Gavves, E., Welling, M.: BOCK: Bayesian optimization with cylindrical kernels. In: International Conference on Machine Learning. pp. 3868–3877. PMLR (2018) 1 27. Padidar, M., Zhu, X., Huang, L., Gardner, J., Bindel, D.: Scaling gaussian pro- cesses with derivative information using variational inference. Advances in Neural Information Processing Systems34, 6442–6453 (2021) 1 28. Salinas, D., Golebiowski, J., Klein, A., Seeger, M.W., Archambeau, C.: Optimizing hyperparameters with conformal quantile regression. In: International Conference on Machine Learning. vol. 202, pp. 29876–29893. PMLR (2023) 1, 2, 4 29. Shahriari, B., Bouchard-Côté, A., Freitas, N.: Unbounded Bayesian Optimization via regularization. In: Artificial intelligence and statistics. pp. 1168–1176. PMLR (2016) 1 30. Snoek, J., Larochelle, H., Adams, R.P.: Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems25 (2012) 1, 4 31. Swersky, K., Snoek, J., Adams, R.P.: Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896 (2014) 2 32. Viering, T., Loog, M.: The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 2 33. Wang, J., Xu, J., Wang, X.: Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. arXiv preprint arXiv:1801.01596 (2018) 1 34. Wistuba, M., Kadra, A., Grabocka, J.: Supervising the multi-fidelity race of hy- perparameter configurations. Advances in Neural Information Processing Systems 35, 13470–13484 (2022) 4 35. Wu, J., Poloczek, M., Wilson, A.G., Frazier, P.: Bayesian optimization with gradi- ents. Advances in neural information processing systems30 (2017) 1FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 7 36. Yang,P.,Akhtar,N.,Jiang,J.,Mian,A.:Backdoor-basedexplainableaibenchmark forhighfidelityevaluationofattributionmethods.arXivpreprintarXiv:2405.02344 (2024) 1 37. Yang, P., Akhtar, N., Shah, M., Mian, A.: Regulating model reliance on non-robust features by smoothing input marginal density. arXiv preprint arXiv:2407.04370 (2024) 1 38. Yang, P., Akhtar, N., Wen, Z., Mian, A.: Local path integration for attribution. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 3173–3180 (2023) 1 39. Yang, P., Akhtar, N., Wen, Z., Shah, M., Mian, A.: Re-calibrating feature attri- butions for model interpretation. In: International Conference on Learning Repre- sentations (2022) 1 40. Zimmer, L., Lindauer, M.T., Hutter, F.: Auto-Pytorch: Multi-fidelity metalearn- ing for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence43, 3079–3090 (2021) 4
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Scalable first-order Bayesian Optimization via structured automatic differentiation",
        "Algorithms for hyper-parameter optimization",
        "Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges",
        "PASHA: efficient HPO and NAS with progressive resource allocation",
        "This looks like that: deep learning for interpretable image recognition",
        "NAS-Bench-201: Extending the scope of reproducible neural architecture search",
        "Neural architecture search: A survey",
        "BOHB: Robust and efficient hyperparameter optimization at scale",
        "Hyperparameter optimization",
        "Sequential model-based optimization for general algorithm configuration",
        "$\\\\pi$BO: Augmenting acquisition functions with user beliefs for bayesian optimization",
        "Non-stochastic best arm identification and hyperparameter optimization",
        "Fast parallel exact inference on Bayesian networks",
        "Efficient hyperparameter optimization with adaptive fidelity identification",
        "Fast inference for probabilistic graphical models",
        "Fast parallel bayesian network structure learning",
        "Fast-pgm: Fast probabilistic graphical model learning and inference",
        "Tabular benchmarks for joint architecture and hyperparameter optimization",
        "Model-based asynchronous hyperparameter and neural architecture search",
        "Accelerating experimental design by incorporating experimenter hunches",
        "A system for massively parallel hyper tuning",
        "Hyperband: A novel bandit-based approach to hyperparameter optimization",
        "Hyper-tune: Towards efficient hyper-parameter tuning at scale",
        "Improving accuracy of interpretability measures in hyperparameter optimization via Bayesian algorithm execution",
        "Explaining hyperparameter optimization via partial dependence plots",
        "BOCK: Bayesian optimization with cylindrical kernels",
        "Scaling gaussian processes with derivative information using variational inference",
        "Optimizing hyperparameters with conformal quantile regression",
        "Unbounded Bayesian Optimization via regularization",
        "Practical Bayesian optimization of machine learning algorithms",
        "Freeze-thaw Bayesian optimization",
        "The shape of learning curves: a review",
        "Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning",
        "Supervising the multi-fidelity race of hyperparameter configurations",
        "Bayesian optimization with gradients",
        "Backdoor-based explainable ai benchmark for high fidelity evaluation of attribution methods",
        "Regulating model reliance on non-robust features by smoothing input marginal density",
        "Local path integration for attribution",
        "Re-calibrating feature attributions for model interpretation",
        "Auto-Pytorch: Multi-fidelity metalearning for efficient and robust AutoDL"
    ]
}
