# config/full_experiment.yaml
# CIFAR-10 OHGW study with four ASHA variations.

experiments:
  - run_id: asha-baseline
    dataset_name: cifar10
    model_name: resnet20
    batch_size: 128
    epochs: 200
    seed: 0
    target_metric: 0.93
    warm_start_mode: none
    eta_h: 0.0
    hg_steps: 0
    num_workers: 4
    num_classes: 10
    hyperparams:
      log_lr: -1.0        # 0.1
      log_wd: -4.0        # 1e-4
      momentum: 0.9
      augment_magnitude: 5.0
      label_smoothing: 0.0

  - run_id: asha-random-warm
    dataset_name: cifar10
    model_name: resnet20
    batch_size: 128
    epochs: 200
    seed: 0
    target_metric: 0.93
    warm_start_mode: random
    random_sigma: 0.01
    eta_h: 0.0
    hg_steps: 0
    num_workers: 4
    num_classes: 10
    hyperparams:
      log_lr: -1.0
      log_wd: -4.0
      momentum: 0.9
      augment_magnitude: 5.0
      label_smoothing: 0.0

  - run_id: asha-ohgw-1step
    dataset_name: cifar10
    model_name: resnet20
    batch_size: 128
    epochs: 200
    seed: 0
    target_metric: 0.93
    warm_start_mode: ohgw
    eta_h: 0.001
    hg_steps: 1
    num_workers: 4
    num_classes: 10
    hyperparams:
      log_lr: -1.0
      log_wd: -4.0
      momentum: 0.9
      augment_magnitude: 5.0
      label_smoothing: 0.0

  - run_id: asha-ohgw-3step
    dataset_name: cifar10
    model_name: resnet20
    batch_size: 128
    epochs: 200
    seed: 0
    target_metric: 0.93
    warm_start_mode: ohgw
    eta_h: 0.001
    hg_steps: 3
    num_workers: 4
    num_classes: 10
    hyperparams:
      log_lr: -1.0
      log_wd: -4.0
      momentum: 0.9
      augment_magnitude: 5.0
      label_smoothing: 0.0

# Note: if after scheduler selection a deeper network should be trained,
# simply duplicate one entry and set model_name: resnet50.

# Parsed by src.main.run_all_experiments()
